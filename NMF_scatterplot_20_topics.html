
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Bokeh Plot</title>
        
<link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css" type="text/css" />
        
<script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js"></script>
<script type="text/javascript">
    Bokeh.set_log_level("info");
</script>
    </head>
    <body>
        
        <div class="bk-root">
            <div class="bk-plotdiv" id="db30cde3-916c-4ee7-83bc-f2b9211d8ad3"></div>
        </div>
        
        <script type="application/json" id="a24b363a-261a-4175-bdfd-570566801880">
          {"1c061917-626f-4654-b535-72395343af79":{"roots":{"references":[{"attributes":{},"id":"8e029c36-0813-425a-b51b-29f7df4a2e6e","type":"ResetTool"},{"attributes":{"callback":null},"id":"fa74f7fe-0ca8-49bf-9223-a61d5c92d757","type":"DataRange1d"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"value":"#1f77b4"},"line_alpha":{"value":0.1},"line_color":{"value":"#1f77b4"},"size":{"units":"screen","value":10},"x":{"field":"x"},"y":{"field":"y"}},"id":"7d8308e1-5940-4a45-94ad-90b552cce448","type":"Circle"},{"attributes":{},"id":"7bb1fedf-707d-477d-9261-d0ab6efa75f5","type":"LinearScale"},{"attributes":{"source":{"id":"eeda0dd3-17e8-4e45-b11e-477771b769a7","type":"ColumnDataSource"}},"id":"6eac7fa1-393d-4733-85a9-a6d49b9a5bcc","type":"CDSView"},{"attributes":{"callback":null,"tooltips":[["Topic","@topic_key"],["Title","@title"]]},"id":"3c216f1e-4a84-4384-84fa-32f1fbd49e0c","type":"HoverTool"},{"attributes":{"plot":null,"text":"Visualization of 20 topics (NMF)"},"id":"9d460809-f665-4d9f-816e-6d07ef818ca3","type":"Title"},{"attributes":{},"id":"e89b6f68-109e-4651-bb4c-a29f9ecdadba","type":"WheelZoomTool"},{"attributes":{"callback":null},"id":"8742ef31-c651-47bd-8fa3-798247ec103b","type":"DataRange1d"},{"attributes":{"callback":null,"column_names":["title","y","topic_key","content","x","color"],"data":{"color":["#e6194b","#f032e6","#808080","#fabebe","#e6194b","#008080","#000080","#808080","#3cb44b","#808080","#008080","#e6beff","#aa6e28","#808080","#808080","#f58231","#e6194b","#aa6e28","#fabebe","#808080","#3cb44b","#008080","#808080","#ffe119","#ffe119","#ffe119","#808000","#808080","#000080","#008080","#808080","#aa6e28","#aa6e28","#008080","#e6194b","#008080","#d2f53c","#000080","#808080","#f032e6","#000080","#008080","#808080","#e6194b","#d2f53c","#f58231","#e6194b","#3cb44b","#46f0f0","#f58231","#aa6e28","#f58231","#008080","#aa6e28","#008080","#008080","#808080","#808080","#000080","#000080","#008080","#46f0f0","#008080","#808000","#808080","#808080","#808080","#000080","#f032e6","#808080","#aa6e28","#808080","#000080","#808000","#000080","#fabebe","#000080","#d2f53c","#3cb44b","#3cb44b","#3cb44b","#3cb44b","#008080","#008080","#008080","#008080","#008080","#f58231","#808080","#f58231","#e6beff","#f58231","#e6beff","#e6beff","#808080","#008080","#808080","#ffe119","#008080","#0082c8","#fffac8","#0082c8","#911eb4","#911eb4","#911eb4","#0082c8","#008080","#3cb44b","#000080","#008080","#ffe119","#ffe119","#808080","#46f0f0","#fabebe","#008080","#008080","#008080","#3cb44b","#e6194b","#0082c8","#e6194b","#aaffc3","#f58231","#3cb44b","#e6194b","#fffac8","#f58231","#46f0f0","#e6194b","#000080","#e6194b","#800000","#008080","#008080","#000080","#808080","#000080","#ffe119","#000080","#000080","#e6194b","#808080","#808080","#008080","#008080","#d2f53c","#808080","#008080","#008080","#fabebe","#000080","#800000","#e6194b","#3cb44b","#e6194b","#800000","#800000","#000080","#008080","#000080","#f032e6","#e6194b","#e6194b","#808080","#000080","#d2f53c","#0082c8","#808080","#0082c8","#d2f53c","#0082c8","#008080","#008080","#aaffc3","#008080","#ffd8b1","#fffac8","#aaffc3","#3cb44b","#3cb44b","#3cb44b","#3cb44b","#d2f53c","#800000","#808080","#800000","#e6beff","#e6194b","#fabebe","#3cb44b","#008080","#008080","#3cb44b","#008080","#008080","#008080","#008080","#911eb4","#000080","#fabebe","#ffe119","#46f0f0","#808080","#808080","#808080","#ffe119","#000080","#e6194b","#46f0f0","#46f0f0","#46f0f0","#e6beff","#46f0f0","#f58231","#f58231","#ffe119","#ffe119","#ffe119","#e6beff","#000080","#f58231","#3cb44b","#008080","#808000","#3cb44b","#e6194b","#3cb44b","#3cb44b","#008080","#f032e6","#ffd8b1","#ffd8b1","#aa6e28","#911eb4","#911eb4","#008080","#008080","#fabebe","#000080","#d2f53c","#aa6e28","#fabebe","#e6194b","#008080","#008080","#008080","#008080","#008080","#0082c8","#e6beff","#000080","#008080","#008080","#e6beff","#808080","#008080","#e6194b","#46f0f0","#d2f53c","#aa6e28","#000080","#d2f53c","#f032e6","#3cb44b","#f032e6","#e6beff","#f032e6","#f032e6","#f032e6","#aa6e28","#e6194b","#ffd8b1","#ffe119","#ffe119","#ffe119","#f032e6","#008080","#800000","#800000","#008080","#008080","#911eb4","#008080","#d2f53c","#008080","#000080","#aaffc3","#808080","#000080","#808080","#808080","#808080","#f032e6","#e6194b","#e6beff","#d2f53c","#e6beff","#aaffc3","#aaffc3","#e6beff","#aaffc3","#008080","#ffd8b1","#ffd8b1","#ffd8b1","#ffd8b1","#ffd8b1","#000080","#808080","#000080","#fabebe","#d2f53c","#e6194b","#3cb44b","#d2f53c","#e6194b","#911eb4","#e6194b","#e6beff","#f032e6","#f58231","#e6beff","#ffe119","#008080","#911eb4","#e6194b","#800000","#911eb4","#008080","#aa6e28","#aa6e28","#008080","#000080","#000080","#aa6e28","#008080","#fffac8","#808080","#008080","#000080","#fabebe","#aa6e28","#aa6e28","#ffe119","#ffe119","#e6194b","#3cb44b","#008080","#008080","#e6194b","#008080","#008080","#008080","#aa6e28","#f58231","#f58231","#f58231","#000080","#000080","#000080","#e6194b","#e6beff","#3cb44b","#aaffc3","#d2f53c","#911eb4","#000080","#3cb44b","#fffac8","#008080","#008080","#aa6e28","#008080","#d2f53c","#808080","#fabebe","#3cb44b","#fffac8","#fabebe","#d2f53c","#aa6e28","#008080","#808080","#808080","#000080","#fffac8","#fffac8","#aaffc3","#d2f53c","#008080","#f58231","#fffac8","#fabebe","#fabebe","#fabebe","#fabebe","#fabebe","#d2f53c","#fabebe","#911eb4","#008080","#008080","#008080","#aaffc3","#ffd8b1","#ffd8b1","#fffac8","#008080","#008080","#ffe119","#aaffc3","#3cb44b","#008080","#e6beff","#3cb44b","#d2f53c","#3cb44b","#008080","#008080","#f032e6","#808080","#f032e6","#008080","#e6194b","#808080","#0082c8","#808080","#3cb44b","#3cb44b","#e6beff","#d2f53c","#3cb44b","#008080","#f032e6","#fabebe","#008080","#800000","#000080","#d2f53c","#911eb4","#911eb4","#911eb4","#911eb4","#911eb4","#808000","#e6194b","#008080","#008080","#008080","#0082c8","#008080","#008080","#fabebe","#fabebe","#e6194b","#fabebe","#000080","#008080","#008080","#008080","#000080","#fabebe","#f032e6","#d2f53c","#d2f53c","#d2f53c","#d2f53c","#d2f53c","#f58231","#f032e6","#ffd8b1","#f032e6","#f032e6","#800000","#f032e6","#fffac8","#fffac8","#fffac8","#fffac8","#fffac8","#fffac8","#fabebe","#46f0f0","#46f0f0","#808000","#e6194b","#3cb44b","#808000","#3cb44b","#808000","#008080","#3cb44b","#d2f53c","#d2f53c","#911eb4","#ffe119","#008080","#000080","#ffe119","#ffe119","#3cb44b","#ffe119","#911eb4","#f58231","#f58231","#008080","#008080","#008080","#008080","#808080","#808080","#e6194b","#008080","#008080","#008080","#e6beff","#008080","#f58231","#008080","#3cb44b","#ffd8b1","#f58231","#3cb44b","#008080","#008080","#3cb44b","#0082c8","#fabebe","#808080","#0082c8","#3cb44b","#000080","#808080","#3cb44b","#fabebe","#0082c8","#000080","#0082c8","#008080","#aaffc3","#d2f53c","#e6194b","#3cb44b","#911eb4","#f58231","#aaffc3","#e6194b","#000080","#808080","#fabebe","#000080","#aaffc3","#46f0f0","#800000","#f032e6","#f032e6","#fabebe","#f58231","#0082c8","#808080","#800000","#000080","#800000","#008080","#808080","#f58231","#ffe119","#808080","#f032e6","#3cb44b","#d2f53c","#f032e6","#000080","#f58231","#e6beff","#f58231","#f58231","#46f0f0","#000080","#e6beff","#fabebe","#800000","#0082c8","#0082c8","#fabebe","#f032e6","#800000","#800000","#800000","#800000","#008080","#aa6e28","#f032e6","#f58231","#46f0f0","#aaffc3","#ffe119","#aa6e28","#fabebe","#fabebe","#fabebe","#f032e6","#808080","#008080","#fabebe","#e6beff","#808080","#008080","#808000","#008080","#008080","#808000","#0082c8","#008080","#3cb44b","#911eb4","#911eb4","#911eb4","#911eb4","#808080","#ffd8b1","#ffd8b1","#000080","#808080","#d2f53c","#f58231","#800000","#800000","#800000","#800000","#fabebe","#800000","#808080","#000080","#e6beff","#808080","#e6194b","#800000","#ffe119","#e6194b","#e6194b","#008080","#008080","#fabebe","#f032e6","#008080","#d2f53c","#000080","#e6194b","#3cb44b","#46f0f0","#000080","#008080","#aaffc3","#911eb4","#fffac8","#ffe119","#ffe119","#46f0f0","#800000","#800000","#800000","#800000","#ffd8b1","#3cb44b","#3cb44b","#000080","#008080","#0082c8","#808080","#0082c8","#808080","#808080","#000080","#808080","#000080","#000080","#911eb4","#ffe119","#008080","#008080","#808080","#e6beff","#f58231","#f58231","#fffac8","#800000","#911eb4","#008080","#f58231","#911eb4","#3cb44b","#e6194b","#e6194b","#0082c8","#0082c8","#911eb4","#d2f53c","#008080","#fffac8","#fffac8","#fffac8","#800000","#008080","#008080","#46f0f0","#e6194b","#f032e6","#008080","#000080","#0082c8","#f032e6","#008080","#008080","#008080","#008080","#008080","#3cb44b","#3cb44b","#3cb44b","#3cb44b","#ffe119","#ffe119","#ffe119","#ffe119","#008080","#008080","#008080","#d2f53c","#e6194b","#d2f53c","#d2f53c","#d2f53c","#e6194b","#000080","#808080","#fabebe","#0082c8","#0082c8","#fffac8","#0082c8","#d2f53c","#0082c8","#e6194b","#000080","#fffac8","#008080","#008080","#e6194b","#911eb4","#808000","#808000","#808080","#fffac8","#d2f53c","#fffac8","#46f0f0","#008080","#008080","#ffe119","#008080","#f58231","#ffe119","#ffe119","#fffac8","#fabebe","#fabebe","#000080","#000080","#e6194b","#008080","#aa6e28","#aaffc3","#d2f53c","#f58231","#ffd8b1","#ffd8b1","#ffd8b1","#e6194b","#008080","#f58231","#f58231","#d2f53c","#800000","#d2f53c","#911eb4","#ffe119","#808000","#d2f53c","#46f0f0","#3cb44b","#808080","#808080","#000080","#3cb44b","#fabebe","#008080","#008080","#e6194b","#e6194b","#ffd8b1","#000080","#000080","#aa6e28","#000080","#000080","#000080","#808080","#008080","#808080","#000080","#008080","#46f0f0","#46f0f0","#46f0f0","#0082c8","#008080","#46f0f0","#008080","#911eb4","#aaffc3","#008080","#aaffc3","#aaffc3","#808080","#808080","#000080","#800000","#008080","#3cb44b","#008080","#808080","#3cb44b","#f032e6","#fffac8","#f032e6","#808080","#3cb44b","#e6194b","#e6beff","#008080","#fabebe","#e6194b","#008080","#aa6e28","#000080","#aa6e28","#808080","#f58231","#ffe119","#808080","#ffe119","#ffe119","#000080","#008080","#3cb44b","#46f0f0","#46f0f0","#46f0f0","#f58231","#e6beff","#e6beff","#e6194b","#f58231","#f58231","#f58231","#e6194b","#e6194b","#808000","#e6194b","#808000","#aaffc3","#800000","#f032e6","#ffd8b1","#808080","#008080","#3cb44b","#3cb44b","#d2f53c","#d2f53c","#3cb44b","#f032e6","#000080","#ffe119","#000080","#008080","#e6194b","#008080","#008080","#000080","#3cb44b","#e6194b","#f58231","#aaffc3","#e6beff","#e6beff","#800000","#800000","#800000","#800000","#800000","#800000","#800000","#e6194b","#fffac8","#f032e6","#46f0f0","#f032e6","#3cb44b","#008080","#008080","#e6194b","#808080","#008080","#3cb44b","#008080","#808080","#d2f53c","#e6194b","#800000","#e6194b","#008080","#fffac8","#3cb44b","#0082c8","#008080","#aa6e28","#e6beff","#aa6e28","#aaffc3","#000080","#808080","#000080","#000080","#ffe119","#ffe119","#ffe119","#ffe119","#ffe119","#fabebe","#e6194b","#e6194b","#e6194b","#f58231","#e6194b","#d2f53c","#800000","#d2f53c","#911eb4","#911eb4","#911eb4","#000080","#e6beff","#0082c8","#3cb44b","#3cb44b","#fabebe","#fabebe","#e6194b","#fabebe","#fabebe","#ffe119","#e6beff","#e6beff","#e6beff","#ffe119","#911eb4","#808080","#808000","#808080","#800000","#46f0f0","#46f0f0","#800000","#f58231","#ffd8b1","#f58231","#aaffc3","#fabebe","#f58231","#3cb44b","#aa6e28","#911eb4","#808080","#e6194b","#000080","#e6194b","#911eb4","#008080","#d2f53c","#808000","#f58231","#f58231","#800000","#aaffc3","#f032e6","#008080","#008080","#d2f53c","#f032e6","#fabebe","#aaffc3","#aa6e28","#008080","#ffe119","#aa6e28","#008080","#000080","#d2f53c","#e6194b","#fffac8","#911eb4","#aa6e28","#008080","#aa6e28","#f58231","#008080","#e6194b","#000080","#46f0f0","#fabebe","#fabebe","#808080","#e6194b","#808080","#e6194b","#e6194b","#0082c8","#911eb4","#000080","#fffac8","#fffac8","#aa6e28","#fffac8","#808080","#f032e6","#e6beff","#f58231","#e6beff","#e6194b","#800000","#800000","#800000","#46f0f0","#e6194b","#d2f53c","#008080","#3cb44b","#800000","#800000","#800000","#800000","#008080","#fffac8","#e6194b","#e6194b","#911eb4","#808080","#000080","#008080","#3cb44b","#f58231","#3cb44b","#aaffc3","#aaffc3","#aaffc3","#008080","#008080","#000080","#fabebe","#e6194b","#fabebe","#e6194b","#808000","#808000","#008080","#3cb44b","#ffd8b1","#ffd8b1","#46f0f0","#46f0f0","#e6194b","#ffe119","#ffe119","#000080","#ffe119","#808080","#ffe119","#911eb4","#aaffc3","#aa6e28","#008080","#000080","#ffe119","#ffe119","#808000","#ffe119","#3cb44b","#3cb44b","#008080","#e6194b","#0082c8","#d2f53c","#808080","#3cb44b","#d2f53c","#d2f53c","#911eb4","#0082c8","#0082c8","#0082c8","#d2f53c","#0082c8","#d2f53c","#808080","#008080","#808080","#808080","#008080","#008080","#3cb44b","#fffac8","#e6194b","#008080","#e6194b","#e6194b","#3cb44b","#ffd8b1","#ffd8b1","#ffd8b1","#ffd8b1","#ffd8b1","#000080","#808080","#fabebe","#000080","#008080","#008080","#000080","#000080","#aa6e28","#000080","#3cb44b","#fabebe","#fabebe","#d2f53c","#e6194b","#008080","#008080","#800000","#800000","#008080","#e6194b","#ffd8b1","#3cb44b","#46f0f0","#ffd8b1","#ffe119","#000080","#e6194b","#e6194b","#e6194b","#e6194b","#e6194b","#fffac8","#f032e6","#808000","#fabebe","#fabebe","#000080","#46f0f0","#e6beff","#e6beff","#e6beff","#808080","#000080","#fabebe","#008080","#808080","#000080","#aa6e28","#ffe119","#aa6e28","#ffe119","#aaffc3","#e6194b","#fabebe","#0082c8","#808080","#46f0f0","#ffd8b1","#008080","#008080","#008080","#fffac8","#f58231","#008080","#808000","#d2f53c","#808080","#008080","#911eb4","#f58231","#808000","#808000","#808000","#d2f53c","#ffd8b1","#d2f53c","#d2f53c","#911eb4","#d2f53c","#fffac8","#808000","#fffac8","#008080","#008080","#aaffc3","#aaffc3","#008080","#f58231","#aaffc3","#aa6e28","#ffe119","#000080","#808080","#e6194b","#f032e6","#000080","#fabebe","#fabebe","#000080","#f58231","#3cb44b","#ffe119","#46f0f0","#808080","#e6194b","#e6194b","#3cb44b","#3cb44b","#f58231","#f58231","#e6194b","#e6194b","#e6194b","#46f0f0","#800000","#e6194b","#e6194b","#911eb4","#e6194b","#800000","#d2f53c","#ffe119","#ffe119","#000080","#3cb44b","#ffe119","#ffe119","#d2f53c","#808080","#808080","#000080","#808080","#808080","#000080","#808080","#008080","#808080","#008080","#800000","#008080","#008080","#808000","#911eb4","#800000","#e6194b","#911eb4","#fffac8","#46f0f0","#46f0f0","#46f0f0","#808080","#808080","#0082c8","#e6beff","#008080","#aaffc3","#aa6e28","#aaffc3","#f58231","#000080","#aaffc3","#f032e6","#3cb44b","#f032e6","#f032e6","#0082c8","#e6194b","#800000","#46f0f0","#46f0f0","#800000","#008080","#008080","#008080","#911eb4","#008080","#911eb4","#911eb4","#911eb4","#e6beff","#911eb4","#911eb4","#008080","#fffac8","#000080","#fffac8","#fabebe","#fabebe","#911eb4","#911eb4","#911eb4","#911eb4","#fabebe","#008080","#008080","#e6beff","#911eb4","#911eb4","#008080","#aa6e28","#aa6e28","#008080","#008080","#fffac8","#fffac8","#fffac8","#fffac8","#fffac8","#f58231","#f032e6","#d2f53c","#e6194b","#46f0f0","#0082c8","#008080","#f032e6","#000080","#808080","#000080","#fabebe","#aaffc3","#e6194b","#911eb4","#911eb4","#aa6e28","#3cb44b","#008080","#008080","#008080","#008080","#008080","#fabebe","#e6194b","#808080","#0082c8","#ffe119","#008080","#008080","#808080","#808080","#800000","#e6194b","#800000","#800000","#3cb44b","#46f0f0","#aa6e28","#911eb4","#911eb4","#911eb4","#aa6e28","#d2f53c","#f032e6","#0082c8","#0082c8","#d2f53c","#008080","#d2f53c","#3cb44b","#d2f53c","#e6194b","#d2f53c","#46f0f0","#3cb44b","#008080","#008080","#008080","#46f0f0","#f58231","#fffac8","#46f0f0","#46f0f0","#aa6e28","#aa6e28","#008080","#e6194b","#0082c8","#e6194b","#fffac8","#fffac8","#fffac8","#008080","#fabebe","#3cb44b","#e6194b","#ffe119","#fffac8","#008080","#f58231","#ffe119","#e6beff","#800000","#800000","#f58231","#800000","#46f0f0","#46f0f0","#46f0f0","#ffe119","#fabebe","#000080","#e6beff","#000080","#e6beff","#f032e6","#ffd8b1","#3cb44b","#3cb44b","#3cb44b","#3cb44b","#008080","#008080","#008080","#008080","#3cb44b","#008080","#d2f53c","#fffac8","#911eb4","#0082c8","#0082c8","#fabebe","#0082c8","#0082c8","#0082c8","#000080","#808080","#008080","#ffe119","#aa6e28","#f58231","#f58231","#f58231","#f58231","#f58231","#f58231","#fabebe","#0082c8","#808080","#0082c8","#808080","#008080","#3cb44b","#e6194b","#0082c8","#aaffc3","#008080","#008080","#000080","#008080","#aa6e28","#e6194b","#fabebe","#808080","#808080","#808080","#911eb4","#911eb4","#911eb4","#911eb4","#0082c8","#0082c8","#0082c8","#0082c8","#0082c8","#aa6e28","#fabebe","#008080","#008080","#808000","#808000","#e6194b","#fabebe","#e6194b","#fabebe","#fabebe","#d2f53c","#008080","#f58231","#008080","#808000","#d2f53c","#0082c8","#ffe119","#911eb4","#008080","#e6beff","#008080","#808000","#fffac8","#008080","#fabebe","#808080","#808000","#e6194b","#3cb44b","#e6194b","#aa6e28","#3cb44b","#800000","#000080","#008080","#aa6e28","#911eb4","#911eb4","#911eb4","#911eb4","#e6beff","#3cb44b","#3cb44b","#808000","#f032e6","#808000","#008080","#008080","#008080","#808080","#808080","#e6194b","#808080","#0082c8","#000080","#0082c8","#808080","#808080","#000080","#008080","#3cb44b","#e6beff","#e6194b","#e6194b","#e6194b","#e6194b","#0082c8","#d2f53c","#aaffc3","#fffac8","#fffac8","#fffac8","#fffac8","#fffac8","#808080","#d2f53c","#008080","#808080","#e6beff","#e6beff","#fffac8","#0082c8","#0082c8","#808080","#fffac8","#fabebe","#3cb44b","#fabebe","#aa6e28","#800000","#f58231","#3cb44b","#0082c8","#fabebe","#e6194b","#fabebe","#e6beff","#f58231","#800000","#46f0f0","#000080","#3cb44b","#d2f53c","#fffac8","#0082c8","#008080","#008080","#fffac8","#008080","#008080","#ffe119","#ffe119","#008080","#ffe119","#808000","#aa6e28","#808080","#e6194b","#aaffc3","#e6beff","#aaffc3","#ffd8b1","#f032e6","#fabebe","#f032e6","#f032e6","#f032e6","#800000","#46f0f0","#46f0f0","#46f0f0","#46f0f0","#f032e6","#ffe119","#f032e6","#fabebe","#008080","#008080","#008080","#008080","#911eb4","#3cb44b","#e6194b","#008080","#008080","#008080","#000080","#e6194b","#fabebe","#f032e6","#e6194b","#808000","#008080","#d2f53c","#f032e6","#808080","#008080","#808080","#008080","#008080","#3cb44b","#000080","#008080","#f032e6","#808000","#008080","#008080","#fffac8","#000080","#808080","#fabebe","#fabebe","#008080","#808080","#000080","#808080","#008080","#808080","#808080","#e6194b","#000080","#911eb4","#e6194b","#fabebe","#0082c8","#e6194b","#ffe119","#ffe119","#808080","#ffe119","#ffd8b1","#ffd8b1","#ffe119","#3cb44b","#d2f53c","#008080","#008080","#008080","#aa6e28","#008080","#3cb44b","#f58231","#fabebe","#e6194b","#f032e6","#800000","#800000","#f58231","#800000","#e6194b","#f032e6","#e6beff","#fffac8","#d2f53c","#808080","#000080","#fabebe","#800000","#808080","#fabebe","#e6194b","#ffe119","#ffe119","#ffe119","#ffe119","#000080","#000080","#008080","#808080","#008080","#808080","#d2f53c","#0082c8","#d2f53c","#911eb4","#008080","#ffe119","#fabebe","#000080","#fabebe","#e6beff","#f032e6","#f032e6","#f032e6","#3cb44b","#e6beff","#000080","#fabebe","#fabebe","#fabebe","#e6194b","#f032e6","#000080","#000080","#46f0f0","#46f0f0","#f58231","#800000","#800000","#e6194b","#f58231","#e6beff","#808000","#808000","#808000","#911eb4","#808000","#0082c8","#0082c8","#0082c8","#d2f53c","#808080","#d2f53c","#fabebe","#d2f53c","#e6beff","#fffac8","#d2f53c","#e6194b","#3cb44b","#f032e6","#000080","#ffd8b1","#808080","#808080","#808080","#008080","#008080","#911eb4","#008080","#808000","#808000","#808000","#808080","#fabebe","#fabebe","#008080","#008080","#fffac8","#008080","#008080","#ffd8b1","#000080","#008080","#f032e6","#fffac8","#0082c8","#fffac8","#fabebe","#e6194b","#fabebe","#f032e6","#e6194b","#000080","#e6194b","#0082c8","#008080","#800000","#aaffc3","#aaffc3","#008080","#46f0f0","#46f0f0","#46f0f0","#46f0f0","#46f0f0","#46f0f0","#46f0f0","#f032e6","#fffac8","#008080","#e6194b","#800000","#e6194b","#e6194b","#800000","#008080","#008080","#808080","#808080","#aaffc3","#aaffc3","#ffd8b1","#aaffc3","#aaffc3","#008080","#008080","#008080","#008080","#000080","#e6194b","#e6194b","#800000","#d2f53c","#0082c8","#000080","#f58231","#e6194b","#800000","#800000","#e6194b","#f032e6","#3cb44b","#000080","#fffac8","#008080","#008080","#808080","#aaffc3","#aa6e28","#aaffc3","#aaffc3","#3cb44b","#808080","#808080","#808080","#008080","#911eb4","#911eb4","#808080","#808080","#911eb4","#fffac8","#ffe119","#fabebe","#ffe119","#911eb4","#008080","#fabebe","#000080","#000080","#808080","#f58231","#f032e6","#808080","#46f0f0","#aaffc3","#808080","#008080","#808080","#d2f53c","#d2f53c","#d2f53c","#808080","#800000","#d2f53c","#f032e6","#f032e6","#e6194b","#fabebe","#ffe119","#ffe119","#ffe119","#ffd8b1","#ffe119","#008080","#ffe119","#000080","#008080","#e6beff","#fabebe","#008080","#000080","#911eb4","#000080","#808080","#fffac8","#f58231","#f58231","#e6beff","#f032e6","#3cb44b","#008080","#f58231","#008080","#ffe119","#ffe119","#f032e6","#ffe119","#808080","#e6194b","#f032e6","#fabebe","#d2f53c","#808080","#aaffc3","#f58231","#ffd8b1","#008080","#008080","#808080","#808000","#808000","#911eb4","#aa6e28","#aa6e28","#aaffc3","#fffac8","#aa6e28","#008080","#46f0f0","#46f0f0","#008080","#46f0f0","#008080","#808080","#000080","#ffe119","#e6194b","#ffd8b1","#e6194b","#911eb4","#800000","#800000","#808000","#d2f53c","#808000","#d2f53c","#f58231","#fffac8","#f58231","#aaffc3","#808000","#808000","#808000","#808000","#000080","#808080","#000080","#000080","#808000","#d2f53c","#911eb4","#008080","#911eb4","#008080","#aaffc3","#ffd8b1","#e6194b","#008080","#008080","#800000","#ffd8b1","#ffd8b1","#ffd8b1","#ffd8b1","#fffac8","#0082c8","#fabebe","#aa6e28","#0082c8","#ffe119","#ffe119","#e6beff","#fabebe","#f032e6","#ffe119","#3cb44b","#008080","#008080","#008080","#e6194b","#000080","#008080","#008080","#008080","#000080","#e6beff","#ffd8b1","#ffd8b1","#aaffc3","#e6beff","#ffd8b1","#ffd8b1","#aa6e28","#e6beff","#f58231","#e6beff","#911eb4","#f58231","#e6beff","#000080","#fabebe","#000080","#f032e6","#e6194b","#808000","#f032e6","#000080","#008080","#008080","#e6194b","#e6194b","#3cb44b","#e6beff","#f032e6","#3cb44b","#000080","#3cb44b","#f58231","#3cb44b","#3cb44b","#008080","#e6194b","#d2f53c","#008080","#008080","#3cb44b","#008080","#008080","#808080","#d2f53c","#d2f53c","#800000","#ffd8b1","#aaffc3","#808000","#d2f53c","#008080","#800000","#f58231","#e6194b","#fabebe","#46f0f0","#aaffc3","#008080","#e6beff","#008080","#008080","#f032e6","#3cb44b","#aaffc3","#911eb4","#ffe119","#46f0f0","#808000","#911eb4","#d2f53c","#911eb4","#aa6e28","#d2f53c","#3cb44b","#911eb4","#e6beff","#fffac8","#008080","#fffac8","#ffe119","#008080","#e6194b","#ffe119","#800000","#aa6e28","#e6beff","#ffe119","#0082c8","#ffe119","#aa6e28","#008080","#e6194b","#3cb44b","#008080","#808000","#f58231","#3cb44b","#3cb44b","#000080","#3cb44b","#aaffc3","#ffe119","#008080","#ffe119","#800000","#aa6e28","#f58231","#911eb4","#d2f53c","#008080","#008080","#800000","#808000","#3cb44b","#008080","#f58231","#f032e6","#f58231","#ffd8b1","#d2f53c","#ffe119","#008080","#ffe119","#808000","#911eb4","#008080","#008080","#fabebe","#008080","#fabebe","#e6194b","#fabebe","#008080","#fabebe","#f032e6","#000080","#f58231","#0082c8","#0082c8","#808080","#fabebe","#000080","#008080","#fabebe","#0082c8","#008080","#0082c8","#008080","#008080","#e6194b","#008080","#e6194b","#3cb44b","#3cb44b","#e6194b","#008080","#008080","#aaffc3","#000080","#000080","#46f0f0","#d2f53c","#ffd8b1","#ffe119","#f58231","#f032e6","#aa6e28","#ffe119","#e6beff","#008080","#000080","#008080","#800000","#808000","#ffe119","#000080","#800000","#f58231","#fffac8","#808000","#e6beff","#f58231","#aaffc3","#0082c8","#ffe119","#fffac8","#aa6e28","#008080","#ffe119","#000080","#f032e6","#e6194b","#ffd8b1","#3cb44b","#808000","#e6194b","#e6194b","#e6beff","#e6beff","#800000","#e6194b","#e6194b","#aaffc3","#800000","#800000","#e6194b","#008080","#911eb4","#aaffc3","#0082c8","#f032e6","#008080","#808080","#ffe119","#008080","#008080","#d2f53c","#e6194b","#d2f53c","#e6194b","#800000","#fabebe","#000080","#808080","#0082c8","#0082c8","#008080","#46f0f0","#911eb4","#808080","#46f0f0","#f032e6","#e6194b","#008080","#000080","#e6194b","#ffe119","#800000","#e6194b","#911eb4","#ffd8b1","#aaffc3","#800000","#e6beff","#911eb4","#f032e6","#f58231","#ffd8b1","#0082c8","#911eb4","#800000","#e6194b","#800000","#800000","#f58231","#f032e6","#f032e6","#d2f53c","#e6194b","#fffac8","#008080","#ffd8b1","#46f0f0","#ffd8b1","#f58231","#ffd8b1","#ffd8b1","#f58231","#46f0f0","#46f0f0","#800000","#46f0f0","#3cb44b","#46f0f0","#3cb44b","#aaffc3","#aa6e28","#ffd8b1","#aa6e28","#aaffc3","#ffe119","#e6194b","#ffd8b1","#fffac8","#911eb4","#3cb44b","#808080","#808000","#008080","#808000","#800000","#d2f53c","#0082c8","#911eb4","#0082c8","#0082c8","#fffac8","#fabebe","#f58231","#808080","#f032e6","#911eb4","#008080","#008080","#008080","#000080","#000080","#808080","#000080","#008080","#808080","#0082c8","#0082c8","#808080","#808080","#f032e6","#808080","#008080","#808080","#808080","#fabebe","#000080","#808080","#008080","#808080","#46f0f0","#808080","#808080","#f58231","#808080","#808080","#f032e6","#808080","#f032e6","#f032e6","#f032e6","#911eb4","#46f0f0","#e6194b","#ffe119","#ffe119","#808080","#000080","#f032e6","#000080","#008080","#e6194b","#ffe119","#f032e6","#fabebe","#e6beff","#008080","#808080","#008080","#008080","#e6194b","#008080","#008080","#e6194b","#fabebe","#f58231","#e6beff","#f58231","#e6beff","#f58231","#f58231","#f58231","#e6194b","#e6194b","#008080","#0082c8","#0082c8","#911eb4","#fabebe","#f032e6","#f032e6","#000080","#808080","#000080","#e6beff","#fffac8","#e6194b","#008080","#e6194b","#fabebe","#aaffc3","#f58231","#000080","#008080","#000080","#e6194b","#000080","#000080","#ffe119","#ffe119","#ffe119","#911eb4","#ffe119","#aaffc3","#aa6e28","#aa6e28","#fffac8","#aaffc3","#008080","#008080","#fabebe","#46f0f0","#fabebe","#ffd8b1","#ffd8b1","#3cb44b","#808080","#aaffc3","#3cb44b","#fffac8","#808080","#d2f53c","#d2f53c","#008080","#3cb44b","#46f0f0","#008080","#e6194b","#0082c8","#808000","#e6194b","#ffe119","#808080","#808000","#aa6e28","#e6194b","#0082c8","#008080","#008080","#911eb4","#911eb4","#911eb4","#008080","#008080","#008080","#ffd8b1","#e6beff","#008080","#f032e6","#008080","#000080","#fffac8","#000080","#fffac8","#fffac8","#008080","#808080","#008080","#008080","#f58231","#aaffc3","#f58231","#f58231","#f58231","#008080","#e6194b","#e6194b","#d2f53c","#3cb44b","#e6194b","#fffac8","#fffac8","#f58231","#f58231","#e6194b","#f58231","#f032e6","#e6beff","#e6194b","#f032e6","#008080","#fabebe","#ffe119","#ffe119","#ffe119","#f58231","#008080","#008080","#e6beff","#ffe119","#e6beff","#ffe119","#e6beff","#ffe119","#ffe119","#ffe119","#e6194b","#f58231","#e6194b","#e6194b","#000080","#fabebe","#0082c8","#0082c8","#808080","#0082c8","#d2f53c","#aa6e28","#808080","#0082c8","#008080","#fffac8","#ffe119","#008080","#008080","#aa6e28","#f032e6","#000080","#e6194b","#3cb44b","#3cb44b","#3cb44b","#aa6e28","#e6194b","#008080","#aaffc3","#008080","#fabebe","#fabebe","#008080","#808080","#aaffc3","#008080","#aaffc3","#d2f53c","#f58231","#d2f53c","#fabebe","#d2f53c","#e6194b","#800000","#46f0f0","#aa6e28","#008080","#008080","#3cb44b","#3cb44b","#808000","#3cb44b","#0082c8","#0082c8","#0082c8","#fabebe","#f032e6","#f032e6","#000080","#fffac8","#808080","#aaffc3","#aaffc3","#f58231","#fffac8","#d2f53c","#fabebe","#f032e6","#f032e6","#e6194b","#808080","#000080","#e6194b","#000080","#ffe119","#ffe119","#008080","#000080","#f58231","#f58231","#aaffc3","#aaffc3","#aaffc3","#3cb44b","#800000","#46f0f0","#f58231","#911eb4","#aa6e28","#008080","#e6beff","#008080","#008080","#911eb4","#d2f53c","#808000","#911eb4","#3cb44b","#800000","#fabebe","#e6194b","#808000","#f58231","#e6beff","#808000","#f032e6","#000080","#008080","#008080","#008080","#d2f53c","#ffe119","#ffe119","#ffe119","#ffe119","#000080","#0082c8","#d2f53c","#0082c8","#0082c8","#808080","#0082c8","#000080","#808080","#008080","#fabebe","#f032e6","#3cb44b","#f032e6","#aa6e28","#911eb4","#800000","#f58231","#808000","#911eb4","#000080","#008080","#fabebe","#808080","#d2f53c","#e6194b","#008080","#fffac8","#fffac8","#e6beff","#e6194b","#e6beff","#e6194b","#000080","#e6194b","#e6194b","#e6194b","#000080","#800000","#0082c8","#000080","#ffe119","#008080","#f58231","#f58231","#008080","#e6194b","#e6194b","#e6194b","#ffe119","#ffe119","#f58231","#808000","#ffe119","#808000","#911eb4","#aaffc3","#3cb44b","#0082c8","#008080","#000080","#000080","#aa6e28","#808080","#0082c8","#008080","#e6194b","#e6194b","#fffac8","#aaffc3","#ffd8b1","#fabebe","#800000","#e6194b","#008080","#008080","#e6194b","#000080","#008080","#3cb44b","#008080","#000080","#fabebe","#808080","#fffac8","#008080","#008080","#e6194b","#000080","#808080","#808080","#808080","#008080","#911eb4","#000080","#808080","#808080","#008080","#911eb4","#000080","#008080","#f58231","#fabebe","#808080","#fabebe","#fabebe","#808000","#808000","#808000","#3cb44b","#000080","#808000","#008080","#e6194b","#008080","#e6194b","#fabebe","#fabebe","#d2f53c","#fffac8","#008080","#ffd8b1","#008080","#008080","#008080","#008080","#3cb44b","#000080","#808080","#808080","#fabebe","#e6beff","#e6beff","#d2f53c","#f58231","#e6194b","#fffac8","#e6beff","#fabebe","#f58231","#911eb4","#aaffc3","#fffac8","#008080","#911eb4","#f58231","#008080","#aaffc3","#aaffc3","#3cb44b","#aaffc3","#aa6e28","#e6194b","#aa6e28","#808080","#0082c8","#fabebe","#f032e6","#fabebe","#808080","#fabebe","#ffe119","#808080","#008080","#e6beff","#e6beff","#e6beff","#fffac8","#d2f53c","#fffac8","#e6beff","#ffd8b1","#aaffc3","#e6beff","#008080","#f032e6","#008080","#aa6e28","#e6194b","#f58231","#e6194b","#3cb44b","#d2f53c","#e6194b","#008080","#911eb4","#808000","#808000","#e6194b","#800000","#800000","#f58231","#808080","#f58231","#008080","#e6beff","#e6beff","#fabebe","#000080","#808080","#ffe119","#000080","#46f0f0","#800000","#008080","#e6194b","#fffac8","#f58231","#e6194b","#008080","#008080","#808080","#808080","#808080","#000080","#000080","#000080","#46f0f0","#008080","#008080","#000080","#008080","#000080","#000080","#008080","#008080","#000080","#008080","#000080","#aa6e28","#000080","#008080","#008080","#808080","#808080","#008080","#e6beff","#ffd8b1","#ffe119","#ffe119","#000080","#e6beff","#008080","#008080","#ffd8b1","#ffd8b1","#ffd8b1","#ffd8b1","#800000","#808080","#000080","#f032e6","#808080","#f58231","#aaffc3","#008080","#e6194b","#008080","#f58231","#e6beff","#e6194b","#000080","#008080","#800000","#d2f53c","#008080","#008080","#3cb44b","#008080","#0082c8","#0082c8","#0082c8","#0082c8","#008080","#808080","#808080","#000080","#008080","#f032e6","#e6beff","#f032e6","#000080","#3cb44b","#46f0f0","#fabebe","#911eb4","#ffe119","#3cb44b","#ffe119","#808080","#000080","#ffd8b1","#800000","#f032e6","#e6beff","#e6beff","#008080","#e6beff","#000080","#000080","#fabebe","#ffe119","#ffe119","#ffe119","#808000","#808000","#808000","#3cb44b","#0082c8","#e6194b","#d2f53c","#d2f53c","#aa6e28","#808080","#911eb4","#e6194b","#e6beff","#0082c8","#aaffc3","#000080","#008080","#008080","#008080","#fffac8","#e6194b","#f58231","#d2f53c","#ffd8b1","#808080","#ffe119","#ffe119","#3cb44b","#f58231","#ffe119","#ffe119","#3cb44b","#808080","#e6194b","#000080","#fabebe","#008080","#d2f53c","#911eb4","#ffe119","#ffe119","#ffe119","#3cb44b","#3cb44b","#aaffc3","#000080","#e6194b","#0082c8","#aaffc3","#d2f53c","#fffac8","#f032e6","#800000","#808080","#f032e6","#d2f53c","#aa6e28","#f58231","#d2f53c","#e6beff","#ffe119","#aa6e28","#aa6e28","#aa6e28","#000080","#e6194b","#e6beff","#f032e6","#f032e6","#000080","#fabebe","#d2f53c","#808000","#f032e6","#ffe119","#d2f53c","#aa6e28","#808080","#0082c8","#808080","#f032e6","#e6194b","#e6194b","#e6194b","#800000","#3cb44b","#911eb4","#911eb4","#fffac8","#ffe119","#000080","#f032e6","#f58231","#f58231","#aaffc3","#aaffc3","#aaffc3","#3cb44b","#008080","#fabebe","#808080","#f58231","#e6beff","#fabebe"],"content":["Network models are in increasing use to describe populations, including socially networked human populations, computer and communication networks, and gene regulatory networks. A network has nodes (e.g., people) and links (e.g., relationships between people). The nodes may have characteristics of interest, and the relationships may be of different types and strengths. Network data, however, generally represent a sample from the wider population network of interest. This short course will cover methods for obtaining samples from networks and using the sample data to make inference about characteristics of the population network. In many cases the only practical way to obtain a large enough sample from the population is to follow links from sample individuals to add more individuals to the sample. For example, in studies of the risk behaviors in people at risk for HIV/AIDS, the population is hidden so standard sampling designs cannot be applied. Instead, researchers follow social referrals from individuals in the sample to find more members of the hidden population. Similarly, in studies of the World Wide Web, links or connections from sites in the sample are followed to add more sites to the sample. Network methods also turn out to be useful for spatial sampling in environmental and ecological sciences where the populations tend to be highly clustered or rare. Link-tracing sampling designs will be described, together with design-based and Bayes methods for estimating population characteristics based on such samples. Computational methods and available software also will be described.","In this paper we explore the effects of a complex sample design on the estimation of a regression tree. To do this we present a simple function representation for a regression tree that more closely resembles standard regression. From this representation we form an estimating equation that allows us to incorporate survey weights in our estimation of the coefficients. In addition our method of pruning allows us to incorporate weights so we can consider the impact of survey weights on the tree pruning procedure. We compare the population estimates to a tree built without incorporating survey weights. ","In contrast to linear mixed models, most supervised learning methods do not take into account whether the data have some clustered structure. Random forest (RF) is a popular technique used to model complex data sets because it tends to produce accurate predictions. We propose a modified RF algorithm called Mixed Random Forest (MRF) that can model mixed-effects data using regression trees to produce robust estimates of group means and random effects. We compare the results of the MRF algorithm with those from linear mixed models for the small area estimation (SAE) problem. Our method is shown to perform better in terms of mean squared prediction error when the underlying function is complex, such as conditionally linear, even though no model is pre-specified. Although we focus on SAE, our method can be used whenever the data are clustered as long as the number of clusters is large. ","This paper explores the accuracy of a simple density estimator, based solely on one histogram, for a variety of possible distribution shapes and bin/sample size combinations, through a simulation study. The density estimator is a piecewise quadratic polynomial chosen to match histogram areas with boundary points initially at midpoints of adjacent histogram bars then improved for smoothness. Performance is measured by the Mean Integrated Squared Error of the density estimates themselves and the Mean Square Errors of the means and a few percentiles derived from the density estimates. To give insight into the performance of this density estimator in practice, the piecewise quadratic density estimator is applied to wage data from the Bureau of Labor Statistics and compared to kernel density estimates using corresponding point data. ","We propose an extension to bootstrap methods for evaluating regression models estimated with data from surveys with complex design. Such methods involve selection of replicate samples formed from simple random samples of sampled clusters within strata. Selection is carried out with replacement, so that about one third of clusters are typically left out of a given replicate sample. Our evaluation method exploits the excluded clusters, using them as cross-validation samples for assessment of a model's prediction error, and at the same time using the bootstrap samples to estimate the variance of regression coefficients.  We also consider the use of a sample of the replicates as a cross-validation sample. ","A Spanish two-way immersion program is a dual language program in which instruction is provided in Spanish and English to native English speakers and native Spanish speakers. The languages of instruction are kept separate and students are grouped heterogeneously. This study was conducted with kindergarteners after their first year either in a Spanish TWI or a traditional program. The study was conducted in two rural and two urban schools in a Midwest state, with a sample size of 114 students distributed unequally through the schools. The children were asked to answer a 14-item survey about their attitudes toward school and learning. A generalized mixed multinomial linear model analysis revealed that TWI programs were associated with significantly more positive effects on the kindergarten students' attitude toward the Spanish language in comparison to their peers in control schools. ","Propensity score matching is widely used to form comparable \"treatment\" and \"control\" groups in observational studies. When forming these groups, a logistic regression model is commonly used to estimate scalar propensity scores by which the two groups are being matched. If the number of groups is larger than two, several methods are available to construct the groups, including generalized propensity scores and scalar balancing scores. This paper presents an application of these methods in selecting postsecondary education institutions into four comparable groups defined by cross classifying institutions on whether they received Child Care Access Means Parents in School (CCAMPIS) grants and whether they have on-campus child-care centers. Moreover, we compare these matching results to those based on simple sequential binary propensity score matching. ","A total of 171 students at a medical school self-rated on 3 questions measuring professionalism, teamwork, and mastery of content with ordinal ratings from 1-5. Each student was also rated by 3 different peers. Pearson correlation coefficients between self-evaluation and the average of 3-peer evaluations and Kappa coefficients comparing the consensus of 3-peer evaluations and the self-evaluation were computed. A multinomial cumulative logistic model was applied. Both Pearson correlation coefficients and weighted Kappa coefficients showed a significant agreement between self- and peer- evaluations. The multinomial logistic regression model suggested peer-evaluation tends to be more generous than self-evaluation. For example, if self-evaluation was 3, there was a 70% probability that peer-evaluation would be 4. ","Modeling of infant survival rate is important for many reasons such as prediction U.S. population, prediction of health-care and daycare facilities, manufacturers (infant food, cloth etc.). We use regression model to simulate infant survival rate. Different approaches are used to handle missing data. Simulations is done for five Southern States including Arkansas, Texas, Tennessee, Alabama, and Missouri. The results of the modeling for 7 years are compared. ","When the outcome variable has many missing data, imputation has been a common method.  But when the response is the number of homicide and contains many zeros, we have to deal with zeros in different ways.  In order to deal with the zero response many methods has been developed.  This paper investigates the procedure of choosing the best fitting model when the response variable has many zeros in the case of homicide.  Few models are in consideration: generalized linear regression (GLM) with Poisson and with negative binomial distribution, zero-inflated with Poisson (ZIP) and with negative binomial distribution (ZINB), zero-altered with Poisson (ZAP or hurdle) and negative binomial distribution (ZANB), and finally two-part model. An example from public health research is used for illustration. ","The objective of this project is to examine how high-school coursework affects attrition of university students using regression analysis. Specifically, previous studies repeatedly show that minority and nonresident students are more likely to drop out from universities. High school GPA and SAT scores do not tell us how performance in different disciplines is related to college persistence. In this study, archival data sets from the data warehouse of a Southwestern university, which contain records of students' high school coursework, will be utilized. Since a high volume of missing data is expected, the final sample size may be small and data points may be sparse. To counteract the problem, exact logistic regression and other remedies will be employed. ","The nonresponse bias of simple linear estimates can be measured directly from respondent data if the response propensity for each survey respondent is known. In reality these propensities are unknown and must be estimated. Traditional estimation methods for propensities are used to produce weighting class and response propensity modeling adjustments for nonresponse. This research explores the use of psychometric methods to estimate propensities from ancillary information plus respondents' answers to questions about their availability and willingness to participate in surveys more generally. Data from a recent telephone survey conducted by the UNC Survey Research Unit are used to compare estimates of nonresponse bias computed from propensities obtained by this approach versus bias from propensities obtained by the traditional methods.  ","Human subject data in psychological research often contain a high level of error. Factor analytic data are no exception. Factor loading instability is common in single administration factor analytic research and results in poor interpretation of the factor pattern. However, Monte Carlo simulations have shown that averaging data across administrations reduces error, resulting in increased explained variance and stable factor patterns. In this study, the Questionnaire for the Measurement of Psychological Reactance (QMPR) was administered multiple times. The data were then averaged and analyzed using principle-components factor analysis with varimax rotation. The results of the study show a notable increase in explained variance and factor-pattern stability, supporting previous simulation findings. Averaging data across administrations is advocated to reduce error in human subject data. ","We compare students' scores between public and private schools in Ireland, using data collected for PISA. This survey employs a two stage sampling design of schools and pupils, but the choice of school by pupils is not under control. Hence, pupils of the two types of school can differ in unknown characteristics, and direct comparison of the sample scores may lead to erroneous conclusions. We compare the performance of commonly used approaches to deal with this problem, such as Hajek and Greg type estimators with weights equal the inverse propensity scores, instrumental and latent variables models and an alternative approach that we developed. This approach fits a model to the data by modeling the outcome distribution under ignorable assignment and the school selection probabilities, which are modeled as functions of the outcome and covariates, thus accounting for informative selection.  ","Classification error analysis aims to measure misclassification rates and to correct estimates of proportions for misclassification. We will trace development of the key models developed for this type of analysis and compare and contrast them. Of particular interest are the latent class models developed by Lazarasfeld and Henry (1968), the Census Bureau Model developed by Hansen, Hurwitz, and Pritzker (1964) and the finite mixture probability models developed by Bross (1954), Tenenbein (1972), Hui and Walter (1980) and others. This paper will show how these three primary methods can be viewed from a broader log linear model with latent variables perspective and how under that framework classification error can be better specified through greater model flexibility. Then, this paper will present examples of how this method has been used in practice. ","Incentive payments to survey respondents have been used extensively for many years, and considerable research evidence supports their effectiveness in improving cooperation and the speed and quality of response. It has become increasingly difficult to achieve high response in many key surveys, and the use of incentives has become increasingly widespread. In March 2008, a one day seminar on survey respondent incentives brought together survey practitioners and methodologists to exchange views and practices related to providing incentives to survey respondents to improve response rates. This invited panel of survey researchers consists of key participants in the seminar who will consolidate and synthesize the main issues and key themes that emerged from the seminar and offer their perspectives on current and best practices and suggest future directions for implementation and research. ","Randy Sitter has made fundamental contributions to resampling methods in sample surveys. In particular, he has developed ingenious methods for variance estimation under stratified multistage cluster sampling designs using balanced repeated replication and bootstrap sampling. In this talk, I will highlight some of Randy's major contributions to this important topic in survey sampling. ","Accurate estimation of elastic modulus of certain nanomaterials like nanobelt is important in many applications. A recently proposed approach was to estimate elastic modulus from a force-deflection model based on the continuous scan of a nanobelt using an Atomic Force Microscope tip at different contact forces. However, the nanobelt may have some initial bending and it may shift or deform during measurement leading to bias in the estimation. In this work we propose a new approach \"Profile adjustment and parameter estimation (PAPE) to account for these various possible errors. It can automatically detect and remove the systematic errors and thus gives a more accurate estimate of the elastic modulus. The advantages of the approach are demonstrated through the application on several data sets. (Joint work with X. Deng, R. Joseph, W. Mai, Z. L. Wang.) ","Design weights in surveys are often adjusted to accommodate auxiliary information and meet pre-specified range restrictions, typically via some ad hoc algorithmic adjustment to a generalized regression estimator. We present a simple solution to this problem using empirical likelihood methods or generalized regression. We first develop algorithms for computing empirical likelihood estimators and model-calibrated empirical likelihood estimators. The first algorithm solves the computational problem of the empirical likelihood method in general, both in survey and nonsurvey settings, and theoretically guarantees its convergence. The second exploits properties of the model calibration method and is particularly simple. The algorithms are adapted to handle benchmark constraints and prespecified range restrictions on the weight adjustments. ","The World Health Organization and many other NGOs utilize HIV infection models in order to make predictions and allocate funding. Examination of these models reveals that numerous assumptions tend to be made, such as model forms, model parameters, and distributions. While such assumptions are often necessary when modeling complex phenomena, we find that the uncertainty introduced by these assumptions has not been adequately quantified, and may be seriously underestimated. We demonstrate one approach to evaluating and quantifying the uncertainty associated with these models. It is our belief that a more accurate understanding of the magnitude of the uncertainty will result in better decisions being made using these models. ","Biosurveillance for Public Health has been a focus of recent legislative and policy initiatives such as the Pandemic and All Hazards Preparedness Act and Homeland Security Presidential Directives. The initiatives produce increased technical requirements for data analysis, information science and communicating biosurveillance results involving the characterization of uncertainty. In addition to traditional analytic epidemiological studies, data and information processing is used to establish a \"common operating picture\" and \"situational awareness\" for public health on an ongoing bases and especially to be used in responding to emergencies. These evolving public health operational requirements augment the complexity of practical biosurveillance. This talk will provide a brief background and history with some examples of challenging components and areas of potential research. ","We describe some of the challenges involved in planning, conducting, and analyzing data from, randomized screening trials that distinguishes them from therapy clinical trials, including issues of lead time, overdiagnosis, and length bias. The present PLCO screening trial, designed to evaluate the effect of screening for prostate, lung, colorectal, and ovarian cancers, will illustrate the issues. The impact of screening on the public, including financial, sociological, and health aspects, will be discussed. ","We examine methodology for developing and validating models to estimate the effect of air pollution (particulate matter, PM) on health. Changes in PM depend on measurement techniques, modeling, and analysis. We use NCHS mortality data on respiratory and cardiovascular disease, and develop and innovative method for assigning distance between place of death and air quality monitoring source. Models to predict mortality use Air quality data from EPRI and weather data from NOAA. We show the value of information from multiple causes of deaths due to respiratory conditions, unintentional injuries, septicemia, and Alzheimer's disease emphasizes the importance of statistics in guiding the proper use and interpretation of model effects on outcomes. ","In 2007, Judkins, Krenzke, Piesse, Fan, and Haung reported on the performance of a new semi-parametric imputation algorithm designed to impute entire questionnaires with minimal human supervision while preserving important first- and second-order distributional properties. In this paper, we report on procedures for post-imputation variance estimation to be used in conjunction with the semi-parametric imputation algorithm. ","The German Federal Employment Agency is using a comprehensive evaluation system to measure the efficiency of the large variety of job-training programs. Because the assignment to any of the possible training programs (i.e., treatments) typically is not based on a random process, corrections have to be done before drawing causal inferences. Basically, we try to answer the question for each job-training program and each trained person, if a person had not been job trained, how much time would have passed until the person found employment? Based on Rubin's Causal Model we multiply impute these missing potential outcomes to get estimates of the individual causal effects. Since this is a massive imputation task, innovative and complex algorithms as well as an automation of the whole process is required. In this talk, we describe our approach, the imputation routines, and present some results. ","Under item nonresponse, imputed values are often generated from a parametric model. When the imputation model belongs to a parametric family of distributions, imputation requires estimated parameter values in order to impute for the missing data. We propose parametric fractional imputation, which is a parametric approach for generating imputed values. The proposed imputation method provides very efficient parameter estimates for the parameters specified in the model and, at the same time, also provides reasonable estimates for parameters that were not considered in the imputation model, such as domain means. Thus, the proposed imputation method is a very useful tool for general-purpose data analysis. Variance estimation is covered and results from a limitd simulation study are presented. ","As the overall speed of computer processing increases, the general perception of record linkage as an unwieldy and time-consuming process diminishes. Similarly, complex coding tasks which have traditionally been viewed as bordering on problematic are now seeing their implementation possibilities evolve towards the plausible. Each of these problem domains employ similar approaches to identifying the \"best\" candidate for either linkage or code assignment. This paper describes Statistics Canada's recent experiences in redeveloping their own generalized record linkage and computerized coding software, and highlights the convergence and associated overlap in techniques which has occurred across these two statistical processing domains. The overall system architecture employed, the techniques and algorithms implemented, and examples of the user interface developed are also presented. ","The model family proposed by Gifi (1990) is a flexible framework for the analysis of multivariate data. The common properties shared by all Gifi-models are the specification of a loss function solved by ALS and transformations of the variables which lead to quantifications of the categories. The latter issue implies the concept of \"optimal scaling\" and allows to account for the variables scaling level. Starting from the basic model called homals (homogeneity analysis aka multiple CA) we present various extensions in terms of rank restrictions (nonlinear PCA) and restrictions on sets of variables (nonlinear canonical correlation analysis). We focus on recent methodological developments and on the R package \"homals\" (de Leeuw &amp; Mair, 2008) which allows for the computation of these models and provides new visualization techniques of the results.  ","This paper introduces and demonstrates the obsSens package for R. It  implements the sensitivity analysis methods for observational studies  presented in the paper \"Assessing the Sensitivity of Regression Results  to Unmeasured Confounders in Observational Studies\" by Lin, Psaty, and  Kronmal. The package will be demonstrated using data from a study on  platelet transfusions in premature babies. The study shows that a practice meant to help the newborns is unlikely to be beneficial and possibly harmful. ","The objective of this paper is to present an implementation of a prototype for   SAFAL: Statistical Analysis Functions Automating Language. SAFAL uses a programming paradigm that is unique. Each statement of SAFAL is not an instruction to a computer, but is a definition of a symbol, using algebraic functional syntax. The statements in a SAFAL program are mathematical definitions (or input output specifications) of the relevant entities. Hence, the order in which they are specified in irrelevant. The design of SAFAL is aimed at statistical analysis in general, but the initial implementation as functions that more relevant to analysis of complex survey data. The basic data structure for SAFAL is a generalized multidimensional array (GMDA) with normal as well as virtual dimensions. The paper describes GMDA its underlying mathematics.   ","Following the publication of the first Autoregressive Conditional Duration (ACD) model by Engle and Russel (1998), numerous new specifications have been published in the literature. At the same time, tests statistics have been developed for the evaluation of ACD models. This paper provides an open source library of statistical procedures for the estimation of the standard linear ACD models, as well as the augmented models family presented by Fernandes and Grammig (2006). Source code of some statistical tests for the evaluation of the models, including distributional misspecification tests (Fernandes &amp; Grammig 2005) and LM tests for misspecified mean equations (Meitz &amp; Ter\u00e4svirta 2006), is provided as well. Several empirical applications are also presented. ","Tobit II models (aka Heckman Selection models) are a standard statistical tool for detecting and correcting selection bias. ML estimation is complicated by the possibility of multiple roots to the score equations. Most software packages ignore this problem and may fail to converge to the global MLE even when consistent starting values are used. Convergence to the global MLE can be insured by use of a two-step algorithm which conducts a grid search over the bounded space of the error correlation, and then uses the conditional ML estimates as starting values for simultaneous estimation. The nature of the problem is illustrated using Monte Carlo simulation. Major software packages are then compared and found to suffer from the same algorithmic errors. Finally, replication of estimates for a sample of published data sets finds that roughly half of the studies report inaccurate estimates. ","Internal pilot designs involve conducting interim power analysis (without interim data analysis) to modify the final sample size. Recently developed techniques have been described to avoid the type I error rate inflation inherent to unadjusted hypothesis tests, while still providing the advantages of an internal pilot design. We present GLUMIP 2.0, the latest version of our free SAS/IML software for planning internal pilot studies in the general linear univariate model (GLUM) framework. The new analytic forms incorporated into the updated software solve many problems inherent to current internal pilot techniques for linear models with Gaussian errors. Hence, the GLUMIP 2.0 software makes it easy to perform exact power analysis for internal pilots under the GLUM framework with independent Gaussian errors and fixed predictors. ","The Statistics of Income Division of the Internal Revenue Service has published data derived from Individual Income Tax Returns filed by taxpayers since 1916.  As with most projects of any kind, the focus of the effort is on getting the current work completed.  Documentation is often an activity that gets secondary attention.  And documentation written with history in mind, where the documentation serves not only to remind the members of the team of what they did, but also serves to explain to future teams what was done and why, gets even less attention.  This paper is the beginning of an attempt at creating historical documentation for the data created by SOI based on Individual Income Tax Returns. ","Statistics of Income of IRS developed a stratified sample of individual returns to study the Form 1040 Sales of Capital  Assets panel (SOCA) in tax year 1999. It was a cross-sectional sample and drawn from the population of all individual returns of tax  year 1999. From this 1999 cross-sectional SOCA sample, a small representative sample was selected to serve as the base-year of the  panel sample. Due to various resource and planning constraints, no refreshment sample has been added to this panel sample since that  tax year. Subsequently, the SOCA panel sample has drifted and is no longer representative. Therefore, a new cross-sectional SOCA  sample will be selected for the tax year 2007 and a new panel sample will be developed from it. To efficiently allocate the sample size  across strata, the standard deviation and cost estimates from the tax year 2005 sample are used. ","Policy research is increasingly being done on panel data; attrition can undermine validity and misrepresent results of many policy analyses. Using the Individual Income Tax Return Panel for Tax Years 1999-2005, this paper will examine panel attrition. It assesses the observed rate to determine the predictability over time. Finally, it will present several tabular representation problems hindering analysis. ","The Statistics of Income (SOI) Division generally compiles statistics based on stratified probability samples, using such classes as size of income, presence or absence of a specific form or schedule, and business activity. In addition, it evaluates these estimates by comparing them with information it collects from extracts of the population of filers. This paper addresses the methodological limitations associated with producing state and county-level tabulations of unincorporated business activity from the information on a Form 1065 or a Form 1040, Schedule C. It presents trends and comparisons among states, counties and business activities across five tax years---2001 to 2005. Discussion includes the methodology for assigning entities to states and counties, as well as comparisons at the national level between this data and SOI estimates drawn from samples. ","Longitudinal studies are subject to patient dropout, which may lead to  biased estimates of the rate of change if the dropout process is informative. Multiple patterns of informative dropout are plausible, in that different causes of dropout might contribute to different patterns. These multiple patterns can be dichotomized into two groups: those exhibiting quantitative interaction and qualitative interaction. We explore a test for qualitative interaction based on simultaneous confidence intervals. The test accommodates the realistic situation where reasons for dropout are not fully understood, or perhaps are even entirely unknown. It also allows for an additional level of clustering among participating subjects, as might be found in a family study. We apply these methods to a longitudinal study exploring rates of change in cognitive functioning for a group of Alzheimer's patients. ","Compartmental models are common in population modeling in the biological sciences but are also used in telecommunications, chemical kinetics and finance. In continuous time, standard compartmental models are based on 'Poissonian' processes which inherit the equidispersion property that the infinitesimal variance equals the infinitesimal mean. Recently developed techniques provide general methods for fitting such models to time series data and this constrain on the mean to variance ratio has been known to affect the goodness of the fit, since overdispersion is usually observed. We present a strategy for constructing a new class of overdispersed continuous time Markov counting processes with a nice interpretation in terms of adding noise to the rate parameters of standard compartmental models. As a motivating example, we consider a multistrain model for cholera incidence in Bangladesh. ","Two most commonly used techniques to analyze longitudinal and cross-sectional complex survey data are: (i) standard regression techniques based on the quasi-likelihood for parameter estimation and the bootstrapping approach for variance estimation and (ii) multilevel technique based on pseudo maximum likelihood. Our study objectives are to explore the standard regression parameter estimation based on quasi-likelihood techniques, bootstrapping approach for variance estimation by using the longitudinal data from Canadian National Population Health Survey (NPHS) and weighted and un-weighted parameter estimation by using the cross-sectional Canadian Heart Health Datasets (CHHD), collected in1986--92, after controlling PSU level. The longitudinal NPHS began in 1994/95 with a sample size of 17,276 and data being collected every two years. ","A common problem in longitudinal studies is missing data. Likelihood-based mixed effects models give valid estimates when the data are missing at random, an untestable assumption without further information. In some studies, additional auxiliary information known to be correlated with the outcome might be available when the outcome of interest is missing; such additional data allows the MAR assumption to be tested and can be used to reduce or eliminate bias when the missing data process depends on the unobserved outcome only through the auxiliary information and the observed outcome. We apply two methods of utilizing the auxiliary information: joint modeling of the outcome of interest and the auxiliary variable, and multiple imputation. Simulations studies and analysis using data from a dementia screening study are reported. Cautions in applying these methods are also discussed. ","Since many epidemiological studies involve the study of individuals of different ages over time, it often becomes necessary to distinguish between and estimate both longitudinal and cross-sectional differences. This paper examines how the choice of age and time in modeling longitudinal data can affect the results. In particular, age can be decomposed into two components: age at entry into the study (first age) and follow-up time. The implication of using age or first age and time is described for a number of possible linear mixed-effects models that may be used to describe the longitudinal data. The two approaches are illustrated using a number of different examples of data taken from the Baltimore Longitudinal Study of Aging (BLSA). The examples illustrate that the added flexibility provided by the first age and time approach is usually necessary to adequately describe the data. ","GEE is often used to assess the effect of covariates on an outcome of interest in longitudinal/clustered data analyses. Parameter estimates obtained from GEE are consistent irrespective of the working correlation matrix. If the correlation matrix is mis-specified, estimates may become inefficient. Inferences from GEE may also be sensitive to outliers in observed data. In addition, GEE lacks an absolute measure of goodness-of-fit (GOF), which makes model selection a difficult task. QIF-an extension of the GEE-provides parameter estimates that are both consistent and efficient irrespective of the working correlation matrix. QIF also provides a GOF test with similar properties to the likelihood ratio test of generalized linear models. We compare results obtained from GEE and QIF, using dataset from NLSCY. The repeated response is a binary measure of hyperactivity-inattention in children. ","This paper summarizes research that was recently conducted for redesigning the NSCAW, a large, national panel survey. In 1998, a two-stage sample was selected consisting of approximately 5400 individuals in 92 PSUs. The sample continued unchanged for 8 years until it ended in 2007. In 2008, a new sample of 5700 individuals will be drawn to restart the survey. A number of issues were addressed for the new sample. Three options were considered for selecting new PSUs: (a) carry forward all 92 previously selected PSUs; (b) replace all PSUs with an independently selected sample; and (c) select a high probability of overlap PSU sample. Other issues investigated include: optimal sample size, optimal sample allocation, frame changes, and domain over-sampling while minimizing the design effects. This paper considers these issues and others and provides our recommendations for the redesign. ","The ACS collects data historically collected by the Decennial Census Long Form. The goal of the ACS is to publish data for small geographic areas cumulating sample over five-year periods with comparable quality to the Census 2000 Long Form estimates. Unlike the Long Form which had an overall sampling rate of approximately 1-in-6 and 100 percent follow-up of nonrespondents, the ACS selects a fixed annual sample of approximately 3,000,000 addresses and samples non-respondents with an overall rate of approximately 1-in-3. These two factors combined with growth and a decline in cooperation rates have led to concerns about the ACS meeting it's stated goal for small areas. This paper looks at the changes in the ACS sample size over time, the distribution of the ACS sampling frame relative to Census 2000 Long Form, and the implications of the fixed sample size on reliability. ","The ACS collects data in three phases: mailout/mailback, telephone interview, and Computer Assisted Personal Interview (CAPI). During the CAPI phase, a sample of mail/telephone non-respondents and addresses deemed to be unmailable is selected for a personal visit. The sampling rate for this process is determined by the CAPI sampling stratum for the tract in which the address resides, which is assigned based upon the mail/telephone cooperation rate of the tract. The initial assignment was made prior to 2005. Based upon recent ACS data, cooperation rates were re-calculated for all tracts, which were then assigned to a new CAPI sampling stratum. This paper discusses the data that led us to revisit the CAPI sampling stratum assignments, and it provides the methodology used to calculate the tract level mail/telephone cooperation rates used to assign each area to a CAPI sampling stratum. ","A redesign of the National Hospital Discharge Survey (NHDS) is planned for 2010. The new design will use a two stage sample of hospital discharges in which a stratified list sample of hospitals will be selected at the first stage. In this paper sample allocations to strata for samples of different sizes are optimized using either a Neyman or a nonlinear programming method.  The different sample allocations are evaluated by estimating and comparing expected relative standard errors (RSEs, aka coefficient of variation or CVs) from the allocations for a set of discharge level and hospital level variables.  This research uses sample discharge data available from the 2005 NHDS and hospital data available from a 2006 list of hospitals eligible for the 2010 NHDS universe. ","The National Health Interview Survey (NHIS) is one of the major data collection programs of the National Center for Health Statistics. The sample design for the NHIS traditionally has undergone a redesign about every 10 years to address new and continuing data needs at the national and subnational levels, and for minority and economic subdomains of the population. The ability to produce reliable annual estimates for older persons of age 65+ years by race and ethnicity is a major design objective. In 2002, research was conducted for the 2006--2014 NHIS redesign to assess options for oversampling older minority persons. Beginning with the 2006 NHIS, minority persons of age 65+ years have an increased probability of selection as sample adults. This paper describes research that has been conducted using the 2006 NHIS to assess the selection and the sample yield for older minority persons. ","The paper \"Update of the Redesign of the National Compensation Survey\" (Izsak et al. 2005) included a proposal to allocate the number of sample establishments among cells using a controlled selection procedure, where cells are area PSUs x industry sampling strata x sampling panels. Since then the procedure has been implemented but with a number of modifications not discussed in Izsak et al. (2005). These modifications and possible future changes are discussed in this new paper. They include: weighting changes necessitated by the use of controlled selection, complications caused by rounding issues and how they were overcome, complexities caused by the need to allocate over five sampling panels, and use of a real-valued minimum allocation for each sampling cell in the controlled selection process in order to avoid very large sample weights and accompanying increases in variances. ","The reinterview method is the usual technique for estimating the response variance and the response bias. It is assumed that the respondent does not remember or is not influenced by the original response. The gross discrepancy rate (GDR) is the ratio of the sum of the two types of discrepancies to the total number of respondents in a yes/no type of question. The response variance is half of the GDR. Response bias is also estimated from the ratio of the net difference between the two discrepancies and the total. However this assumption that the respondent neither remembers nor influenced by the original response is not realistic. A measure of the memory effect is derived in this paper. The GDR, the sample response variance and the response bias are rectified based on this measure of the memory effect. ","It is important to evaluate data quality in any survey, but especially after changes in protocols and measurement systems. When revisions are made in a longitudinal survey, it is important to estimate the error rates for the data collection process and possibly for specific domains associated with the process. We examine this problem from the context of the National Resources Inventory, a longitudinal survey of natural resources. Our goal was to evaluate the quality of the data collection process after protocol and organizational changes were implemented for the 2005 survey. Estimates of error rates were of interest, so we used a probability sample to select segments for review. The quality review sample design allowed for oversampling of units more prone to errors. We evaluate the design strata and illustrate the value in this approach to assess problems in the data collection process. ","Simulation modeling has become the most commonly used tool for performance evaluation of stochastic dynamic systems in science and engineering. The field operations of surveys can be one of these systems. At Census Bureau, the Simulation Modeling of Field Operations in Surveys (SIMFOPS) was developed to estimate the cost, response rate, and timing of new or continuing surveys. In this paper, we describe the simulation of simplified field operations for NHIS (National Health Interview Survey). We apply the simulation methodology to the field operations. We use the 2004 NHIS CHI (Contact History Instrument) data for the study. From this study, we have shown that SIMFOPS can be used for optimizing the field operations by setting the controllable parameters before a decision is made and implemented. The cost savings might be enormous and would not be at the expense of the response rate. ","With the increasing complexity of survey design, survey managers need better tools to assess the data collection survey process. The development of data collection computer-assisted methods has opened the door to a wide scope of process data (\"paradata\") that can be used to evaluate the data collection survey process status in a timely manner. Active Management at Statistics Canada is a set of plans and tools developed to manage survey data collection while it is in progress. This includes monitoring progress, timely analysis of these indicators, identifying problems, implementing and communicating corrective actions, and evaluating success. The paper begins with an introduction to the key components of the Active Management and discusses a series of practical example currently used to monitor issues and data quality indicators at Statistics Canada. ","Previous research by Tucker et al. (2005) and Tucker et al. (2006) attempts to identify a latent construct that predicts the amount of measurement error in expenditure reports on the Consumer Expenditure Survey (CEIS). While this work was successful in identifying a construct that predicts measurement error in expenditure reports, it is more sensitive to falsely negative reports of the entire purchases than it is to the underreporting of the amount of expenditure for those purchases. Current research focuses more deeply on the underreporting of expenditure amounts for a number of different commodities. Together, with previously explored indicators such as, the number of contacts, missing on the income question, the length of the interview, and the use of records, we examine a new indicator measuring the number of missing sections of the interview, resulting in a new latent construct. ","Errors in data haunt practitioners. We have to prioritize observations that are most cost-effective to recontact and correct. One approach is to predict the true value of one observation using available information and take the norm of the difference between the predicted and reported value. This is a number referred to as an item score. Usually we want to edit and verify all observations on the same unit rather than each item separately. We discuss ways of forming a unit score out of a generic set of p item scores. A unit score function that unifies item scores often used in statistical editing is presented. Based on the unit score several subsample designs are available (e.g., pps or cutoff sampling above a threshold). We discuss ways of selecting this threshold value. The problem of prioritizing manual statistical editing of business survey data is our motivating example. ","Statistically valid analyses of longitudinal trials can be problematic in the presence of missing data. In a recent randomized, double-blind, placebo-controlled, 8-week clinical trial to assess the efficacy and safety of PRX-00023 in subjects with major depressive disorder, about one-quarter of the subjects withdrew prematurely. In anticipation of such a dropout, a likelihood-based mixed-effects model repeated measures analysis had been prospectively chosen as the primary analysis.  In this paper, the appropriateness of the design and the related statistical analysis in the presence of missing values is highlighted with results of the study itself. Emphasis is placed on how the data-based robust efficacy results strengthened the sponsor's position to take a bold decision. In addition, we propose a three-pronged solution to the problem of missing data in the highly-regulated environment. ","A longitudinal data set is defined as a data set in which the response for each experimental unit is observed on two or more occasions. There are different traditional methods to analyze the longitudinal data including: repeated measures ANOVA and mixed effects models. Random Coefficient Models (RCM) are anther way to analyze the longitudinal data by using the time as continues variable and estimating the slop, intercept, and the regression line of each subject. Clinical trials data with high dropout rate were used to compare the results of RCM versus the traditional methods and to evaluate the missing data effect. Using the time in the model as fixed effect versus random effect or as class versus continuous variable were examined. The final results show that the RCM (using the time as random continues variable) have greater power and better model fitting. ","In this paper we consider a new analytical framework that is a combination of the individual and combined data analyses, based on an estimating equation approach. The proposed analyses utilize a stochastic model for the two drug combinations and derive the mean and the variance terms based on Ito's calculus. The proposed estimation methods are used to estimate model parameters from individual and combined data provides the ground for the model free tests. The strength of the fit of the model to the data is examined by the statistical measures and the graphical method. Simulation studies were performed to show the strengths of the proposed approach in the estimating the model parameters. A synergy test of the model fitted by the individual subjects confirmed that the combination of the drugs under study is synergic in nature.  ","The pretest/post-test setting is commonplace in clinical trials. One typical trial objective is to detect difference in the response between two treatment groups. The response variable is measured at or before randomization (pretest) and at prespecified follow-up times (posttest). In search of the most efficient way to analyze this type of data, we compared various common analysis methods typical in Phase I HIV vaccine trials and concluded that the ANCOVA method is generally best with valid assumptions. However, due to small sample sizes and outlying observations, the normality assumption is often suspected. As such, we further looked into semiparametric (GEE), robust parametrics (RAVE) and nonparametric (rank based) methods. Simulations were used to compare the different methods under various conditions, and practical recommendations for statisticians will be highlighted. ","In longitudinal clinical studies, after randomization at baseline, subjects are followed for a period of time for development of symptoms. A mixed model for repeated measures (MMRM) can be used to analyze such data. To accommodate safety and tolerability, in some studies, the treatment has to be titrated to the optimal dose. Then, subjects will stay on their optimal dose until the end of the study. In an MMRM analysis, one can ignore the titration visits because including them could add extra variability unnecessarily. However, when patients drop out during the titration period, ignoring the titration visits would result in missing data in these patients. In this paper, we evaluate the impact of excluding and including titration visits in an MMRM analysis by a simulation study. We evaluate the approaches based on the bias and the coverage accuracy of the confidence interval. ","In this talk we address statistical issues arisen from post marketing evaluations of pharmaceutical products, which focus particularly on disease treatment that gives rise to phase-dependent effects and volatile longitudinal responses. We employ geometric Brownian motion process in this talk to accommodate the two aforementioned longitudinal characteristics. Our model formulation also adapt to firstly finding what individual conditions determine the individual trajectory of disease progression and volatility, and then secondly how to summarize individual information on population level. A two-step modeling approach is proposed, is demonstrated with a data set from a rheumatoid arthritis study in which patients were randomized to three treatment groups and the index measurements disease activity score based on 28 joints (DAS28) were recorded at multiple time points. ","How to handle cell phone numbers has become a major concern in telephone surveying in the US. This roundatble will discuss key issues in coverage, sampling, nonresponse, measurement, and weighting that affect U.S. telephone surveys that reach cell phone numbers, especially surveys meant to sample the general public with an RDD frame. We also will discuss unique legal and ethical issues that come into play when reaching U.S. cell phone numbers, compared to what happens when dialing U.S. landline numbers. In addition, operational considerations (including cost implications) will be included in the discussion. The information presented at the roundtable will include updates from new research and thinking since the publication of the December 2007 special issue of Public Opinion Quarterly on \"Cell Phone Numbers and Telephone Surveying in the US.\" ","This paper uses administrative data to evaluate how test-score ceiling effects influence value-added estimation.  There is no evidence of a test-score ceiling in the raw data, which is particularly appealing for this project. Starting with the no-ceiling baseline, we consider the effects of numerous artificially imposed test-score ceilings on school rankings based on value-added.  Over a wide range of test-score ceiling severity, school-level value-added estimates are only negligibly influenced by ceiling effects.  However, schools' value-added rankings are significantly altered by ceilings equivalent in severity to those found in minimum-competency testing environments.    ","One dimension untested in value-added (VA) research is the sensitivity of teacher effect estimates to rules for defining student-teacher links. Student-teacher links are complicated by an array of factors, including students transferring among schools mid-year; transferring between classes mid-year; taking multiple same-subject courses; or being habitually absent. Because performance measures may have high stakes in merit pay plans rule changes that result in small adjustments in student-teacher links may have a significant impact for some teachers. Our study fills this gap by: reviewing how VA studies, to date, have defined student-teacher links; creating a taxonomy that captures rules used when defining student-teacher links; and conducting a simulation study that examines the sensitivity of teacher VA estimates to decision rules made when defining student-teacher links. ","Incomplete data are a common concern for analyses that estimate teacher contributions to learning from longitudinal student achievement data. Students with missing scores are at greater risk for low performance and data might not be missing at random (MAR). We develop models for estimating teacher effects that allow for missing not at random data. We consider a selection model where the number of observed test scores depends on a student's latent general level of achievement and a pattern mixture model that allows the means and covariances of test scores to depend on the student's pattern of observed scores. We fit these models to five years of longitudinal test score data from a large urban school district. These models yield very similar teacher effects as models that assume data are MAR. We discuss likely explanations for the robustness of estimates to assumptions about missing data. ","There is increasing interest in using longitudinal measures of student achievement to estimate individual teacher effects. Current multivariate models assume scores come from a single vertical scale and that each teacher has a single effect on student outcomes that persists undiminished to all future test administrations (complete persistence) or can diminish with time but remains perfectly correlated (variable persistence). However, vertically linked tests might not be unidimensional, and not all state assessments use a vertical scale. We develop the \"generalized persistence\" model, a Bayesian multivariate model for estimating teacher effects that accommodates longitudinal data that are not vertically scaled by allowing a teacher's effects on her student's current and future outcomes to have less than perfect correlation. We illustrate the model using mathematics assessment data. ","A key difficulty in drawing inference of school effect from student test score gains is the fact that test scores are noisy measurements of students' academic achievements. In this paper we examine competing inferential methods of dynamic panel data models using noisy data. In particular, we consider a score-level model where the score of the current period depends on the score of the previous period. We compare Monte Carlo simulation results of estimators of these model and estimates using the student test score data of Missouri. ","Studying the relationship between two multivariate random vectors is an important problem in statistics. Canonical correlation coefficients are used to study these relationships. Canonical correlation analysis (CCA) is a general multivariate method that is used to study the relationship when both sets of variables are quantitative. In this talk, we provide a generalization of this method to determine the relationships between two sets of repeatedly or longitudinally observed vectors. Assuming a block Kronecker product variance covariance matrix to account for the dependency of the vectors observed over t time periods, we provide methods to obtain canonical correlations and canonical variables. ","Inverse regression, or statistical calibration, uses the estimated relationship between a response Y and a covariate x to infer the values of unknown x's from their observed Y's. Typically x is univariate but Y may be multivariate. A brief review of the basic theory will be given, followed by consideration of the problems involved in extending these approaches to longitudinal data, ie where the training data consists of groups of observations on distinct individuals. A Bayesian analysis using MCMC is shown to give a flexible framework for solving these problems. An example concerning the age determination of tern chicks from their wingspan and weight measurements will be used for illustration. ","Recently, we proposed an autoregressive linear mixed effects model for the analysis of longitudinal data in which the current response is regressed on the previous response, fixed effects, and random effects (Funatogawa et al. 2007 Stat Med). The model represents profiles approaching random equilibriums. Because intermittent missing is an inherent problem of the autoregressive (conditional) mode, we provided the marginal (unconditional) representation of the model and the likelihood. In this study, we further provide a state space form of our model for calculating the marginal likelihood without using large matrices. We modified the method proposed by Jones (1993) for a state space form of a usual linear mixed effects model. We analyzed parathyroid hormone and serum calcium measurements in treatment of secondary hyperparathyroidism. ","Amyotrophic Lateral Sclerosis (ALS) is a fatal neurodegenerative disease characterized by progressive muscle weakness, for which only relatively insensitive markers exist. ALS varies highly among people, with non-linear initial progression followed by linear decline. Electrical impedance myography was used to measure muscle impedance longitudinally in 18 ALS patients at different disease stages, 27 normal subjects and 20 patients with disuse myopathy. The data were fitted with a mixed effects model to account for disease duration, time between measurements and covariation of measurements from muscles in the same anatomical group. Sensitivity and specificity of multi-frequency resistance, reactance and impedance phase were assessed for progression. Electrical resistance differentially varied in ALS patients; reactance and phase spectra varied non-specifically in ALS and disuse myopathy. ","The mixed-effects models with two variance components are often used to analyze longitudinal data. For this kind of model, we compare the two approaches to estimating the variance components, the analysis of variance approach and the spectral decomposition approach. We establish a necessary and sufficient condition for the two approaches to yield identical estimates, and some sufficient conditions for the superiority of one approach over the other, under the mean squared error criterion. Applications of the methods to circular models and panel data are discussed. Furthermore, simulation results indicate that better estimates of variance components do not necessarily imply higher power of the tests or shorter confidence intervals. ","Heagerty (2002) and Lee and Daniels (2007) have proposed marginalized transition models for the analysis of longitudinal binary data and ordinal data, respectively. In this paper, we propose similar models for longitudinal count data. We also propose a model to accommodate overdispersed count data. Fisher-scoring algorithms are developed for estimation. Methods are illustrated with a real dataset and are compared with other standard methods. ","In a typical application, ranked set sampling (RSS) is a cost-effective approach if ranking sampling units is easier and cheaper than measuring them. In this case, RSS is useful compared to classical random sampling. In genetic studies, genotypes and phenotypes (traits) of individuals have to be measured and obtained in genetic association and linkage studies. In many applications, genotyping cost is much more expensive than measuring phenotypes. Therefore, RSS can be applied. In this talk, we present two examples, where RSS can be easily applied, compared to using random sampling and the truncation approach. The first example is to map linkage disequilibrium and the second example is to test linkage using affected sib-pairs. In both examples, the idea of extreme rank selections will be presented. Simulation results will also be presented to compare different approaches. ","We generalize the idea of ranked set sampling by allowing it to incorporate dependent order statistics of various sizes. The data so obtained is used to estimate the distribution function and compare two distributions. Asymptotic results are established and are compared with simulation runs. An application using weather data is also provided. ","Many inferential procedures based on a ranked set sample data do not allow the possibility of ranking error. On the other hand, in a typical application, ranking error may not be eliminated and statistical inference may be impacted by the severity of this error. In this talk, we introduce a nonparametric maximum likelihood estimator for Bohn and Wolfe (1994) judgment ranking model. It is shown that the MLE of the cumulative distribution function (cdf) of the underlying population exist for any legitimate Bohn-Wolfe model. We also show that the MLE of the judgment ranking probabilities exist for any legitimate CDF estimator. We estimate the Bohn-Wolfe model by iterating  between these two estimators until we have a convergence. The properties of the estimators are investigated in a simulation study. The estimated Bohn-Wolfe model is used to calibrate the impact of imperfect ranking. ","The maximum likelihood estimator (MLE) and several different pivots will be used to construct different confidence intervals for the logistic location parameter. These confidence intervals will be constructed by using simple random sampling (SRS) and ranked set sampling (RSS). Difference confidence intervals will be compared via their expected lengths and the standard deviation of their lengths using simulation. We expect that the confidence intervals base on RSS to have shorter expected lengths and smaller standard deviation from there competitors using the SRS. ","One is often interested in estimating the population proportion of a rare event, where conventional sampling techniques (such as SRS and balanced RSS) are wasteful and inefficient. Moreover, one is also often interested in the units of a population that exceed a certain elevated threshold; for instance, the proportion of fraudulent accounts in a population of accounting records, and the distributional properties of the largest amounts of dollar fraud among the fraudulent accounts. We will explore how sequentially-guided RSS techniques can lead us to target sparsely distributed occurrences of the rare event more accurately. ","The NHANES has been conducted since the 1960s and is unique in that physical examination data are obtained. The 1999--2006 NHANES is designed so that estimates of health and nutritional status, risk factors and health conditions can be estimated in age-gender specific domains of the non-Hispanic (NH) black, NH white, Mexican American, and low income populations. In addition to the examination, other unique information collected in NHANES includes dietary intake data, data from household specimens (water and dust), personal exposure monitors and activity monitors, and from tuberculin skin testing. Examples of estimates such as chronic disease prevalence, seroprevalence, exposure levels to environmental chemicals and dietary intake obtained using NHANES data that highlight disparities will be presented Limitations of the NHANES data in making such estimates will be discussed. ","The National Health Care Surveys are a family of nationally-representative provider-based surveys that collect data about health care providers, their patients, and their care. The surveys cover health care providers across a broad spectrum of ambulatory, hospital, and long-term care settings. The resulting data are used in many ways, including to assess disparities in quality of and access to care, and differences in health status among different U.S. populations. We present an overview of the surveys and the data that each collects. Examples will be provided of analyses that characterize disparities according to race and ethnicity. Initiatives to better assess racial and ethnic disparities are also described. ","This presentation will provide background on the strengths and weaknesses in the analysis of vital statistics and of data from the National Survey of Family Growth for measuring health outcomes using health disparities as an example. Measures such as infant mortality, life expectancy, low birth weight and prematurity, emerging and rare causes of death, and high risk sexual behaviors will be mentioned. Additional discussion will cover new activities that will have a positive impact on data quality and timeliness. ","Examples from published analyses show the usefulness of data from the National Health Interview Survey (NHIS) and the State and Local Area Integrated Telephone Survey (SLAITS) for studying health disparities: Married adults were generally healthier than adults in other marital status categories (NHIS results); the prevalence of diabetes was higher among non-Hispanic black persons and Hispanic persons than among non-Hispanic white persons (NHIS); Hispanic persons were more likely to be without health insurance coverage than non-Hispanic black persons and non-Hispanic white persons (NHIS); and children in step, single-mother, or grandparent-only families had poorer health than children living with two biological parents (SLAITS).  Also, the major role played by the NHIS in providing data to track disparity-reducing objectives of the Healthy People Program is described. ","Contextualized microdata is one way to safely release geographic data without identifying the location of survey respondents. This study informs the design of such datafiles with its needle-in-haystack approach to disclosure and its discussion of associated methodological concerns. Drawing a sample of counties, tracts, and blockgroups, I illustrate how the reidentification of individuals is shaped by aggregating geographies into look-alike sets. I detail the complexity of reidentification patterns by assessing the likelihood that young adult white and black males would be pinpointed within reconstituted haystacks given: (1) the size of the total population of aggregated contexts; (2) the amount of error in population counts; and (3) differential search costs stemming from spatially dispersed contexts. ","Disclosure limitation is an important consideration in the release of public use data sets. It is particularly challenging for longitudinal data sets, since information about an individual accumulates over time. We consider problems created by high ages in cohort studies. Because of the risk of disclosure, ages of very old respondents can often not be released, as stipulated by the Health Insurance Portability and Accountability Act (HIPAA). Top-coding of individuals beyond a certain age is a standard way of dealing with this issue, but it has severe limitations in longitudinal studies. We propose and evaluate an alternative to top-coding for this situation based on multiple imputation (MI). This MI method is applied to a survival analysis of simulated data and data from the Charleston Heart Study, and is shown to work well in preserving the relationship between hazard and covariates. ","Conventional wisdom suggests that the availability of commercial databases with identifying information and key demographic variable coupled with powerful record linkage techniques would increase the risk of disclosure of survey respondent. The objective of this paper is to address these concerns by evaluating such risk of disclosure using national surveys and also assess the accuracy of information in the commercial database. The results from these experiments suggest that the disclosure threat from linking entities in the commercial list with those in public data may be exaggerated. Any assessment of risk of disclosure and procedures used to treat the data should take into account the data accuracy used by potential intruder. ","Government agencies must simultaneously maintain confidentiality of   individual records and disseminate useful microdata.  We propose a  method to create synthetic data that combines quantile regression, hot deck imputation, and rank swapping.  The result from implementation of the proposed procedure is a releasable data set containing original values for a few key variables, synthetic quantile regression predictions for several variables, and imputed and perturbed values for remaining variables. The procedure should provide quality data to the user and simulataneously protect the confidentiality of respondents.  To measure the disclosure risk in the resulting synthetic data set, we extend existing probabilistic risk measures that aim to imitate an intruder attempting to match a record in the released data with information previously available on a target respondent.   ","In this paper, we follow the lead of Don Rubin, Jerry Reiter, Arthur Kennichell, and Julia Lane, who have recommended a synthetic data set be created---one that produces the same key analytic results as the original data set. By this, we mean in a way to be defined in detail that the original and synthetic data sets are sufficiently close statistically for the synthetic data set to be an \"analytically sufficient\" substitute. We do not offer a general solution to this proposition. Instead, we discuss the degree of \"success\" of one such effort, involving a linkage of Survey of Income and Participation (SIPP) data with benefit and earnings data from the Social Security Administration and the Internal Revenue Service, respectively. We look at the analytic strengths and weaknesses of the resulting synthetic data set. ","Many randomized response (RR) procedures for surveying sensitive binary variables are available in the literature. We shall present a common framework for all RR surveys of binary variables. The unified approach is focused on the substantive issues relating to respondents' privacy and statistical efficiency and is helpful for fair comparison of different procedures. We shall describe an approach for comparing RR procedures, taking both respondents' protection and statistical efficiency into account. For any given RR procedure with three or more response categories, we shall show that one can design an RR procedure with a binary response variable that is as protective and at least as informative. This result suggests that one should use only binary response variables when designing an RR survey of a dichotomous population. Finally, we shall characterize all admissible RR designs. ","The development of randomized response models for personal interview surveys has attracted much attention since the pioneering work of Warner (1965). Several randomized response models have been developed by researchers for collecting data on both qualitative and the quantitative variables, but none of these models discuss matched pair data. In this paper, we develop a new randomized response model and study its application to an important political question. ","The American Time Use Survey (ATUS) provides estimates of how people divide their time each day in the United States. Households that have completed their eighth and final monthly interview in the Current Population Survey (CPS) are eligible for selection in the ATUS. Since the initial data collection in 2003, the ATUS response rate has been below 60%. The low response rate has caused concern over the potential for bias in the ATUS estimates. In 2005, the Bureau of Labor Statistics conducted research on nonresponse in the 2004 ATUS. We follow with an analysis of nonresponse on the 2006 ATUS. We link the ATUS weighted output files to the call history files for nonresponse unit information, and link to the CPS files for available frame variables. We use SAS PROC SURVEYLOGISTIC to model response propensities in the ATUS for individuals or households with different characteristics. ","Concerns about declining response and landline telephone coverage rates in Random Digit Dial (RDD) surveys led to the development and administration of an independent Bias Study that was conducted in conjunction with the 2007 National Household Education Surveys Program (NHES:2007). For the Bias Study, a clustered sample of addresses was selected, telephone numbers were matched to the addresses to the extent possible, and the matched sample used the regular RDD protocol. Addresses that could not be matched or completed by telephone were sent into the field for in-person attempts. In this paper, we describe the methods used to estimate nonresponse and noncoverage bias, present estimates of the two types of bias, and interpret the findings in the broader context of other nonresponse and noncoverage bias studies. ","The widely reported phenomenon of socio-economic status (SES) bias in response rates to surveys remains one of the major problems which researchers face when dealing with survey data. The purpose of this paper is to present the results of a study, conducted in Ireland, which provided an unprecedented opportunity to examine the characteristics of both the non-respondents and respondents to a survey and to compare characteristics of both with the target population. Anonymized records on 41,280 participants in the Irish Quarterly National Household Survey (a nationwide survey of households conducted by the Irish Central Statistics Office) provided data for this study into SES bias. Our findings appear to contradict the perceived position on SES bias in surveys. ","Survey nonresponse bias is difficult to assess since the difference between responders and nonresponders are rarely measurable. Sensitivity analysis is increasingly used to explore the impact of nonresponse. Date from the Current Population Survey are used to study different types of sensitivity analysis in reporting the potential for bias. ","We consider assessment of nonresponse bias for the mean of a survey variable Y subject to nonresponse. We assume that there are a set of covariates observed for nonrespondents and respondents. To reduce dimensionality and for simplicity we reduce the covariates to a proxy variable X that has the highest correlation with Y, estimated from a regression analysis of respondent data. We consider adjusted estimators of the mean of Y that are maximum likelihood for a pattern-mixture model with different means of Y and X for respondents and nonrespondents, assuming missingness is an arbitrary function of a known linear combination of X and Y. We propose a sensitivity analysis, sketch Bayesian versions of this approach, and propose a taxonomy for the evidence concerning bias based on the strength of the proxy and the deviation of the mean of X for respondents from the overall mean. ","This talk discusses model visualization methods that have been developed for a project analyzing records of political speech in legislatures. These records contain billions of words (more than Wikipedia) uttered over timescales from minutes to centuries. Our statistical models---dynamic Bayesian mixture models to study topic attention, dynamic feature selection models to study political content, and dynamic scaling and item response models to study political positioning---produce millions of meaningful parameters. Model visualization is central to both interpretation and communication of these results. ","Use of cartograms and other amorphologic representations of data has been important in studies of the World Wide Web since the last decade. Cartograms have also been around for a long time, but are more frequently seen in contemporary journalism, since the invention of the Newman-Gastner diffusion algorithm. The use of statistics in social science is often to describe patterns that exist on a real geography, but for the most part neither geographic (morphologic) nor representational (amorphologic) graphics have found their way into the standard toolkits of social scientists. I explore the use of amorphologic presentations of a variety of political/geographical data. ","This talk will present graphical techniques for interpreting and presenting regression and regression-like models. The focus will be on applications in political science; however the results are applicable to a wide audience. Additionally, the visual displays are easily implemented using an R graphics package called \"tile,\" written by the speaker. ","Imputation is a common pragmatic approach for handling missing data---missing values are replaced by estimates and analyses are conducted on the filled-in data set. The obvious drawback of imputation is that it is \"cheating,\" since it \"makes up\" data. As a result, inferences from the filled-in data overstate the amount of information, yielding confidence intervals that are too narrow, and tests that do not achieve nominal levels. Multiple imputation addresses this concern by creating multiple data sets with different sets of plausible values imputed. The basic idea and underlying theory of multiple imputation will be reviewed, including multiple imputation combining rules, imputation models, controversies about the method, available software, and pros and cons relative to other missing data approaches. ","Often we can think of our data as an imperfectly observed version of a hypothetical complete data set in which some or all observations are known only imprecisely. When the degree of imprecision is so great that standard continuous-data models are no longer reasonable approximations, we say that the data are coarse. Some prominent examples of coarse data include censored data (e.g., survival times in clinical trials), grouped data (as incomes), and heaped data (data with multiple degrees of coarseness, such as reported ages or daily cigarette consumption). I will present a general analysis strategy for coarse data, discuss ignorability conditions, and illustrate the methodology with one or two live examples. ","Small area estimation based on area level models typically assumes that sampling error variances for the direct survey small area estimates are known. In practice we use estimates of the sampling error variances, and these can contain substantial error. This suggests modeling the sampling variances to improve them and to quantify effects of their estimation error on small area inferences. We review papers that have attempted to address these issues. We then provide some results on the latter issue, showing, in a simple framework, how error in estimating sampling variances can affect the accuracy of small area predictions and lead to bias in stated mean squared errors. ","Each month, the Bureau of Labor Statistics publishes estimates of employment for industrial supersectors at the metropolitan statistical area (MSA) level. The survey-weighted ratio estimator that is used to produce estimates for large domains is generally less reliable for MSA level estimation due to the unavailability of adequate sample from a given MSA. We also note that the effect of a few establishments, which are influential in terms of unusual employment numbers or sampling weights or both, could be prominent for the small area estimation. In this paper, we develop an empirical hierarchical Bayes method based on a unit level model. Empirical evaluation using the population data from administrative file shows our proposed method to be less sensitive to influential establishments when compared to the direct survey-weighted ratio estimator or estimators based on an area level model. ","Empirical best linear unbiased prediction (EBLUP) estimators of small area means have been obtained under unit level nested error regression models. But EBLUP estimators can be highly influenced by the presence of outliers in the data. We propose a resistant method for small area estimation which is useful for downweighting any influential observations in the data when estimating small area means. A parametric bootstrap method is used to estimate the mean squared error (MSE). A simulation study is conducted to study the efficiency of the proposed robust estimators relative to EBLUP estimators and the relative bias of the bootstrap MSE estimators in the presence of outliers. The proposed robust method is also applied to some real data reported in the published literature. ","This paper develops expressions for the expected value and variance of the dual system estimate relaxing the autonomous independence assumption. The development assumes all members of the population are exposed to possible inclusion in both the coverage measurement survey and the census. Taylor Linearization is used as in Wolter's 1986 JASA paper; however, the captures of persons in the same housing unit are no longer assumed to be independent. A sensitivity analysis is done to analyze the effect on the bias and multinomial model variance of the dual system estimate for varying amounts of capture dependence. Data from the Census 2000 Accuracy and Coverage Evaluation Survey is used to estimate potential levels of capture dependence for persons in the same housing unit. ","The Census Bureau uses dual system estimation as one method to evaluate the coverage of the decennial census. This estimation for previous censuses has used post-stratification to minimize the impact of correlation bias on the population estimates. For 2010, we are planning on using logistic regression modeling instead of post-stratification cells. Logistic regression gives us the option of using variables in the model as continuous variables instead of having to form groupings. This research present our initial results of the impact of using certain continuous variables in the model development and the resulting population estimate. ","The 2010 Census Coverage Measurement Program (CCM) is preparing to use logistic regression modeling in the estimation of net census coverage error rather than post-stratification, the approach used for previous censuses. The most important objective for the CCM is to obtain separate estimates of erroneous census inclusions and census omissions. The plan for estimating census omissions is to sum estimates of net coverage error and erroneous enumerations. The net error estimates will be based on dual system estimation formed with separate logistic regression models for the correct enumeration rate and the match rate. Direct estimates at the block cluster level aid in variable selection by comparing the accuracy of estimates based on logistic regression models (or post-stratification designs) with and without a variable for groups of the clusters with different characteristics. ","Many small domain estimates require a precise, direct estimate of the within domain variability as one component. However, due to small sample size, the precision of within small domain direct variance estimates of census coverage is questionable. In this paper, Markov chain Monte Carlo (MCMC) techniques are applied to develop a model-based estimate of the within domain variability as part of the estimation process. For this particular application, variability within state is modeled via a random effects model where the census block is the replicate. Ultimately, this block-level model is applied to evaluate synthetic error of the small domain. ","The role of Demographic Analysis (DA) is to provide a coverage measurement estimate of decennial results independent from the (survey based) dual system estimator. In 2000, demographic analysis proved its worth. However, there are many improvements that should be considered. There are two major foci of interest: Constructing national DA measures and subnational demographic \"benchmarks,\" and evaluating components of DA measures. These research foci have proven important given the success of DA in 2000 and the need for continual improvement. This paper will discuss the potential of several new research topics in each focus area, and discuss their feasibility within the operational limits of the 2008 dress rehearsal and Census 2010. ","This study explicates practice analysis methods using ambulatory care nursing as an exemplar. Data derived from a focus group technique were used to develop a survey that was completed by 499 ambulatory care nurses. The construct validity of the survey instrument was assessed using confirmatory factor analysis with LISREL. The focus group developed a questionnaire that produced 34 knowledge statements delineating ambulatory care nursing practice. The previous study produced five factors as initial patient assessment, professional nursing issues and standards, client care management skills, technical/clinical skills, and system administrative operations. To assess the adequacy of the factors, we conducted confirmatory factor analyses on a five-factor model (Joreskog and Sorbom, 1992). Fit indices suggested that the five-factor model provided a satisfactory fit for the data. ","Modeling an ordinal categorical response variable is often performed using a cumulative logit link function. The proportional odds assumption (McCullagh, 1980) is key to the usage of this model. A likelihood-based version of this test is available in many statistical software packages. However, these available tests are not capable of taking into account a complex design that can include stratification, clustering, without replacement sampling and weighting or conducting the test when inference is based on replicate methods, such as Replicate Weight Jackknife and Balanced Repeated Replication. SUDAAN Release 10 will be capable of performing the test for proportional odds when any of the previously mentioned aspects are present in the data. This talk will discuss the theoretical implications of performing the proportional odds test and will demonstrate this test using survey data. ","Currently there are no federal level rules or mandates addressing or regulating the removal of plastic from the environment as a bi-product of global pesticide manufacturers releasing non-refillable plastic containers into U.S. agriculture. Degradation of the environment from non-refillable plastic pesticide containers could have a lasting affect on agricultural land and the economic sectors that rely on healthy land for the production of food and fiber for their income generation. Adding to the complexity of recycling plastic is also the need for controlling that the plastic that is recycled is clean an free of toxic pesticide residuals in containers. Contingent Valuation (CV) is used to estimate the monetary value of a proposed change to the provision of products to pesticide licensees. ","Missing data are a serious problem for research in the behavioral, biological, economic, health, and social sciences. When respondents do not provide the desired data, subsequent analyses can be biased and less precise. Ignoring the fact that some observations are missing and reporting results based on those that are observed, although expedient, is not a recommended procedure. In longitudinal studies in which data are collected on subjects over time, the loss of statistical power due to missing information can become severe. Methods of multiple imputation are explored for use with the Iowa Family Transitions Study. The study is a multi-year multi-informant data collection effort concerned with financial hardship, emotional well-being, and martial and other relationships. Effective methods for missing data are critical for maintaining statistical power and effective sample size. ","We evaluated alternative approaches to imputation for univariate estimates and multivariate regression analyses of physiological health measures collected in the 2003-2004 National Health and Nutrition Examination Survey (NHANES). From the NHANES public use data files we selected 5041 respondents age 20+ who provided questionnaire or medical exam data. Measures collected at interview (e.g., demographics, self-reported health status) and measures collected at physical examination (e.g., height, weight, blood pressure, cholesterol, hemoglobin, Hematocrit, and iron) were evaluated for rates of item missing data (i.e., item nonresponse). The properties of several imputation methods (including single and multiple imputation) were evaluated with respect to univariate estimates and a regression model using age, sex, race, height, weight, cholesterol, and marital status to predict blood pressure ","Often the probability of responding depends directly on the outcome value. This case can be treated by postulating a parametric model for the distribution of the outcomes before nonresponse and a model for the response mechanism. The two models define a parametric model for the joint distribution of the outcomes and response indicators, and therefore the parameters of these models can be estimated by maximization of the likelihood corresponding to this distribution. Modeling the distribution of the outcomes before nonresponse, however, can be problematic since no data is available from this distribution. We propose an alternative approach that allows estimation of the parameters of the response model by first estimating the outcomes distribution of the respondents, and then solving an estimating equation defined by the census likelihood of the response indicators. ","We compare soft-boundary hot-deck (SBHD) imputation with model-assist hot-deck (MAHD) using simulation method. In SBHD, donors and receivers are matched in cells defined by a list of variables. There is no control on the number of observations in each cell and the number of the variables used to define the cells is limited. If at least one cell runs out donors, the last variable on the list will be dropped to order to enlarge the donor pool. In MAHD, the donor-receiver grouping is defined through the intervals and percentiles of predicted values by a model. It greatly reduces or eliminates those limitations of SBHD. Therefore it provides flexibility to optimize the procedure and possibility to produce better results. We also study grouping which utilizing propensity score of missing. ","\"Scrambled data are as good as scrambled eggs.\" In the present investigation, it has been shown that scrambled responses on sensitive variables such as income, drugs used, induced abortions, etc. can also be imputed by following Singh, Joarder and King (1996) and can be jackknifed to estimate the variance of the resultant ratio type estimator by following Rao and Sitter (1995). Results have been simulated under different levels of untruthful reporting by following Singh, Joarder, and King (1996) and are compared with those from Rao and Sitter (1995) study. ","The purpose of the research was to ascertain the effectiveness of using the SERVQUAL Model (Parasuraman et al 1988) to assess the differences between college student expectations going into a course at the beginning of the semester and their final perceptions at the end of the semester. Based on past research in employing the SERVQUAL Model in other service industries, such as banking, a \"gap\" usually occurs because expectations generally exceed perceptions (Schneider and Bowen 1985). The author will also examine the advantages and disadvantages of the SERVQUAL technique and offer suggestions on how to improve the technique's effectiveness. ","This poster describes a planned simulation study comparing methods to uncover latent structure. Exploratory factor analysis (EFA) will be contrasted with the more explicitly causal correlation constraint analysis (CCA). The project will compare CCA and EFA in terms of accuracy, invariance, and sensitivity to variability within 100 simulated samples built to specifications and repeated with high and low levels of variability. The study will test whether:  1. CCA is more sensitive to lower levels of variability than EFA. 2. CCA results are more factorially-invariant than EFA results. The proposal will determine if CCA correctly recovers the 'true' latent variables and structure from simulated observed data more consistently than EFA (\"accuracy\"), recover the same 'true' model 95% of the time (\"invariance\"). Replication of results in samples with high/low variance will support sensitivity. ","There are documents that provide very good high-level guidance for different aspects of the survey process. This high-level guidance is typically implemented by computer programs. This poster provides a quality approach of how to translate high-level guidance into well-documented and understandable statistical computer programs. The quality approach consists of three interrelated documents: (1) a text file that contains the executive summary of the program and a detailed section for each of the major steps of the program; (2) a flow chart containing the major steps and how they are related; and (3) computer programming techniques to facilitate the understanding and quality control (e.g., variable naming conventions, commenting, and program structure that results in a self-documenting statistical computer program). ","Health surveys often contain a questionnaire item on whether sample persons' general health is excellent, very good, good, fair, or poor. The Medical Expenditure Panel Survey (MEPS) includes this type of question at each of 5 separate interview rounds. The MEPS is conducted via CAPI and one adult responds for all members in the household. The MEPS also includes a self administered questionnaire (SAQ) which is completed by sample adults around the time of the 2nd and 4th household interviews; it includes the same health status question as in the CAPI. In this paper, we assess the extent that type of respondent (self versus other family member), questionnaire mode (in-person CAPI versus self-administered paper), and characteristics of the respondent and sample person affect responses to the perceived health status question. ","Sample size at each level is important to consider when estimating multilevel models. Although general sample size guidelines have been suggested, the nature of social science survey research (e.g., large number of level-2 units with few individuals per unit) often makes such recommendations difficult to follow. This Monte Carlo study focuses on the consequences of level-2 sparseness on the estimation of fixed and random effects coefficients in terms of model convergence and both point estimates and interval estimates as a function of the level-1 sample size, number of level-2 units, proportion of singletons (level-2 units with one observation), collinearity, intraclass correlation, and model complexity. SAS IML was used to simulate 1000 data sets across 5760 conditions. Results are presented in terms of statistical bias, confidence interval coverage, and rates of model non-convergence. ","The NHIS, a national face-to-face household survey, and the BRFSS, a system of state-based RDD surveys, provide a major source of health information needed by policy makers. Estimation for small areas (e.g., for counties and states) has become important, but the NHIS' inadequate sample sizes for small areas and the BRFSS' telephone coverage bias along with high nonresponse rate make direct estimates for small areas problematic. A Bayesian model-based approach, suggested by Raghunathan et al., 2007, provided a method to combine the data from both surveys and correct for the individual deficiencies. In this paper we consider a more traditional approach of using linear mixed models to combine data from the two surveys. We focus on models that can be applied to a wide range of problems and be easily applied by data users. We provide evaluations of the Bayesian and traditional models. ","The variance of variance of finite samples taken from a finite population with replacement is expressed in terms of the sample size and the second and fourth order moments of population.  ","Audit files produced by CAI software present a record of the actions and entries made by interviewers as an interview is completed. These data provide objective feedback on the interviewing process, which can lead to changes in training or supervision. In this study, a software tool was developed that searched for and provided frequency counts of text that described selected interviewer actions. Survey production files were then analyzed to answer a series of questions including how interviewers chose to enter data (keyboard vs. mouse), when and how they navigated in a complex instrument, how interviewers handled edit messages when they were triggered, and how more complex sections of the instrument (e.g., tables) were handled. The advantages and disadvantages of using paradata for these purposes are discussed. ","As has been widely reported in the literature, response rates for telephone surveys---particularly RDD surveys---have been declining significantly in recent years. Mathematica Policy Research (MPR) has conducted several rounds of the Community Tracking Study Household Survey (CTS) for the Center for Studying Health System Change since 1996, to inform health care decision makers about changes in the U.S. health care system, and how such changes affect people. Obtaining high response rates for the CTS has been increasingly difficult each round, and round 5 (2007--08) posed the greatest challenges yet. MPR implemented various incentive and other strategies, including varying dollar amounts and the form and timing of payments. This paper shows how these strategies, implemented within random sample replicates, affected response rates and the effort needed to obtain completed interviews. ","The National Health Interview Survey (NHIS) is a multi-purpose health survey conducted by the National Center for Health Statistics (NCHS). Public-use microdata files and complex sample design variance estimation structures are available at the NCHS Internet web site. In 2007, NCHS expanded the variance estimation structures to cover all available years of public-use microdata files, which enabled researchers to compute appropriate variance estimates when conducting analyses of NHIS annual data pooled across years. We present research findings about the level of correlation in NHIS estimates from year-to-year, which provide insights in areas such as: 1) the amount of error in variance estimates for pooled data within a sample design period if the annual data are incorrectly assumed to be uncorrelated; 2) the trend in correlation over a sample design period. ","Hardy-Weinberg equilibrium (HWE), which specifies the relationships between allele frequencies under random mating, has been central to genetics for 100 years. One important application in modern times is in quality control for high-throughput genotyping, where departures from HWE are a valuable diagnostic. Some large cohort studies are conducting large-scale genotyping on a subsample of their participants, to reduce costs. HWE will typically not hold in the subsample, especially when participants experiencing medical events are oversampled. I will discuss HWE testing under subsampling. It is straightforward to develop a large-sample, design-based test, which I will present, but large-sample tests for HWE have notoriously poor performance at practical sample sizes and allele frequencies. I will also present a modification of Fisher's exact HWE test for unequal probability sampling. ","One challenge faced by the Commercial Buildings Energy Consumption Survey (CBECS) has been that of surveying strip shopping malls. These malls consist of unrelated establishments sharing a common building. Respondents knowledgeable about energy and related characteristics of the entire mall are difficult to find. This presentation describes the multilevel data collection strategy adopted by the 2007 CBECS. Interviewers compiled rosters of mall tenants, which were used to draw a PPS systematic sample of establishments. Interviews were conducted with the mall manager, as well as with the selected establishments. Estimation involves developing a strategy for combining these different pieces of information to reconstruct the single mall building. Auxiliary information, from the roster and from other malls, assisted in the reconstruction; inconsistency and nonresponse were problems. ","While increasing response rate by converting refusals aims to minimize nonresponse error, it can possibly lead to higher measurement error if reluctant respondents are less motivated to attend carefully to respondent tasks. Using data from the second round of European Social Survey conducted in 2004/2005 in Netherlands, we investigate whether reluctant respondents tend to provide lower data quality. Willing and reluctant respondents are compared on measures of acquiescence, straight responses, don't know responses, middle and extreme responses, random selection of response alternatives (coin-flipping), as well as interviewer evaluation of respondent effort. The implications of the study for the future waves of the European Social Survey are discussed. ","As the size of the U.S. population that lives in households with no landline telephones has burgeoned, Arbitron has undertaken a plan of study aimed at finding methods to mitigate potential bias due to frame noncoverage of this population in their radio audience measurement surveys. This plan of study encompasses a number of different facets, including an examination of the cost-effectiveness of sampling the cell-phone-only (CPO) population. This paper reports the results of statistical analyses designed to study the characteristics of the CPO population and how the information gleaned from these analyses might be used to improve the quality of Arbitron's audience measurement estimates in the face of the CPO issue. ","Principal Components Analyses are typically conducted without taking into account the sampling design.  Controlling for variables that are part of the sample design may affect the interrelationship of variables in a manner that is difficult to interpret.  Controlling for clusters, strata or probabilities of selection may affect the substance or the stability of the results. To examine this issue, PCAs were conducted for several random, cluster, and stratified designs using a database of zip code areas. Clustering diminished the sample's ability to reproduce the population PCA, with adjustments producing mixed results.  Stratification with a similar number of units selected per stratum and proper weighting led to results at times better than random sampling in spite of inequality of weights. Simulations also indicated that stability of results is in part a function of sample design. ","Coevolution in evolutionary algorithms allows solutions to two interdependent optimization problems to be determined simultaneously, similar to the evolution of symbiotic species in nature. There are many methods for determining multivariate optimal allocations in stratified sampling, including the use of an evolutionary algorithm (Day, 2006). There are also widely accepted methods for determining optimal stratum boundaries. This work presents a method for simultaneous determination of multivariate optimal allocations and stratum boundaries using the concept of coevolution in evolutionary algorithms. ","There is always the question of whether to incorporate sampling design into the analysis of complex survey data. Failure to account for aspects of sampling design can lead to creation of mis-specified models and a lack of understanding of the basis for a given type of analysis may lead to spurious inference. We carry out analysis of data from the Jamaica Healthy Lifestyle Survey 2000 with and without accounting for two aspects of sampling design - clustering and weighting of observations. The analyses examine risk factor relationships with chronic disease outcomes and reveal that, for some outcomes, the inference differs depending on the aspect of sampling design that has been omitted. We give reasons for the differences in inference and suggest recommendations for when to use sample clustering and sample weights in the analysis. ","The Monitoring Sexually Transmitted Infections Project is an RDD telephone survey being conducted in Baltimore, Maryland from 2006 through 2009. The project screens households for people 15-35 years of age and then randomly selects one eligible person. To gain efficiency and reduce costs, three methodological changes were adopted after several months of data collection. These changes were 1) in the wording of the screening questions aimed at identifying households with someone 15-35, 2) implementing a sampling strategy that incorporated the use of lists that identified the ages of residents within some households, and 3) altering the probability of selection of an eligible respondent within a household to favor selecting someone who initially answered the telephone. This paper will examine the effects of these strategies on survey cost and sample composition. ","The collection of income information is difficult in most survey. High nonresponse rate on income causes the limitation of data use. The income questions in telephone survey are often designed in a cascading sequence of questions in which the interview would ask a cascading sequence of income questions when respondent fails to provide the exact income amount. Using this partial information of income, we can generate the income range to improve the accuracy of income imputation. The paper discusses the characteristics of respondents who provide income information in full, partial or none. The purpose of this paper is to investigate the impact on income imputation by comparing the data from the California Health Interview Survey (CHIS) and Current Population Survey (CPS). ","Voter confidence and transparent elections are essential to modern democracies. States are legislating precinct-level, sample-based audits. New Jersey just passed legislation requiring elections be audited to achieve particular power levels to detect election altering miscounts, and Florida is working on legislation modeled after New Jersey. A statistical foundation in the development of election auditing procedures aids transparency and improves voter confidence in election results and the democratic process in both the United States and abroad. Come share ideas concerning statistical methodology in the election process. Statistical issues may include random recounts in all elections, 100% recounts in close elections, paper trails, Six Sigma applications, process control, exit polling, and more. ","We derive a closed-form posterior distribution for the difference of two Poisson rate parameters using both data subject to under-reporting and error free data. We use a Monte Carlo simulation study to examine the characteristics of posterior distributions for the rate difference. We also derive a closed-form posterior distribution for the rate difference when using informative priors in the absence of the error free data. We perform a Monte Carlo sensitivity analysis to examine the effects of the priors on the posterior distribution of the rate difference. We then apply our methods to an example comparing automobile accident rates for males and females. We demonstrate a considerable difference in credible sets for the rate difference found using our two Bayesian methods that account for under-reporting versus a Bayesian interval that does not account for under-reporting. ","In randomized clinical trials the primary outcome of interest often is time to the occurrence of an event. Cox-Regression models are commonly used to analyze such type of data. The standard Cox model assumes that the censored data are non-informative. These assumptions are usually not testable from the observed data and may well introduce biased. This paper presents a method for analyzing time to event data using Cox-Regression when censoring is informative. We propose a Bayesian model to analyze this type of data by introducing informative prior distributions to identify the model. Sensitivity analysis is then used over a range of these prior distributions. The method is applied to analyze the data from the Trial of Preventive Hypertension Study.  ","The Bayesian Euler's Approximation Method (BEAM) has recently been proposed to estimate the parameters in a non-linear model involving with ODEs, especially when analytical closed form solutions are not available. In this article, the BEAM is extended to handle datasets with missing or censored observations. The proposed method is based on data augmentation algorithm. A simulation study based on growth colonies of paramecium aurelium is presented to compare the performances of the proposed method for various percentages of missing and censored data cases and results are compared to complete data case. Finally the method is illustrated with a real data of AIDS Clinical Trials Group Protocol 315. ","In data systems with complexities due to nested/non-nested clustering  and multiple-membership, missing values present an added analytic challenge to the  statistical analyses. We develop model-based multiple imputation (MI) inference which has been a popular method in the analyses of missing data.  Adaptations of multivariate generalizations of the mixed-effects models are used as imputation model. These models are modified to handle multivariate responses and observational units with possibly overlapping membership of clusters that are not necessarily hierarchical. Markov Chain Monte Carlo techniques are used to simulate and draw imputations from underlying joint posterior predictive distributions. Relevant concepts on both multiple-membership and non-nested clustering are demonstrated  longitudinal administrative data with panel missingness as well as arbitrary item nonresponse. ","In many applications, it is of interest to study the relationship between a predictor X and a response Y without imposing restrictive parametric assumptions on the conditional response distribution of Y given X. The focus of this article is on developing a semiparametric Bayes approach for flexible conditional response distribution modeling, accommodating predictors that are measured with error without imposing parametric assumptions on the distribution of the missing predictor. Our approach relies on a joint modeling strategy, which uses Dirichlet process mixture models for the distribution of X and kernel stick-breaking process mixtures for the conditional distribution of Y given X. Identifiability issues are considered, and an efficient MCMC algorithm is developed for posterior computation. The methods are illustrated using simulation examples and an epidemiologic application. ","Bayesian variable selection has recently become a popular research topic, and has been applied successfully in many situations. However, for the situation of missing covariates, less progress has been made. To find good subsets of variables from large amount of covariates, and at the same time to deal with the problem that a certain percentage of the covariates are missing, we propose a Bayes factor driven search algorithm to find best subsets. With missing data it is difficult to find a closed form for the Bayes factor, so we derive a bridge sampling type approximation. We prove that the stationary distribution is preserved and ergodicity is achieved when the approximate  Bayes factors are used in a stochastic search algorithm. To increase the computational speed, we use a matrix inversion identity to accommodate the situation. ","A single purpose design may be quite inefficient for handling a real-life problem. Therefore, we often need to incorporate more than one design criterion and a common approach is simply to construct a weighted average, which may depend upon different information matrices. Designs based upon this method have been termed compound designs. The need to satisfy more than one design criterion is particularly relevant in the context of random fields. It is evident that for precise universal kriging it is important not only to efficiently estimate the spatial trend parameters, but also the parameters of the variogram or covariance function. Both tasks could for instance be comprised by applying corresponding design criteria and constructing a compound design from there. Modern techniques for such first and second order characteristics will be suggested and reviewed in the presentation. ","Inferences for spatial data can be affected substantially by the spatial configuration of the network of sites where measurements are taken. Most network design literature considers univariate spatial inference problems, such as kriging and variogram estimation. This presentation considers one aspect of network design for multivariate spatial inference. In particular, I examine the effect that the proportion of collocated design sites has on multivariate spatial prediction, and I show how this effect depends on the relative strengths of spatial correlation and cross-correlation. ","This paper describes and evaluates two basic approaches for estimating the Spatial Mean of the Temporal Trend: 1. a pure model-based approach with purposive sampling in space and in time, and model-based inference; 2. a mixed approach with probability sampling in space, and purposive sampling in time. In the mixed approach the statistical inference depends on the pattern of the observations in the space-time universe. When sample locations are revisited in all sampling rounds, a first option is to estimate first temporal trends at the sample locations by model-based inference, followed by design-based estimation of the spatial mean of these location-specific temporal trends. A second option is to estimate first the spatial means at the sampling times by design-based inference, followed by model-based inference of the temporal trend of these spatial means. ","One goal of the Clean Air Act Amendments of 1990 (CAAA) is to reduce ambient concentrations of atmospherically-transported pollutants. Monitoring data from EPA networks can be used to estimate regional trends of these pollutants to evaluate the effectiveness of the CAAA. This paper presents spatial network design methodology to optimize the network's ability to detect and quantify future regional trends in air pollution by adding or relocating monitoring sites. The 2000--2005 PM2.5 data in the Midwest region of US is analyzed to illustrate the design methodology. ","Support Vector Machines (SVM), being robust non-parametric classifiers, have gained considerable success in many data modeling fields. Based on Statistical Learning Theory, they provide a well-founded framework for extracting non-linear dependencies from empirical data in high dimensional spaces. This study develops a novel method for spatial sampling design using SVM for spatial classifications which cover a wide range of tasks in mapping of categorical variables. It was observed, that optimal solutions provided by SVM are often sparse: a larger part of data does not contribute to the decision, while only the support vectors, contribute to describe the decision boundaries between the classes. SVM algorithm gives priorities to new sampling points which the most contributes to the decision process. Spatial sampling design using SVM can be considered as an active learning process. ","The U.S. Census Bureau's Small Area Income and Poverty Estimates (SAIPE) program produces model-based estimates of income and poverty using data from Census 2000, the American Community Survey (ACS), administrative records, and intercensal population estimates. This work assesses SAIPE county poverty models under log rate and log count data transformations. Equivalent rate and count models are described as the basis for comparison, which differ only in their corresponding dependent variables. Scale invariance and homoskedasticity are assessed, and \"goodness of fit\" comparisons are made in terms of root mean square error (RMSE). The estimation results support use of the log count model. ","Time series data with change points at the end of a series can make forecasting particularly complicated because these points can disrupt estimations of fit in the holdout sample. However, poor performance with a holdout sample is not a sign that the model is totally invalid. The current study sought to examine the predictive validity of various time series models, and then extended model fit estimations by forecasting with multiple horizons in order to address the complexity of the data. Implications of this study point to the importance of testing multiple origins and horizons for time series in which the effects of change points are difficult to unmask. ","Checking on normality is desirable in multivariate data analyses, if statistical inferences are made under the assumption of normal distribution. Normal probability plots or Q-Q plots provide a good visual check and are considered to be adequate for the purpose of assessing normality by many researchers.  Transforming the data to make it 'normal' is an appropriate procedure if departure from normality is suspected in the plots. Once the problem of normality is resolved by means of transformation, the desired statistical inference can proceed in a fairly standard fashion. To illustrate data transformations and to apply multivariate statistical inferences, waiting and treatment times for emergency department visits along with some socio-demographical characteristics were used from the National Hospital Ambulatory Medical Care Survey (NHAMC).  ","The Energy Information Administration (EIA) is researching estimation methods with the goal of developing an experimental Energy Consumer Price Index (ECPI), based almost entirely on EIA data.  For some major energy sources, EIA collects universe or large-sample price and sales data, which can be used to compute price indexes with very low sampling error.  Also, EIA's model-based projections of future energy prices and consumption levels can be used to develop CPI forecasts for some energy components.  Because the experimental indexes are being computed in a research environment rather than in a large-scale production environment, the process of incorporating data from new energy surveys will be streamlined.  This paper provides background information and preliminary results of EIA's price index estimation research. ","To estimate the number of unique valid signatures on a petition, the authors have recently shown that a new non-linear estimator, W, is preferably to the more commonly used Goodman-type estimator provided that a signature's being replicated on the petition is independent of the signature's being invalid for other reasons, such as not a registered voter or wrong address. Through an extensive enumeration, this paper investigates the power of the Chi squared test of independence to determine empirically the preferred estimator for a petition with known levels of petition size, sample size, percent replicated signatures, percent invalid signatures, and degree of association between replicated and otherwise invalid signatures. ","Individual-level data are often not publicly available due to confidentiality. Instead, masked data are released for public use. However, analyses performed using masked data may produce biased parameter estimates. We propose a data masking method using spatial smoothing. The method allows for varying both the form and the degree of masking by utilizing a smoothing weight function and a smoothness parameter. We investigate for GLM the bias of parameter estimates resulting from analyses using the masked data, and we show that data masking using a smoothing weight function that accounts for prior knowledge on the spatial pattern of exposure may lead to less biased estimates. We apply the method to the study of racial disparities in mortality, and we find that the bias of the association estimate when using the masked data is highly sensitive to both the form and the degree of masking. ","Face-to-face interviewing using area probability samples remains the gold standard method for conducting representative surveys, but the increasing cost of these studies has led researchers to look for economies in all phases of survey design and execution. GeoFrameTM is an innovative use of digital photography and geospatial technology that reduces the cost of field enumeration to make construction of area probability samples more affordable. GeoFrame'sTM utility can be extended to other situations where sampling frames may be difficult or too costly to create, such as when creating probability samples to evaluate disaster recovery efforts or conduct surveys in developing nations. This paper describes the initial application of this new technology to construct a sampling frame for a survey of tobacco use in the colonias in El Paso County, Texas. ","In area household surveys, a multi-stage sampling approach is frequently used to create a nationally representative sample. The first stage is the selection of primary sampling units (PSUs) consisting of counties or groups of counties. The second stage of selection involves the formation and selection of smaller units within the PSUs, often referred to as segments, consisting of blocks or groups of blocks. In the United States, segments formed for area samples are typically based on population or housing counts for the specific blocks or groups of blocks, as collected in the most recent decennial census.    ","The NIS-Adult, a nationwide telephone survey conducted by NORC for the Centers for Disease Control and Prevention, was designed to monitor vaccination rates for persons aged 18 years and older. The initial sample for the NIS-Adult was selected from adult respondents to the National Health Interview Survey (NHIS) who completed the household interview and provided contact information. The NHIS sample had an insufficient number of older adults. To supplement adults aged 50-64 years and 65+ years, an age-targeted list sample based on directory-listed telephone numbers was used. The age-targeted list contributed a larger fraction of the final sample than expected-59%. In this paper, we compare the demographic profiles of the NIS-Adult with those from the Current Population Survey and the NHIS and show that the age-targeted list sample performed above our expectations. ","In area probability samples, secondary sampling units (SSUs), or segments, are often constructed using census blocks, block groups, or tracts. There are numerous challenges in forming segments that meet statistical, operational, and study goals. Creating segments that are composed of contiguous units is often a desirable and challenging goal that can be a challenging task, whether a manual or automated solution is used. A complex approach involves forming segments which are more heterogeneous based on one or several variables.  There may also be some advantage in creating more compact segments. An important operational consideration is to create segments with visible boundaries (e.g., roads, highways, rivers), rather than invisible boundaries (boundaries with no physical demarcation; these are often municipal, county, or state boundaries.).  All of these challenges are explored in this p ","The Master Address File (MAF) is a national inventory of addresses for living quarters in the United State that the Census Bureau continually updates. One of the goals of the 2010 Demographic Household Survey Redesign is to switch to using a MAF-based frame for current demographic household surveys. To create the best possible current household survey frame from the MAF, a set of filter rules must be established to determine which MAF records should be accepted into the frame. The quality of the filter can have a major impact on the coverage of the resulting frame. We assess the effectiveness of various filter criteria by comparing the resulting frames with a benchmark address list collected from the field listings of a nationally representative probability sample. ","The NSCG is a longitudinal survey that collects information on employment, educational, and demographic characteristics of scientists and engineers in the U.S. The current NSCG sample was selected from the Census 2000 Long Form. In 2010, the NSCG plans to refresh its sample to address attrition and coverage concerns. With the ACS replacing the Long Form, the Census Bureau is evaluating using the ACS as a sampling frame for the NSCG. The evaluation will initially examine the change in available and effective sample if the ACS is used as the sampling frame under the current NSCG design. The evaluation will then examine the potential increase in sampling efficiency for the NSCG associated with the additional demographic information available on the ACS. This paper provides an overview of this evaluation and presents recommendations for the 2010 NSCG based on the evaluation results. ","In literature, stratum creation, take-all unit identification and sample allocation methods usually try to optimise the variance or the cost for the estimation of a total. For a complex statistic like a ratio or a regression coefficient, a sampling plan based on the total of one of the variables in the statistic isn't necessarily optimal. We propose to use a method built on a Taylor linearization of the complex statistic. The statistic is first linearized and then, the common stratification and allocation methods can be used. We will measure the efficiency of the method through a simulation study. We will show an example of the application using the Canadian Survey of Employment, Payrolls and Working Hours. ","The etiologic effects of complex traits are often conferred by multiple risk factors. Cancer epidemiology studies typically establish a large cohort to study risk factors for complex diseases and disease phenotypes. While some putative risk factor data are obtained from the full cohort, new risk factors such as biomarkers need to be constantly examined as studies continue. Evaluating the biomarkers on the full study cohort may not always be feasible due to logistic or economic concerns. Therefore, it may be useful, or sometimes necessary, to identify an informative sub-cohort for evaluating new risk factors in an efficient manner. This talk examines a novel stratification scheme for identifying an informative sub-cohort for biomarker evaluation. Our work is motivated by the ongoing Study of Nevi in Children the investigates risk factors associated with nevogenesis and nevus evolution. ","Increasingly, educational, administrative and policy interventions are being carried out to improve public health, quality of medication use and/or contain costs. Several statistical methods have been suggested to implement the intervention procedures and to assess its impact. These methods may vary from randomized controlled trials to Auto-Regressive Integrated Moving Average (ARIMA) to segmented regression analysis or parameter-driven models in regression analysis. Clearly, the methodology is partially guided by nature and availability of data. Thus, ARIMA is more suitable for data with serial dependence. But, it also requires a large number of data points for better results. Segmented regression assumes linearity which often may hold only over short intervals. This method may also not be appropriate for highly auto-correlated data. In this paper, these methods will be compared. ","In 1996--7, AIDS Clinical Trial Group study 320 randomized 1156 HIV+ US patients to combination antiretroviral therapy (ART) or highly active ART with equal probability. In one year of follow up, 96 patients incurred AIDS or died, 51 dropped out, and 290 dropped out or stopped their assigned therapy. Noncompliance likely results in a null-biased intent-to-treat hazard ratio (HR) of AIDS or death comparing highly active ART to combination ART: which was 0.75 (95% confidence limits [CL]: 0.43, 1.31) for follow up within 15 weeks. Noncompliance correction using inverse probability-of-censoring weights yielded a 63% stronger HR of 0.46 (95% CL: 0.25, 0.85). Weights were estimated conditional on randomization arm and measured baseline and time-varying covariates. These methods may help resolve discrepancies between randomized studies with differing amounts of compliance. ","For measuring the degree of association between a disease and a risk factor, we have developed a new epidemiological design that can serve as an alternative to the conventional design for cohort and case-control studies. When the prevalence rate of disease is greater than the prevalence rate of exposure, this new design is superior in terms of the sample size and of the efficiency of estimate. A \"reversed weighting\" procedure is adopted for estimating the required sample size. The relative efficiency of the new design in comparison with case-control study results under a wide variety of conditions is also examined. "," In a simple stratified case-control study, a prospective cohort is stratified according to some variables known for the whole cohort. Separate samples of cases and controls are then drawn from each stratum and values of covariates are obtained. In a two-stage study, some of the more expensive or difficult covariates are not measured on all the sampled units, but only on a subsample drawn from them. This process can be continued indefinitely. We derive a set of estimating equations for the efficient semiparametric maximum likelihood estimator with an arbitrary number of stages. These take the form of penalized \"pseudo-likelihood\" equations, with a term corresponding to an ordinary prospective likelihood plus an extra penalty term for each additional stage of sampling. We illustrate with a three-stage example. ","Introduction and increased utilization of cancer screening induces remarkable transient effects on cancer incidence and mortality. When the dust settles we are interested in estimating the achieved benefits comparing a steady state after the intervention with the one before the intervention. We propose methodology for addressing this problem. A model of population under screening is built using methods of queuing theory. The disease free stage and the preclinical disease stage represent two series-connected servers. Stationary states of the system before and after the intervention are derived and linked by a regression model with intervention process characteristics as covariates. The methods are applied to Surveillance, Epidemiology and End Results (SEER) data. ","Characteristics of hidden populations (e.g., population of injection drug users) cannot be studied using standard sampling and estimation procedures. This article considers methods for estimating the population proportion of hidden population using social network. We describe an one step sampling procedure for collecting data from both the population and its social network, and provide a method to estimate the population proportion efficiently. We further drive a formula to compute an estimate of the variance of the proposed estimator. Simulation study is provided to illustrate the new sampling and estimation methods. ","In 2010, the U.S. Census Bureau will publish the first set of 5-year period estimates from the American Community Survey (ACS), based on data for 2005-2009. Published for small places, tracts, and other small areas, the 5-year ACS estimates will attempt to replace the long-form data from recent decennial censuses. Because the effective sample size for the ACS will be somewhat less than half that of previous censuses, users will face an increased challenge to distinguish true variation from sampling error. The concept of the false discovery rate has become increasingly useful in other disciplines confronted by large numbers of estimates, such as micro-array analysis in genetics and fMRI studies of the brain. The paper will review this concept and suggest its possible future application, based on a preliminary analysis of published data from the ACS Multiyear Estimates Study. ","For correlated sample survey estimates, a linear model with  covariance matrix in which small areas are grouped into clusters by a similarity measure based on spatial locations is proposed. In the context of correlated data, a novel asymptotic framework, a hybrid of infill asymptotics and increasing domain asymptotics is introduced. The hybrid asymptotic framework assumes that the number of clusters and the number of small areas in each cluster grows with sample size. Under the  previously mentioned asymptotic framework, the proposed parameter  estimators are sqrt(k) consistent, where k is the number of clusters. The proposed model is implemented for county-level civilian  employment growth data. ","In this study, we used a two-stage method to estimate the proportion of alcohol drinking among teenagers. Data for this study was the 2001 National Health Interview Survey in Taiwan. We first used generalized linear mixed effects model including the random intercept to estimate the probability of alcohol drinking among teenagers for townships or cities in Taiwan. The estimated random intercept was related to some spatial variables. Then, we used the spatial model to predict the value of the random intercept in the area, where no sample was taken, and their probabilities of alcohol drinking. The results of model show that the highest regional probability of alcohol drinking was about 0.3 in mountainous areas. The model provides reasonable estimates. ","In the context of small area estimation, hierarchical Bayesian (HB)  models are often proposed to produce more reliable estimators of small  area quantities than direct estimates. A method that benchmarks HB estimates with respect to  higher level direct estimates and measures the relative inflation  in the posterior mean square error of distributions due to benchmarking   is developed to evaluate the performance of hierarchical  models. The benchmarked  HB posterior predictive model comparison method is  shown to be able to select proper models effectively in a simulation  study.  The method is then applied to fitting models to a stratified  multi-stage sample survey conducted by Iowa's State Board of Education.  The survey strata  serve as small areas for which HB estimators are  suggested.  Here the method is used to select a generalized linear  mixed model for the survey data. ","Fisher et al. (2006) developed a hierarchical Bayes model to estimate the number of people without health insurance within demographic groups for states. The Centers for Disease Control and Prevention are interested in estimates of women without health insurance by demographic groups in families that earn less than 200% of the poverty line. Our approach jointly models survey estimates from the Annual Social and Economic Supplement to the Current Population Survey and estimates from tax, census, food stamp, and Medicaid data using a multivariate, hierarchical approach. We have made important enhancements to the models by including additional administrative data, further elaborating the expectation and variance models for both the survey estimates and the administrative data, and developing an approach to reflect the variation due to raking to survey estimates in the variance estimates. ","A general small area model is a hierarchal two stage model, of which special cases are mixed linear models, generalized linear mixed models and hierarchal generalized linear models. Such models naturally lend themselves to both Bayesian and non-Bayesian analysis. In recent times, parametric bootstrap  for small area models has become an active field of study. Although resampling techniques like the parametric bootstrap are inherently non-Bayesian, there are some deep connections between resampling and  Bayesian analysis. This talk will focus on some of these connections in the context of small area problems. ","The performance of the polls in election years is a marker for the quality of work in the profession generally. The accuracy of published polls in general elections has been very good, but this year there were some estimation problems in the early primary and caucus polls. This paper will discuss range of sources of such error and whether they might present problems for the fall election. ","This paper presents a variety of answers to the question of \"Why Do Election Polls Differ?\"  It examines issues of timing, sampling, questionnaire wording, weighting and the identification of \"likely voters\" among others ","This paper will present examples of problems in accurately measuring preference in election polls, focusing on poll timing, measuring intensity, and the interaction between respondent and interviewer. Have interviewer characteristics like race, gender and age affected poll accuracy? When they report polls, are journalists giving polls more precision than they deserve? ","The Energy Information Administration (EIA) has taken several steps to improve its communication with survey respondents. In the past 30 years the energy industry has changed substantially because of deregulation and the restructuring of industry business practices. Even though EIA's business surveys are mandatory, we have had to change our approach to communicating with respondents to maintain our response rates and a high level of data quality. Our efforts have focused largely on pre-survey notifications of survey changes, providing multiple modes of data collection, making our surveys correspond to business record keeping practices, and insuring the confidentiality of sensitive price information. Since many of our largest respondents complete multiple EIA surveys, this paper will also highlight EIA's move toward a common look and feel for its internet data collection systems. ","The mission of the UK Office for National Statistics (ONS) is to improve understanding of life in the United Kingdom and enable informed decisions through trusted, relevant and independent statistics and analysis. The ONS is therefore responsible for conducting, analyzing, and disseminating a broad range of social surveys, business surveys, and the decennial census. Respondent communication is regarded as one of the central elements to facilitating participation in our surveys.This paper will review, compare, and discuss social and business survey respondent communication at ONS. The paper will include an overview of the types, modes, visual design, and content themes of our respondent communication. ","The Bureau of Labor Statistics (BLS) provides key economic statistics such as payroll employment, the unemployment rate, and the consumer price index, as well as statistics on compensation, productivity, workplace safety and health, and other topics. The BLS has undertaken a number of customer relations initiatives in recent years designed both to inform data users and to encourage respondents to provide data. Among the topics to be discussed are new publications, Internet sites geared toward respondents, improvements in electronic data collection, and a test to determine what effect targeted changes to materials provided to respondents had on survey response rates. ","Despite the best efforts to recruit and train the best interviewers, there is still the risk interviewers will try to falsify cases. The standard technique of call-back validation is slow and unreliable in detecting falsified interviews. Various techniques have been developed, such as Benford's Law, sampling of audio files of interviews, looking for unlikely response combinations, and newer techniques such as GPS tracking. All these methods have limitations. The goal of this roundtable is to bring together researchers who can share their experiences to move toward the goal of preventing the falsification of interview data. ","The MEPS is designed to provide nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian noninstitutionalized population. Each year a new panel of households is selected from households that participated in the previous year's National Health Interview Survey (NHIS). For most years, the MEPS sample includes an oversample of targeted policy relevant subpopulations which include selected minorities and households with low income. The status of individuals' income in the next year is not known at the time the sample is drawn. However, a wide range of potential predictors are available from the NHIS to inform the oversampling of households predicted to be poor. This paper reports on the modeling procedure and the results of updating the prediction model using data from the 2003 NHIS and from the 2004 MEPS. ","MEPS is a national probability sample survey designed to provide nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian noninstitutionalized population. Depending on the type of medical event, there are varying levels of item nonresponse on medical expenses as collected in the MEPS household interview. MEPS also collects expenditure data in the Medical Provider Component (MPC) of the survey. Missing expenditure data for health care events are completed through a weighted sequential hot deck procedure with MPC data as the primary donor source. Studies in 2004 and 2005 examined the impact of imputation on estimates of variance for MEPS health care expenditures. This study updates this research by investigating multiple imputation as a method to assess the impact of imputation on the variance estimates. ","MEPS is a nationally representative survey of U.S. civilian households. Respondents are asked about their medical insurance, healthcare utilization and expenses through five rounds of interviews conducted over a two-year period. Annual estimates for the US are made by combining the data from the panel in its first year with the one in its second. This research examines if the level of reporting of medical events differs between each panel's first and second years and if the drop occurs primarily between the first and second rounds compared to later rounds. An assessment of the impact of any differential reporting on the annual MEPS expenditure estimates is made using simulation. ","The MEPS is a longitudinal panel survey. A new panel is fielded annually and data are collected via five CAPI interviews that cumulatively cover two consecutive years. Each annual MEPS panel is selected as a subsample of respondents to the prior year's National Health Interview Survey (NHIS). In 2006, a new sample design was implemented for the NHIS and the changes carried over to the 2007 MEPS (MEPS Panel 12). Also, in 2007 the MEPS survey instrument was converted from a DOS-based system to a Windows-based system. These two changes have the potential to affect point estimates and standard errors of MEPS estimates. In this paper, we conduct an assessment of the impact of the changes on preliminary 2007 estimates.  ","The Medical Expenditure Panel Survey (MEPS) is a national probability sample survey. The Insurance Component of MEPS provides national and state level estimates of insurance coverage for the U.S. civilian noninstitutionalized population. The demand for reliable data at the state level and below, regarding healthcare insurance has greatly increased. Previous research has been conducted to produce direct design-based estimates using the MEPS Insurance Component design structure. However, the number of Metropolitan Statistical Areas (MSAs) for which direct estimates can be produced with acceptable reliability is limited. In this paper, we evaluate mixed models and Bayesian models, incorporating a time covariate, to produce MSA level estimates for smaller MSAs in ten states. We examine estimates of MSE and RSE of two types of estimates based on direct and indirect estimation techniques. ","Statistics Canada launched the Household Survey Strategy (HSS) to be able to increase its capacity to conduct ongoing and new household surveys in a cost-effective manner. The main methodological component of the HSS is the master sample, which will group the respondents from major household surveys to produce a sample frame which will be used by smaller household surveys to select their samples. Many methodological challenges will confront these multiphase, multistage and multiframe smaller surveys, including sample coordination, weighting and estimation. This paper will focus on the variance estimation for the smaller surveys. A new variant of the bootstrap method is planned to be used to estimate the variance of these surveys. A simulation study trying to recreate the HSS environment using census data shows that this method should give fairly accurate variance estimates. ","Multilevel models (MLMs) are often used to analyse clustered survey data. Several commercial software packages developed for fitting such models to survey data have implemented a model-based sandwich estimator to estimate the design-based variance (DBV) of the model parameters. However, there is empirical evidence that this estimator underestimates the DBV due to ignoring the stochastic adjustments to the sampling weights (Kovacevic et al. (2006)). Therefore, resampling methods have been proposed to more accurately estimate the DBV. However, none of the software that can currently fit MLMs to survey data has implemented these methods.     ","The German Institute for Employment Research has just completed the first wave of a new panel survey focusing on low income households. It is designed as a dual frame survey, the first frame being the register of households receiving some kind of unemployment benefits, the second frame consisting of an address register of the whole population. 6000 households were selected from each frame (with stratified PPS sampling of zip codes in the first stage), resulting in a large variance of design weights between the two samples. In this paper, we describe the sampling design and the construction of final (single frame) weights, and we show the results of a simulation study that tries to evaluate the performance of bootstrap variance estimators for our survey. ","We compare two methods, employing possibly misspecified parametric  models for nonresponse, of estimating theoretical variances for nonresponse adjusted estimates of survey totals. The first method is based on a formula of Sarndal and Lundstrom (2005) requiring known joint inclusion probabilities and a calibration model for response probabilities. The second method is based on balanced repeated replicates (BRR). Both methods are compared with the correct design-based variance in a superpopulation framework with independent random response indicators.  Numerical calculations and simulation results are given for a split-PSU design, with simple random sampling within half-PSU's and with weight factors for nonresponse constant over adjustment cells. The accuracy of variance estimators is related to the population balance across half-PSU's of intersections of true and working adjustment cells. ","Data on employee benefits is one of the key products produced by the National Compensation Survey (NCS).  The NCS uses a stratified three-stage sample: areas (PSUs), establishments, and occupations.  The weighting procedure includes adjusting sample weights for establishment non-response, occupation non-response, any unusual data collection situations, and post-stratifying to known employment control totals.   This paper illustrates how Fay's method of balanced repeated replication (BRR) was applied to estimates generated from this complex sample design to obtain variance estimates for the NCS benefits products. The following topics will be presented: an introduction to the NCS stratified three-stage sample; a description of our utilization of Fay's method, including formation of variance strata, variance PSUs, and replicates; a detailed analysis of the magnitude of estimated variances. ","The Survey of Income and Program Participation, conducted by the U.S. Census Bureau, is a longitudinal survey plagued by severe cases of missing data. So, the cross-sectional Horvitz-Thompson estimator for subdomain totals cannot be computed, much less its variance. One solution to this problem is to prorate partial subdomain estimators from previous cross-sections, in lieu of the missing data. The result is a \"backdated\" HT subdomain estimator. We use MLE's of transition probabilities (Bishop Fienberg Holland 1975 page 267) to prorate. We compute the MLE's through poststratification (Little 2004, 1993). The variance formula combines large sample approximations and BRR. This approach takes advantage of the survey design and the power of the poststratification. A careful diagnosis is conducted to validate the model. ","Often there is a need to predict the probability that an individual with specific characteristics (risk factors) will suffer from a certain disease or a health condition. Sometimes the real interest is in providing the aggregate predicted risk estimates at the population level or at the level of a subpopulation (e.g., males age 65+). A related problem is prediction of such a probability for an individual who does not belong to the surveyed population. While the estimation of these probabilities follows from fitting an appropriate model to the available data, the design-based standard error estimation of so-obtained estimates is not obvious. We are proposing a bootstrap method for estimation of the standard errors of predicted individual and aggregate risks. The method is illustrated using data from the Canadian Community Health Survey. ","The 2003 Lancet paper on child survival reviewed interventions feasible for delivery at high coverage in low-income settings. It showed that if key interventions were universally available, over 60% of then current global under-5 deaths could be prevented. Furthermore, while health facility-centric interventions are necessary, household-based interventions, with limited need for external material inputs, could prevent over half these deaths. This is the theory, what is the practice? A child survival project in four countries in West Africa serves as the basis for assessing the situation, not only in terms of intervention coverage and mortality measures, but also the more qualitative aspects of people motivation and management, logistics, adaptation to local conditions, and impact on policies. Some of the challenges of interpreting such information are considered. ","Mortality in children (10 million annually) remains unacceptably high. Children can be saved by improving medical technology (MT), by improving delivery and utilization (DU) of the technology or by a combination of both. (1) We estimated that MT could reduce child mortality by 22%. This reduction is 1/3 of what could be achieved if DU of existing technologies were adequate. (2) We found a serious discrepancy between research needs and research funding: a mere 3% of research grants funded by NIH and the Gates Foundation were directed toward DU; 97% were related toward MT, with its relatively low potential to save lives. (3) We estimated the millions of lives currently saved by public health programs. Conclusion: effectively implemented programs could save millions of lives. Statistics plays a key role in the design and evaluation of effective programs. ","Numbers can't provide food, shelter, medical treatment, or security. But well-executed statistical analyses can help to identify programs that are successful in providing these basic needs under challenging circumstances---poverty, corruption, social and political instability, etc. Although the Health and Human Rights Report Card is a work in progress, and is admittedly quite limited in scope, it does provide a starting point to identify states that appear to be out-performing their economic peers on key health indicators. Further research could then be conducted within these states to uncover successful strategies for fulfilling the right to health at high levels of population coverage under poor economic conditions. More generally, the report card should be viewed as an example of how statistics can be used to identify the most effective strategies for improving health outcomes. ","To save children's lives, trial programs need to be developed and evaluated. If a program proves to be more cost-effective than other possible programs, such becomes a candidate for national funding. A problem is that the United States Agency for International Development (AID) has chosen to remain largely ignorant of proper evaluation methods, practicing a LQAS method that can conclude a project has been successful even when there's a 90% chance that it has not. Likely a major reason for avoiding good evaluation is the fact that such would mean it would be more difficult to spend money -a primary goal. Personal experiences are given. ","This paper will report on preliminary results from a research project to develop empirical information to assist Michigan in the development of an election audit system. One purpose of an audit is to ensure that the electoral system is functioning in a way that provides accurate counts. Another purpose is to increase and maintain public confidence in the electoral system, suggesting that the audit should be regular and ongoing rather than episodic. The study is being conducted in three carefully arranged phases, each successive one building upon prior work and involving discussion and consultation with local election officials. This paper will report on the second phase, involving the secondary analysis of ballots from the 2006 election to simulate the results of different sampling approaches to the auditing function. ","Previous election reforms designed to increase turnout have often made voting more convenient for frequent voters without significantly increasing turnout among infrequent voters. A recent innovation---Election Day vote centers---provides an alternative means of motivating electoral participation among infrequent voters. Election Day vote centers are non-precinct based locations for voting on Election Day. The sites are fewer in number than precinct-voting stations, centrally located to major population centers (rather than distributed among many residential locations), and rely on county-wide voter registration databases accessed by electronic voting machines. Voters in the voting jurisdiction (usually a county) are provided ballots appropriate to their voter registration address. ","We present the results of two nationwide surveys in the United States, one from the 2006 general election and one from Super Tuesday 2008. The surveys involved large national samples to allow measurement of what are low-probability events. We measure the incidence of problems relating to voter registrations, voting equipment difficulties, lines, polling place operations, and poll workers, as well as measures of the reasons for nonvoting (a variant of the Current Population Survey question). No problem affects more than a small percent of voters, but about 1 in 5 voters experience at least one problem. Among other findings: administration of voter identification differs with race of respondent; use of electronic voting equipment and requirement for voter identification leads to longer lines; and a small percent express dissatisfaction with poll workers and polling place operations. ","This paper proposes a local post-stratification approach to the dual system estimation for evaluating coverage in a Census. The local stratification is achieved by nonparametric estimators for the Census enumeration and correct enumeration probabilities, which may be employed as an alternative to the existing post-stratification in the Census dual system estimation. We propose an imputation based estimator for general nonparametric regression with missing values, which can handle both continuous and categorical covariates and is used for model diagnostic and checking. Our theoretical analysis and simulation studies indicate attractive properties of the imputation based estimator. The proposed approach is applied to analyze research data files from the 2000 Census dual system surveys. ","We consider two important issues in the hierarchical Bayesian inference, namely the selection of prior for the variance component and highly accurate simple approximations to various posterior moments involved. We propose a prior distribution for the variance component that yields a positive estimator having good frequentist properties when posterior mode is considered as an estimator. We discuss several approximate hierarchical Bayes methods which have several advantages over the complicated Monte Carlo Markov Chain methods. By applying our methodology on data related to the SAIPE project of the U.S. Census Bureau, we demonstrate the utility and accuracy of our approach in solving real life small area estimation problem relative to some other existing methods. ","We propose a Bayesian penalized spline prediction (BPSP) estimator for a finite population proportion in an unequal probability sampling setting. This new method allows the probabilities of inclusion to be directly incorporated into the estimation of a population proportion, using a probit regression of the binary outcome on the penalized spline of the inclusion probabilities. The posterior predictive distribution of the population proportion is obtained using Gibbs sampling. Simulation studies and a real example in tax auditing show that the BPSP estimator is more efficient, and its 95% credible interval provides better coverage with shorter average width than the Horvitz-Thompson (HT) and generalized regression (GR) estimators. Compared to parametric model-based prediction estimators, the BPSP estimators achieve robustness to model misspecification and influential points in the sample. ","Several statistical agencies use, or are considering the use of, multiple imputation to limit the risk of disclosing respondents' identities or sensitive attributes in partially synthetic public use data files. Methods for obtaining inferences for scalar and multivariate estimates have been developed for partially synthetic datasets, as well as several other types of multiply-imputed datasets. Methods for conducting model selection with multiply-imputed data are very limited, and this paper moves toward filling this need. As a first step, a simple case is considered in the context of partially synthetic data where the analyst has some knowledge about the imputation procedure. In this scenario, a Bayesian approach to implementing model selection with partially synthetic datasets and a Bayes factor approximation similar to the BIC are derived and illustrated. ","In the statistical disclosure limitation literature work has been done on summary and release of contingency table data in terms of evaluating the bounds on cell entries in k-way contingency tables given sets of marginal totals. Narrow bounds, especially for cells with low counts, could pose a privacy risk. The focus of this paper is measuring disclosure risk when tables of rates, that is tables with conditional probabilities, are released. We derive the closed-form solutions for the linear relaxation bounds on cell counts given full and partial conditional probabilities. We also compute the corresponding sharp integer bounds via integer programming and show that there can be large differences in the width of these bounds, suggesting that the use of linear relaxation is often an unacceptable shortcut to estimating the sharp bounds and disclosure risk for the tables of rates. ","To assess transient balance-control disturbances in astronauts after flight, their ability to maintain a stable upright stance was measured under several challenging stimuli using a moving-base platform. Performance was quantified by \"equilibrium\" scores (EQs), transformed maximum postural sway angles scaled from 0 to 100, where 100 represents perfect control (no sway) and 0 represents loss of balance (no meaningful sway angle is observed). By comparing post- to pre-flight EQs for astronauts vs. controls, we built a classifier to decide whether an astronaut has recovered. Future diagnostic performance depends on both the sampling distribution of the classifier and the distribution of its input data. Taking this into consideration, we constructed a predictive ROC by simulation, after modeling P(EQ = 0) in terms of a latent EQ-like beta-distributed random variable with random effects.   ","Pattern mixture models (PMM) stratify the dataset by dropout patterns and model the repeated measures within each pattern. We propose a hierarchical PMM (HPMM) that incorporates not only dropout patterns but also various dropout reasons. Participants are classified into random and nonrandom dropout groups and the dataset is stratified according to dropout patterns within each group. Since the completers' potential dropout reasons are unobserved, an ad-hoc latent variable model is proposed to classify them into the dropout groups.  Since classification of dropout reason is subjective, a simulation study is conducted to examine the potential bias from group misclassification. Estimates from the ad-hoc method are compared to MLEs for the HPMM. These methods are also compared to the standard PMM and a selection model assuming ignorable dropouts. ","Capture-recapture models have long been used to give biologists information about population dynamics. In the past 10--20 years there has been a proliferation of capture-recapture methods based primarily around increasingly complex data. We show how thinking of the capture-recapture experiment in terms of a missing data problem can (i) help to unify the abundance of models into a common framework and (ii) makes estimation of models that describe complex population dynamics more accessible to biologists. ","Missing data are common in large-scale surveys, arising mainly due to nonresponse in cross-sectional studies. Results are biased when missing data are ignored at the analysis stage. The present study aims to determine the bias associated when missingness is ignored and to investigate whether or not multiple imputation technique is a possible solution to address the issue of bias. The objective of the study will be achieved using the Public Use Micro Data File of Joint Canada/U.S. Survey of Health (JCUSH). JCUSH is a cross-sectional survey, which started collecting data in November 2002 and ended March 2003. The final sample contains 8,866 participants: 3,505 Canadian and 5,183 American participants. The bias will be tested using (i) available cases (only complete information), (ii) complete cases only (removing incomplete information), and (iii) multiple imputation method. ","In a longitudinal study of biomarker data collected during a hospital stay, observations may be missing due to administrative reasons, the death of the subject, or the subject's discharge from the hospital, resulting in nonignorable missing data. Standard likelihood-based methods for the analysis of longitudinal data (e.g., mixed model) do not have a mechanism to account for the different reasons for missingness. Rather than specifying a full likelihood function for the observed and missing data, we have proposed a weighted pseudo likelihood method. Using this method, a model can be built based on available data by accounting for the unobserved data via weights treated as nuisance parameters in the model. The performance of the proposed method is compared with a number of standard methods via simulation, and the method is illustrated using an example from the GenIMS study. ","Respondent driven sampling (RDS) is a Markov chain sampling scheme that can be compared to Markov chain Monte Carlo. Starting values, transition kernels, communication and convergence are discussed for RDS. RDS was used to sample HIV at-risk populations in Los Angeles in the SATHCAP study. We discuss some of the apparent lapses of RDS in this population in Los Angeles. In particular, the RDS sampling scheme may not be stationary, and for this particular survey study participants were heavily very low income, unemployed and homeless. An explanation is given why HIV can transmit in Markov fashion through a given population while RDS may not. We argue that prior to the start of an RDS trial it is necessary to prove that the stationary distribution of the sampling scheme is the intended population. ","Traditional landline random-digit-dialing (RDD) surveys only include landline telephone numbers, however, the number of households in the U.S. that only have cellular telephone service has grown rapidly in the past few years.  One approach to improve the coverage of RDD surveys is to use a dual frame design whereby a sample of cellular telephone numbers is also drawn and a screening instrument is used to identify individuals that only have cellular telephone service.  An important issue for this type of sample design is the development of a weighting methodology, especially for state or sub-state surveys. ","The National Immunization Survey (NIS)---a nationwide, list-assisted RDD survey conducted by the NORC for the Centers for Disease Control and Prevention---monitors vaccination rates of children between the ages of 19 and 35 months. As in any RDD survey, non-landline telephone (i.e., no-phone and cell-only) households are not sampled in the NIS. To compensate for this noncoverage, a special adjustment (Keeter, 1995) is applied, in which households with an interruption in landline service are used to represent nonlandline households. With the increasing proportion of cell-only households among the nonlandline households, this adjustment may no longer be very effective. Also, the adjustment introduces a considerable variation in sampling weights. The paper evaluates the impact of this adjustment on bias and variance to assess the effectiveness and to identify an optimal level of adjustment. ","Previous studies have shown that individuals with access to a cell phone only are different from those with access to a landline telephone in terms of demographics and important outcomes. This makes it important to cover the cell phone-only group for many different types of surveys. However, random digit dialing (RDD) telephone surveys have traditionally included only those with a landline. One way to cover this group is through a cell phone RDD survey. An alternative is through a mail survey. The purpose of this paper is to examine the success of using a mail survey to generate national estimates that include the cell phone-only population. In this paper, we first study the demographic characteristics of cell phone-only HINTS respondents and then compare the health-related measures between cell phone-only respondents and those with landlines.  ","It is well documented that nonresponse and noncoverage rates have increased steadily in random digit dial (RDD) surveys. As these rates continue to rise, there are increasing challenges to reducing bias and increasing operational efficiency. This paper discusses several methods to consider when conducting RDD surveys. We also discuss the benefits and shortcomings of the approaches - shortcomings if it is decided not to implement a particular procedure. For example, there is a growing concern with nonresponse bias; what is the impact of by-passing the household nonresponse adjustment? We also consider the impact of cell phone only households, purging nonresidential numbers, mailings incentives, subsampling of cases prior to refusal conversion, level of effort, and other uses of auxiliary data.  ","The size of the wireless-telephone-only population has increased in recent years, raising concerns about the accuracy of RDD-based telephone surveys. Previous research on the impact of wireless-only on population estimates has found mixed results. In 2007, the National Immunization Survey (NIS) conducted a pilot study to determine the impact of wireless-only households on childhood vaccination coverage estimates. The pilot survey collected vaccination information from 99 wireless telephone users in Illinois. This paper compares vaccination coverage estimates of children in landline households, wireless-only households, and combined. We discuss the implications of these results for RDD surveys and, specifically, for the NIS.  Given the low response rate and small sample size of this study, the results presented here should be considered speculative, rather than conclusive, evidence. ","This study shows a tailored RDD survey design by using two types of data available regardless of response status. The first coming from survey operations exhibit the history of all calls made to each sample and the indicators of survey design features, such as incentive provision. The second is prepared by linking geographic identifiers of all sample cases to the external data. As regarding survey response behaviors as a stochastic process, we fit models to predict how response behaviors change given hypothetical calling schedules and design features. The model is applied to any sample with the same set of variables, and the response behavior of a new sample can be predicted before fielding the survey. Based on the expected response behavior, the design features and calling schedules may be tailored for each case so as to maximize positive response behavior for fixed costs. ","RDD studies are facing growing challenges, including eroding rates of coverage and response. Moreover, when the study population includes rare subgroups, surveys can require significant resources for screening. It is estimated that about 18% of New York adults are current smokers, where per CDC a current smoker has smoked at least 100 cigarettes and currently smokes. In order to increase the rate of locating smokers in New York, a segmentation technique was applied to prior survey data to identify high smoking areas. Subsequently, telephone exchanges associated with such areas were targeted to increase the number of interviews with smokers. The employed disproportionate stratified sample design was implemented while managing the anticipated variance inflation due to unequal selection probabilities. This paper provides an overview of this design along with a summary of key findings. ","In the Pre-Elementary Education Longitudinal Study (PEELS), imputation of item missing data was done using AutoImpute (AI) software, which uses semi-parametric modeling to form imputation classes. In this paper, we summarize PEELS experience with AI, investigate the bias aspect of the imputed data for the PEELS teacher questionnaire data, and study the variance estimation of imputed data using multiple imputation by AI. In the study of variance estimation, we look into the bias issue for the multiple imputation method and the performance of AI multiple imputation on domain estimation. ","The Quarterly Financial Report collects income and balance sheet data for most manufacturing corporations and for large mining, wholesale trade, and retail trade corporations. Historically, imputation for nonrespondent certainty cases consisted of carrying forward prior quarter data, and adjustments were made for non-respondent noncertainty cases by reweighting respondent data. Two problems with these historical methods are: a failure to adequately account for market changes, and inconsistent adjustment across companies for unit non-response. We conducted a simulation to test an alternative imputation method that uses cell means and ratios of current to prior quarter respondent data. This paper describes the design and implementation of this simulation and presents an analysis of the results. The proposed method yields estimates with smaller bias and comparable standard errors. ","Surveys frequently have missing values for some variables for some  units. Imputation is a widely used method in sample surveys as a  method of handling missing data problem. We provide an new imputation  procedure for various imputation models retaining many of the  desirable properties of model-based imputation estimation and hot-deck  imputation under fractional imputation. The main objective of this  procedure is to construct an easy-to-use data set for general purpose  estimation.   We provide an extension of fractional imputation methods to general  patterns of missing data via maximum likelihood calibration. ","The CFS is a quinquennial, nationwide survey that collects data on the shipment of goods within the United States. As part of the Economic Census, it is conducted jointly by the U.S. DOT's Research and Innovative Technology Administration, Bureau of Transportation Statistics, and the U.S. Census Bureau. A sample of the 2007 CFS completed questionnaires was selected for in-depth investigation of missing or bad responses to items. This research aims to assess existing item nonresponse, type of items most affected, and relationship of item nonresponse to the job title of the respondent. Results indicated that about 70% of respondents are in management occupations, transportation-related occupations had the lowest item nonresponse, items requiring consulting establishment records had higher item nonresponse, and item nonresponse varied by industry classification of establishment. ","Ignoring the missing data mechanism, especially when data are missing not at random (MNAR), can result in biased inference. Current methods use sensitivity analysis, via modelling the response mechanism, to assess the missing data mechanism. We proposes a method that does not require modeling the response mechanism, but rather compares parameter estimates of a measurement model fit to all of the data and to a selected subset of the data. Discrepancy in the parameter estimates of the two sets is assessed via confidence intervals, and is utilized as a tool to detect data that are not MCAR. Simulation studies and an application of the proposed method to a real data set to assess the methodology will be presented. ","Surveys have focused on the response rate as a key measure of data quality. As a result, survey research organizations have sought to maximize the response rate. However, nonresponse bias is a function of not only response rates, but also the differences between responders and nonresponders. New measures of survey data quality are needed. The fraction of missing information is one possible metric. Developed along with methods for analysis with missing data, this measure uses a model to quantify our uncertainty about the values we would impute for nonresponders. The fraction of missing information goes beyond the response rate by also involving the survey data. Therefore, this measure may be more useful for guiding survey research organizations toward collecting the highest quality data. Data from the National Survey of Family Growth are used to demonstrate the use of this metric. ","Policymakers use national surveys to paint a picture of the U.S. population along a variety of dimensions such as poverty status, receipt of program benefits, demographic characteristics and health insurance coverage. Inferences are drawn about need and eligibility for federal programs based on estimates produced by these surveys. Findings are presented from research that develops comparable measures of income, family structure and poverty across four surveys. It examines whether the same picture of the U.S. population is presented by these surveys. The four surveys are the Annual Social and Demographic Supplement to the Current Population Survey, the Medical Expenditure Panel Survey Household Component, the National Health Interview Survey, and the Survey of Income and Program Participation. ","Data on the value of people's assets and debts are particularly difficult to collect because respondents often cannot provide balances without consulting records, which may not be readily accessible. Moreover, the value of certain major assets, such as a home, property or business, is not typically available from any records. These factors are compounded by the sensitive nature of wealth holdings, resulting in high item nonresponse and substantial error in reported values. Some policy applications of wealth data---assessing program eligibility---require greater accuracy than the data are able to provide. This paper examines the measurement of wealth in four surveys: Survey of Income and Program Participation, Survey of Consumer Finances, Panel Study of Income Dynamics and the Health and Retirement Survey. Estimates are presented for the entire and policy relevant subpopulations. ","Researchers using survey data matched with administrative data benefit from the rich demographic and economic detail available from survey data combined with detailed programmatic data from administrative records. The research benefits of using these matched data are too numerous to mention, but there are also drawbacks which have received less systematic attention from researchers. We focus on survey data matched with administrative data from the Social Security Administration and address the strengths and weaknesses of each for program participation and benefits, disability and health information, earnings, and deferred compensation. We discuss the implications for decisions that researchers must make regarding the appropriate data source and definition for the concepts in question and draw some general conclusions about measurement issues associated with using matched data. ","Health insurance coverage in the U.S. and the characteristics of the uninsured population are of substantial importance because health coverage is related to individual health. Researchers have developed many explanations for why the national and state health insurance estimates from the Current Population Survey's Annual Social and Economic Supplement (CPS-ASCE) differ from state-initiated survey data collection activities, the National Health Interview Survey, the Medical Expenditure Panel Survey Household Component and the Survey of Income and Program Participation. We present results from a systematic comparison of health insurance estimates from these surveys to the CPS-ASCE using bivariate and multivariate techniques to see where the largest pockets of difference among these surveys is coming from. ","While most analytic applications using public data files are well done, there is potential for misuse and misinterpretation due to the complexity of the data base. Most large national surveys are designed to meet multiple analytic objectives yet, for ease of use and consistency of results, a single sample weight is provided to the public for design based estimation. Issues of use and interpretation of the design based estimates can arise due to misunderstanding of the target population, design efficiencies versus inefficiencies for certain characteristics, item non-response, and other factors. This presentation provides examples from the NHANES data where the sample weight is misused or misinterpreted in published articles. Examples include applications to specific population characteristics (such as income), subnational estimates, and analyses where item nonresponse is a factor. ","The National Health and Nutrition Examination Survey (NHANES) is one of a series of health-related surveys that have furnished a rich source of national data for over 50 years. A unique feature of NHANES is the complete medical examination carried out in Mobile Examination Centers (MECs) that travel to survey locations (mostly individual counties) in the country. Thus the survey requires extensive cooperation of sampled counties to locate and place the MECs in appropriate sites. In addition, NHANES has an extensive outreach program, including local media coverage and contacts with local organizations, to reach as many sampled persons as possible. To minimize the risk of county-level refusal, we consider the question of whether counties can be rewarded for their cooperation by providing them with selected county-level estimates. We sketch a general approach to do so, combining approaches. ","The objective of this paper is to describe the analytic challenges of data files obtained by linking air quality data from the EPA with health data from the NHANES. The combination of health data with routinely monitored environmental exposure data has the potential to enhance our understanding of environmental impacts on health. However, air monitors are placed for regulatory not scientific purposes and may not correspond to areas sampled in national surveys. As a result, between 20% and 70% of the NHANES respondents may be linked, depending on the pollutant, the spatial and temporal criteria for exposure assignment, and the location of the NHANES sample units. The demographic profile of the linked records differs from that of the full NHANES sample. ","Concentrations of chemical compounds, such as pesticides, are found in various environmental and biological media. Some pesticide concentrations are reported below the analytical limit of detection (LOD), even though the pesticide may be present in small, but undetectable amounts. Concentrations reported below LOD are referred to as left-censored. Maximum likelihood estimation (MLE) can incorporate information regarding the proportion of the dataset that is reported below the LOD to fit various probability distributions to such left-censored data. This paper provides a comparison of fitting the lognormal and Johnson's SB distributions to pesticide concentrations obtained from NHANES biomonitoring data. The Office of Pesticide Programs will continue to explore and analyze NHANES biomonitoring data to help inform its pesticide risk assessments in the future. ","In survey sampling practice, analysis weights generally incorporate the initial sampling weights (the inverse of the selection probabilities) and adjustments for nonresponse and to match external totals (post-stratification or raking). Variation in the analysis weights may be by design (for example, oversampling and undersampling specific populations) or be unexpected (low response rates in subgroups). Extreme variation in these weights can result in large sampling variances and a few extreme weights can offset the precision gained from an otherwise well-designed and executed survey. Various methods are in use for developing trimming levels. The purpose of this roundtable is to discuss the current methods being used, new methods (Elliot and Little 2000, Liu et al. 2004) and issues in trimming sampling weights and the use of trimming methods in practice. ","The Health Information National Trends Survey (HINTS) provides health communications researchers with ongoing U.S. population estimates on cancer-related knowledge, attitudes, and behavior. Due to decreasing telephone survey response rates, the most recent HINTS is a mixed-mode data collection with dual sampling frames: an RDD telephone survey combined with a mail questionnaire sent to a sample of addresses from a national listing provided by the U.S. Postal Service. Data collection from over 7,000 households was completed in early 2008. There are numerous open-ended questions in the telephone mode where interviewers classify responses into fixed categories. The mail survey provides these categories to respondents in a closed-ended format. This presentation will report on the extent of mode differences found in the responses to these HINTS measures. ","Studies on mixed-mode survey designs and mode effects so far have produced mixed findings. Such findings have not been analyzed in the context of special populations such as US veterans. This paper aims to contribute to the literature by examining the mode effects in three recent surveys of veteran populations administered on paper and online. The three surveys are: the US Army MWR Leisure Needs Survey, Survey of Veteran Satisfaction with the VA Home Loan Guaranty Process, and the 2008 Veterans Burial Benefits Survey. Study results indicate that the mail survey mode is largely preferred over the web, with some variation in mode preference across surveys. The analysis examines survey response rates, respondent profiles and demographics, and studies key statistical estimates for mail and web modes. Chi Square tests, t- tests, and logistical regression were used to compare survey modes. ","This paper reports results of a study to improve understanding of household coverage on the Cheyenne River Indian Reservation in the 2006 Census Test. Matched data sources from 31 reservation households are used to identify and explore types and sources of coverage error to suggest improvements to enumeration methods, not to estimate coverage rates. Matched data include: 1)the actual Update/Enumerate household roster; 2) the Census Coverage Measurement (CCM) household roster collected 3 months later; 3) the CCM interview audiotape; and 4) an immediate ad-hoc debriefing to resolve any coverage anomalies. We identify types of coverage errors (e.g., omissions, erroneous enumerations); types of persons missed; and sources of error (e.g. mobility, living situations, cultural differences, non-city addresses, questions, recall issues). Suggestions are made to improve reservation enumeration. ","This paper presents a method using administrative records and models to increase the accuracy and efficiency of the 2008 Census Dress Rehearsal Coverage Follow-up operation.  As part of the 2008 Census Dress Rehearsal, some households will receive a follow-up interview to reduce within household undercoverage.  This paper presents one statistical method used to identify households that may have incorrectly enumerated the number of people in the household.  Using models built from Census 2000 and administrative records, the expected number of undercounted individuals within 2008 Census Dress Rehearsal households was calculated. ","The 2007 National Census Test provided an opportunity to study the response results of two Spanish/English bilingual questionnaire designs in areas that contained a relatively heavy concentration of Spanish-speaking people with a need for English language assistance. Results from the 2005 National Census Test showed that while the bilingual form significantly increased self-response rates nationally, it yielded higher nonresponse rates for the household items compared to the English-only form. The 2007 National Census Test was conducted as a follow-up to the 2005 Test to determine if the bilingual form item nonresponse issues could be resolved through improved forms design and updated content. This paper compares the two 2007 bilingual form designs with variations in cover letter placement to determine the better design and then compares that design to the English-only form. ","The NSDUH is an annual national survey of substance use and mental health measures in the U.S. civilian population aged 12 or older. We show that we were able to reduce the number of income questions in the 2006 NSDUH, thereby reducing respondent burden, without significantly impacting estimates of reported income. In this survey, 5.7% of the 67,802 respondents received the reduced set of questions, and analyses showed an impact in only one case. A question on Social Security income was eliminated, and as a result, it appeared to be confused with Supplemental Security Income, which was retained. A decision was made to reintroduce questions about Social Security in later surveys. Simulated imputation analyses showed that the new questions would have little impact on imputed estimates, and timing data analyses showed a 24% reduction in median time to complete the reduced set of questions. ","Statistics Canada uses the Goods and Services Tax (GST) data as auxiliary data in several sub-annual surveys. Given that GST remittance can be done on an annual, quarterly, monthly or on a more frequent basis, the GST data must be calendarized.  Calendarization means that the data have to be standardized so that they refer to a common reporting period.  The calendarization system, which was built and implemented in 2002 (Brodeur and Pierre, 2003), is now being revised in order to improve the quality of the data by solving some of the issues that were raised by the users. This paper will discuss these issues and the solutions to be implemented. ","One approach for estimation in a longitudinal panel survey is to use marginal generalized least squares with panel totals. For indicator variables, the sum of the totals for the indicators often is constrained to equal a known total, such as the population size. One approach to satisfying the constraint is to use the same weighting matrix for each marginal regression. An alternative approach is to use a direct estimator of the covariance matrix for the weights for each marginal regression and ratio adjusting to match the constraint. The second approach is a linear combination of estimators of the best linear unbiased estimators, but is more computational intensive than the first approach. We evaluate the loss in efficiency from using the same weight matrix compared to direct covariance estimators in the context of the National Resources Inventory design. ","This research addresses the issue of linking baseline and followup surveys in longitudinal studies. Social Security Number (SSN) is commonly used as a matching variable between survey waves, but reluctance to provide this information reduces matches. Optical scanning errors can also produce mismatches between otherwise matching surveys. Two matching methods were developed for a smoking survey that had no other identifying information except SSN. The variable matching method was explored using a similar smoking survey on a parallel population. The benefit of these matching methods will be discussed in terms of sampling precision and bias reduction in sparse domains.  ","The Census Bureau is carrying out research with the intent of improving the housing unit-based method for estimating population totals. One approach is to use the Decennial Census as the baseline for measures and update them annually using American Community Survey data to estimate their change since the census year. This study looks at the possibility of using an Empirical Bayes approach to produce county estimates of change with smaller variances than direct county estimates. Results from national and state models are compared and the problem of modeling states with few counties is considered. Data from the 1990 and 2000 long form samples are used to represent ACS and data from the 1990 and 2000 short form are used as independent variables in the models. In the actual application, sources for the latter would be the Master Address file and Administrative Records. ","This paper discusses issues with the estimation of temporal change in the presence of measurement error - a problem commonly referred to as \"regression toward the mean\". We propose the SIMEX method as a technique for dealing with measurement error in this case and present the results of applying the method in a simulation study.  ","Recent methodology on combination of multiple-survey data through generalized regression is adapted to certain variants of double sampling arising in practice. A computationally simple calibration scheme that gives rise to efficient generalized regression estimators for characteristics surveyed in the second-phase sample is investigated within a framework of optimal regression estimation. This, one-step, calibration scheme makes efficient use of all available auxiliary information in the first-phase and second-phase samples and greatly facilitates variance estimation. ","An outcome-dependent enriched sample results from adding a random sample to a stratified sample, where the stratification is based on the outcome. Consider binary outcomes with probability determined by predictors according to a given model. This study applies the profile, the partial, and the weighted likelihood methods to estimate model parameters from an enriched sample without imposing any parametric forms for the distribution of predictor variables. Although the three methods all consistently estimate model parameters, estimators given by the partial likelihood method have variance dramatically larger than the other two. Our simulation results indicate the profile, and the weighted likelihood methods perform well for moderately large sample sizes. Guidelines for the optimum allocation of sample sizes are also given for this enriched sampling scheme. ","All statistical agencies face the dilemma of how to provide microdata access to scientists while at the same time guaranteeing the confidentiality of data providers. Traditional access modalities have included public use files, licensing, and increasingly, Research Data Centers (RDCs). However, these approaches have come under increasing criticism. As a result, new innovations have recently emerged -- notably the use of multiple imputation techniques to create public use files, and technological solutions to provide remote access. This paper describes the lessons learned from the German experience, from an international workshop that was held in Nurnberg Germany and from a project proposal for the 7. EU Framework Programme. It summarizes the current best practices, together with the implications for current and future research access to microdata. ","A national tax agency's data can be leveraged through proper identification, capture, processing, and integration for statistical research. Access and budgetary constraints can be transcended with a partnership of analysts. Important questions include what fosters entrepreneurship; why the nonprofit sector's increasingly significant; outsourcing and independent contractors; the effect of true effective tax rates on economic performance. Barriers to research include public and private researchers are inadequately aware of the roles each need to play; budgets and legal authorizations limit public analysts on access and data quality options; private researchers often do not know or appreciate these constraints and the vast data potential, including its limitations. Proposed measures might serve the research community's needs outside the historical zero sum game of access and barriers. ","The NORC Data Enclave, developed by NORC in conjunction with the National Institute of Standards and Technology, provides remote and on-site researcher access to confidential micro data. It combines elements from the computing and social sciences, resulting in a secure remote data access platform and high-level technical security protocols certified by NIST. Other protocols include: on-site researcher training; a review process and legal agreements to ensure that only authorized researchers from approved institutions access data; audit logs and audit trails to monitor research behavior during data access; and full disclosure review of statistical results before they are permitted to leave the secure environment. This paper summarizes some of the practical lessons learned over the past year and identifies next steps that can be applied to similar approaches. ","Research Data Centers (RDCs) have been the poor relations of data access. Recently, however, developments in technology have altered the landscape. Whilst effective anonymization becomes more difficult, RDCs are now able to surmount many of their drawbacks and make more use of their advantages. Hence, there has been a resurgence in RDCs and analysis of the most confidential microdata. However, this has raised new issues in terms of disclosure control of outputs, transparency of access, international data sharing and so on. This paper reviews developments and provides an overview of providing access to confidential microdata, concentrating on developments in RDCs and \"data enclaves.\" It looks at the data access \"spectrum,\" reviews what we know about managing RDCs, highlights known problems and unresolved issues, and discusses some of the practical barriers to improved data access. ","This session will honor Mollie Orshansky, who developed the federal measure of poverty. In July 1963 and January 1965, Ms. Orshansky, a statistician for the Social Security Administration published two articles presenting a set of poverty thresholds for identifying families and persons in poverty, and analyzing the poverty population identified by these thresholds. Her poverty thresholds were adopted by the Office of Economic Opportunity on a quasi-official basis in May 1965 then designated as the Federal Government's official statistical definition of poverty by the Bureau of the Budget in 1969. The multigenerational panel will focus on Ms. Orshansky's pioneering efforts in methodology, public policy, federal and other poverty programs, and her leadership and mentoring skills that inspired younger generations in research on underserved populations. ","The paper addresses small-area estimation problems when the response is bivariate binary, and some of the covariates may be partially missing. Hierarchical Bayesian models which accommodate this missingness are considered. Estimators of small-area means, along with the associated posterior standard errors and posterior correlations, are provided. The method is applied to the analysis of a real data set, and the superiority of the hierarchical Bayes estimators over the direct estimators is established. ","Calibration can be used to adjust for unit nonresponse when the model variables on which the response/nonresponse mechanism depends do not coincide with the benchmark variables in the calibration equation. As a result, model-variable values need only known for the respondents. This allows the treatment of what is usually considered nonignorable nonresponse. Although one can invoke either quasi-randomization or prediction-model-based theory to justify the calibration, both frameworks rely on unverifiable model assumptions, and both require large sample to produce nearly unbiased estimators even when those assumptions hold. We will explore these issues theoretically and with a small empirical study. ","In this talk we utilize relationships between the population, the sample and the sample-complement distributions developed previously. These relationships allow expressing the distribution of the missing target outcomes as a function of the distribution under full response and the response process. The missing outcome distribution is used for imputing the missing values and for estimating the population mean of the outcomes. The proposed approach is extended to situations where the population means of the covariates are known, but the covariates themselves are unknown for the nonresponding units. In such situations the response process for the covariates cannot be ignored even if the outcome nonresponse given the covariates is missing at random (MAR), requiring a proper imputation of the missing covariates. We illustrate the proposed approach using simulations and a real data set. ","The goal of Graduated Drivers Licensing programs is to provide teenage  drivers the ability to gain driving experience while limiting their exposure to the most dangerous driving situations. The examination of these programs has been conducted using crash data and with various statistical techniques leading to very different estimates for GDL effectiveness. Failure to account for time and seasonal effects can result in overestimation of this public health intervention. This study uses data from Iowa as an example to demonstrate how various methods can have very different results. Time series models tend to provide the best estimate when compared to general linear models, relative risks, and rate ratios. ","Transportation-related fatality risk is a function of many interacting human, vehicle, and environmental factors. Statistically valid analysis of such data is challenged both by the complexity of plausible structural models relating fatality rates to explanatory variables and by uncertainty about the probability distribution of the data. But, fortunately, generalized linear modeling and maximum quasi-likelihood estimation together provide an extraordinarily effective set of statistical tools for the analysis of such data. The goal of this talk is to illustrate and promote application of these tools to fatality risk analysis. ","The Omnibus Household Survey is a random-digit-dial survey of slightly more than 1,000 non-institutionalized residents in telephone households in the U.S. Information is collected to monitor expectations of and satisfaction with the transportation system. Since the response rate for the survey is approximately 50%, there has been considerable interest in gauging the nonresponse bias in this survey. There are mainly three options for assessing nonresponse bias. The first option, used frequently, and hence widely accepted, is an intensive follow-up of nonrespondents. This was not an option here because of cost. The second option is external validation -- benchmarking against some other data, and this was done here for commonly used demographic variables.  Another option is to use data on survey operations. This presentation discusses results from using call history data to estimate nonresp ","Signalized intersections are among the most dangerous locations on a highway network. There are more than 2.8 million intersection-related crashes in year 2000. In this paper we propose a Bayesian hierarchical approach to model accidents at signalized intersections. The data include 197 signalized intersections in the State of Florida. The proposed model incorporates the effects of geometric design features, traffic control features, and traffic characteristics. Furthermore, we argue that signalized intersections along a certain corridor will affect each other, especially for those in close proximity. We incorporate this spatial correlation into the model by using a conditional autoregressive prior for an intersection specific random effect. The results of this study will benefit intersection geometric design and traffic management. ","Many highway agencies face the problem of contract time delays in  transportation construction projects. Such delays can have adverse  effects on the owner, contractor and the facility users. Time delay may be caused by any party to the contract; its causes may include changes in engineering design and the factors of production, and certain characteristics of the contract bidding process and project environment. Focusing on the latter, in this study a methodology is developed to investigate the likelihood and extent of time delay in highway construction projects. Logit and hazard-based duration models are estimated using data from a state agency, and including explanatory factors relating to project type, project environment and contract bidding characteristics. The demonstrated methodology can help agencies identify the factors that contribute to time delays and to better predict them. ","Two phase designs are useful for increasing estimation efficiency for reasons including deep stratification, improved nonresponse adjustment and calibration controls, and for integrating survey designs by using the first phase as a master sample. Despite these benefits, such designs are not in wide use due to difficulty in variance estimation unlike the multi-stage single phase designs where the WRPSU assumption is often invoked for simplified variance estimation. It is shown that by a simple modification of two phase cluster designs in which the second phase sampling within each first phase cluster/PSU is performed pps independently across PSUs conditional on the first phase sample, the usual simplified variance estimation remains applicable even if the second phase variance is not estimable. The size measures for the pps correspond to sampling rates determined at the second phase.  ","Often surveys employ weight adjustment procedures to compensate for unit non-response. Cell collapsing is typically performed when the respondent counts within individual unit nonresponse adjustment cells are small. Additionally, surveys may impose a collapsing criteria based on the magnitude of the adjustment factor. For replicate-based variance computation, there are few guidelines for which weighting cells should be used for the replication process (i.e., whether to use the weighting cells from the full sample in all replicates or to determine weighting cells individually in each replicate sample). We address how weighting cell collapsing should be handled for the simple delete-a-group jackknife replicate variance estimation procedure when using the \"quasi-randomization\" estimator to account for unit nonresponse via a simulation study modeled after a typical business survey. ","This paper provides the theoretical framework for estimating the variance of the difference in two years' totals estimated under the stratified Bernoulli sample design. We provide a design-unbiased estimator that accounts for two practical problems: a large overlap of sample units between two years' samples and \"stratum jumpers,\" which are population and sample units that shift across strata from one year to another. Both problems affect estimating the covariance term in the variance of the difference. The estimator is applied to data from the Statistics of Income Division's individual tax return sample. Na\u00efve variance estimates using only the separate years' variances are compared to show the effect of ignoring the estimated covariance. ","The American Community Survey (ACS) releases data every year including a Public Use Microdata Sample (PUMS) File.  PUMS allows data users to tabulate their own characteristic estimates by publishing responses from individual households and persons.  All identifying information is removed from PUMS to ensure confidentiality. There are two methods available for users to calculate standard errors: generalized variances (design factors) and replicate weights (direct standard errors). Standard errors calculated using replicate weights are more accurate than those using design factors.  Using that as a benchmark, this paper investigates whether the design factors for counts, means and medians are an acceptable substitute for their direct standard errors. ","A common issue in large-scale complex surveys is the detection of outliers in the data. Such outliers can be caused by frame imperfections or by errors during data collection. The existence of the outliers in a survey dataset can cause bias in the inference for population quantities of interest. We describe a procedure to identify suspicious points in survey data, taking into account the structure of the finite population. We compute the distance from each point in the dataset to the center of a subpopulation that it belongs to, and define a measure of point outlyingness as the tail probability on the resulting distance distribution function. We also extend the procedure to periodically updated surveys when detection rules are built based on historical data and applied to recently collected data. We apply the proposed approach to National Resources Inventory, a large-scale longitudinal complex survey. ","The quasi-likelihood estimating function (EF) properly weighted for survey data can be used to fit logit models. In practice, an asymmetric logit-Wald confidence interval (CI) is built due to skewness of estimates of low or high prevalence outcomes. However this method has instability in the variance estimate for moderate sample sizes and/or for low or high prevalence outcomes, leading to poor coverage properties. We propose an alternative based on the randomly recentered estimating equations (RREE) idea of Singh and Dochitoiu (2005). Replicate parameter estimates are created by equating the standardized EF to random values drawn from the N(0,I) distribution. They are used to compute new variance and interval estimates. Simulations are used to evaluate the performance of RREE relative to logit-Wald in terms of bias and variance of the variance estimator, length and coverage of the CI. ","Statistics Canada's General Social Survey is an annual cross-sectional household survey for which 20 complete cycles of data now exist. Recent cycles provide a final calibrated survey weight as well as sets of bootstrap weights for variance estimation. Analysts have requested bootstrap weights for earlier cycles, but after extensive research, we have determined that it is not feasible to construct them for the earlier cycles using current methods. However, detailed design information and final weights for each cycle do exist. We present a proposed method of creating \"approximate\" bootstrap weights for past cycles and evaluate them with a simulation study. Although this describes a solution to a specific problem, our method may be of wider use: the possibility of creating sets of pseudo-design based final bootstrap survey weights when only some limited information is available. ","The Behavioral Risk Factor Surveillance System (BRFSS) is a state-based telephone survey of the adult civilian non-institutionalized population residing in the United States. Consequently, the BRFSS final weights that are currently available in the public use data files are designed to produce unbiased estimates of health conditions by socio-demographic characteristics at the State level. In addition to State level BRFSS estimates, there is interest in the health status of adults residing in the 25 U.S. counties contiguous to the United States - Mexico border. The purpose of this paper is to apply an alternative approach for post-stratification by minimizing conditional global mean square error of BRFSS health estimates for adults residing in the 25 counties contiguous to the United States - Mexico border. ","In the Canadian Census, basic person and dwelling information is gathered on a 100% basis, and additional information is collected from a 20% sample of private households. Initial weights of approximately five are applied to the sampled population but are then calibrated independently for approximately 6,600 geographical regions through a multistep regression estimation process to produce the final weights. Within each region, auxiliary variables are discarded from the weighting process for being collinear, nearly collinear or causing outlier weights. The weighting process is evaluated by comparing the estimates based on the final weights with the 100% totals for all the auxiliary variables. In addition, biases in the census sample for the auxiliary variables are studied by comparing estimates based on the initial weights to the 100% totals at both the national and regional levels. ","Since its inception, the SDR has used the weighting class approach to compensate for nonresponse. This approach incorporates a small set of variables in defining the weighting classes even though additional variables may also contribute to bias reduction. Changing to the propensity model approach would help overcome this limitation. Under the propensity model approach, weighting adjustments would be based on propensity scores predicted by multivariate models. Using the 2006 SDR data, this paper compares the two weighting approaches in terms of the distribution of the weights and the weighted estimates of key SDR variables. We conclude that the propensity model approach may represent a viable alternative to the current weighting class approach. The propensity models may be further refined and future SDRs may consider adopting the new approach to nonresponse adjustments. ","The Kauffman Firm Survey (KFS) is a national-level longitudinal panel survey of new businesses, gathering data on the resources and processes to initiate and to sustain businesses. The sample is a stratified random sample of firms starting in 2004, with over-sampling of high technology firms. The purpose of this paper is to identify the characteristics of respondents for the first and second follow up surveys (baseline data collection was completed in 2005--2006) and describe the non-response adjustment methods. Both cross sectional weights and longitudinal weights were developed to accommodate analytical objectives. We will also discuss the response patterns status for these two weights. The non-response adjustments are based on models using Chi-squared Automatic Interaction Detector (CHAID) and logistic regressions. ","Multilevel modeling has become common in large scale assessments with multistage sampling and unequal probabilities of selection (Raudenbush, 2000, Koretz &amp; McCaffrey, 2001). Pseudo-likelihood estimation with partitioned weights produce asymptotically unbiased estimates of parameters in many applications (Pfeffermann, et al, 1998). However, a case often found in evaluations, longitudinal data of individuals nested within groups, has gotten little scrutiny. In this paper simulation studies will be used to illustrate the effect of informative weights with nonresponse corrections on parameter estimation in the nested longitudinal case with correlated individual-level effects. Continuous and categorical outcomes will be examined. ","Data from school survey samples are typically weighted to account for unequal probabilities of selection, and non-response at the student and school levels. Weight adjustments are usually performed in two stages based on weighting class adjustments and on post-stratification. Weighting class adjustments are based on weighting cells defined by school or classroom using student enrollment data. Post-stratification adjustments capitalize on external data available for post-strata defined by grade, race/ethnicity and gender. This paper compares alternative approaches to weight adjustments using simulations and school survey data. In forming adjustment cells, we consider the role of potential interactions between class/school and demographic characteristics. ","We begin by reviewing the key concepts in classical power and sample-size analysis for regular (frequentist) hypothesis testing. More deeply, we ask: How should such planning help convince us that a significant p-value indeed reflects a true research hypothesis? By using judgments about the probability that the null hypothesis is false, we apply Bayes' Theorem to assess the probability that a significant p-value will be a Type I error or a nonsignificant p-value will be a Type II error. We dub these the \"crucial\" Type I and II error rates, and we show that they can differ greatly from their classical counterparts. All ideas are illustrated by examining a small preliminary study that tested a very speculative treatment for atherosclerosis and became enthusiastically reported due to its significant p-value supporting efficacy. Unfortunately, the crucial Type I rate may have been over 85%. ","An internal pilot design provides an easy and low-cost path to a good sample size despite starting with inadequate information. For Gaussian data, an internal pilot uses an interim variance estimate to give a sample size adjustment (without interim data analysis). A brief introduction highlights the practical benefits and limitations of internal pilot designs. In contrast to many types of adaptive designs for sample size re-estimation, internal pilots come from a solid analytic foundation, which allows balanced control of sample size, statistical power, and cost. The real world introduces some barriers that can usually be overcome by simple strategies. Ready to use methods for large and small designs will be described and recommended for study planning, interim sample size adjustment, and final analysis. ","While collecting data on sexual orientation, sexual attraction and sexual behavior has become routine for some research purposes, such as studies of STDs, several studies, such as the Institute of Medicine report on lesbian health, have recommended broader data collection in order to enrich the analytical possibilities of a data set. Because sexuality and sexual behavior are often multifaceted, the specific measure of such attributes needs to be closely attuned to the research question. The panel will discuss when it is appropriate to collect data on sexual orientation or behavior, how to sample and contact potential respondents, what to ask, how to design a survey instrument, what mode of data collection is most effective, and what effect asking these questions has on response rates, drawing extensively on cognitive studies and experiences gained through state and national surveys. ","In randomized trials that require repeated visits, it is inevitable that some people will miss some visits. Data should be collected despite these protocol violations, but it is difficult to obtain immunogenicity or reactogenicity data for missed visits. Some sort of imputation is required if ITT analyses are to be done using all randomized subjects. Multiple imputation has been shown to be valid under a broad range of circumstances. CDC's current Anthrax Vaccine Trial presents challenges because of the ~2000 measurements per person and the ~200 subjects in each treatment arm. We employed a novel ordered monotone-blocks multiple-imputation method. The evaluation involved creating 200 datasets with realistic missing data patterns for 4 immunogenicity and 28 reactogenicity measures. Here we present the results from this evaluation of the validity of the multiple imputations in this trial. ","In an impact analysis using random assignment, researchers often deal with missing values in both the covariates and the outcome variables of regression models. Clearly rigorous methods are needed to impute missing values in the outcome variables to minimize the potential bias in impact assessments. When imputation is applied to covariates of the regression analyses, the effect of imputation is less clear on impact analyses. This paper assesses this effect, using a random assignment evaluation of the Growing America Through Entrepreneurship (GATE) program. Two outcome variables used in the original evaluation are modeled against a set of 10 covariates, a treatment indicator, and variables associated with the site of the evaluation. Impacts are assessed with different types of missingness in the covariates with values imputed using mean imputation and sequential hot deck. ","The German Institute for Employment Research (IAB) is using multiple imputation to generate a public use file of the IAB Establishment survey. The imputations serve to protect data confidentiality while enabling valid inferences for wide classes of estimates. In this talk, we describe some of our imputation strategies, focusing on the tradeoffs among data quality, disclosure risks, and computational feasibility. ","It is important to evaluate the efficacy of child restraints for protecting children of different sizes in crashes. Child injury status and parent-reported height and weight were obtained from a complex survey, and a logistic regression model is fitted. However, ignoring the misreporting and missing values in height and weight might lead to bias in model estimates. We propose to account for the missingness and reporting error using a two-stage multiple imputation approach. In the first stage, incomplete parent-reported height and weight are imputed. In the second stage, the true height and weight are imputed using a measurement error model established from ancillary data. Complex survey design is taken into account in both stages of imputation as well as the analysis model. ","Recently, there have been increases in the number of postdoctoral researchers (postdocs), the duration of postdoc appointments, and the number of non-US postdocs. Understanding the impact of these increases on the US research enterprise and employment markets requires information that is currently unavailable from survey data or other sources. The National Science Foundation (NSF) is engaged in a multiyear project to determine the feasibility of obtaining national-level postdoc statistics for US and non-US doctoral degree holders across all sectors that employ postdocs. A formidable project challenge is frame development and testing methodologies that obtain needed postdoc data from the nonacademic sector and for postdocs with doctorates from foreign universities. This paper will describe the alternative frame development approaches that NSF is exploring to gather postdoc information. ","Current national-level statistics for postdoctoral researchers in the United States maybe flawed due to coverage gaps in sampling frames. To begin to rectify this situation, the National Science Foundation is experimenting with methodologies to build a comprehensive sample frame of academic and nonacademic institutions where postdocs work. The methodologies are designed to minimize building an inadequate sample frame that adversely impacts coverage. The multi-step frame building process involves both qualitative and quantitative methods that include administrative list reviews and rapid turnaround surveys. Key to the effectiveness of this process is determining each list's contribution before merging it into a comprehensive sample frame. This paper will discuss the outcomes of the multistage process for the academic institutions component of the comprehensive sample frame for postdocs. ","The Commodity Flow Survey (CFS) is conducted approximately once every five years to provide data on the movement of goods in the United States. Information on the frame available for CFS was inadequate for determining shipper status or developing accurate measures of shipping activity for auxiliaries (establishments of large companies that provide support to the rest of the company). For the 2007 survey, steps were taken to address this issue. This paper will describe the problem and its effect on the 2002 CFS, discuss the new approach and its implications for the frame and subsequent sample design for 2007, and provide an evaluation of the approach. ","The National Science Foundation (NSF) has been collecting data on research and development since 1953. In 2004, the NSF embarked upon a series of activities designed to evaluate and renew this important survey. As a result, the entire structure of the survey is being changed. The survey will consist of five distinct parts each focused on different types of data. The five sections are aimed at collecting data on (1) R&amp;D paid for by the company, (2) R&amp;D management, (3) R&amp;D paid for by external parties, (4) R&amp;D employees, and (5) intellectual property, technology transfer and innovation issues. Different people within the business will need to answer different parts of the survey. This paper focuses on the development of the paper questionnaire and touches on the contact strategies needed to get the right section to the right person. ","Many in the media dubbed both 2005 and 2006 the \"Year of the Data Breach,\" given well-publicized breaches across the private sector, academia and government. The Office of Management and Budget (OMB) led federal efforts with the issuance of a key policy memorandum addressing data breach notification, unnecessary collection and retention of personally identifiable information (PII) (e.g., social security numbers), and a number of other privacy and security aspects. While government-wide in scope, the guidance understandably has a distinct effect on statistical agencies whose missions often involve collecting PII and whose success rests on maintaining the public's trust. This paper focuses on OMB requirements as they affect statistical agency programs, and suggests ways that agencies can move beyond simple compliance to a strategic approach to data stewardship. ","Upon the release of the Identify Theft Task Force's September 2006 memorandum, the Census Bureau recognized the need to strengthen its processes to address potential data breaches. Both the report of the Identify Theft Task Force and the draft OMB memorandum were used as the foundation for the enhanced policy. The bureau decided to use a widely accepted model of developing a risk score based on the likelihood of the event occurring and the impact of the event, an approach that was included in the OMB memorandum. The bureau incorporated an existing internal board into the revised process, the Data Stewardship Executive Policy Committee (DSEP). The DSEP reviewed the policy, and it was adopted on December 15, 2006. This paper will describe the Census Bureau's processes and its experiences since they were adopted. ","A longitudinal study with four rounds of data collection planned over a 5-year period experienced a data loss during field operations in the third and fourth rounds of data collection. Cases with data loss were evaluated for risk of disclosure. All cases with data loss were notified, and the level of potential risk was reported to each case. Those with a potential risk of disclosure were offered a year of credit monitoring. Cases with data loss were allowed to opt out of the data collection and and to have their data removed from the computer files. The disclosure risk analysis is described; the numbers of cases impacted are reported, along with the number of cases that opted out during the third and fourth rounds of data collection; and the participation of the impacted cases compared to the rest of the sample is compared in round three of the data collection. ","Estimating mortality in Iraq is unique in many ways. With the history of wars in the country, an authoritarian regime lasting for decades, the invasion, and the political winds, researchers face many challenges. Several studies have attempted to estimate mortality in Iraq for many reasons and the numbers could be as powerful as bombs taking the lives of many innocents! In this paper, we walk through the different stages of the survey cycle and outline the challenges associated with the task of estimating mortality. At the end of the paper, the reader would have to decide what to believe!  ","In 2004, the Lancet published results from a survey that estimated the death toll attributable to the conflict in Iraq until then at almost 100,000. Since that moment, the topic has been subject of a large debate among scientists. Besides this publication, four other surveys conducted in Iraq reported data on mortality due to the conflict. Although each one of these studies had its own specific objectives and methods, some clear patterns common to each of them can be identified. These common characteristics are quite distinct from those of surveys in other conflict settings such as the Sudanese region of Darfur and the Democratic Republic of Congo. This paper looks into the specificities of mortality surveys in Iraq, compares them with surveys conducted in other conflict settings and suggests alternatives in order to assess mortality in Iraq. ","The recent paper \"Violence-Related Mortality in Iraq, 2002--2006,\" published in the New England Journal of Medicine by the IFHS study group, focused mainly on the violence-related mortality and reported only on the unadjusted pre- and post-invasion nonviolence rates. Estimating the nonviolence deaths and the overall and cause specific excess deaths are equally important. In this paper we will analyze the IFHS mortality data with more emphasis on the nonviolence deaths and we will attempt to assess the post-invasion excess deaths. As for the violence-related deaths, adjustment methods will be used to attempt to account for under-reporting and other sources of bias in estimates of nonviolence deaths. Mortality rates by selected background characteristics will be presented and the limitations of the data and the methods used will be discussed. ","I consider the second Lancet survey of mortality in Iraq published in 2006. I give evidence of ethical violations against the survey's respondents including endangerment, privacy breeches and shortcomings in obtaining informed consent. Violations to minimal disclosure standards include non-disclosure of the survey's questionnaire, data-entry form, data matching anonymized interviewer IDs with households and sample design. I present evidence suggesting data fabrication and falsification that falls into nine broad categories. ","In this paper, we carry out an in-depth investigation of diagnostic measures for assessing the influence of observations and model misspecification in the presence of missing covariate data for generalized linear models. Our diagnostic measures include case-deletion measures and conditional residuals. We use the conditional residuals to construct goodness of fit statistics for testing possible misspecifications in model assumptions, including the sampling distribution. We develop specific strategies for incorporating missing data into goodness of fit statistics in order to increase the power of  detecting model misspecification. ","Many statistical problems can be formulated as discrete missing data problems (MDPs). Examples include change-point problems, capture and recapture models, sample survey with non-response, zero-inflated Poisson models, diagnostic tests, bioassay and so on. This paper proposes an exact non-iterative sampling algorithm to obtain iid samples from posterior distribution in discrete MDPs. The algorithm is essentially a conditional sampling, thus completely avoiding problems of convergence and slow convergence in iterative algorithms such as Markov chain Monte Carlo. The key idea is to first utilize the sampling-wise inverse Bayes formula to derive the conditional distribution of the missing data given the observed data, and then to draw iid samples from the complete-data posterior distribution. We apply the method to contingency tables with one supplemental margin for an HIV study. ","We develop a novel modeling strategy for analyzing data with repeated binary responses over time as well as time-dependent missing covariates. Covariates are assumed to be missing at random (MAR). We use the generalized linear mixed logistic regression model for the repeated binary responses and a joint model for time-dependent missing covariates using information from different sources. A new Bayesian method is developed to identify the importance of each covariates and the sensitivity of the specification of the missing covariates models is investigated. A Markov chain Monte Carlo algorithm is developed for computing the Bayesian estimates. A real plant dataset is used to motivate and illustrate the proposed methodology. ","From a Bayesian perspective, we propose a unified model for analyzing mixtures of continuous, ordinal and nominal repeated measures. The continuous measures are linked to latent variables for the ordinal and nominal measures via a multivariate normal model. MCMC inference is used with a step for sampling the restricted covariance structure. We will examine the application of this model in an incomplete data setting where our Bayesian implementation allows for simultaneous imputation and inference. Alternatively, separation of the imputer and analyst roles may be preferable in some situations. We compare our approach to various alternatives, including the general location model and switching regressions, in the context of a longitudinal study on functional outcomes of hospitalized older medical patients. ","Motivated by large survey data conducted by the U.S. Veterans Health Administration (VHA), we propose new methods for the analysis of longitudinal survey data using structural equations models (SEMs). A reparameterized version of the longitudinal SEM is developed to ensure model identifiability and facilitate efficient Bayesian posterior computation for large survey data. Several structural equations models are considered and compared via a variation of the deviance information criterion to investigate the importance of facility and covariate effects in the presence of missing data for the VHA study. We also propose an innovative model for the missing covariates for longitudinal survey data when no individual respondents can be traced over the survey period under investigation. A detailed analysis of three VHA all employee survey data is presented to illustrate the proposed methodology. ","Highly disproportional sample designs have large weights, which will introduce undesirable variability in statistical estimates. Weight trimming fixes a cutpoint weight and sets larger weights to this cutpoint value, reducing variability at the cost of introducing some bias. Previous work developed Bayesian \"weight smoothing\" models to produce general model-based weight trimming estimators of population statistics, but has been limited to the context of stratified and post-stratified sample designs. This paper extends the \"weight smoothing\" methodology to a more general class of complex sample design that include single or multistage cluster samples and/or strata that \"cross\" the weight strata. The methods are applied to linear regression models. We also discuss to use the variable selection approach to explore more flexible and robust \"weight smoothing\" models. ","Statistics of Income of IRS started a panel sample in tax year 1999. It was a stratified sample of individual returns, where stratum boundaries were based on the individual income. Because of the large number of population returns and the much skewed income distribution, sampling rates across strata were quite different. This poses a problem for 'stratum-jumper' returns that shift between strata in different years, as their yearly income change dramatically. This particularly affects returns that were selected with low sampling rates, thus being assigned higher base weights. If the income on a return grows such that it moves into a stratum with a lower probability of selection, then the associated original weight is no longer appropriate for cross-sectional estimation. In this paper, we review the ad hoc procedures and also develop more options for the treatment. ","This paper discusses research on alternatives to the ACS weighting methodology to adjust for non-response, which remains unchanged since 1996. The study universe is the 2006 ACS tabulation sample. Methods using the inverse of the propensity score as the adjustment factor, the mean inverse of the propensity score as the adjustment factor, and forming the adjustment cells using the propensity scores were investigated. The methods were tested on a dataset where a sample of respondents were recoded to non-respondents and compared to a dataset that only contained respondents by analyzing several population, household and housing unit estimates. The variances were analyzed in order to ensure that there is not an increase due to the alternative methodologies.  ","The objective of this research is to assess whether the use of independent population estimates produced by the Census Bureau's Population Estimates Program improves the estimates produced by the American Community Survey (ACS). Six alternatives to the current population controls were investigated by using controls either at higher levels of geography or with less demographic detail. An expected worst-case bias scenario for the population estimates is studied by applying the April 1, 2000 population estimates based on the 1990 Census as controls to the Census 2000 Supplementary Survey data. Various county-level demographic estimates were calculated for each of the seven methods. The county-level relative bias, variances, and relative mean square errors to the Census 2000 data were then calculated to compare the relative quality of the estimates for each method. ","This paper compares two weighting procedures for the Federal Human Capital Survey, a sample of nearly 440,000 federal employees stratified by agency and supervisory status to gauge perceptions of various workplace issues. The first weighting method involves a basic nonresponse adjustment using stratum information only. The second method utilizes extra demographic data on the frame known of both respondents and nonrespondents in a CHAID analysis to construct nonresponse cells, concluding with a raking step. This paper interprets results from a nonrespondent follow-up to explain possible mechanisms of nonresponse and compares means and variances of estimates in an effort to evaluate which method is more appropriate. ","In January 2008, the National Health Interview Survey released its first annual Paradata File, containing information about the data collection process for the 2006 NHIS. Analyses of paradata can be used to explore case characteristics and allow for a better understanding of NHIS respondents and the interviewer-respondent dynamic. This discussion will provide an overview of the 2006 NHIS Paradata file, including the scope of the cases included on the file and a summary of major variables related to interviewer strategies, measures of contactability and cooperation, measures of time, mode of interview, and reasons for partial interviews and break-offs. Applications of the data as a stand-alone file and as a file linked to the 2006 NHIS health data files will also be discussed. ","Paradata provide a unique opportunity to learn about interactions between interviewers and respondents in the National Health Interview Survey, conducted annually by the National Center for Health Statistics, Centers for Disease Control and Prevention. Interviewers record textual entries on a variety of topics including reasons for partial and break-off interviews, reasons for telephone interviews (given that the NHIS is primarily a face-to-face survey), and general information about the family's availability, cooperativeness, and circumstances surrounding participation. Using qualitative and quantitative analysis, this paper will examine textual entries recorded in the 2004--2007 NHIS. The paper will describe the process of analyzing textual data, present findings, and explain how the findings have led to changes in the collection of paradata and modifications to field procedures. ","Nonresponse is a growing concern to survey researchers. One strategy for reducing the effect of nonresponse is statistical adjustment of survey estimates. Methods available for nonresponse adjustment include post-stratification and ratio adjustment. Another approach starts with populating the sample frame with a set of auxiliary variables that are correlated with both the probability of response and the survey variables of interest. The recently released 2006 National Health Interview Survey (NHIS) paradata file contains information collected during the process of recruiting sample households to participate that could be potentially useful for nonresponse adjustment. Information collected includes indicators of respondent reluctance and strategies for recruiting sample members. The goal of this paper is to test hypotheses about correlations between paradata variables and survey outcomes. ","Measurement error arising from respondent-interviewer interactions during the interview has received considerable attention among survey researchers. Less understood are the effects on data quality of respondent concerns and reluctance expressed during initial doorstep interactions. While some suggest that doorstep concerns foretell a respondent's level of commitment to the interview, others suggest that stated concerns carry little intrinsic meaning. Using paradata (data about the collection process) and health data from the 2006 and 2007 NHIS, we explore the associations between respondent doorstep concerns and reluctance (e.g., \"too busy\") and various measures of data quality. We also attempt to extend prior research by exploring the effects of combinations of respondent concerns. We discuss the implications of our findings for interviewing and quality assurance procedures. ","Survey methodology often emphasizes risk factors that arise at relatively fine levels of aggregation. For many surveys, however, it is important to consider risks that occur at a systemic level (e.g., risks arising from operational constraints; changes in the target population; changes in collection and production systems; loss of personnel; and limitations on timely and accurate survey process information). This roundtable will explore constructive steps survey organizations can take to characterize, evaluate, and manage systemic risks, with emphasis on three questions: 1. What are the most important systemic risks encountered by your survey organization?; 2. What are good approaches you have found to ameliorate these risks?; 3. What are practical ways to communicate these risks---and your program of risk management---to your primary stakeholders? ","A series of experiments have compared the 2002-2006 General Social Survey with data collected from the Web-enabled Knowledge Networks Panel surveys. First, the results indicate that the level of don't knows are highly contingent on format and layout. However, it is possible to design in-person and web surveys to produce similar and comparable levels of item on-response. Second, the substantive distributions are not statistically different across modes for the majority of items. Third, statistically significant and substantively large mode effects do appear for an appreciable minority of items. These differences probably relate to the different demand characteristics of an interviewer-administered vs. self/computer administered survey. In particular, social-desirability and impression management dynamics are quite different across the two modes and may account for much of the differences. ","SHPPS 2006 collected data on school health policies and programs from nationally representative samples of states, districts, schools, and classrooms. District-level questionnaires were designed for telephone administration, but about half of respondents completed and returned paper questionnaires. Chi-square tests were used to test for differences in population estimates obtained from mail vs. telephone data collection and in the proportions of missing and don't know responses. No strong evidence that population estimates were affected by mode of response was found for any of the 7 questionnaires. As expected, the percent of missing data was higher for mail and the use of don't know was higher for telephone. For example, for the Health Education questionnaire, 3.3% (mail) and 0.0% (telephone) of responses were missing whereas the percentage of don't know responses were 0.1% and 2.4%. ","PRAMS is a mixed mode (mail/telephone), state population-based surveillance system of women who recently delivered a live-born infant. Because of the sensitive nature of some of the topics addressed, we examined the extent to which the mode of completion biased survey estimates. We selected a nested matched case-control sample (n=8617) from all respondents, with telephone respondents treated as cases and mail respondents as controls. Matching variables were related to the likelihood of response and to mode of response. We examined 20 sensitive questions. Several socially undesirable events were reported more frequently by mail respondents. The magnitude of the bias ranged from 0.03 to 1.04 percentage points for the 6 events with the strongest mode effects. Our results indicate that mode effects do produce bias in some prevalence estimates from PRAMS, but the magnitude is small. ","For the first time in 2006, Canadian households had the option of responding to the Census via the Internet. Almost one in five households chose to report their data with this new collection mode. A study was undertaken to check for the presence of an Internet mode effect in the data. Different means were used to assess the presence of a mode effect: non-response rates to each question by response mode were looked at, as well as answers distributions. Among other means, the propensity score method was used to compare answer distributions. This method allowed us to take into account some of the differences in characteristics of Internet reporters and Paper reporters and hence make them more comparable. The findings of the 2006 Census Internet Mode Effect Study are given in this paper. ","In sampling rare populations, it is expensive to obtain individuals with the characteristic of interest from a random digit dialing survey. In a study of individuals with a disability, we obtained opinions about their use of air travel. Two approaches were used to select a sample. One approach used a list of individuals that are members of a disability association. A random sample of these members was mailed a questionnaire and opinions about air travel was collected. A second but more expensive approach used random digit dialing methods to obtain a sample of individuals with a disability. In both approaches, once an individual with a disability who traveled by air was recruited for the study, they were randomized to complete a questionnaire using telephone, mail or web. The results obtained from the two frames and for the different modes will be presented. ","The analysis of panel data with nonmonotone nonresponse often relies on the assumption of ignorable missingness. It is important to assess the impact of departure from the ignorability assumption on such an analysis. Nonmonotone nonresponse in longitudinal outcomes, however, can often make such sensitivity analysis infeasible because the likelihood function often involves high-dimensional integrals with respect to missing outcomes. We propose extending a method of local sensitivity analysis to measure the potential effect of nonignorability in panel data with nonmonotone nonresponse. The proposed method overcomes computational difficulty because it completely avoids evaluating the difficult-to-evaluate integrals. We demonstrate the validity and computational advantage of the proposed method with a simulation study. We then illustrate the methodology in the Smoking Trend data set. ","A mixed-effects, flexible B-Spline model was used in modeling longitudinal and survival data. A longitudinal model was specified with natural B-Splines and then fitted data were incorporated into the Cox proportional hazard model. This method was applied to the Acute Infection and Early Disease Research Program database, which included individuals with acute and early HIV-1 infection from multiple centers, and used to assess a difference in time from study eligibility date to initiation of antiretroviral therapy (ART) in 123 women as compared to 2140 men. Longitudinal data were available for viral load, and CD4 count; number of observations per person ranged from 1 to 28, median=3, mean =5. Estimates of the difference in time to ART by sex from this method were compared to estimates from an extended Cox proportional hazards model with time-varying covariates. ","A method is developed for the longitudinal missing binary data problem with incomplete data for the outcome and covariate on some subjects, but in which the auxiliary information is always observed. This method applies EM algorithm using the method of weights (Ibrahim, 1990 JASA 85:765-9) on an augmented data set derived from the observed data in a regression framework. A missing at random mechanism is assumed. Weights are calculated using the augmented data completing the E-step. Parameter estimates are then determined through the M-step, using a weighted generalized linear mixed model approach that accounts for the correlation structure over time. We assume a simple fixed effects model with a random intercept. Using simulated data, we compare the results from a complete case analysis, multiple imputation and the proposed method and show that the proposed method performs better. ","The robust covariance estimator for generalized estimating equations (GEE) is widely used in the statistics and econometric literature. It is known that use of the robust covariance estimator may lead to inadequate confidence interval coverage when the number of independent clusters is small. Kauermann and Carroll (2001) and Mancl and DeRouen (2001) proposed corrected covariance estimators for GEE with improved small-sample properties. We propose new covariance estimators for generalized estimating equations. The new estimators are comparable to those of Kauermann and Carroll and Mancl and DeRouen in terms of bias and width, but improve upon them in terms of variance in many scenarios. We show in simulations that this reduction in the variance of covariance estimators results in interval coverage that is closer to nominal in small samples. ","The Survey of Occupational Injuries and Illnesses (SOII) uses a stratified sample to produce State and National estimates for nonfatal workplace injuries and illnesses. Private industry estimates are produced separately for 42 States, the District of Columbia, and three U.S. territories (Guam, Puerto Rico and the Virgin Islands). The level of industry detail for which State estimates are available varies widely and is based on the needs determined by each State. Additionally, estimates for injuries and illnesses for State and Local Government workers are available for 26 of these States. This paper will describe the frame development, target estimation industry identification, sample allocation, sample selection, and estimation methodologies used to produce the number and frequency (incidence rates) of nonfatal workplace injuries and illnesses. ","The 2003 National Assessment of Adult Literacy (NAAL) and the international Adult Literacy and Lifeskills (ALL) surveys each involved stratified multi-stage area sample designs. During the last stage, a household roster was constructed, the eligibility status of each individual was determined, and the selection procedure was invoked to randomly select one or two eligible persons within the household. The objective of this paper is to evaluate the within-household selection procedure used and update the procedure for future literacy surveys. The analysis is based on current household composition data and intracluster correlation coefficients using the adult literacy data.  In our evaluation, several feasible household selection rules are studied, considering effects from clustering, differential sampling rates, cost per interview, and household burden. In doing so, an evaluation of within ","Site selections for qualitative substance abuse evaluation studies are usually constrained due to cost concerns not only by the limited number of sites selected but also by the casual and subjective method for selecting particular sites. A methodology aimed at identifying twin-counties with similar demographics yet different substance use and mental health profiles is developed. Social distance matrices are constructed to list the degrees of similarity among all possible pairs of counties within each state based on predetermined source variables. The distance matrices are obtained separately through socio-demographic characteristics and through the substance abuse, mental health and service coverage measures. Two ranking indexes are created and composite scales for ranking are established. The methodology is further assessed with qualitative case study result from multipaired twin sites. ","The goal of the study was to generate national level estimates of the nutrient content of average daily dose of adult multivitamin minerals (AMVM) sold in the United States from a representative sample of the products from each of the groups: 1. For the top 16 brands, the objective was to obtain estimates of the nutrient content for 35 of the most important products. This group accounts for approximately 86% of the total market-share of all AMVM. 2. For the next 12 brands, the objective was to obtain estimates of the nutrient content for the most representative product. This group accounts for approximately 7% of total market-share. 3. For all other brands, the objective was to obtain estimates of the nutrient content for a sample of 15 statistically representative products/brands. The total marketing share for the 77 brands in this group was approximately 7%. ","Kim, Heeringa, and Solenberger (2006) suggested two model-based sampling methods for reducing the variance of the Horvitz-Thompson (1952) estimator. Their methods are whole sample procedures based on optimization theory under a superpopulation model. With respect to the sample selection probabilities, we theoretically present the differences between those procedures and popular draw-by-draw procedures such as sampling methods of Mizuno (1952), Brewer (1963), and Murthy (1957), when the sample size is two, which is a common situation in nationwide samples with many strata. We also compare the efficiencies between them for natural populations in the published literature. It appears that the methods of Kim, Heeringa and Solenberger (2006) may be preferable to those draw-by-draw procedures in many situations.    ","Through the National Household Survey on Drug Abuse (the predecessor of the current National Survey on Drug Use and Health), we calculated intraclass correlations (IC) for both cognitive and behavioral measures on drug use at the census tract and census block group levels within six major metropolitan areas respectively. We demonstrate that IC should not be overlooked in the substance abuse fields and discuss further the utility of empirical knowledge of intraclass correlations of pertinent measures in future sample design, estimations, and for policy oriented preventions and interventions. ","Four methods of sampling from any given distribution are considered: natural, inversive, rejective (especially, Lahiri's method), and geometric. In natural sampling, equally-likely sampling is done on a finite population that obeys the distribution approximately. Inversive sampling refers to choosing a random sample from a uniform distribution and applying the inverse of the cumulative distribution function (cdf), and to higher-dimensional generalizations. Rejective sampling refers to choosing a random sample from a product of independent uniform distributions and rejecting units that violate preimposed constraints. Lahiri's method, in particular, avoids computation of integrals and inverses. Geometric sampling exploits features of the distribution to transform the sampling into rejective and/or uniform sampling. One application is equal area/volume sampling from a surface/body. ","A common technique used to reduce nonresponse bias is to adjust the sampling weights of responding units to account for nonresponding units within a set of weighting cells. In the National Compensation Survey (NCS), which is an establishment survey, the weighting cells are formed using available auxiliary information: ownership, industry, and establishment size. At JSM 2006, we presented a paper in which we explored the effectiveness of the formed cells in reducing potential bias in the NCS estimates and presented results for one NCS area. Since 2006, NCS has expanded this study to several additional survey areas and time periods. In this paper, we present results from this additional research. We include areas of different size and with different levels of nonresponse. Also we compare the direction and magnitude of bias across time and across areas. ","Raking ratio adjustments are used to benchmark sampling weights to known control totals for a variety of reasons. Raking is used to reduce sampling error through the use of auxiliary variables correlated to survey response. In recent years, raking has been used to reduce non-response bias. It also produces weights that have face validity. The use of raking can expedite the creation of analysis weights by simplifying the number of weighting adjustments. This paper is an empirical evaluation of analysis weights created by raking sampling weights without nonresponse adjustments. The raked based weights are compared with analysis weights created by applying sequential weighting adjustments at each stage of nonresponse in a telephone survey (i.e., screener interview nonresponse, extended interview nonresponse). As part of the evaluation of these weights, estimates and their standard errors ar ","Typically, the end of a survey field period is a point when a survey struggles to achieve what is seen as an acceptable response rate.  Response is shaped by the decisions of field staff to continue applying effort and respondents to be open to persuasion.  The situation is an uncomfortable one in that it is quite difficult to apply measurable standards to the process in a way that can be mapped with any precision into response probabilities.  The Survey of Consumer Finances (SCF) has long gone to the bitter end of the field period where the remaining possibilities of completed interviews are very slim, and in doing so has held response rates approximately constant over time.  The hope is that this operational approach will allow as much constancy across time as possible in any response biases.  The paper discusses costs and benefits and presents evidence on the role of \"late\" interviews ","With low response rates of 20-30% and evidence of differences between respondents and nonrespondents, it is clear that nonresponse bias is a problem that plagues many organizational surveys, yet the problem remains largely unaddressed (Assael &amp; Keon, 1982; Cycyota &amp; Harrison, 2006; Paxson, 2002; Smith, 1997). Nonresponse bias is a function of (a) the likelihood of responding, and (b) the correlation between the variable of interest and the likelihood of responding. One strategy to reduce nonresponse bias is to collect auxiliary variables (Zs) that are associated with the variable of interest (Y) and the propensity to respond (P) and to use them in a weighting adjustment (Groves, 2006). In this paper, we demonstrate the use of auxiliary variables in several post-survey adjustments for the Community Health Measures Survey and propose future work on the use of Zs in organizational surveys. ","In many longitudinal studies, subjects may drop out due to a variety of reasons. For example, in a study monitoring the post-surgery progress of infants suffering from Biliary atresia, a serious disease in which the ducts that carry bile from the liver to the gallbladder are injured or underdeveloped, subject may drop out due to death, liver transplant or loss-to-follow up. The distribution of responses across the drop out groups could vary systematically and the missing data mechanism may be nonignorable for some of them. We develop a multiple imputation method where the completers are matched to subjects with a particular reason for drop out based on covariates and the observed outcome and then constrain the imputation of the missing values to matched subjects. We illustrate this methodology using the data from Biliary Atresia Research Consortium Network. Simulation study is presented. ","A web-based study (n=6400) with hypothetical vignettes was used to explore willingness to participate (WTP) in surveys, varying disclosure risk, topic sensitivity, &amp; mode. We also asked respondents for their preferences for risk communicated in words or in numbers, &amp; measured objective and subjective numeracy. We present 3 sets of analyses: 1) relationship between objective &amp; subjective numeracy, &amp; correlates of numeracy, 2) how numeracy relates to preference for words or numbers in describing risk, &amp; 3) effect of numeracy on stated WTP given the risk manipulation. Numeracy relates to preferences for risk description, &amp; interacts with the risk manipulation in predicting WTP: the effect of the risk manipulation is larger for those with higher levels of numeracy &amp; for those who prefer numbers over words. The findings have implications for disclosure risk communication in surveys. ","Following the decennial census, the Census Bureau and other agencies that sponsor major demographic surveys, such as the Current Population Survey, perform a redesign of those surveys. The overarching goals of the redesign are to ensure that survey requirements are met, to incorporate new or changing requirements, and to assess and improve statistical methods and operational processes in the ongoing survey programs. These goals are met through a comprehensive and integrated program of evaluation, research, development, and implementation. This paper will give an overview of the 2010 Redesign Program at the Census Bureau. We will highlight research on potential changes in statistical methods, such as replacing the census long-form data with the American Community Survey, using a single frame for sample selection, and incorporating administrative records in the sample design. ","This paper will present an overview of historical and current issues in the definition, stratification, and selection of Primary Sampling Units (PSUs) in large demographic surveys. By PSUs, we mean the large clusters (usually geographic areas such as counties) that are the sampling frame in the first stage of a multi-stage sample design. While simple in concept, the details of defining, stratifying, and selecting PSUs can prove to be surprisingly complex. We look at developments pertaining to PSUs (as used by the U.S. Census Bureau) over the past half-century, and describe some current problems. The issues discussed include (1) constraints on PSU size and boundaries, (2) choosing \"building blocks\" for PSUs, (3) methodology for and objectives of PSU stratification, (4) coordination among multiple surveys, and (5) coordination with preceding designs (maximizing or minimizing overlap.) ","One of the goals of the 2010 Demographic Survey Redesign is to use the continually updated Master Address File as the primary source to develop the sampling frames for current demographic household surveys. To support this goal, the Census Bureau is conducting several evaluations to compare the coverage of a MAF-based frame and the current four frames. The focus is on the overall quality at the national level as well as in two sub-universes: the sub-universe of new addresses added since the last census and the sub-universe currently covered by an area frame that is primarily in rural areas. This paper will present the most recent findings from these evaluations. ","The Redesign Program at the Census Bureau ensures that survey requirements are met and new requirements are incorporated for demographic surveys. The redesign also provides an opportunity to evaluate and improve statistical methods and operational processes. However, timing limits the amount of research and innovation that can be done within a single redesign. The new samples must be designed, selected, and fielded for interviewing under a strict schedule. To increase the focus on long-term strategy, the Census Bureau created a permanent Redesign Program. This program is responsible not only for managing the current redesign but also for researching topics that otherwise might not be examined due to tight deadlines in the current redesign. This paper discusses some of these long-term research ideas, and also reviews some innovations of past redesigns and the ensuing benefits. ","Quality control is important in many fields, especially in industry. We adapt and develop methodology in quality control to monitor data collection in epidemiological studies. To date, we do not know of any procedures to evaluate quality control during the actual process of data collection but only after the data has been collected. In this regard, we focus on two important processes during data collection: instrument calibration and population sampling. To evaluate instrument calibration, we present methods utilizing Shewhart control charts and Westgard stopping rules. To evaluate population sampling, we present methods utilizing regression analysis. The proposed methodology is beneficial to investigators to help assess the quality of data they are collecting and to allow them to adjust for data collected that is of low-quality during real-time data collection ","Multiple indicators, multiple causes (MIMIC) models are useful for studying the effects of a latent variable on several outcomes, when causes of the latent variable are observed. The error in the causal equation of a MIMIC model is a Berkson error and the classical Berkson error model is a MIMIC model. Previous work has focused on linear MIMIC models, where the causes of the latent variable are observed without error. We generalize the MIMIC model to allow non-linear relationships and to allow classical measurement error in the explanatory variables of the causal equation. We propose estimation procedures to estimate the resulting G-MIMIC ME model parameters based on the Monte Carlo EM algorithm. We apply our methods to data collected on A-bomb survivors to estimate the effect of radiation dose on cancer mortality, adjusting for both classical and Berkson measurement errors. ","This paper investigates the problem of modeling change in some imprecisely measured outcome and precisely measured predictors in the absence of auxiliary data (e.g., replicates, validation data). For models where adjusting for the baseline outcome as a covariate is appropriate, a measurement error induced bias will likely result. Ordinary regression analysis cannot be used to account for measurement error biases; standard measurement error modeling techniques cannot be implemented without auxiliary data. We present a method to investigate associations between change and predictors by parameterizing large sample estimators as functions of the reliability or the measurement error variance and employing sensitivity analysis. This approach can be implemented provided acceptable bounds for the reliability or measurement error variance are specified. An illustration is provided. ","Data in the social, behavioral and health sciences frequently come from observational studies instead of controlled experiments. Observational data are filled with different sources of uncertainty such as missing values, unmeasured confounders, and selection biases. As such, a single data set may not provide all the necessary information, so multiple data sources are often required to identify biases and inform about different aspects of the research question. In this manuscript, we present a unified Bayesian modeling framework that will account for multiple biases simultaneously and give more accurate parameter estimates than standard approaches. We illustrate our approach by analyzing data from a study of water disinfection byproducts and adverse birth outcomes in the U.K. ","We consider two problems in observational studies when unmeasured confounding exists: The estimation of average causal effect (ACE) of an exposure and the quantification of unmeasured confounding. We use an instrumental variables (IV) method, combined with contextually appropriate assumptions, to place bounds on ACE. We account for measured confounding using inverse probability weighting (IPW) and incorporate a sensitivity parameter into the IPW estimating equations to encode the unmeasured confounding. The bound on ACE from IV allows us to infer the plausible range of the sensitivity parameter, and hence the magnitude of unmeasured confounding. We apply our methods to the HIV Epidemiology Research Study, where one primary interest is the initial stage ACE of highly active antiretroviral therapy on CD4 count among HIV+ women, and the existence of unmeasured confounding is of concern.    ","In environmental epidemiology, researchers sometimes assume the existence of exposure thresholds above which the risk of adverse effects begins to increment. In this work, the research question of interest is to identify the relationship between a health-related outcome and whether or not the subject-specific mean and/or variability of the exposure exceed known thresholds. As a subject's true exposure mean and variability cannot be observed directly, misclassifications usually arise. Building off of random effects models for repeated exposure measurements and assuming balanced data, methods based on regression calibration and matrix methods are demonstrated for mean exposure only. For unbalanced data, and to incorporate categorizations based on both exposure mean and variance, a maximum likelihood approach is introduced. Simulation results and a real study example are also presented. ","In radiation epidemiology cases, the true dose received by those exposed cannot be assessed directly. Physical dosimetry uses a deterministic function of the source term, distance and shielding to estimate dose. Biological dosimetry inverts a well-defined dose-response curve at a biological indicator of dose. For the Atomic Bomb survivors, the physical dosimeter system is well established, and biological indicators have recently come available. We propose a likelihood-based dose-estimation method that incorporates both the physical and biological indicators. The classical measurement error plaguing the location and shielding data supporting physical dosimetry system is well known. More recently, researchers have realized the added complexity of Berkson error in the dose estimates. Our proposed method accounts for both error types. ","Protecting confidentiality is an essential duty of all Health Departments. However, existing guidelines have important limitations, including: 1) poor specification of disclosure, 2) failure to consider multiple data releases, 3) arbitrary rules-of-thumb and 4) conflation of privacy risk and reliability. We address these issues in an increasingly common setting: a queriable web-based reporting system for health events and vital statistics. We compare traditional approaches to more rigorous statistical methods. Two microdata methods (K-anonymity and L-diversity) are applicable but must be modified to address multiple tabular releases, for which we use a guidance matrix (Dandekar). The resulting method is easily applied and in our example verifies that the proposed releases contain no disclosure risk (hospitalizations, births), or only require minor modifications to ensure safety (deaths). ","The U.S. Census Bureau's Center for Economic Studies (CES) and academic researchers are developing synthetic microdata public use files from the Longitudinal Business Database (LBD). The LBD longitudinally links variables for business establishments from the Census Bureau's Business Register annual files. Synthetic microdata sets are designed to mimic the distributions of the underlying \"real\" microdata data and allow researchers to carry out analytically valid studies, without disclosing confidential information about respondents. The \"synthetic LBD\" files contain synthesized data on establishments' employment and payroll and on their birth and death years, as well as industry and geography. After briefly describing how we are creating the synthetic LBD and evaluating its analytic validity, we describe the confidentiality protection measures we are applying. ","This paper discusses the combined use of model fitting to data and matching using the model predicted values to reproduce the aggregate behavior and the main features of a data set. We approximate the data using semiparametric regression models combining a simple additive structure with the flexibility of the nonparametric approach. Then we use the model predicted values to simulate new data to preserve confidentiality. In the case of highly sensitive data this method provides the necessary protection from disclosure. ","Multiple variations of \"conventional\" data rounding procedures have been proposed and are practiced to protect sensitive tabular cells. Irrespective of the inherent procedural variations, all these procedures fall under a general category of \"indiscriminant\" rounding. The \"indiscriminant\" rounding procedure results in reduced data utility and quality. To improve the overall utility and quality of the published tabular data, we instead propose using a selective rounding procedure for tabular cells. The proposed procedure could be implemented by using multiple different techniques/tools currently available to the statistical agencies. ","The truncated triangular distribution has been used for masking microdata. The random variable following the truncated triangular distribution can serve as a multiplicative noise factor for the masking. The most desirable candidate distribution is the symmetric one which is centered at and truncated symmetrically about 1. This is because the multiplicative noise factor of 1 or very close to 1 does not protect confidentiality at all. The probability density function of the truncated triangular distribution has been developed by Kim [4] and applied to the 2006 Korean Householder Income and Expenditure Survey (HIES) data. Formulas for the domain estimation for the data masked by the multiplicative noise mentioned above are developed. In this paper, we will show domain estimation formulas and some results of the application of the truncated triangular distribution on the HIES data. ","This paper considers sensitivity analyses intended to supplement standard simulation-based evaluations of survey procedures. The principal ideas are motivated by, and illustrated with, a study of bootstrap variance estimators for the U.S. International Price Program.   ","The National Resources Inventory (NRI) is a large-scale longitudinal survey conducted to assess trends and conditions of nonfederal land. A key NRI estimate is year-to-year change in acres of developed land, where developed land includes roads and urban areas. Since 2003, a digital data collection procedure has been implemented. Data from an NRI calibration experiment are used to estimate the relationship between data collected under the old and new protocols. A measurement error model is postulated for the relationship, where duplicate measurements are used to estimate the error variances. Analyses suggest that therefore the parameters in current use are acceptable. The paper also provides a way to model the measurement error variances as functions of the proportion of developed land, which is essential for estimating the effect of measurement. ","A measure of physical activity is time spent in moderate to vigorous physical activity (MVPA). Daily or weekly estimates of MVPA can be obtained with self-reports (e.g., recalls and questionnaires) and/or with monitoring devices (e.g., accelerometers). Self-reports are inexpensive in survey settings but estimate MVPA with considerable measurement error due to recall errors and social desirability effects. Monitoring devices such as accelerometers provide estimates with less error, but are more expensive to administer. Using data from the National Health and Nutrition Examination Survey (NHANES) 2003-2004, we develop a function that approximates MVPA as measured by accelerometer from self-reported physical activity ","We describe a split-questionnaire survey (Raghunathan and Grizzle, 1995) in a national RDD telephone survey of medical decision making among persons age 40+ in the US. Respondents who were eligible for up to 9 different medical conditions (3 cancer screenings, 3 medication, and 3 surgeries) were only asked detailed questions on up to 2 conditions. The selection of modules was inversely proportional to the expected marginal prevalence rate for each condition, and every pair of modules was administered across sampled individuals. Of the 3010 respondents, 1279 were eligible for 3 or more modules and were subject to random assignment. A total of 4528 of the 6696 possible modules were administered. We describe the selection algorithm and the subsequent multiple imputation to recover complete module-level information for all eligible modules on all 3010 respondents. ","Mislevy's 1991 Psychometrica paper introduced a model in making inferences about the distribution of a latent construct ? under complex sample design by applying \"multiple imputation\" techniques to Large Scale Assessment, such as NAEP. That presentation developed the case for covariates treated as fixed effects, and mentioned the extension to random effects in passing.  This study extends the discussion of random effects under the classical test theory framework. We developed analytical solution for the mean and variance of the imputation on  for clustered samples, following Rubin's formula for Multiple Imputation. We explored the effect on the variance of the mean of the imputed values by changing the relative sizes of the three variance components, between cluster variance, within cluster variance and error variance, and sample size. Findings were also confirmed by a simulation study.  ","The Consumer Expenditure Interview Survey is an ongoing panel survey of U.S. households in which sample units typically receive the same survey protocol during each interview. Because of the high burden associated with the survey request, the BLS is exploring alternative designs that, if implemented, would change many features of the data collection process. One such alternative is adaptive matrix sampling. Matrix sampling involves dividing a survey into sections and then administering each to subsamples of the main sample. To potentially compensate for the resulting loss of information, we can adapt section assignment probabilities based on data from the first interview. We use historical data to explore potential efficiency gains incurred by the use of adaptive matrix sampling, develop point estimators for expenditures collected under this design and evaluate their variance properties. ","In surveys that are conducted annually, new questions may be added and old questions may be eliminated from year to year. For the sake of continuity and/or trend analysis, it may be helpful and informative to predict responses to these questions in the survey years when the question did not appear. In the National Survey on Drug Use and Health (NSDUH), an annual national survey of substance use and mental health measures in the U.S. civilian population aged 12 or older, a method has been used in which regression models are fit using data from the years when the question was included in the questionnaire, and predicted values are calculated for respondents during the years when the question was not included. The assumptions required for this method will be delineated and tested using real data from the NSDUH; and the analytic risks involved in such an approach will be noted. ","Typically, in every recent election, we have been inundated with polling results. The focus of these has been on the potential winning candidate and who will or did vote for him or her. Seldom have there been surveys that asked voters about the voting experience, itself. This paper discusses the first national attempt at such a poll. Issues of design are covered, with sampling and especially nonsampling issues given prominence. There have been smaller efforts of this sort in 2004 and 2006, but not a national attempt. Our focus then will be on what in other contexts might be called a national customer satisfaction survey. Key here, of course, is obtaining the interviews before the winners are announced. ","The underlying objective of voting reforms is to improve public confidence in the election system; however, little is known about the conditions voters actually face and how this affects their confidence. In 2006, the Center for the Study of Elections and Democracy at Brigham Young University collected exit poll data on the voting experience and characteristics of individual voters together with data of the actual conditions at the polling locations in Franklin (Columbus) and Summit (Akron) counties in Ohio as well as statewide in Utah. The findings reveal good reasons for boards of elections to invest in improving service at polling places. The service voters receive at the polling place affects their confidence that ballots will be counted accurately. Equipment differences across jurisdictions, wait times, voter sense of privacy, and voter partisanship also affect confidence. ","Many states are currently writing or revising laws for election auditing. ASA members have been actively working with voting rights activists, computer scientists, state legislators, county supervisors of elections and secretaries of state to ensure the use of credible audit procedures for elections. Issues range from highly pragmatic (e.g., how to randomly select precincts in a way that is credible to public observers, how to efficiently and accurately tally votes from multiple races) to more theoretical (e.g., initial sampling designs, efficient sequential procedures for when findings from the initial sample are equivocal, post-hoc power calculations). The work is important, and the window of opportunity to improve all aspects of electoral process is open. ","Statistical Agencies need to make informed decisions when releasing sample microdata from social surveys with respect to the level of protection required according to the access mode. These decisions should be based on objective quantitative measures of disclosure risk and data utility.  Disclosure risk is a function of both the unknown population and the sample counts in cells of a contingency table spanned by identifying discrete key variables. Disclosure risk measures are estimated using probabilistic modeling. Based on the disclosure risk assessment, appropriate Statistical Disclosure Limitation (SDL) methods are chosen. Information loss measures are defined to quantify the impact on statistical analysis. We demonstrate a Disclosure Risk-Data Utility assessment on a sample drawn from a Census in order to validate procedures. ","Some National Center for Education Statistics (NCES) data are only released as restricted-use files and/or through NCES' web-based Data Analysis System (DAS). The DAS allows the user to create programming files, tables of estimates, regression models, and view output. Because of this, there has been a recent policy shift away from PUMFs and towards the DAS as the main means for releasing public-use data. This policy shift has implications for the DAS: 1) meeting data user needs for increased access to data, 2) maintaining the confidentiality of the data, and 3) increasing the number and type of statistical methods provided for the user. A NCES task force was recently charged with examining the DAS data protection methods versus users' statistical and data needs. Some of the task force findings will be presented with a description of how the DAS currently protects data. ","The U.S. Census Bureau collects its survey and census data under Title 13 of the U.S. Code which promises confidentiality to its respondents. The agency also has the responsibility of releasing data for the purpose of statistical analysis. In common with most national statistical institutes, our goal is to release as much high quality data as possible without violating the pledge of confidentiality. We apply disclosure avoidance techniques prior to publicly releasing our data products to protect the confidentiality of our respondents and their data. This paper discusses three areas of current disclosure avoidance research: noise for tabular magnitude data, synthetic tabular frequency and microdata, and a remote microdata analysis system. ","This paper presents a review and synthesis of some previous literature on survey callback procedures and related issues arising from changes in contact and collection modes during the callback process. We examine several standard approaches within a general modeling framework defined by fixed-subpopulation analyses, selection models, pattern-mixture models, and measurement error models. Cost structures, bias-variance trade-offs and temporal aggregation receive special attention. ","Two-phase sampling and other differential contact strategies can help control costs, maintain weighted response rates and reduce non-response bias. When the relative cost of later call-backs is sufficiently high, these strategies may increase the efficiency of the data collection process, as measured by sampling precision per unit cost. When the respondents obtained through these strategies more effectively represent the pool of nonrespondents, we can reduce nonresponse bias. Using call history data from recent NORC face-to-face surveys, we explore the effects of field strategies that tailor contact protocols to predicted likelihood of response, and find strategies that reduce the bias and variance of the estimates for a given data collection budget. ","Reluctant respondents due to respondent burden and lack of motivation have resulted in increased data collection costs as survey researchers have attempted to maintain traditionally high response rates. This reluctance may be related to the amount of time it takes to complete an interview in large-scale multi-purpose surveys, such as the National Survey of Recent College Graduates (NSRCG). Recognizing that respondent burden or questionnaire length may contribute to lower response rates, the NSRCG offered its nonrespondents near the end of the data collection period an opportunity to complete a much abbreviated interview consisting of a few critical question items. This paper describes an investigation of critical-item-only respondents and the extent to which they are different from regular respondents. ","For many household and establishment surveys, initial contact and interview attempts by themselves do not produce satisfactory response rates. This often leads data-collection organizations to use various callback procedures to collect data from sample units that initially were nonrespondents. Analyses of the resulting data depend implicitly or explicitly on models for the response mechanism, conditional on the specific callback procedure. These models generally are acknowledged to be, at best, approximations to more complex underlying response processes. Consequently, it is important to explore the extent to which the properties of the callback-adjusted estimators may be sensitive to deviations from the assumed models. This paper develops a framework for exploring the impact of moderate deviations, and presents some related simulation results. ","Group-randomized trials (GRTs) require design and analytic methods that take into account intraclass correlation due to assignment of intact social groups to study conditions. Binary outcome variables in GRTs are common, and investigators have increasingly responded to the challenge of developing methods for these trials. Early GRTs with binary outcomes were often analyzed with two-stage methods, involving analysis of group-level event rates. Generalized estimating equations (GEE) provided a method for taking intraclass correlation into account while avoiding the specification of the joint distribution; recently proposed corrections have addressed GEE's poor performance in small GRTs. The use of mixed-model approaches has increased dramatically due to availability in commercial software. Each of these methods will be reviewed, highlighting recent developments and areas for future work. ","Even though the data analysis in most cluster studies with Gaussian data uses some type of weighting to account for imbalance, current methods for computing power assume equal cluster sizes, for which exact power can be computed. Unbalanced data typical of cluster designs can make actual power differ substantially from the planned power. We describe how to align power calculations with a hybrid method of data analysis which combines mixed model estimation of intraclass correlation with exact transformation to a univariate model for approximate hypothesis testing. Analytic results, enumerations, and simulations all support the approach. The method allows accounting for within-cluster covariates. A power analysis for a study of adolescent drinking behavior illustrates how easily the method can be implemented with standard data analysis and power software. ","Gaussian clustered data are often analyzed using a mixed model on individual data or least squares regression of cluster means. Both techniques provide unbiased hypothesis tests for data when the number of clusters and number of observations within each cluster is large. In small samples with unbalanced data, even moderate imbalance in cluster size across treatment groups can lead to test size bias. The magnitude of bias in type I error is compared for the mixed model analysis and weighted and unweighted least squares analysis of cluster means. Of the 10 methods considered, the analysis of cluster means with means weighted by their estimated theoretical variance best controlled type I error. Several methods provided unbiased inference when treatment groups were close to balanced in the number of clusters and observations per cluster, as is often common in group randomized trials. ","Internal pilot designs allow revising the variance estimate at an interim stage to adjust the final sample size up or down, as needed, to achieve the power desired. Clinical trialists have begun using such designs with univariate outcomes. However repeated measures make an internal pilot difficult because the nuisance parameter becomes a covariance matrix rather than a single variance. Compound symmetry within clusters allows applying Gurka, Coffey, and Muller's (2007) exact method for internal pilots with equal cluster sizes. For unequal cluster sizes, an easy to implement hybrid approach uses mixed model estimation of intraclass correlation and exact transformation to an inference-equivalent univariate model. Estimated weights allow using existing univariate internal pilot techniques. Simulations and enumerations demonstrate the accuracy and benefits of the approach. ","The Agricultural Resource Management Survey (ARMS) is conducted by the USDA and collects detailed economic data from US producers. As with most surveys, ARMS suffers from unit nonresponse (70.5% in the 2005) with the potential to introduce nonresponse bias. Nonresponse bias was assessed by matching records sampled for the ARMS with those from the 2002 Census. Mean relative bias was assessed for 17 variables by comparing estimates based on census data for all ARMS cases (respondents and nonrespondents) versus ARMS respondents, using uncalibrated sampling weights and calibrated weights. Nine of the 17 had significant bias using the ARMS base weights. The ARMS calibration weights reduced the bias so that it was no longer significantly different from zero in 90% of the study variables. This suggests that calibration is an effective tool in reducing nonresponse bias to acceptable levels. ","The Canadian Community Health Survey is a cross-sectional survey designed to produce nationally comparable estimates of health risk factors, health status and health care services.  Annually, data from 65,000 respondents are collected using a combination of an area frame and a telephone frame.  To increase the precision of estimates, provinces can provide extra funds to increase the sample size for their regions.  Almost all of these 'buy-in' units use the more cost-effective telephone frame option, thus leading to a larger than average ratio of telephone to personal interviews.  For variables affected by the mode of collection, this larger ratio may lead to comparability issues.  This paper identifies the presence of a mode effect for some variables due to the addition of buy-in units and suggests a method to control the problem using a weighting technique. ","Research has shown that forced-choice questions promote deeper processing and less satisficing, resulting in greater numbers of responses being selected (Smyth et al., 2005). The authors tested whether deeper processing can also occur when this format is applied to other types of questions. In a survey for the National Science Foundation, respondents are asked to update a list of science and engineering departments offered at their school. The authors tested two versions of presenting the listing task to respondents. In one group, respondents were asked to update their listing as needed and then check a box at the bottom of the page to confirm they had updated their listing. In the other group, respondents confirmed or deleted each unit individually - a forced-choice format. The authors hypothesized that the forced-choice format would promote deeper processing in respondents. ","Random digit dialing can be a very costly methodology for a rare population, but less costly listed samples are not representative and can induce bias. When the list source has less than 30% coverage of the population, the maximum contribution of the listed sample to precision may be severely limited. In a telephone survey of child care for 3- 4-year-old children in California, we conducted parallel RDD and listed samples and parallel weighted estimation within each sample. We combined these parallel estimates to produce minimum MSE composite estimates for parameters of interest, with weights inversely proportionate to the estimated MSE of each estimate. In particular, we estimated the bias of the listed estimate using the RDD estimate as a gold standard. This approach allows the listed sample to contribute differently to different parameter estimates as a function of estimated bias. ","Innovation has long been credited as a leading source of economic strength and vitality in the United States. Existing data sources can be leveraged to develop better measurement of innovative activities and a better understanding of the innovation process. This paper reports on the results of a data linking project that matches nearly 65,000 participants in four federal science and technology (S&amp;T) programs with Census microdata. This paper uses fuzzy-matching techniques to link the data collected through these programs with establishment and company level data in the Census Bureau's Longitudinal Business Data (LBD) and Business Register (BR) and Survey of Industrial Research and Development (SIRD). This matching will the census frame and lead to a better understanding of the innovation process and federal program impact. ","Missing values are common issues in empirical studies. The performance of the methods analyzing missing data strongly depends on the missing-data mechanism. If missing mechanism is ignorable, the likelihood based inferences will only depend on the observed data. For two variables X and Y where both of X and Y are missing, however, missing-data mechanism may not be ignorable, since the missingness of Y can possibly depend on the values of X which are missing. We propose a nonignorable missing-data mechanism, in which X is MCAR and Y is MAR given the value and missingness of X. The non-iterative maximum likelihood estimates exist and data are excluded for estimating certain parameters. Extensions of this type of mechanism will be also discussed. ","Survey estimates of Medicaid tend to be lower than those compiled from records used for program administration. Studies point to false-negative reporting about enrollees in surveys as the main explanation, however they find different levels of misreport. It is unclear how much study differences owe to genuine discrepancies in how different surveys measure Medicaid. This study helps to clarify this question by comparing the results of using one set of variables, derived from the same administrative database, to separately model Medicaid misreport in the Current Population Survey Annual Social and in the National Health Interview Survey, both fielded in 2001. Results suggest that survey design has an important effect on Medicaid reporting, and most notably that differences in reference period are enough to explain differences in the probability of false-negative reporting in CPS and NHIS. ","This paper investigates differences in employment figures gathered from unemployment insurance tax filings under the Quarterly Census of Employment and Wages and employment reported through the Current Employment Statistics survey. Since these two Bureau of Labor Statistics programs both collect monthly employment from an establishment for the same reference period, the employment figures should generally be identical. However, differences exist at the micro and aggregate levels, both at a point in time and in seasonal patterns. We analyze employment differences for 200 establishments with large employment differences in 2005-06 and determine the scope of and patterns in the differences. We then report on findings from a response analysis survey in which the 200 establishments were asked about reasons for differences in employment reported on the tax and survey forms. ","A portion of the administrative data used at Statistics Canada (STC) comes from the T2 database, which is related to corporations and is made up of financial and fiscal data. These administrative data are provided by the Canada Revenue Agency (CRA). STC also receives from CRA information related to the Goods and Services Tax (GST) that businesses have been remitting to the CRA since 1991. The GST information provided includes total revenue and the taxes collected on products and services over a given reporting period. Most corporations are present on both sources of data and it is therefore possible to compare their revenue variables and assess the strength of the relationships between them. The presentation will focus on the different results obtained from the comparisons and discuss improvements that can be brought into the methodologies used to process these data. ","In the US, survey and administrative data sources are frequently blended to support a variety of research purposes. Because these data sources are primarily designed for different purposes, one inherently research oriented and the other to administer government programs, blending them poses unique challenges. This paper will focus specifically on income data derived from two sources: the Surveys of Consumer Finances (SCF) sponsored by the Board of Governors of the Federal Reserve System, and Federal income tax return data collected by the Statistics of Income (SOI) Division of the Internal Revenue Service. Utilizing multiple years of data, we will examine key similarities and differences between these two data sources and demonstrate methods for reconciling estimates produced from them. ","Ranked set sampling is a technique of efficient data collection when measuring of units (on the variable of interest) is expensive or time consuming but one can easily rank them. In practical situations however, any such ranking procedure employs visual or surrogate-based ranking, which may be prone to errors, giving rise to biased inferences. In this paper, a method has been developed to estimate the underlying true sampling distribution in the presence of ranking errors that automatically corrects for the bias introduced. Asymptotic results are derived and are illustrated with a numerical study. ","In many education and social intervention programs it is thought efficacious and ethical to offer a treatment to individuals based on a measure such as need or worthiness. This entails assigning subjects to the treatment group if they are at or above some cutoff score on the measure. The outcome of the treatment is then analyzed using the regression discontinuity design technique assuming that the treatment effect can be detected at the assignment cutoff through regression analysis. In this study the power of such a design is analyzed to determine the sample size necessary for implementing the parametric regression model under some common assumptions. Violations of those assumptions threaten the validity of the RDD study. Ways to address such violations are also discussed.     ","The Volunteer Supplement of the Current Population Survey database is used to answer questions relative to the volunteering of the American Baby Boomer population. This population consists of individuals born between 1946 and 1964. As of 2006, persons in this age group represents more than 26 percent of the U.S. population and obviously makes considerable contributions to many organizations and services by volunteering. Using the Volunteer Supplement, we will present many characteristics of Baby Boomer volunteering, describe distinctions between them and other generations, and describe how their volunteering is changing over time. Synthetic design variables were created to estimate sampling variance and logistic regression used to show that strong predictors of volunteerism can be determined from home ownership, business ownership, and voting frequency. ","Splines are a flexible application for nonlinear modeling. Estimating both the number and locations of knots as free parameters (join points in a \"free-knot\" spline) can optimize model parsimony. We designed a spline framework that estimates nonlinear relationships between a binary outcome variable and a continuous prognostic variable in the presence of covariates in complex nationally representative samples. We use direct search methods to maximize likelihood equations with piecewise linear free-knot B-splines. Model selection and inference incorporate parametric and nonparametric bootstrap methodology. Parameter estimates are structured for interpretability so that results look similar to a logistic regression. Unlike other nonlinear approaches, our framework handles complex multistage sampled data. Our presentation will include a summary of methodological details and simulations. ","The variance of a probability expansion estimator is sensitive to sample design and can be large when the design is subject to administrative and physical constraints.  Models and model based estimates provide a more efficient alternative but are dependent on models of questionable validity and sacrifice the impartiality of randomization.  There is a third estimation technique that retains the comforting impartiality of randomization and uses this randomization to impose a model on the sample data under which there is a Best Linear Unbiased Estimator (BLUE).  Since the model is imposed by the statistician through designed randomization, model failure tends toward a non-issue.  Examples from actual surveys are provided where the sampling variance of the Combined Ratio Estimator is tens to hundreds of  times greater than that of the BLUE.   ","We discuss a robust prediction tool based on the residuals of a model-based approach to survey data. The observed residuals can be used to estimate the error distribution under the i.i.d. assumption. The resulted prediction interval has better accuracy than the asymptotic normal or t prediction interval. We illustrate the potential by the analysis of historical trend of obesity in American young adult from the national health survey BRFSS. ","It sometimes happens that two separate samples from a population, having perhaps quite distinct designs and mode of sampling, contribute information on the same variable of interest, and it becomes an important question how to combine the data from the two samples. An example is the Occupational Employment Statistics survey (OES) and the National Compensation Survey (NCS), carried out by the Bureau of Labor Statistics, both contributing information on occupational wages. We discuss some new options for combining data from two samples and achieving unified estimation. ","For better inference of the population quantity of interest, ratio  estimators are often recommended when certain auxiliary variables  are available. Two types of ratio estimators, modified for adaptive  cluster sampling via transformed population and initial intersection  probability approaches, have been studied in Dryver and Chao (2007).  Unfortunately, none of them are a function of a minimal sufficient  statistic, and therefore can be improved with Rao-blackwellization  procedure. The purpose of this paper is to obtain new ratio  estimators that are not only more efficient than the original ratio  estimators proposed by Dryver and Chao, but simple to calculate.  Additionally, explicit formulas for the approximated variance of  these easy-to-compute estimators are derived. ","Under-reporting is a key issue when dealing with nutrition surveys. This paper estimates under-reporting of energy in the Canadian Community Health Survey (CCHS), identifies characteristics of under-reporters and assesses the impact of under-reporting on data analysis. Total energy expenditure based on equations from the Institute of Medicine is compared to energy intake. Confidence intervals inspired by the Goldberg cutoffs are used to identify under-reporters. In CCHS, under-reporting is estimated at 9.6%. Age, sex, being overweight or obese and physical activity are associated with under-reporting. Identifying under-reporters results in a positive association between energy intake and being obese. ","Statistical methods to estimate usual (i.e., long-run average) daily intake of a food or nutrient are required to properly conduct dietary surveillance. The dietary assessment portion of the NHANES includes up to two 24-hour recalls (24HR) per person. Day-to-day (within-person) variability in diet ensures that 24HRs measure usual intake with substantial error. For dietary components not consumed daily, 24HR reports may be zero, even for individuals with positive usual intake. Distributions of measured and usual intake are often very skewed. We present a nonlinear mixed model for usual intake and demonstrate its use in estimating the population distributions of usual intake for major food groups and nutrients. The balanced repeated replication method was used to approximate the standard errors of estimated quantiles of usual intake. ","This presentation provides a history of the evolution of the methods for the collection and analysis of nutrition data at NCHS/CDC. The NHANES Survey began collecting nutrition data in 1971. The nearly 40 years of NHANES experience is summarized in terms of the many changes in mode (paper questionnaires, CAPI, and telephone), methods (repeated 24 hour recalls and food frequency), survey participation rates, and the data base for processing the nutrition information. In addition, the survey design and the statistical methods for analyzing the data have changed over time. NHANES has moved from a 6-year periodic survey to an annual survey with data released in two year cycles. The current design is recognized to have analytic issues related to seasonality and small number of PSUs. The impact of these changes will be discussed in terms of their impact on time trends for nutrition data. ","Statistical methods for the analysis of categorical data in the form of contingency tables date back to the turn of the last century and were transformed by the development of log-linear model methods in the 1960s and 1970s, when computation via maximum likelihood estimation became widely accessible via the major statistical packages. This course is based on materials from an MS-level course taught at Carnegie Mellon and will introduce participants to log-linear models and methods for fitting them to multidimensional contingency tables. The material will include the use of graphical models and their interpretation and the applicability of the methodology to large sparse tables.","Use of models for estimation from survey samples is becoming progressively more important. Practitioners must understand how to efficiently use available data to both increase precision and correct potential bias due to declining response rates. This course will provide an overview of the use of models to design efficient samples and improve estimators through the use of auxiliary data. We will compare design-based and model-based estimation and discuss the integrated approach of model-assisted estimation. This course will cover the types of auxiliary information typically available in samples of business establishments, institutions, and households; models that may be appropriate; use of models in determining sample designs and creating estimators; and software with particular emphasis on packages available in R. Participants will receive a topic-specific list of references to use for follow-up reading.","The bootstrap methods for complex survey data have been around for more than two decades, yet their treatment in textbooks and software implementation has been limited. This course is intended for survey statisticians who want to familiarize themselves with the bootstrap methods for complex survey data and find solutions for situations when the standard resampling methods, such as BRR, are not applicable. Basic ideas of the bootstrap will be introduced, and modifications necessary for complex samples from finite populations will be considered. Implementation of the bootstrap procedures by the method of replication weights will be demonstrated. Extensions leading to the bootstrap methods with missing/imputed data will be overviewed. Practical demonstrations will be conducted using Stata and R software packages. Attendees must be familiar with the basic concepts of sampling (i.e., stratification, clustering, varying probabilities of selection, use of sampling weights), design-based estimation (i.e., Horvitz-Thompson estimator, Taylor series linearization for variance estimation), and at least one of the aforementioned software packages. Familiarity with resampling variance estimators (i.e., BRR, jackknife) is an advantage. If you bring your laptop to follow the practical demonstration and perform self-check exercises, please have Stata or R installed. Availability of the power outlets in the room will be limited. Make sure you have a fully charged battery. Performing computationally intensive tasks such as bootstrap depletes batteries at a faster rate than your regular laptop use.","The Bureau of Labor Statistics collects employment figures through two programs: the Quarterly Census of Employment and Wages (QCEW) and the Current Employment Statistics (CES) survey. These two programs collect monthly employment counts from an establishment for the same reference period so the employment figures should generally be identical for each establishment. However, differences exist in the monthly employment figures from the two programs - at the micro and aggregate levels, both at a point in time and in seasonal patterns. Using results from a 2008 response analysis survey (RAS) in which 3,000 establishments with large seasonal differences were asked about reasons for differences in employment reported to CES and QCEW, we analyze the employment differences and potential reasons behind them.  ","The Occupational Employment Statistics (OES) Survey conducted by U.S. Bureau of Labor Statistics (BLS) collects occupational wage data within pre-defined wage intervals. Current practice is for OES to use point data from the National Compensation Survey (NCS) also conducted by BLS to derive interval means used in calculation of occupational wage estimates. In past analysis this method tested less biased than using interval mid points or geometric means. This paper examines an alternative for estimating wages when employment has been reported in pre-defined wage intervals. We apply O'Malley's Piecewise Quadratic Density Estimator (PQDE) to OES wage interval data. This paper assesses the resulting pure OES wage estimates. ","This paper examines efforts to interview a very wealthy part of the sample for the 2007 SCF. Only about a quarter of the group was interviewed. But at the close of the field period, more than a third of this sample was judged to be still workable. Progress of the field work was driven both by the behavior of respondents and the behavior of the field staff. The paper uses the formal data coded in the call records for each case to describe the work. But that information is inconclusive about the factors that drove the work. Informal notes in the call records do provide a clear picture of the points of resistance among respondents. Although it was difficult to locate, contact, and convince respondents of the legitimacy and value of the survey, it appears that the ultimate constraint in a large proportion of cases was time. Auxilliary data show little evidence of nonresponse bias. ","This paper will examine the effect of late-filed returns on population estimates for three Statistics of Income (SOI) programs. Estimates for populations of interest for each study are produced by drawing stratified, random Bernoulli samples of tax and information returns as they are filed, over periods that span a predetermined number of years. While this methodology results in the inclusion of the majority of targeted returns, a small number of returns for each program are filed beyond the data collection period. These late-filed returns may introduce non-response bias into the population estimates, which might be mitigated by post-stratification or weighting adjustments. This paper will examine the effects of truncated sampling periods on population estimates, using the three SOI projects as case studies, and will provide a comparative analysis. ","In the wake of globalization in the twenty-first century, organizations and corporations are increasingly competing on a multinational scale. Purchasing goods and services through the Internet has become a trend that is attracting the interest of importers and exporters around the world. Indeed, both American businesses as well as global consumers subscribe to different forms of electronic commercial transactions and shopping, largely because of the advantages and convenience that are globally associated with this dev elopement and its process. Over the years, many importers and other buyers who have purchased goods through the electronic commerce indicate that the process is convenient and time-saving. Recent studies indicate that majority of Americans engage in online shopping as well as conduct research on the quality and brand of products they plan to purchase. ","Response rates are routinely reported and used as a quality indicator; the credibility of survey statistics is often linked to response rates. However, response rates in Economic Surveys might not be an important concern for policy makers to maintain better quality. The belief is that as the economic experiences of the respondents increase, errors are lowered which means better estimates on inflation expectations. To investigate that, an experimental method based on treating respondents as non respondents is applied to the Survey of Expectations data of the Central Bank of Turkey and New Zealand to see how response rates at different levels affect the survey estimates. As a result, this study focuses on determining whether the data that comes from increased/high response rate of Economic Surveys is a better, worse or the same proxy of inflation expectations. ","The purpose of this panel topic contributed session is to bring together some leaders from academia, government, and industry with young professionals in statistics to provide them guidance to be successful in their career as well as informing the opportunities available for them. ","Calibration estimators use auxiliary data to improve the efficiency of survey estimates. Traditionally, the control totals, to which sample weights are calibrated, are assumed to be population values with no sampling variance. Often, however, estimates from other surveys are used because the population controls either do not exist or are not readily accessible. In this situation, many researchers apply traditional variance estimators to cases where the control totals are estimated, thus assuming that any additional sampling variance associated with these controls is negligible. We compare the MSE for linearization and replication variance estimators of proportions when the uncertainty in the control totals is either addressed or ignored. Illustrations are given of the effects of different levels of variability in the estimated controls on the overall variance estimates. ","Post-stratification is a calibration estimator that is often used to reduce the variance of the estimates (efficiency) and to reduce bias due to noncoverage or nonresponse. In this paper we examine the efficiency of post-stratification in the full response and coverage situation. Virtually all results on the efficiency of post-stratification in the literature assume a simple random sampling. We expand this to look at the efficiency in complex designs including stratified and multi-stage cluster samples. In addition, we show that the efficiency of post-stratification differs substantially depending on the type of estimate. In particular, the reduction in variance for totals and is very different than the reduction for means and ratios. We attempt to describe the conditions where the post-stratification produces the largest gains in efficiency. ","In this talk we will discuss variance estimation for surveys where the number of first-stage sample units in one or more strata is one. Unbiased variance estimation in such cases is not possible. We will review relevant ideas that have been discussed in the literature, particularly collapsing of strata. We then propose a new approach based on components of variance from different stages of sampling. The motivation and illustrations come from the Canadian Health Measures Survey, in which the number of PSUs per stratum is very small (in fact, just one for the Atlantic region) due to operational and financial constraints. ","Considering two different sampling schemes (Tille and Pareto), we present the results of a Monte Carlo simulation studying the statistical properties of several variance estimators on a synthetic data set, modeled on establishment data. We test the validity of including a varying finite population correction in the formulation of the stratified jackknife (as done with the Yates-Grundy-Sen estimator), and we compare the effects of direct replication of a rate to a Taylor linearization formulation. ","Generalized variance functions (GVFs) can provide useful approximations of error variances, especially for complex-survey cases in which (1) standard variance estimators have insufficient degrees of freedom for direct use, or (2) confidentiality restrictions prevent the release of design information used in direct variance estimation. Much of the GVF literature has considered variances of estimators for population proportions, and used population totals and sample sizes as predictors in the resulting variance-function models. Some surveys, however, have variance-estimation settings that meet criteria (1) or (2) above, but involve point estimators that are complex nonlinear functions of the data. This paper derives the functional forms and predictors for a GVF in this setting; applies the results to a class of price-index estimators; and evaluates the properties of the resulting GVFs. ","A method is outlined for developing guidelines for acceptable use of 1-year estimates rather than 3-year estimates for areas with more than 65,000 total population in the American Community Survey (ACS). The method is based on the coefficients of variation of the estimates. It is applied to estimates from the 2006 ACS and the results are presented. ","The delete-a-group jackknife can be effectively used when estimating the variances of statistics based on a large sample.The theory supporting its use is asymptotic, however.Consequently, analysts have questioned its effectiveness when estimating parameters for a small domain computed using only a fraction of the large sample at hand.We investigate this issue empirically by focusing on heavily post-stratified estimators for a population mean and a simple regression coefficient, where the post-stratification takes place at the full-sample level.Samples are chosen using differentially-weighted Poisson sampling.The bias and stability of delete-a-group jackknife employing either 15 of 30 replicates are evaluated and compared with the behavior of linearization variance estimators. ","Statistical innovation produces opportunities for knowledge advancement and policy improvement. In the last two hundred years an important thread of innovation has been the visualization of data. Minard's 19th Century diagram on Napoleon's army - cited as \"the best statistical graphic ever drawn\" - possessed clarity and communicated much statistical and geographic data in a limited space. Now information technology and data modeling have permitted the creation of a dynamic, online mapping and reporting tool, called OnTheMap, that analyzes and clearly displays vast amounts of data for detailed geography while preserving confidentiality. OnTheMap is the product of a visionary idea at the U.S. Census Bureau and among its state partners that a new, cost-effective, 21st Century statistical system can be built by integrating existing administrative records with census and survey data. ","The Statistical Support Section of the Statistics of Income (SOI) Division of the Internal Revenue Service (IRS) is comprised largely of mathematical statisticians, who provide general statistical consulting services for the customer service, tax administration and research divisions of the IRS. The Section supports its customers by developing samples and surveys, designing measurement systems, analyzing data from various IRS databases, and providing statistical training to national office and field staff. With emphasis placed on quality and meeting customer needs, the Statistical Support Section strives to provide technically accurate, up-to-date, easy-to-use, and well-documented final products. This paper will provide an overview of how statisticians interact with their customers as well as provide highlights of several Statistical Support Section projects. ","EIA is currently developing a new agency-wide Internet data collection system called ISMS. Once developed, ISMS will standardize survey forms creation, the collection of respondent data, and the data processing procedures used by the individual project area at EIA. The system's success, however, requires a diverse group of individuals to work together to develop, test, and finalize ISMS in a timely manner. This requires each individual to not only present their area of expertise but also requires cooperation and understanding across different areas and different disciplines. This mix of individuals keeps the project both interesting and challenging as it often forces us to look outside our personal expertise to other areas across the agency. It is also rewarding to know that decisions that I help to make now will affect the future of Internet data collection for the entire agency.  ","Agriculture has evolved from mule and plow into a high-tech business. Likewise, cutting-edge techniques to produce agricultural statistics used to guide agricultural policies have sprouted from the innovation of statisticians at the USDA's National Agricultural Statistics Service (NASS). Advanced remote sensing techniques that collect data without physical contact, more specifically, satellite imagery, have been the crux of the \"Census by Satellite.\" The yield, a Cropland Data Layer, can be used in many geographical information systems. In addition, pioneering uses of data mining techniques have been applied to improve the quality of data from collection to estimation. With their ability to illustrate complex statistical results in an easily digested form, data mining and remote sensing are just a few of the areas where innovations in agricultural statistics are growing in NASS. ","The Consumer Expenditure Quarterly Interview Survey (CEQ) is an ongoing panel survey which collects detailed expenditure information from a national sample of households. High data quality is essential to accurately reflect the spending habits of American consumers. This study examines CEQ data quality in terms of editing required during the data processing phase. Editing methods include imputation of item missing data and allocation of expenses reported in aggregate across different items. We explore the relationship between data quality and a variety of variables, including respondent characteristics, size of household, interview characteristics (e.g., length of interview, use of interviewing aids) and contact history (e.g., number of contact attempts). We also compare data quality across expenditure category (e.g., utilities, clothing) and wave. ","In 2008 CDC conducted a study comparing web-based vs. paper-and-pencil administration of the Youth Risk Behavior Survey, a biennial national survey of U.S. high school students that assesses priority health-risk behaviors. Within each of the 85 participating schools, four classrooms were selected and randomly assigned to one of four administration conditions (three web-based, one paper-and-pencil). Findings on data quality and privacy included less missing data for paper-and-pencil administration (1.5% vs. 5.3%, 4.4%, 6.4%; p &lt; .001), less perceived privacy and anonymity among web-based respondents, and a lower response rate for \"on your own\" web-based administration than for in-class administration by either mode (28.0% vs. 91.2%, 90.1%, 91.4%; p &lt; .001). These findings do not favor use of web-based administration for this survey. ","Survey researchers have used multi-mode data collection methods to improve response rates on establishment surveys, often mixing a visual mode (such as a mail or web survey) with an aural mode such as telephone. A better understanding of the potential mode and quality effects of conducting multi-mode establishment surveys can inform future data collection efforts, as well as the characteristics of establishments and informants choosing the different modes. This paper will use data from the Kauffman Firm Survey (KFS), a longitudinal survey of new businesses offering both a web and CATI mode, to shed light on mode and data quality effects between modes. Specifically, the paper will conduct a longitudinal analysis among three groups of respondents: those completing all rounds through web, all rounds through CATI; and those completing the first round by CATI and the follow-up surveys by web. ","Many applications present data that are collected on multiple scales of resolution, as for example when data are aggregated across different scales both longitudinally and by economic sector. Frequently, such data sets experience missing observations in a manner that they can be accurately imputed using our Bayesian multiscale multiple imputation method. This method borrows information both longitudinally and across different levels of aggregation to produce accurate imputations of missing observations. One important implication of such methodology is its potential effect on confidential databases protected by means of cell suppression. In a large scale empirical study using the U.S. Bureau of Labor Statistics Quarterly Census of Employment and Wages, we find that several of the predicted cells are within 1% accuracy, thus causing potential concerns for data confidentiality.  ","This paper summarizes the methodology and quality assessment of the most recent version of the SIPP Synthetic Beta (SSB v5.0), a public use data set that combines long earnings histories and benefits information from administrative data sources with the detailed demographic data collected in the SIPP. We use multiple imputation to create partially synthetic microdata that contains no confidential data but should preserve many covariate properties of the underlying confidential data of interest to analysts. This method of disclosure protection has the benefit of not requiring any unusual software or sophisticated techniques to run traditional analyses and perform proper statistical hypothesis testing. In addition to describing our methods, we attempt to assess the analytic validity of the synthetic data and determine the disclosure risk of making such data available to the public. ","Data releases to the public should ensure the privacy of individuals involved in the data. Several privacy mechanisms have been proposed in the literature. One such technique is that of data anonymization. For example, synthetic data sets are generated and released. In this paper we analyze the privacy aspects of synthetic data sets. In particular, we introduce a natural notion of privacy and employ it for synthetic data sets. ","There is a growing demand for public use data while at the same time increasing concerns about the privacy of personal information. One proposed method for accomplishing both goals is to release data sets which do not contain real values but yield the same inferences as the actual data. The idea is to view confidential data as missing and use multiple imputation techniques to create synthetic data sets. In this paper, we compare techniques for creating synthetic data sets in simple scenarios with a binary variable. ","The survey process as measured by paradata may be different for different respondents. Those initially reluctant to participate may be convinced by the interviewer to cooperate, or the reluctance may produce attrition. Difficulty to contact respondents may be related to interviewer effort and busy respondent schedules, or it may be a form of reluctance. This paper will use contact history data for the Consumer Expenditure Survey in a mixed model where interviewer effects are part of the model. The goal is to identify subsets of respondents and interviewers whose paradata relates differently to survey outcomes and measures of survey quality. If characteristics of respondents can be related to different interviewer behavior, then adjusting interviewer behavior may improve response rates or data quality.\" ","Auxiliary variables that are available for all sample units and related to both the probability of response and the survey variables of interest are potential candidates for nonresponse adjustment variables. The National Health Interview Survey (NHIS) paradata file includes two potentially interesting sets of auxiliary variables measuring the cooperation and contactability of households in the NHIS sample. In an initial study, these paradata variables were found to have moderate correlations with the probability of response, but weaker correlations with a subset of variables on the NHIS family file. This paper expands this previous study by testing social scientific theories that provide a link between the cooperation and contactability paradata variables and a few key health variables on the NHIS family and sample adult files. ","Modern survey data collection is fraught with uncertainty. If one assumes a beneficial realization of the various uncertainties, overruns of time or costs are the key risk. If one assume a perverse realization of the various uncertainties, smaller samples of respondent cases are the risk. This argument is the rationale for so-called \"responsive designs,\" which alter the data collection protocol base on paradata collected during the data collection period. This paper reviews a set of paradata have been monitored over the past year to assess the performance of a face-to-face survey data collection. It describes the conceptual framework of the paradata, illustrates the monitoring tools, and then evaluates interventions in the data collection designed to alter the course of the data collection in a fashion judged desirable by the survey management. ","The response rate has been a key indicator in judging the risk of nonresponse bias. Recent investigations have shown that the response rate is not always a good indicator for nonresponse bias. This paper proposes indicators that rely on more of the available data than just the response indicator. Under an assumed model relating frame data and paradata to the observed survey data, two statistics can be developed that are useful monitoring tools. The first is the difference between the complete case mean and an adjusted mean (either based on imputations or nonresponse weights). The second is the fraction of missing information. This statistic quantifies our uncertainty about the values we would impute for the nonresponding units. These monitoring tools may lead to improved data collection practices. Examples from a personal visit survey are demonstrated. ","The NHIS interview protocol includes a household interview, a family interview (for each family in a household), and subunit interviews (within each family) with a randomly selected adult 18 or older (sample adult) and about a randomly selected child under the age of 18. While the household and family response rates remain high (87% in 2007), the sample adult response rate has dropped to under 70%. Since critical health information is collected from sample adults, more attention should be given to their participation rates and the potential for bias in key health estimates. In this paper, we use paradata, including measures of respondent reluctance, along with frame, family-level, and sample adult data (health and sociodemographic data collected with the family interview), to model sample adult participation and explore bias due to sample adult nonresponse. ","Obesity/overweight (Ob/Ow) are associated with several chronic diseases. In the 1980--90s prevalence of obesity increased in the US. Researchers tried to determine the impact of Ob/Ow on mortality by estimating the number of deaths associated with body weight. Questions were raised about the methodology of these estimates. In response CDC and NCI investigators developed a methodology based on estimating attributable risk with accounting for confounding in order to carry out the estimation. These investigations used data from the National Health and Nutrition Examination Surveys (NHANES) I,II,III and their mortality follow-up along with prevalences of Ob/Ow and confounders from current NHANES and #s of deaths from Vital Statistics. These estimates have been used to establish the impact of Ob/Ow on mortality. The statistical methodology, results and sensitivity analysis will be presented. ","National health policy is increasingly driven by quantitative analyses. These analyses are used to advocate or oppose policy changes, to estimate the costs of the initiative, and to implement the chosen option. Examples will be drawn from proposals for universal health insurance, expansion of home care for people with disabilities, and Medicare payment and quality measures. Many of these analyses draw on surveys and statistical techniquest to adjust for patient characteristics. All too often, however, uncertainty is ignored and the illusion of precision dominates. Emphasis will be on the use and misuse of statistical analyses, with suggestions on how statisticians can better communicate with policymakers. ","Congress relies on data in various forms when formulating and implementing policy. GAO, CRS, CBO and the Federal statistical agencies, while policy neutral, often have the opportunity to utilize statistical methods to transform data from various sources into policy relevant information to aid in supporting or justifying policy strategies. Further, agency leadership can tap statistical expertise to aid in monitoring and evaluating programs. This presentation will provide a brief exposure to the history and methods of program evaluation and the relationship of program evaluation to performance measurement in the context of the Government Performance and Results Act of 1993. Then, the use of statistical expertise in policy and program evaluation and in informing policy and decision-makers as found at GAO as well as in federal agencies will be highlighted through several examples. ","Capacity building is the development and strengthening of knowledge, skills, resources, infrastructure, institutional structures and processes, and policy and legal frameworks---all with the goal of empowering individuals and institutions to achieve their potential and meet the needs of the communities they serve in a sustainable way. In all countries, lack of statistical capacity hinders the gathering, maintenance, and use of data for evidence-based policymaking. Panelists will discuss global and regional challenges in capacity building; the need for greater statistical capacity for policymaking and achieving international development goals; the need for statistical education at all levels, with greater inclusion of women and minorities; changing needs for human capital in the USA and abroad; and the role of professional statistical societies in promoting statistical capacity globally. ","Survey samplers weight included units by the inverse of their probability of inclusion, known or estimated. The technique is closely associated with the design-based approach to survey inference, with the idea that units in the sample are representing a certain number of units in the population. I discuss weighting from a modeling perspective. Some common misconceptions of weighting will be addressed, including the idea that modelers can ignore the sampling the weights, or that weighting necessarily reduces bias at the expense of increased variance, or that units entering the calculation of nonresponse weights should be weighted by their sampling weights. A robust model-based perspective suggests that selection weights cannot be ignored, but there are better ways of incorporating them in the inference than via the standard Horvitz-Thompson estimator and its variants.  ","Recent work developed model-based approaches for regression parameter estimation in complex sample designs by interacting covariates of interest and probabilities of inclusion (Elliott 2007, 2008). An alternative approach develops nonparametric regression estimators of means as a function of inclusion probabilities (Zheng and Little 2003, 2005). This work brings together elements of these approaches, modeling each sampled outcome as a mixture regression on covariates, where the mixing fractions are a function of the probability of selection. This allows for a data-driven trade-off in robustness and efficiency, accounting for model misspecification as well as skewness and overdispersion in the residual errors. We consider both finite normal mixture models (McLachlan and Peel 2000) and Bayesian density estimation via extensions to Dirichlet process mixture models (Dunson et al. 2007). ","Gelman (2007, Statistical Science) is absolutely correct that \"Survey weighting is a mess.\" Dealing with excessive weights is a part of this mess, with the most popular method being the rather ad hoc Winsorization method (a.k.a. weight trimming). We propose a more principled method based on shrinking the log of weights, motivated by the expectation that imperfections (e.g., measurement errors) in forming the raw weights typically affect all sampling units multiplicatively. The new method amounts to raising the raw weight to a power a, essentially a compromise between using equal weights (a=0) and using the raw weights (a=1). We conduct theoretical and empirical investigations of this new method, in comparison with the Winsorization method. We illustrate the new method using data from the National Latino and Asian American Study (NLAAS) for mental health and service utilization. ","The increased availability and usage of spatial statistics has fostered a growth in the spatio-temporal understanding of the dynamics associated with criminal offending. Using homicide data from the city of Pittsburgh, PA, this examination extends earlier spatial clustering methodologies used in identifying patterns of crime mobility introduced by Cohen and Tita (1999). Ultimately, this examination introduces the use of new bivariate spatial clustering techniques as a way of identifying patterns of crime mobility over a specified temporal period. This builds upon the resourceful and innovative work of Cohen and Tita (1999), which employed the use of the univariate LISA (Local Indicator of Spatial Association) statistic at different time periods. The results are promising and suggest a more robust application of clustering techniques in the identification of diffusion processes. ","Data available to analysts are often obtained from complex sample surveys in which population units are selected by stratified multi-stage designs with unequal selection probabilities. Unweighted estimators of the model parameters may be severely biased in such cases if the selection probabilities are related to the outcome values even after conditioning on the model covariates (informative sampling). Probability weighting reduces the bias but does not eliminate it, unless the sample sizes at each level of the model hierarchy are very large. In this paper a general approach for bias correction based on bootstrap is proposed. The method is assessed by simulation study using probability weighted estimators of two-level model parameters when fitting survey data under informative sampling designs. The proposed method showed to be effective in bias reduction in all the cases considered. ","Unit level logistic regression models with mixed effects are sometimes used for estimating small-area proportions, with normality commonly being assumed for the random effects. However, in practical applications the random effects often exhibit significant departures from normality. To reduce the risk of model misspecification, we propose an hierarchical Bayes estimation approach in which the distribution of the random effects is chosen adaptively from the exponential power class of probability distributions. The richness of the exponential power class ensures the robustness of this hierarchical Bayes approach against departure from normality. We demonstrate the robustness of our proposed model using both simulated data and real data. ","In this paper we applied unit level linear mixed model to estimate the finite population means for small areas. As an inferential procedure we used Bayesian approach that needs specification of prior for the hyperparameters. Following some objective criteria, we propose a prior distribution for the ratio of variance components, along with a standard uniform prior on the regression coefficients. To approximate the posterior moments of small area means, we apply Laplace method. Our choice of prior avoids the extreme skewness, usually present in the posterior distribution of variance components. This property leads to more accurate Laplace approximation. Our simulation study shows that the resulting approximate Bayes estimators (with new prior) of small area means have good frequentist properties such as MSE and coverage rate. ","The analysis of variance (ANOVA) model suffers from a series of flaws that effect the conclusions and undercut its many potential applications. In this paper, we propose new generalized ANOVA models to concurrently address all these fundamental flaws so that it can be applied to many immediate and potential applications ranging from addressing an age-old technical issue to cutting-edge methodological challenges arising from the emerging effectiveness research paradigm. By integrating the classic theory of U-statistics with state-of-the-art concepts such as the inverse probability weighted estimates, we develop distribution-free inference to address missing data for longitudinal clinical trials and cohort studies. We illustrate the proposed models with both real and simulated study data, with the latter investigating behaviors of model estimates under small and moderate sample sizes. ","Households in the Medical Expenditure Panel Survey (MEPS) are drawn as a sub-sample of respondents to the prior year's National Health Interview Survey (NHIS). The procedure used for selecting MEPS samples for 1996-2009 was to stratify the NHIS eligible frame and select units systematically within strata. Sampling rates were designed to improve precision for targeted subgroups over that expected from a design without stratification. A strategy to reduce the variance of MEPS weights is to use PPS sampling with the NHIS weights as measure of size. While this strategy would reduce the variance in sampling weights and improve the precision of national estimates, PPS sampling in the absence of stratification does not insure minimum precision levels for minority subpopulations of interest. This analysis evaluates the potential gains/tradeoffs of various PPS sampling options for MEPS.  ","The Medical Expenditure Panel Survey (MEPS) is a nationally representative panel survey of the U.S. civilian noninstitutionalized population. The annual survey collects information to produce estimates of health care use, health status, health conditions, medical expenditures, sources of payment, insurance coverage, and health care access for the nation as well as for policy-relevant subgroups.  The MEPS has a complex survey design that uses the National Health Interview Survey (NHIS) as the sampling frame. The NHIS has a stratified, multistage area probability design and the MEPS sample is selected from households that responded to the prior year's NHIS. Design effects are used as a measure of precision to help assess the quality of survey estimates. In this study, design effects for various MEPS survey estimates by selected population subgroups will be examined for 2001 through 2008. ","Nonresponse in surveys can lead to potential bias of estimates. A strategy to address potential nonresponse bias is to carry out weighting adjustments to reduce nonresponse bias in the survey estimates. Auxiliary variables available for all sample persons are used in constructing nonresponse adjusted weights. With the sample for the MEPS being drawn from respondents to the National Health Interview Survey (NHIS), variables from the NHIS sampling frame are available on both MEPS respondents and nonrespondents. In this study, we compare the characteristics of MEPS respondents and nonrespondents by covariates, and then we examine the correlation between the covariates and selected survey variables. In addition, we assess the correlation between the nonresponse adjusted weights and various survey variables as a measure of the relationship between response propensities and survey variables. ","MEPS is a nationally representative survey of U.S. civilian households. Respondents are asked about their medical conditions and insurance coverage as well as their healthcare utilization and expenses through five rounds of interviews covering a two-year period. Annual estimates for the U.S. are made by combining the data from the panel in its first year with the one in its second. This research examines the differential event reporting by MEPS event type---including visits to office-based physicians, outpatient clinics, emergency departments, inpatient stays, dental visits and prescription drug usages. Event-type specific models will be developed and used to estimate the magnitude of differential event reporting by round in MEPS and it's impact on the annual utilization and expenditure estimates from the survey. ","The Medical Expenditure Panel Survey (MEPS) is a national probability sample survey designed to provide nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian noninstitutionalized population. Depending on the type of medical event, there are varying levels of item nonresponse on medical expenses as collected in the MEPS household interview. MEPS also collects expenditure data in the Medical Provider Component (MPC) of the survey. Missing expenditure data for health care events are completed through a weighted sequential hot deck procedure with MPC data as the primary donor source. Studies in 2004, 2005, and 2008 examined the impact of imputation on estimates of variance for MEPS health care expenditures. This study updates this research by investigating fractionally weighted imputation. ","There are several disclosure risk factors to consider prior to publishing tables or releasing an on-line data tool with underlying restricted use data. For example, secondary analysts may want to produce tables at a geographic level lower than allowed for public use microdata. Rules are needed from the data owner or licensor that balance the risk with data utility; preferably based on data driven analyses. A framework is presented for an initial assessment of disclosure risk when releasing a limited number of data tables. Considerations for risk factors include the matchability to existing files, level of detail in variables, impact of data masking, among others. As an illustration, risk factors are quantified using public use data from the American Community Survey to help measure the disclosure risk associated with the planned Census Transportation Planning Products. ","This paper focuses on data utility measure for complex survey data. Two data utility measures based on the weighted empirical distribution functions are proposed, where weighted empirical CDF is estimated from not only original and masking data but also survey weights attached to records. A simulation study conducted on 2006 Korea welfare panel data shows that the existing measures based on IID data are much lower than the proposed measures, which means that the existing measures may report data utility to be much more useful than actual usefulness in case of complex survey data. ","Beginning with the 2007 data release, the Survey of Business Owners (SBO) will employ random noise instead of cell suppression to perform disclosure avoidance processing. This paper reports the results of a simulation study conducted to assess the impact of noise infusion on the statistical properties of the SBO variance estimates. We present two alternative methods of estimating the additional variance component due to noise infusion, while using the SBO random group variance estimator to estimate the sampling variance component. We examine the coverage and bias properties of the alternative variance estimators for level estimates and percentage change estimates over repeated samples. Our study showed that the effects on the variance estimates over repeated samples due to the addition of noise was negligible for the SBO estimates, due to the survey's large sampling variances. ","Using the theory of terminology for special languages, we investigate data, their representations, the semantics of data, and allowed computations.  These three components for understanding data are related, and terminology supplies the link.  The key insight, using terminology, is that a datum is represented by a signifier that stands for a certain kind of concept.  The concept is part of the semantics.  The kind of concept describes the computation. The paper contains a thorough exploration.  A rich descriptive framework is the result, and this framework may significantly alter the way users find statistical data.  Currently, users usually must know which agency has the data they are looking for.  Reducing or eliminating this dependence will greatly increase the ease and flexibility with which users find US federal statistical data. ","GeoFrame is an innovative use of digital photography and GPS technology that reduces the cost of field enumeration to make construction of household sampling frames more affordable. GeoFrame's utility can be extended to other situations where sampling frames may be difficult or too costly to create, such as creating probability samples to evaluate disaster recovery efforts or conduct surveys in developing nations. This paper describes the initial application of this technology to construct a sampling frame for a survey of a difficult to reach population by traditional means. It also presents the results of a field study conducted by RTI International that compares and contrasts the cost and coverage of the GeoFrame methodology versus traditional field enumeration. ","This paper describes a modeling framework to produce synthetic microdata that better corresponds to external benchmark constraints on certain aggregates (such as margins) and on which certain cell probabilities are bounded both below and above to reduce re-identification risk. Rather than use linear constraints (Meng and Rubin 1993), the modeling methods use convex constraints (Winkler 1990, 1993) in an extended MCECM procedure. The methods preserve analytic properties and are presented as a computationally tractable alternative to epsilon-privacy (Dwork 2008). Epsilon-privacy and its extensions have not yet been shown to preserve analytic properties (e.g., Dwork, McSherry, and Talwar 2007, section 5). A lone exception is Machanavajjhala et al. (2008) that preserves analytic properties in a narrowly focused \"on-the-map\" application. ","The American Community Survey (ACS) is a general-purpose survey with a diverse set of data users. The National Academy of Sciences recommended that the Census Bureau develop a comprehensive program of user education, outreach, and feedback about the ACS. This panel discusses several initiatives that were undertaken in response to these recommendations including a survey on users' preferences on the presentation of sampling error measures and the development of a series of educational materials targeted at different ACS data users. Panelists will discuss the challenges faced in developing tools for ACS data users and the difficulties in advancing the public's basic grasp of statistical literacy to include complex concepts. ","Since 1970, the U.S. Census Bureau has implemented an experimental program as part of the decennial census to evaluate a variety of alternative methodologies and questionnaire design strategies. For Census 2010, a robust program is planned that includes an ambitious test of race and Hispanic origin question changes, an alternative address collection for improved within-household coverage, and a 2000 Census short form-style form. In addition, other treatments include a deadline message with and without a compressed mailing schedule, and two alternative privacy messages. Finally, a nonresponse follow-up contact strategy experiment will examine the feasibility of reducing the number of interviewer contact attempts in order to save costs. This paper presents the sample design for each of these experiments, and includes a discussion of the unique challenges in achieving the optimal design. ","The collection methodology for the Canadian Census of Agriculture will change in 2011, moving from a primarily drop-off/mail-back approach to a primarily mail-out/mail-back one. To ensure that the new collection methodology does not have a negative impact on farm coverage, modifications to the existing methodology are required. Prior to the census, improvements are needed to both the quality and coverage of the Farm Register to ensure a good mailing list. A redesigned survey to identify new farming operations will help to achieve this. During census collection, the strategy to prioritize and select non-responding farms for telephone follow-up will play a key role in maintaining coverage. Following census collection, imputation will be used to help maintain coverage. In addition to covering these topics, the methodology for evaluating farm coverage will be reviewed. ","The census must count everyone once, in the right place. This is a challenge in our diverse, mobile society: Census 2000 had an estimated 5.8 million duplicated persons (Mule 2002). Following the NAS Residence Rule Panel suggestion to try to identify duplications on the census form itself, the Census Bureau designed a 2010 Alternative Questionnaire Experiment questionnaire to identify persons' alternate addresses and where to count them. This paper presents results of cognitive testing of this experimental overcount census form in household types prone to duplication. We also elicited descriptions of household members' living situations to check coverage. We present question wording, comprehension and layout results. We assess accuracy and completeness of the overcount screener, address, and residence data, and identify wider implications for the census and for survey methodology. ","The 2010 Census Coverage Measurement Program (CCM) will evaluate the coverage of the 2010 U.S. Census. New technologies and methods are being incorporated into the 2010 CCM to address the failure by the Accuracy and Coverage Evaluation Survey for Census 2000 to identify substantial numbers of enumeration errors, particularly duplicate census enumerations. Also, the 2010 CCM will provide estimates of components of coverage error (erroneous enumerations and omissions) separately in addition to estimates of net coverage. As the new methods add operational complexities, they particularly need to be evaluated to assure their effectiveness on the large scale of a census environment. The 2010 Census and CCM also provide opportunities for investigating new ways to improve census coverage measurement methodology. This paper contains an overview of the 2010 CCM Evaluation studies. ","In the 2010, The Census Coverage Measurement (CCM) will estimate the components of census coverage in addition to net error. This includes estimates of correct and erroneous enumerations in the Census. This paper focuses on missing data methods examined for handling census records in the estimation with unresolved enumeration status who also did not report name and two characteristics during their enumeration. ","Previous research has shown that the CPS ASEC under-counted Medicaid participation and over-counted the number of uninsured. This study examines the extent of an undercount of Medicaid participants and the subsequent impact on the percent uninsured. The Medicaid Statistical Information System (MSIS) is an administrative database for the Medicaid program. A field test of the American Community Survey that tested the overall quality of the new questions on health insurance. The MSIS database is linked to the persons in the ACS field test. The linked data will be examined at the household and person level for the types of health insurance reported and conflicts in what the survey and MSIS reports. This research shows that the count of Medicaid participants is underestimated and the uninsured rate should be lower, given the administrative records are without error. ","The Statistics of Income (SOI) of IRS started the Individual Income Tax Return Edited Panel Sample in 1999. It has been used for both longitudinal and cross-sectional purposes. Because of the panel drift over time, the efficiency of the panel is reduced in terms of the longitudinal study of taxpayers' behavioral changes. Further, the supplemental sample is unable to capture enough new high-income returns to support the cross-sectional estimations. Therefore, it was decided to start a new panel in 2007. The new panel follows the same stratified random sample design as the old panel. The key is the sample size reduction and allocation. Using the information from the current panel, the stratum sample sizes are determined by balancing the cost and precision and factoring in the needs of panel users. In addition, a refreshment sample is designed to better represent the out-year populations. ","Using data from the 1999--2006 Edited Panel this paper will attempt to determine whether the factors associated with intermittent behavior and dropout behavior are statistically different from continuous filing behavior and each other. Failure to recognize differences may lead to false statistical results and thus cause policy makers to pursue inappropriate policy. In a previous paper \"Attrition in the Individual Income Tax Return Panel Tax Years 1999--2005\" (ASA 2008) seven possible factors of attrition were introduced. By applying a Multinomial logit specification model this paper will help to quantifiably ascertain which, if any, of these seven factors can successfully predict continuous filing, intermittent filing, or dropping out. While death and income are the two most likely factors causing a change in filing behavior this paper will also examine other factors. ","Administrative records from tax returns are a reliable source of information on the distribution of personal income and taxes. This paper is the ninth in a series examining trends in the distribution of individual income and taxes based on a consistent &amp; comprehensive measure of income derived from individual income tax returns. In this analysis, we examine changes in the income distribution and tax burdens between 1979 and 2007, including the effects of Federal income and Social Security payroll taxes on the after-tax distribution of income. We also examine tax rates by income levels and within income levels to determine the extent to which tax burdens vary among people. Further, we estimate Lorenz curves and Gini coefficients to see how income inequalities have changed over time, using both cross-section (1979--2007) and panel (1999--2006) data. ","For several tax years, the Statistics of Income (SOI) Division of the Internal Revenue Service (IRS) has merged its U.S. Individual Income Tax Return sample data with selected information documents from the IRS Individual Returns Master File (IRMF) System to create a repository of sampled data and matched information reporting documents. This database has been further enriched by assigning industry and occupation codes to individuals who have filed income tax returns. The occupation codes are based on taxpayers' entries on the income tax return and the industry codes are based on SOI matching procedures for employer information documents.  In this paper, we take a first look at selected SOI individual income tax data by taxpayer occupation classifications and industry sectors for Tax Year 2005. ","For developing public policies and research purposes, income-related statistics are frequently needed for different small geographic regions. Previous research using the Statistics of Income (SOI) Division's Individual sample suggests that some IRS data, though free from the usual sampling error encountered in small area estimation, can be subject to nonsampling error. However, the SOI sample estimates, based on a large national sample of cleaned tax data, are subject to sampling variability for small domains. We use empirical and hierarchical Bayes methods to improve estimators of small-area totals and apply our estimators to data from SOI's 2004 and 2005 samples to evaluate the impact of an increased sample size. ","Consider a group-randomized school trial of schools, where either schools or classrooms are randomized, there is a strong set of covariates at multiple levels, and where the outcomes are a mixture of binary items, Likert agreement scale items, and standardized cognitive assessment scores. The panel will discuss alternative approaches to the analysis. Particular attention will be paid to the implications of different approaches for the communication of study results to broad audiences. Specific questions for the panelists: What language would you recommend for the press release and/or executive summary on how to interpret any error bands that you would generate in the analysis? How would you describe the generalizability of the results (again to a nonstatistical audience)? An education policy official will comment on the communications aspects of the solutions suggested. ","The Committee on Professional Ethics in Statistics will present a thought-provoking panel discussion where eminent statisticians describe a specific ethical dilemma they faced in their careers. The real-life ethical scenarios are broadly generalized to different content areas and include medicine and epidemiology, law, and government. The final outcome of each scenario will then be revealed by the panel. In keeping with the theme for JSM 2009, the real-life change in policy that followed the resolution or non-resolution of each scenario will be highlighted. ","In a famous paper on the design-based approach to survey inference, Hansen, Madow, and Tepping (1983) (HMT) consider estimation of the finite population mean of a variable Y from unequal probability samples, with an auxiliary variable X measured for all units in the population. They compare the ratio model estimator and the design-weighted combined ratio estimator and show in simulations that the \"model-based estimator\" can be seriously biased for the population mean even under moderate violations of the ratio model, whereas the \"design-based estimator\" has better frequency properties. From a robust Bayesian perspective, both of these estimators have limitations discussed in Little's paper (2004). The robust Bayesian approach leads to the separate ratio estimator and its shrinkage-type modifications for small samples. These alternatives are compared with the HMT estimators by simulation. ","In sampling theory when the correlation between study variable and auxiliary variable is positively high, the classical ratio estimator (RE) is the most practicable estimator to estimate the population mean. Sisodio and Dwivedi(1981) and Upadhyaya and Singh(1999) suggested to use population information of the auxiliary variable to increase the efficiency of the RE. Kadilar and Cingi(2004) combined this suggestion with the estimators in Ray and Singh(1981) and proposed novel ratio estimators. In this study, we adapt robust regression to the Kadilar-Cingi estimators (KCE) and obtain the conditions where these adapted estimators are more efficient than KCE theoretically. We support the theoretical results with simulations and compare the adapted estimators both with KCE and the classical RE. We study the robustness properties of the new estimators via simulations and give a real life example. ","Efficient probability sample design is inhibited by operational constraints and design based selection-probability estimators, although unbiased, are seldom variance-efficient. Model based estimators are both minimum variance and unbiased but rely on models gleaned from potentially misleading sample data. This paper extends model based inference to models imposed on sample data by designed randomization. This combines the advantages of randomization with the inferential power of models under statistical warranty. This is done by random construction of the sample units called pre-sampling. In the multivariate application described here, the model based BLUE has a variance that is orders-of-magnitude smaller than that of design based competitors. Pre-sampling methodologies appear to be robust against rough estimation of the model parameters and are evaluated under repeated sampling. ","This paper presents the small area model considered to produce estimates of the number of persons with disabilities and disability rates for health regions and selected municipalities using the 2006 Participation and activity limitation survey data. The paper describes the transformations applied to direct estimators and to the variances associated with these estimators in order to meet certain fundamental criteria. The log linear unmatched model to which the hierarchical Bayes (HB) approach was applied by relying on the Gibbs sampling method, and the results from the latter model are presented. Lastly, the paper presents the data sampling methodology used to ensure that the final statistics take into account province level results. ","Since Hansen and Hurwitz (1943), a variety of sampling techniques with unequal probabilities have been developed, but the variance of estimates of interest may be quite sensitive to the procedures of selection, especially in the case of small populations. Kim, Heeringa, and Solenberger (2006, 2008) developed model-based sampling methods. Their approaches are based on a fairly practical linear superpopulation model and optimization theory, and may consistently yield optimal sampling designs reducing the variance of the Horvitz and Thompson (1952)'s estimator. In this paper, we suggest new model-based sampling methods and empirically compare them with the previous methods and the traditional sampling methods of Brewer (1963) and Murthy (1957). Also, we compare the efficiencies of those alternative methods according to some chosen estimation methods of model parameters. ","The problem of statistical disclosure control has a venerable history, and an extensive literature spans multiple disciplines: statistics, theoretical computer science, security, and databases. We revisit the problem from a cryptographic perspective. The focal point of the talk is an \"ad omnia\" notion, which we call differential privacy. Roughly speaking, differential privacy ensures that any possible outcome of an analysis is \"almost\" equally likely, independent of whether any individual opts in to or opts out from the data set; the probability space is over randomness in the analysis, not in the data set. As a result, only a limited amount of additional risk---of anything!---is incurred by participating in a data set. A blossoming literature has already yielded both general techniques and targeted applications. Some highlights will be presented. ","Statistical disclosure limitation had focused on balancing the risk of disclosure of confidential information collected from individuals or establishments against the utility of released data for statistical uses. Staples of the theory that has evolved over the past decade include the R-U confidentiality map and a formal decision-theoretic framework. In this presentation we reconsider the effectiveness of this statistical approach to privacy and confidentiality in light of recent developments from computer science on what has been described as differential privacy. We consider ways in which the different approaches can be combined. ","This paper considers some of the techniques highlighted in the Federal Committee on Statistical Methodology's Working Paper 22 (revised December 2005), which is the working \"bible\" of official statistical disclosure limitation methods in the US, from the cryptographic privacy perspective. An essential feature of cryptographic models of confidentiality protection is the randomized sanitizer, which generates the conditional probability distribution of the release data, given the confidential data. Properties of the randomized sanitizer determine the extent of provable confidentiality protection provided by a disclosure limitation technique. Analysis of some standard methods in WP-22 using their implied sanitizers reveals the points of failure that might be addressed by using methods that borrow from the cryptographic privacy literature. ","Coverage intervals for a parameter estimate are frequently derived from a survey sample by assuming that the randomization-based parameter estimate is asymptotically normal and that the associated measure of the estimator's variance is roughly chi-squared. In many situations, however, the size of the sample and the nature of the parameter being estimated do not support the use of the conventional Wald technique, especially when a one-sided coverage interval is needed. We will propose a method of coverage-interval construction that \"speeds up the asymptotics\" so that the resulting one-sided intervals can have much better coverages than the corresponding Wald intervals while sharing the Wald intervals' large-sample randomization-based properties. For a mean or total computed from a stratified, simple random sample, no model need be assumed. ","The method of weighted quasi-likelihood (wql) is commonly used for point estimation in survey data analysis along with the Taylor method for variance estimation (VE) and Wald for interval estimation (IE). However, for finite samples, it is known that the above VE may be unstable and IE may have poor coverage properties. We consider ways to improve standard VE and IE by using Gaussian replicates of the pivotal estimating function (EF) derived from the wql-score vector. The basic idea is that the (nonstudentized) pivotal EF is closer to normal than the wql-estimator. The replicate estimators are obtained by setting the pivotal EF equal to random draws from the standard multivariate normal distribution. The computational problem of solving a multivariate EF for each replicate may be quite difficult . We propose an EF-based MCMC under a frequentist framework for this purpose. ","Calibration estimation has been developed into an important field of research in survey sampling. It is now an indispensable methodological instrument in the production of statistics. A few national statistical agencies have developed software designed to compute calibrated weights based on auxiliary information available in population registers and other sources. However its application in general statistics outside of survey sampling is limited. In this talk, we will demonstrate that the simple calibration method is a powerful tool to handle the general missing data problem when the parameters of interest are defined by unbiased estimating equations. In contrast to the traditional calibration method, the calibration weights depend on the unknown parameters of interest and must be estimated by the calibration estimating equations. Large sample results and simulations are included. ","The American Community Survey (ACS) currently has a fixed annual sample size of housing unit addresses; the number of addresses on the sampling frame, however, increases every year. This results in address sampling rates that decrease from one year to the next, thereby contributing to a decrease in reliability of small-area ACS estimates; it also negatively impacts the desired relationship between the coefficients of variation (CV) for ACS and Census 2000 long-form estimates, for tracts of average size. An increase in the annual ACS sample would both improve the reliability of small-area ACS estimates as well as restore the desired relationship between ACS and long-form CVs. This paper explores the effectiveness of various sample design options using such an increase in the annual sample; each varies the sampling strata as well as the sampling rates within the strata. ","Prior to 2006, the American Community Survey (ACS) had produced inconsistent estimates of households and householders and inconsistent estimates of husbands and wives in married couple households even though logically these estimates should be equal. With the Family Equalization Project, research was undertaken to remedy these differences using a three-dimensional raking methodology where, for two of the dimensions, the marginal control totals are derived from the survey itself rather than an independent source. The results from that research led to changes in the weighting methodology for the 2006 ACS. This paper evaluates the effects of that change in both the 2006 and 2007 ACS single-year estimates. The focus of this paper is to assess the effects at the weighting area level where the raking was performed. We also looked at the effects on estimates for cities and towns. ","The American Community Survey (ACS) uses independent housing unit and population estimates, produced by the Census Bureau's Population Estimates Program (PEP), as controls. These controls are applied to county based weighting areas. The PEP also publishes estimates of totals for sub-county areas, which are incorporated places and minor civil divisions. The ACS estimates for totals in these sub-county areas are normally different from the PEP estimates, and in a few cases substantially different. These differences are disconcerting to many data users. In this paper we evaluate the effects of incorporating the PEP estimates for sub-county areas into the ACS weighting process. The main goal is to make ACS estimates of total housing units and population for sub-county areas closer to the PEP estimates, with minimal effect on distributions of housing and population characteristics. ","The American Community Survey (ACS) provides estimates of detailed characteristics of persons both in housing units (HU) and group quarters (GQ). However, the GQ sample frame, sample design and estimation methodology differ from those of the HU. Importantly, the sample and estimation methodology for the GQ population are designed to be optimal for state level estimates, whereas those of the HU population are designed for smaller geographies. These differences present challenges for the usability of estimates of GQ and total population at substate geographic levels, particularly for one-year estimates. For example, for some substate geographies there may be anomalies in the estimates of poverty. This paper describes these challenges and reviews proposed and implemented methodological changes to enhance the usability of GQ and total population estimates for smaller geographies. ","The American Community Survey (ACS) published nationwide multiyear estimates for geographic areas with 20,000 or more population for the first time in 2008. This set of estimates were produced using data collected from 2005 through 2007. In December 2010, ACS will publish its first full implementation 5-year data products covering data collected from 2005 through 2009. The paper describes data quality filtering methodology currently used by the ACS and alternate filtering methods that could potentially be used to further reduce the number of estimates with \"poor\" reliability that are published. The paper will focus on examining the reliability of multiyear estimates and the effect of filtering on these estimates, especially for areas with population below 65,000. ","Results are presented from a simulation study of alternative weighting class adjustments for nonresponse when estimating a population mean from complex sample survey data, in an effort to extend the previous work of Little and Vartivarian (2003, 2005) to a complex sample survey setting involving stratified cluster sampling from a finite target population. Results from 30 different simulations suggest that the use of weighted response rates within weighting classes that are defined by values on an auxiliary variable and the sampling strata can be beneficial (in terms of reduced RMSE) when working with survey data collected from \"complex\" stratified cluster samples, particularly when response rates are low and the auxiliary and stratum variables are correlated with both the survey variable of interest and response propensity. Suggestions for survey practice and future research follow.  ","To control potential noncoverage bias, ratio adjustment methods are often used to adjust survey population estimates to represent relevant subgroups in the target population. The National Immunization Survey (NIS)-a nationwide, list-assisted RDD survey fielded by the NORC for the Centers for Disease Control and Prevention-monitors the vaccination rates of children between the ages of 19 and 35 months. The NIS uses various ratio adjustment methods based on multiple population controls and distributions to adjust survey estimates. The NIS also utilizes the Keeter adjustment method using information on telephone service interruptions to adjust for noncoverage of nonlandline telephone households. This research evaluates the potential impact of adjusting across multiple population controls on survey weights, estimates, and variances, and seeks to identify a best strategy for refinements. ","In many surveys, field procedures address nonresponse with a combination of callback efforts and changes in the mode of data collection. To analyze the resulting data, one generally needs to account for the relevant features of the underlying population, the sample design, and the nonresponse follow-up plan. This paper reviews and extends some standard randomization-based approaches to such analyses, with primary emphasis on population-level estimating equations that account for (1) group membership determined by nonresponse status; (2) random assignment of sample units to specific callback plans; and (3) parameter-identification restrictions. This approach leads to relatively simple estimators for population means and for parameters related to callback patterns and collection modes. The paper closes with a detailed simulation-based evaluation of the properties of these estimators. ","In the Survey of Doctorate Recipients at National Science Foundation, adjustments were made to the design weights to compensate missing units. In search of sensible adjustment methods, we conducted a simulation study to evaluate five different adjustment factors: the inverse of unweighted response rate in weighting cell formed by cross-tabulation of significant main effects; the inverse of weighted response rate in weighting cell formed by deciles of predicted propensity values estimated by logistic regression; the inverse of unweighted response rate in weighting cell formed by deciles of predicted propensity values estimated by logistic regression; the inverse of individual predicted propensity values from an unweighted logistic regression model; and the inverse of individual predicted propensity values from a weighted logistic regression model. ","Common methods to adjust sampling weights to account for survey nonresponse are the weighting cell technique, response propensity modeling, or a combination of both. Each raises several issues; for examples, which covariates to use to construct the weighting cells or to model response propensities, whether weights are used in modeling, and whether to weight the adjustment factor. To address these issues, we used a simulation based on a data from the National Survey of Recent College Graduates maintained by the National Science Foundation to evaluate these weighting methods. In the end, we expect that the weighting adjustment will have successfully accounted for possible nonresponse bias. ","The German Institute for Employment Research has just completed the second wave of a new panel survey focusing on low income households, which is designed as a dual frame survey. The first frame is a register of households that are currently receiving some kind of unemployment benefits; the second frame consists of an address register of the whole population. In this paper, we describe the challenges of weighting and calibration for the second wave of the survey, including nonresponse adjustment strategies for households and individuals, a comparison of different weight share methods to tackle changes in household composition, and the use of convex weighting to integrate a second wave sample of births from the smaller sampling frame (i.e. new households in need of benefits). ","Various algorithms have been developed for computing sample weight adjustments to account for features such as nonresponse and coverage errors (post-stratification). Some of these algorithms are now available in commercial software products such as SUDAAN\u00ae and WESVAR\u00ae. In addition, some methods for obtaining weight adjustments can be found by adapting existing procedures and functions in general purpose statistical software packages such as SAS\u00ae. This paper will present an overview of software that can be used to create weight adjustments. The focus is on options that already exist in software products and are relatively easy to use, particularly for small to mid-size studies. ","The CES survey, conducted by the Bureau of Labor Statistics, is a large monthly survey of over 390,000 business establishments. The CES program publishes monthly estimates on employment, hours, and earnings by industry for the nation, states, and metropolitan areas. CES sample-based employment estimates are benchmarked - or controlled - annually to the Quarterly Census of Employment and Wages (QCEW), which is a near-universe count of employment based on unemployment insurance tax account records. Because the QCEW data are available quarterly, it is technically possible to benchmark CES estimates four times a year. Several alternative quarterly benchmarking procedures were evaluated to determine if the quality of the CES estimates could be improved. The authors describe the alternative procedures and examine the costs and benefits of benchmarking on a quarterly basis.    ","The CES survey is a large monthly survey of businesses used to estimate employment, hours, and earnings by industry and geographic area. The Business Employment Dynamics (BED) program is a quarterly program that disaggregates population business data into four components: opening establishments; businesses whose employment expanded; businesses whose employment contracted; and closing establishments. The data from the BED program are available about 7 months after the end of the calendar quarter. It is possible to estimate two of these series---the expansions and the contractions---using CES survey reports. Research to produce an experimental CES-BED series has raised a number of estimation and administrative issues. Many of these issues are still being examined. The authors present some preliminary results and discuss some of the issues that still remain to be resolved. ","The Bureau of Labor Statistics presents a wealth of information to the general public. Much of this information is published in the form of tables and simple charts; however recent research has shown that humans are able to interpret information from visual cues better than they can interpret through tables of numbers. This paper applies principles from leading researchers in the field of data visualization, such as Dr Edward Tufte, to the data sets available from the Business Employment Dynamics program. New ways of thinking about the presentation of information, combined with tools from SAS and Google, can better inform the public about current issues in labor statistics. ","The Bureau of Labor Statistics firm size class data provide an important needed platform to discuss the impact of small and large firms on job creation. In the 2007 publication \"Employment dynamics: small and large firms over the business cycle,\" Helfand, Sadeghi, and Talan explored the behavior of the size classes in the recessions of the early 1990s and 2001 and the subsequent recoveries and found that small and large firms played markedly different roles in each of the downturns and expansion. This paper expands the data series, following the recovery from the 2001 recession into the current 2008 recession. It then compares the performance of small and large firms in the recessions of 1991, 2001, and 2008, answering the question of which size classes have been responsible for job creation and destruction over the previous two decades' business cycle. ","This paper builds upon previous research (Knaup and Piazza) on the survival and longevity of businesses to explore the dynamics of business growth and new job creation. Using longitudinal data available in the U.S. Bureau of Labor Statistics (BLS) Longitudinal Database (LDB), a methodology is developed to examine the growth rate patterns and employment trajectories of business start-ups over a 7 year period. Specific attention is given to identifying the characteristics of high-growth firms and/or \"gazelles\" as defined by the OECD/Eurostat Entrepreneurship Indicators Program. ","The Job Openings and Labor Turnover Survey (JOLTS), conducted by the U.S. Bureau of Labor Statistics, is a panel sample of 16,000 establishments, in which participants remain in the sample for two consecutive years. Every year a sample of approximately 8,000 establishments is drawn from the 1st quarter data from the Longitudinal Database. Each month one new panel is rolled into the sample while the oldest panel in the sample is rolled out. Previously, each annual sample was weighted to the current sample while previous samples remained unchanged. There also no consideration made for younger units, which are thought to be more dynamic and should be included in the sample as early as possible. With the new sample procedures, all establishments will be updated to reflect the current frame, with quarterly birth samples will be taken to update JOLTS with younger establishments. ","In 2006, the CPI's All-US-All-Items 12-month standard errors increased by more than 50% over the previous year's median average, returning to regular pre-2006 levels in 2007 and 2008. Since overall sample size had not been appreciably reduced in the 2006 time period, our hypothesis was that one or more of the individual (replicate) variance pieces were contributing an excessive amount of variance to the overall variance. A decomposition analysis of the Stratified Random Groups variance calculation system was produced, and the results showed one or two major area-item-replicates, at the lowest aggregate level, producing as much as half of the entire All-US-All-Items variance. This paper will investigate the nature and genesis of these anomalies, their impact on the overall CPI variance, and compare how different variance methodologies would have handled these anomalies. ","Election polling in the 2008 presidential campaign faced many challenges, including non-coverage and non-response, the possibility that racial conservatives would be underrepresented in samples or would mislead about how they intended to vote, and greater difficulties in gauging turnout among critical groups including blacks, Hispanics and young people. Yet polls performed well nationally and statewide. Some polls had a small pro-Democratic bias in estimates for white voters, but an offsetting bias among non-whites. The potential for bias in landline pre-election surveys was confirmed by the exit poll. Among Election Day voters, 20% could have been reached by pre-election surveys only by cell phone; those with a landline phone supported Obama by only a one point margin (50%-49%), narrower than the final 6.5% margin. ","ABC News pre-election polling in 2008 consisted of 28,917 RDD telephone interviews in 18 individual surveys, including 10,213 interviews in our final 19-day pre-election tracking poll. We faced a range of methodological challenges and questions in producing this research, including the inclusion of cell-phone-only respondents in the sampling frame, posited racial and race-of-interviewer effects and issues in early-voting measurement. My presentation will explore our approach and results as they pertain to these and related methodological issues. ","8 research firms in Election 2008 conducted telephone polls using recorded voice. To be examined briefly: A) Did recorded-voice pollsters perform similarly, or dissimilarly, to each other? B) Did recorded-voice pollsters have smaller or larger errors than firms using other methods of data collection? To be examined more substantively: C) In what ways were un-weighted 2008 data more representative than un-weighted 2004 data? In what ways less representative? D) To what extent was weighting able to correct for coverage bias and non-response bias? To what extent could different weighting algorithms have further reduced error? E) In 2012, will there be a robust role, a diminished role, or no role for recorded-voice telephone surveys? F) Beyond 2012, is recorded-voice less likely, more likely, or no-more-or-less likely to survive than other methods of data collection? ","The performance of matched sampling is assessed using data from the    2008 U.S. Presidential election. The assumptions necessary for the    validity of matched sampling, including ignorability, are described.    With a matching ratio of about five, the matched sample reproduces the    joint demographic distribution of the target population very closely.    The sampling distribution of the associated state-level vote estimates    is approximately standard normal with unit variance, suggesting little    or no selection bias conditional upon a full set of demographic    controls. The results are compared to RDD telephone and Internet    samples, none of which are clearly better and some (such as the 2008    ANES Internet panel) are substantially worse. ","Reducing socioeconomic (SES) disparities in health continues to be an important health policy issue. Although recent reports have shown differences in life expectancy by SES, several were based on ecologic data or had examined only one SES indicator. Using nationally representative longitudinal data made available through the linkage of National Health Interview Survey (NHIS) respondents to their death records, this study examines life expectancy at ages 25, 45, and 65 by sex and race/ethnic groups for both education and income. Examination of the specific causes of death and health factors, such as smoking, contributing to observed differences in life expectancy by SES as well as time trends will be explored. The NHIS linked mortality files, with SES and health information, provide a unique data source for examinations of the causes and implications of disparities in life expectancy. ","Linking survey data to program administrative data is a powerful approach for building imputational models that can be used to adjust fallible survey data on program participation to produce timely policy research. Currently, data linkage is often done after the survey data have been disseminated. Also linked data cannot be widely disseminated (if at all) because the agreements that facilitated the linkage forbid it. Our research objective is to leverage older vintage data linkage activities to improve policy analysis of Medicaid using the most recent public use survey data currently available. We develop imputational regression models that use only public use data elements as predictors of administrative Medicaid enrollment in a linked data set to partially correct Current Population Survey and National Health Interview Survey estimates of Medicaid enrollment and uninsurance. ","Using the National Health Interview Survey linked to Medicare records, we investigated whether use of Medicare services beginning at age 65 is related to health insurance status before age 65. We found that the previously uninsured were less likely than the previously privately insured to have positive Medicare expenditures. Among those with positive expenditures, expenditures were more than one-third higher among the previously uninsured compared to others. Compared to others, the uninsured had more emergency room and inpatient though fewer physician visits. In a companion analysis using the Health and Retirement Study, results indicated that different patterns of Medicare service use between the previously insured and uninsured were not due to differences in supplementary insurance coverage. Future research should analyze reasons for these different patterns of service use. ","Early studies of chronic air pollution exposure and mortality have informed U.S. air quality standards; yet, were largely limited to whites. With persistent racial disparities in mortality, understanding if reported effects pertain to all racial groups is important. The NHIS linked mortality files, with a large diverse sample, combined with EPA air quality data, provide a unique resource to examine factors related to air pollution and mortality. Initial survival models for fine particulate matter and heart disease mortality indicate similar hazard ratios, but larger confidence intervals, for black compared to white men. Though statistical issues related to the NHIS linked mortality files are increased with the addition of EPA exposures, which are systematically missing and geographically correlated, findings from these linked files should be useful for targeting US air quality policies. ","Analysts in many disciplines often fit models on survey data, which are provided to them by the data collection organization as a weighted data set. The data set might also be accompanied by replicate weights or generalized variance functions, to be used in statistical inference. A major issue facing analysts is how to properly account for the sampling aspects of the data in their model estimation and inference. A number of different approaches are described in the statistical literature, but it is fair to say that none are currently fully accepted outside of the survey statistics community. We review the major approaches and discuss ways in which survey-appropriate estimation and inference can be better integrated into common statistical practice. ","Health surveys should yield useful and valid estimates of disease burden and morbidity, with the ultimate goal of providing policy makers with data that can be used to guide appropriately targeted policy decisions and development of culturally appropriate interventions. Health surveys are faced with several challenges and a survey statistician is invaluable to successful execution of a complex health survey. Using examples from national health surveys carried out in Jamaica, West Indies, and elsewhere our discussion will examine the role of a survey statistician in the important stages of a health survey---from sample size determination and sample selection through data gathering to successful dissemination of results---and steps that can be taken to overcome the associated challenges. ","This paper discusses the quality and usefulness of estimates from the American Community Survey (ACS), specifically estimates for very small areas and population groups. The ACS is intended to replace the long form survey in the 2010 census. Since the long form is unique as a source of information about smaller population groups, a priority objective of the ACS design has been to provide good quality estimates and information about smaller groups. The foundation and general premise of the ACS design is that by spreading the \"long form\" sample across several years, it is possible to provide comparable information to the long form for all sizes of population groups. The paper focuses on assessing the quality of ACS estimates for small and very small population groups by exploring two aspects: reliability and the concept of period estimates. It shows the trade-off between lag and reliability. ","This research examines the utility of age/sex distributions below the county level, focusing on Bronx Public Use Mircodata Areas (PUMAs). While the ACS controls by age, sex, race, and Hispanic origin are determined at the county level, they are used uniformly across potentially heterogeneous PUMAs within the county. An important issue in a heterogeneous county is whether county controls reflect nonresponse among individual PUMAs. The key question examined is whether the age/sex distributions that are a product of this process are useful at the PUMA level. We conclude that sub-county controls should be pursued in the ACS, especially in counties that exhibit high levels of heterogeneity, since they would benefit from weights based on PUMA-specific attributes. Understanding the improvements that would result from such actions needs to be high on the Bureau's research agenda for the ACS. ","The transportation data community has been one of the largest users of the Decennial Census Long Form. Using the last four decennial census results (1970 through 2000), the transportation community has commissioned a custom tabulation called the Census Transportation Planning Products (CTPP). Any future CTPP will now use the ACS results. A welcome addition to the ACS standard tabulations are workplace-based tables. Ideas for improving the utility of 3-year ACS data for the transportation community include: 1) adding a home-to-work flow tabulation, 2) adding a home-to-work distance variable calculated using transportation road network files, 3) establishing a geographic unit with nation-wide coverage for 3-year ACS tabulations using 2010 census tracts as building blocks, 4) finding alternatives to data suppression for both the 1-year and 3-year ACS standard and custom tabulations. ","We explore use of posterior predictive checks to assess the adequacy of imputation models. One strategy applies analyses of substantive interest to both the completed data with imputations and replicated copies of the completed data under the imputation model and compares the results. Posterior predictive p-values for the differences of these estimates quantify the evidence of misfit of the imputation model. A variant of this strategy integrates out the missing data and their replicates using multiple imputation, yielding posterior predictive p-values that are generally more powerful than those estimated using the completed data. The checking procedure can be easily implemented in many cases using standard imputation software by treating re-imputations under the model as posterior predictive  replicates, and thus can be applied for methods that are not fully  Bayesian.  ","Many researchers have access to different survey sources, each with similar variables.  These researchers are often interested in the appropriateness of bringing together the data from the different sources for the purpose of data analysis, particularly when each source has a small sample size for the question being studied. We address a variety of topics that the researcher should be aware of such as the comparability of the variables across surveys, and the suitability of positing a model for the variables in the different surveys. We discuss possible approaches for combining the information. ","In missing-data problems complicated not only by the underlying data-generating mechanism (e.g. multi-stage surveys) but also skip patterns or censored items, an imputation strategy proceeding sequentially through the variables has been gaining a great momentum among practitioners. While this approach offers unmatched attractions, it does not necessarily follow a conventional joint modeling approach. Our work assesses the performance and compatibility of sequential approach to a joint modeling approach in multilevel settings. This sequential approach uses a set of hierarchical regression models each of which follows the appropriate format for the underlying variable subject to missingness. A comprehensive simulation study will also be presented, where the performance of the sequential approach is reasonably well and even better than the more convention methods in some scenarios. ","When using survey data, researchers must evaluate how to effectively handle missing data. For social survey data, full information maximum likelihood methods are often implemented when the researcher is interested in structural equation models. This strategy is convenient to implement and provides acceptable results. Yet it does not incorporate any imputation methods for assessing the missing information. We examine the benefits of imputation methods as an alternative for managing missing data, particularly in longitudinal surveys where missingness may be conditioned on previous panel data. Our goal is to outline the practical use of imputation and resulting gains in estimation. We apply these procedures to the Family Transitions Project, a longitudinal survey of more than 550 participants, which focuses on familial relationships and socioeconomic stress induced by economic hardships. ","For historical data, sequences of transactions that are supposed to be consecutive may contain gaps. A hot-deck based method of imputation, combined with a degree of randomization, is proposed. It is common in sequences of financial transactions to periodically \"pay off the balance\" with a single payment, returning the balance to zero. This feature is incorporated into the imputation procedure. The imputation procedure has been developed for imputing gaps in sequences of transactions in financial accounts. The procedures will be illustrated on data that are fictitious but retain interesting features found in the real world data that the method was devised to address. ","The Ohio Family Health Survey (FHS) is a telephone survey of the health and health insurance status of adults and children in Ohio. The FHS prescribed confidence intervals for estimates of insurance status for several population subgroups: rural regions, ethnic minorities, families in poverty, etc. This paper presents and assesses the imputation methods developed for the survey with special attention to the imputation of income and Medicaid status, two inter-related variables with a substantial amount of missing data. Both Medicaid status and income were imputed using multivariate regression models for ordinal and binary outcome variables (Proc MI). The predictors included the following variables: Gender, Age, Education, Race/ethnicity, Tenure (Own vs. Rent), Employment status (full time versus not), Insurance status, Household size, and Marital status. We developed 5 models. ","The multivariate normal (MVN) distribution is arguably the most popular parametric model used in imputation and is available in commonly used software packages (e.g. SAS PROC MI).   When the incompletely-observed variables include nominal variables, practitioners often  apply  techniques such as creating a distinct ``missing\" category or disregarding the nominal  variable from the imputation process, both of which may lead to biased results.  In this work,  we propose practical rounding rules to be used with the existing MVN-based imputation methods,  allowing practitioners to obtain usable imputation with small biases. These rules are calibrated in the sense that values re-imputed for observed data have distributions similar to those of the observed data. A simulation study demonstrating the advantages of this approach is presented.   ","The Economic Census, conducted every 5 years to measure and profile the American economy, is the largest collection conducted about businesses. Data from the census are used to release more than 600 data products and to benchmark key economic statistics, including the Gross Domestic Product. Response rate goals for the 2007 Economic Census were set higher than those achieved for the 2002 census. The new rates were negotiated with the Office of Management and Budget as part of their administration of the Performance Assessment and Rating Tool to continue to \"rate\" the census as effective. Several strategies were used to elevate response to the new levels. This paper discusses these strategies, their effectiveness in improving response, and identifies new research that needs to be conducted in formulating plans and response policy for the 2012 Economic Census. ","The Economic Programs Directorate conducts about 70 surveys that measure a variety of economic activity in the United States. These programs are conducted on a monthly, quarterly or annual basis and provide varying degrees of detail. Over the past couple of years, program managers have been experimenting with new strategies to improve response rates for their programs. These strategies have improved the overall response rates, and data quality for programs. This paper discusses these strategies used for current surveys conducted within the manufacturing and wholesale sectors of the economy, and the improvements resulting from these strategies. ","In March 2007 the U.S. Census Bureau conducted the 2007 Census of Government Employment, a voluntary census of state and local governments measuring public civilian employment and payroll. The following methods were used in an effort to bolster response rates: training in non-response follow-up methods, targeted contact with chronic non-respondents prior to mail-out, mail-out of a reminder letter to non-respondents one month after the initial mail-out, substantial increase in site visits to chronic or large non-responding governments, and use of the National Processing Center's telephone center to conduct non-response follow-up calls. The unit response rate improved from 76.5 percent in 2002 to 89.3 percent in 2007. ","We report on the design and implementation of the Privacy Integrated Queries (PINQ) platform for interactive privacy-preserving data analysis. PINQ is designed to expose an analysis language like SQL, allowing many data transformations and aggregations, but is carefully implemented to provide strong (and formal) differential privacy guarantees under arbitrary use. Importantly, the system itself provides the privacy guarantees, and does not require trust or expertise of the users, or privacy sophistication of the data providers. PINQ creates the potential to open statistical analysis of arbitrary sensitive data to arbitrary non-expert analysts, while still providing some of the strongest formal privacy guarantees possible. As well as describing PINQ, we give some examples of new analyses possible in its framework that have previously vexed privacy experts. ","This talk discusses recent results on designing differential private analytical methods that are asymptotically efficient, meaning that they produces answers (estimates) that converge to the correct value at the same rate as the optimal estimator, as the number of sample points increases. These results show a range of settings in which privacy is achieved at no (asymptotic) cost. They provide further evidence that rigorous definitions of privacy are compatible with valid statistical inference. ","The concept of differential privacy as a rigorous definition of privacy has emerged from the cryptographic community. We evaluate how the proposed ideas of differential privacy can be applied to discrete response data such as Binomial and Poisson random variables. In particular we explore utility and validity of statistical analysis of contingency tables and log-linear models, and focus on sample size calculation such that we achieve both statistical efficiency (the confidence level and the power) and differential privacy. We consider advantages and disadvantage of the new proposed framework when compared to more traditional statistical disclosure limitation approaches when dealing with categorical data.    ","Considerable effort has gone into understanding issues of privacy protection of individual information in single databases, and various solutions have been proposed depending on the nature of the data, the ways in which the database will be used and the precise nature of the privacy protection being offered. Once data are merged across sources, however, the nature of the problem becomes far more complex and a number of privacy issues arise for the linked individual files that go well beyond those that are considered with regard to the data within individual sources. In the paper, we propose an approach that gives full statistical analysis on the combined database without actually combining it. We focus mainly on logistic regression, but the method and tools described may be applied essentially to other statistical models as well. ","Two types of plots are described to clarify how modeling decisions interact with system design decision-making. The first type is called \"consequence-likelihood diagrams\" with Box-Meyer posterior probabilities as the likelihood measure and box-and-whisker plots of the objective function under various hypotheses representing the consequences. The usefulness is illustrated with real world applications in welding engineering. The second type of plot is called \"tolerance-box yield plots\" which help the user explore robust design decision-making. These plots are based on regression modeling with interactions in the model and the possible inclusion of noise factors. Tolerance-box yield plots are compared Taguchi marginal plots. The benefits include a natural ability to integrate multiple responses, incorporate separate models for mean and variance, and display the yield which is intuitive. ","Weapons stockpiles are expected to have high reliability over time. Prudence demands regular testing of sampled units to demonstrate that aging effects have not affected reliability. Managers need to know how reliability estimates will be affected if surveillance sampling is curtailed or ramped down to lower levels to save money. The question is not easily answered because reliability estimation is a diverse process carried out across multiple organizations. This poster describes a model for a stockpile expected to have high reliability from one year to the next, but with an ever-present possibility that reliability could begin to decline at any time. The model provides a framework for answering questions about the effect of reduced sampling on reliability estimates and confidence bounds. ","Adapting scatterplots to data from complex probability samples is challenging because different points have different weights, because cluster sampling induces correlations, and because data sets are often large. I show how to use partial transparency and hexagonal binning to produce useful scatterplots and conditioning plots for large survey data sets. The correlations induced by cluster sampling can be examined with interactive graphics, using brushing on cluster. ","In order for the U.S. Federal Statistical System to collect and release reliable and valid data, it must gain the cooperation and trust of its survey respondents (persons or establishments). To achieve this, federal statistical agencies must pledge confidentiality (under penalty of law) to protect respondents' data prior to collection and public release. Survey data released to the public is usually in the form of microdata and tabular data. This poster will focus on techniques for disclosure avoidance of two-dimensional tabular data. Tabular disclosure limitation techniques that will be presented are: complementary cell suppression, minimum-distance controlled rounding, unbiased controlled rounding, controlled rounding subject to subtotals constraints, and controlled tabular adjustment (CTA). Before and after perturbation results will be compared. ","For pay-for-performance and network-tiering programs, health plans often profile physician performance using quality of care and resource use. There has been great interest in characterizing the reliability of these scores. Reliability in this application is typically based on a simple hierarchical linear model. This separates the observed variability in physician scores into two components: variability between physicians and variability within physician. Feedback to physicians may have different reliability standards than public reporting or network exclusion. We show the relationship between reliability and misclassification probabilities for several common rules used to map physician scores into reporting categories. Report cards giving providers one to three stars or systems flagging providers as high performing and statistical testing and percentile cut points are considered. ","The Office of Management and Budget (OMB) released a set of Standards and Guidelines for Statistical Surveys in September 2006. The OMB standards codify the professional principles and practices that Federal Statistical Agencies are required to adhere to when conducting surveys. The standards also provide guidance on the level of quality and expected effort in conducting the statistical activities. The OMB guidance was not designed to be completely exhaustive in all the efforts an agency may undertake to ensure the quality of it statistical information. In fact, agencies are encouraged to develop additional, more detailed standards focused on their specific activities. The Division of Science Resources Statistics (SRS) within the National Science Foundation (NSF) has been undertaking major innovation and change in many of our national surveys. ","The IPP's Import and Export Price Index Surveys provide estimates of the change in prices of goods and services purchased by U.S. residents from foreign merchants or sold to foreign buyers by U.S. residents. In order to meet new Office of Management and Budget (OMB) standards and guidelines for statistical surveys, research was conducted using a three-phase approach to assess the effect of nonresponse bias in the IPP price indexes. The first phase modeled response rates using logistic regression. The second phase used a generalized linear mixed model with a random intercept term to investigate item price trends with varied response rates and company and item characteristics. Finally, using an equation provided by OMB to estimate nonresponse bias, we determined whether the level of nonresponse error in the price indexes was significant. ","The Current Population Survey (CPS) has been the source of the official U.S. poverty estimates since their inception. Since then, many changes have occurred in society and in the willingness of CPS respondents to report the cash income used to construct these poverty figures. Improvements have also occurred in the CPS data collection instruments. We believe the series of annual CPS cross-sections provide a meaningful measure of changes in poverty. The current paper examines trends in poverty rates by type of imputation from 1981 until 2007 focusing particularly on how poverty series for reporters and for those with item and whole imputes have trended over this period. Differences for blacks and whites are also presented. This paper provides a beginning analysis of a detailed reconstruction of imputation and poverty series for the period 1976 to 2007. ","Many surveys use administrative records to obtain demographic information and then request permission to obtain detailed information on behaviors from the respondent. A complete response results when both the demographic information and the detailed survey data are obtained. Non-responses arise from non-contact and from refusal to participate. If non-response rate is high and complete responders differ from non-responders, statistics that use only data from complete responders may be severely biased. If complete responders underreport undesirable behaviors in surveys and thus produce false responses, measurement error bias would be resulted. Through the combined asse of paradata, survey data, and biomarker data, this paper describes the extent of the possible biases, factors associated, and implications of adjustment. Data from the Arrestee Drug Abuse Monitoring System are used. ","This paper describes the nonresponse patterns among the participants in the Early Head Start Research and Evaluation Project, a Congressionally mandated random assignment study that began in 1995. Across the 17 study sites, 3,001 infants and their families were randomized at the time of enrollment to be in the program or control group. The program group children were allowed to participate in the Early Head Start program in their site; the control group children were not. Information was collected by program staff at baseline, and the children were followed up by research staff when children were 14, 24, and 36 months of age, and in pre-kindergarten. To better understand response patterns across waves, the paper looks at how the retained sample differs from the baseline sample, and the patterning of loss; that is, how attrition differed (if at all) between the program and control groups. ","In a survey of Maryland children between 2005 and 2006, 55% of those who returned a questionnaire(collecting demo-social characteristics and self-reported oral health) participated in a screening exam(collecting objective oral health). This paper examines the differences in socio-economic and oral health status among screening examination respondents and nonrespondents and the effect of non-response on estimates of oral health. Two-sided z-tests at a significance level of 0.05 are applied for all comparisons. Population estimates of demo-social characteristics among screening examination respondents were not significantly different from nonrespondents. Although an overall low response rate may potentially bias our population estimates, if the sample is representative of the target population, then weight adjustment, stratification, and imputation at the analysis stage can limit the bias. ","Random samples of US veterans were surveyed using a web-based survey and a traditional paper survey format. Responses among each mode were compared to determine if the survey mode has an impact on the sampling design, coverage, and non-response. Response rates for each mode were also determined and compared. We found that, while there were no significant differences in reliability of responses, there were significant differences in response rates based on mode of collection. Traditional paper surveys yielded the high response rate with a significant drop to the web surveys. We propose a bi-modal method of data collection to minimize costs and time. Using data collected from three veteran studies we suggest the usage of the bi-modal approach based on notifications of web surveys and follow-ups using traditional paper instruments. ","This paper builds and advances a previous publication on how to compute response metrics for online panels (Callegaro &amp; DiSogra, 2008). It starts by describing how multi-stage response rates are computed for probability-based web panels (recruitment, profile, and completion rate) and their use in a cumulative response rate calculation. We then focus on standing panels that rely on continuous recruitment from multiple independent recruitment samples and the concept of recruitment cohort. We will also present a special case for studies that use a screener and refinements that take panel attrition rates into account. Lastly it is discussed how to combine mixed mode recruitment methods in the computation of the recruitment rate. ","To address the need for National and State estimates of Serious Mental Illness (SMI), a study was designed to use adult mental health measures in the National Survey on Drug Use and Health to predict diagnostic status on a Structured Clinical Interview for DSM IV Disorders (SCID) interviews conducted by mental health clinicians. The study was carried out January-December 2008. NSDUH interviews were administered via ACASI in the homes of a nationally representative sample of respondents. A probability sample of over 1500 was recruited and completed clinical interviews over the phone. This presentation will describe the model-based methods developed to calibrate the NSDUH mental health data to SCID interview data and to use the models to generate National and State level SMI estimates from the total NSDUH sample. ","Despite widespread concern about the \"nonmedical use\" or \"misuse\" of prescription psychotropic drugs, there is no universally accepted definition of the construct and no single way to measure it. Among major epidemiologic surveys, the National Survey on Drug Use and Health (NSDUH) defines nonmedical use as use without a prescription of one's own or simply for the feeling or experience the drug causes. This presentation will outline issues with the definition and operationalization of the construct, collection of data for specific medications versus aggregate categories, balancing continuity of drug questions for trend analysis against the need to adapt questions as new drugs emerge and old drugs become obsolete, and related issues. It will discuss our ongoing methodological work on these issues supporting the redesign of NSDUH. ","Understanding coverage and reporting biases in survey estimates is critical in interpreting results, and also important in making design changes. A useful alternative to direct measurement of biases through costly field studies is to conduct systematic analyses comparing data across different surveys that use varying methods to measure the same phenomena. The Office of Applied Studies (OAS), SAMHSA, DHHS, is the primary source of national data on the prevalence, treatment, and consequences of substance abuse in the United States. In conjunction with efforts to redesign of the National Survey on Drug Use &amp; Health (NSDUH), OAS is conducting cross-survey analyses to improve understanding of the current data and identify potential survey design improvements. Included in this study will be analyses of substance abuse treatment data from NSDUH and OAS's facility surveys. ","In 2006, staff of the National Survey on Drug Use and Health (NSDUH) carried out a Reliability Study, in which 3,136 respondents completed the ACASI - and CAPI - administered interview twice, with a 5 to 15 day interval between occasions. A primary objective of the Reliability Study was to identify survey items and measures that did not meet reliability criteria, thus providing a basis for investigation of potential improvements to those measures. This study was carried out within the context of an overall NSDUH redesign being planned for implementation in 2013. This presentation will cover the main findings of the Reliability Study, and attempt to extract principles related to the construction and administration of reliable survey questions about drug use, mental health and demographics. ","In the context of small area estimation, hierarchical Bayesian (HB) models are often proposed to produce more reliable estimators of small area quantities than direct estimates, such as design-based survey estimators. A method which benchmarks the HB estimates with respect to the higher level direct estimates and measures the relative inflation of posterior mean square error due to benchmarking in the posterior predictions is developed to evaluate the performance of hierarchical models. Both numerical and graphical summaries of posterior predictive discrepancy measures are available. The benchmarked hierarchical Bayesian posterior predictive model comparison method is shown to be able to select proper models effectively in a simulation study. The method is then applied to selecting a generalized linear mixed model for complex sample survey data. ","Fay-Herriot models relate direct estimators of small area means to vectors of area level auxiliary covariates. Estimation of error variances in these models is a problem because of the lack of data within areas. A nonparametric approach is proposed for estimating these variances. Estimators of the remaining model parameters are derived and their asymptotic properties are studied. Moreover, small area estimators that incorporate the estimated error variances are obtained and several simple estimators of the mean squared error of these estimators are proposed. Simulation experiments study the small sample performance of the new small area estimators and compare different estimators of the mean squared errors. Finally, the results are applied to the estimation of unemployment proportions in Spanish domains. ","Building on the Fay-Herriot model and Henderson's method of obtaining best linear unbiased predictor (BLUP), we propose an extended small area model that does not require an explicit two-level parametric modeling. This extended model may be used in several ways, including robustness studies, quantile based analysis, nonlinear mixed effects modeling and studying extremes. A few of these applications are discussed in detail, and data examples are provided. ","In modern survey applications, National Statistical Institutes have always the pressure to reduce costs. This task plays an important role in the next European Census round 2010/11, where some countries try to employ a register-based Census which may help to reduce costs by far. Small area estimation methods are expected to allow for high-level results for small areas and domains under the given budget constraints. The present article focuses on the estimation of higher educated people as example for a variable which is not overserved in the register. Based on the classical basic unit and area-level models, binomial mixed-models will be elaborated. Finally, spatial small area models will be assessed. The entire study will be accompanied by a Monte-Carlo study which will foster comparisons of all models within a realistic framework. ","Annual or periodic cross-sectional health surveys in the US provide the opportunity to track health indicators over time. Various methods, using survey software, exist to investigate whether and how health indicators change over time. Several of these methods are demonstrated with SUDAAN analyses of multi-year BRFSS data sets from selected states: (1) chi-square test, (2) polynomial relationship between year and prevalence, (3) two recently added tests in SUDAAN CROSSTAB that assess a monotonic relationship between the health indicator and time (ACMH and TCMH), and (4) logistic regression with predicted marginals. These methods can be used also when other variables are controlled (e.g., age). Results show that all methods give consistent results, but some analyses are more informative than others under certain conditions. ","The use of complex survey sampling designs in population-based case-control studies is becoming more common, particularly for sampling controls. Researchers have proposed various methods for analyzing data from case-control studies with stratified SRS designs. There are, however, limited methods for case-control studies under general complex sampling designs. The standard design-based approach to inference is not suitable for the analysis of population-based case-control studies because of the extreme variation of the sample weights and the mismatched-frequencies of the matching variables between cases and controls. In this paper, we proposed a weighting method to reduce the variation of the sample weights and also to recapture the efficiency due to frequency matching. The performance of the proposed method was studied using extensive simulation studies and a real data application. ","We adapted a method of comparing distributions that uses projection splines, which fit a regression line that allows the slope changes over the range of x, for use with complex survey data. We used SUDAAN to estimate the percentile distribution of gestational age among the infants of black women and those of white women. We calculated y as the difference between the percentile of the two groups divided by the variance of the difference and assumed y was identically and independently distributed. We compared the distributions by regressing y on the average of the two percentiles, starting with a model with a node every 7 days. We eliminated nonsignificant nodes stepwise until all nodes were significant at an alpha = 0.01. The final model had 6 significant nodes, indicating the gestational age distribution of infants of black women is shaped differently from that of infants of white women. ","Survey regression techniques for count data can be used to model the number of events such as health care visits among individuals in complex sample surveys. The Geographic Accessibility of Health Care in Rural Areas Study was a cross-sectional stratified multi-stage sample survey conducted to study health care utilization in a rural area. A total of 1059 adults from 12 counties in western North Carolina were interviewed and asked questions regarding personal geographic factors including activity space and transportation use, as well as sociodemographic, cultural and health status characteristics. We discuss the use of the SUDAAN software package, and particularly, its LOGLINK procedure, to quantify associations between individual characteristics and utilization rates, adjusting for features---unequal weighting, stratification, and clustering---of the sample design. ","We propose a cross-validated version of the design-based variance estimator of survey estimators, and describe its use in several survey applications. The estimator is based on the same \"leave-one-out\" principle as traditional cross-validation, but takes the design effects on the variance into account. We apply the cross-validated estimator as a design-based model selection tool for regression estimators, and show that it is effective in minimizing the asymptotic design mean squared error of regression estimators, both those using parametric and nonparametric models. The cross-validated variance estimator is also proposed as an alternative to the commonly used linearized variance estimators, with a reduced design bias. ","Popular model-based small area estimates face a problem that when aggregated over a larger domain, the result may not agree with the direct estimate. The latter is felt reliable as a survey is designed for better accuracy at larger domain. While an agreement with an aggregated direct estimate at some level protects against model failure, it is often politically necessary to convince the utility of small area estimates. For an agreement benchmarking is used to modify the model-based estimates. Raking, popular in benchmarking, scales the small area estimates by a factor so that two weighted totals agree. It is used for convenience with no firm statistical principle and it has some deficiency. We use constrained Bayes estimators that benchmark the weighted mean, or the weighted mean and variability. Interestingly, many frequentist solutions including raking are special cases of our result.  ","When there is a rare disease in a population, it is inefficient to take a random sample to estimate a parameter. Instead one takes a random sample of all nuclear families with the disease by ascertaining at least one sibling (proband) of each family. This problem, which arises in population genetics, is analogous to the well known selection bias problem in survey sampling. Here, we develop a Bayesian analysis to estimate the segregation ratio in nuclear families. We consider the situation in which the proband probabilities are allowed to vary with the number of affected siblings, and we investigate the effect of familial correlation among siblings within the same family. We discuss an example on cystic fibrosis and a simulation study to assess the effect of the familial correlation. In addition, we discuss an application in the study of single nucleotide polymorphism. ","Over the last three decades, Small Area Estimation (SAE) became one of the main topics of research and application in survey sampling. The great interest in this topic is in response to a continuing demand for reliable small area statistics, even for areas or domains with very small samples or no samples. Interest in SAE methods has further enhanced due to the recent trend in many countries to base their censuses on administrative records, supplemented by small area estimates that serve for testing, correcting and supplementing the administrative data. In the present talk I shall overview some of the main developments in SAE in recent years, distinguishing between Bayesian and frequentist approaches, applied to cross-sectional and times series models. The overview will be preceded by a brief discussion of earlier developments, which will serve as a background for the new developments. ","The principal goal of this paper is to argue the presupposition of its title. More specifically, the claim is that survey research has fallen approximately 20 years behind developments in relevant basic science. The paper limits its scope to a single but broad topic, research on memory. A timeline is offered to establish both parts of the claim, namely (1) a qualitative claim that survey research overlooks important basic findings in memory, and (2) a quantitative claim that the gap is approximately 20 years. The timeline comprises papers and books chosen to illustrate advances in the basic science or implications of memory research for other areas of psychology and behavioral science generally. The paper offers a few examples of key issues in survey research where the effect of the 20-year gap is evident. The discussion section suggests a few answers to the why question of the title. ","As demand for call scheduling efficiency in a telephone survey increases, two areas that have received scant attention in call scheduling are the so-called seasonality effect and most effective time of day to call. We will use detailed calling history data from 2008 Behavioral Risk Factor Surveillance System (BRFSS) to identify ways to reduce efforts for cases which have historically yielded very few completes and to reduce interviewers' burden by calling more efficiently. We will use logistic regression to test whether seasonality affects the likelihood of a first attempt answer for calls at different time period of day. We'll examine which time period of day the first attempt calls made could be significantly more likely to result in a higher rate of completion. We'll further demonstrate significant difference in this correlation by seasonality (quarterly) and geographic areas. ","Collaboration between STATCOM at DePaul, a graduate student run not-for-profit (NFP), and Community Building Tutors resulted in a research project that utilized sample statistics to quantify the community impact of a 2-day summer festival. Three types of surveys were implemented at or after the festival with varying success: non-random sample based on a raffle, a stratified sample performed at event exits, and a non-random email survey conducted 3-months post event. Each method has different limitations but taken together allow for robust estimates of the festival's community impact. Our findings resulted in the creation of new metrics to quantify the impact of community festivals: festival stimulated volunteerism and repeat visit to community space. These and other metrics allow for festival benchmarking and stronger funding arguments for community centric festivals. ","Matrix sampling methods involve dividing a lengthy questionnaire into subsets of questions and administering each subset to subsamples of a full sample. In a panel survey, information about a sample unit can be learned during the first interview and this information can be used both to assign questions and to impute missing quantities at later interviews. Previous research has considered estimators based on available cases and simple adjustments to the design weights (Gonzalez and Eltinge 2008). Here we extend this research by developing an imputation procedure for recovering the data not collected from a sample unit at subsequent interviews. We use data from the Consumer Expenditure Quarterly Interview Survey to explore potential efficiency gains incurred from incorporating these imputation methods into the estimation procedures of an adaptive matrix sampling design for a panel survey. ","Genetic data collected during the second phase of the Third National Health and Nutrition Examination Survey (NHANES III) enable us to investigate the association of a wide variety of health factors with regard to genetic variation. The classic question when looking into the genetic variations in a population is whether the population is in the state of Hardy-Weinberg Equilibrium (HWE). Our objective was to develop test procedures using family data from complex surveys such as NHANES III. We developed six Pearson ?2 based tests for a diallelic locus of autosomal genes. The finite sample properties of proposed test procedures were evaluated via Monte Carlo simulation studies and the Rao-Scott first order corrected test was recommended. Test procedures were applied to three loci from NHANES III genetic databases, i.e., ADRB2, TGFB1, and VDR. ","In preparation for the 2010 Census, the U.S. Census Bureau pretested two forms that are targeted at populations that could be missed using traditional census procedures. The populations of interest are those who have no \"usual place to stay\" but who frequent transitory locations like RV parks and marinas; people experiencing homelessness; and people who do not have a city-style street address (including basement apartments and very rural addresses). This paper reviews the pretesting of the Be Counted and Enumeration of Transitory Locations forms, examining results specific to populations in atypical living situations. Issues of particular concern are, having a questionnaire that is easy to navigate given the low levels of literacy and form literacy of some respondents and the ability to gather geographically specific address information from respondents who are often highly mobile. ","Can a one contact produce defensible survey results in a census of 184,000 licensed professional health workers? In response to a legislative mandate health workers were contacted by letter and asked to respond to a web survey. The survey was limited to a one time contact only for the majority of respondents. Anticipating a concern for low response and representativeness, an experiment was implemented for 32 groups to allow evaluation of the effect on response rates, data quality, and characteristics. One third were randomly assigned a multi-contact mixed mode treatment for comparison. License characteristics were available for analysis and comparison to survey completions for 32 professions. Results show significant differences in response to the \"one time only\" compared to \"multi-contact mixed mode\" surveys. ","The present work investigated how helpful \"do not include\" statements (DNI), instructions to exclude certain items from categories, are to respondents' decision making. DNI statements can occur anywhere in a survey and are intended to guide respondents so that their answers are as intended by survey designers. Study participants were asked to consult an industry description and determine whether the descriptions accurately reflected a business described in an accompanying scenario. Some industry descriptions contained \"do not include\" statements while others did not. Accuracy measures were tabulated. The observed findings indicate that when DNI statements were present, respondents made more correct rejections. By contrast, respondents made more correct acceptances without DNI statements. ","Over the last two decades advances in computing technology have provided tools for data visualization that have changed the way statisticians analyze data. However, this revolution in data visualization has not been effectively incorporated in the analysis of complex survey data. In part, this is due to the fact that many visualization techniques are not designed to incorporate survey weights or multi-stage clustered designs in a way that mimics a conventional simple random sample data set. We explore the possibility of changing the structure of the complex survey data through the use of inverse sampling in order to allow the use of common visualization tools. In particular, we examine regression diagnostic plots. ","This study investigates methods for assessing variability of estimated life-table functions when data are longitudinal and involve a complex survey. Traditional methods for estimating such variability, developed by Chiang, are appropriate when the observations are independent. With longitudinal data and a complex survey design, however, there can be two types of dependence: (1) within age groups, due to, e.g., clustering in the survey; and (2) across age groups, due to observations for a single individual contributing information for more than one age group. Chiang's method is compared to balanced repeated replication in the context of a study of disparities in life expectancy, for which data are obtained through linkage of respondents in the National Health Interview Survey to death records. Differences in the two methods due to the two types of dependence discussed above are explored. ","In this study we consider model-based methods that can be used to account for clustering, stratification and weighting effects in complex-survey-design data.  Generalized linear mixed effect models were developed based on the adult sample from the public-release of 2007 National Health Interview Survey (NHIS).  For this public-release there are only two available levels of clustering, strata and Primary Sampling Units (PSUs) for use in the model-based method.  Model-based multilevel variance/covariance structures were estimated using algorithms given in the SAS procedure GLIMMIX. These model-based methods will be compared empirically with the design-based method of the SUDAAN software, as well as with a fixed effect model in the SAS procedure LOGISTIC.    ","This paper provides the theory for estimating the variance of the difference in two years' domain-level totals estimated under the stratified Bernoulli sample design. We modify the Henry et al. (2008) variance estimator for estimated change in domain-level totals, for \"planned domains,\" domains that are related to the sample design variables, and \"analysis domains,\" unplanned domains of interest at the analysis stage. Our variance estimator also takes into account three practical problems: a large overlap of units between two years' samples; changing compositions of units across years that produce \"stratum jumpers,\" which are population and sample units that shift across strata from one year to another; and changes in sampling rates across years. These problems affect estimating the covariance term in the variance of the difference. ","Two types of procedures for masking microdata are used: adding independent noise and multiplying by independent noise. The triangular distribution is one of the options for incorporating multiplicative noise. Kim (2007) developed the distributional form and investigated the first two moments of the truncated triangular distribution known to be better than an unmodified triangular distribution for multiplicative noise. Investigation of the statistical properties of data masked by using the truncated triangular distribution requires knowledge of the moments of the distribution. In this paper, we first derive the characteristic function for the truncated triangular distribution. Then the moments are derived from the characteristic function. We present a general formula for the moments. ","Respondent fatigue is often a problem in surveys. Participants may stop responding to a survey or may start choosing default answers. We developed a model to guide the order in which to ask survey questions. Our conclusions were that if we could ask all questions of all participants, we should ask them in a fixed order. If this was not possible, a subset should be randomly assigned at baseline. Monotonicity assumptions about item response could limit the number of follow-up questions. For example, if a person reported chronic health problems at baseline, it might be reasonable to assume the same at follow-up. Follow-up questions that needed to be asked should be asked in the same order as at baseline. Although not immediately intuitive, respondent fatigue can actually increase the power to detect effects in certain situations by decreasing estimator variance. ","The Telephone Point-of-Purchase Survey (TPOPS) is a random-digit-dialed, computer-assisted-telephone-interview survey that provides the Consumer Price Index with a sampling frame of retail establishments for the most of items priced in the index. TPOPS is a rotating panel survey administered across the country, in only households with residential landline telephone numbers. TPOPS response rates have fallen precipitously from an average of 70% in the late 1990s to about 45% more recently. While the primary concern with non-response is the missing cell-phone only population, other demographic cohorts may be underrepresented, also. This paper discusses the results of a TPOPS non-response bias analysis. Spending patterns of underrepresented populations are compared to the sampled landline population, and methods which can be used to mitigate non-response in the short-term are recommended. ","It is often the case that a survey of persons at institutions (e.g. teachers at schools or staff at offices) requires that one or more persons be selected from each institution by an administrator at the institution. A common way of doing this is to ask that a list of eligible potential respondents be drawn and that a set of random numbers provided to the administrator be used to draw the sample. Experience shows that this task is both difficult and burdensome. An alternate approach when the number to be selected from each institution is small is to provide a list of names (first and last) and ask that the person who follows each alphabetically be selected. This method has several potential biases when the population as a whole or in some institutions differs from the one from which names were drawn. These biases are explored here through simulations. ","We describe a methodology and software to compute respondent weights for complex samples that finds an optimal set of non-negative weights satisfying a given set of restrictions on the weights. The problem is formulated as a constrained optimization problem where the objective function is a quadratic function of the weights. Empirical results from a comparison of this weighting methodology to a raking approach will be presented to illustrate the advantages of the methodology, focusing on its capability as well as weight efficiency. ","Creation of replicate weights for the jackknife procedure should reproduce the adjustment patterns of the main weighting. Often this is not possible because all the information is not available or the process is too cumbersome. When nonresponse adjustments using propensity scores are done, and there is no post-stratification, there are four possible ways of adjusting replicate weights. Thus complete reproduction of the weighting procedure may not be necessary. One can build replicates with the initial sample and reproduce the propensity categories, accept the categories but recalibrate the adjustments, estimate the change in adjustment coefficients from the respondents only, or apply the same adjustments as were applied in the original sample. Using simulated data, estimates from the four approaches were compared under different conditions. ","This article highlights yet another application of empirical Bayes estimation. The empirical Bayes (EB) approach for parameter estimation has the capacity to yield more precise estimates than methods based on sampling theory. It can give sensible results in situations where sampling theory falters. It is this desirable property that has motivated this application in estimating indicators of vehicular traffic density (i.e. proportion of students who use personal motor vehicles) at the main campus of the University of Lagos (UNILAG for short). The EB estimator of a proportion P is the shrinkage estimator which is a weighted average of the prior proportion estimator and the sample proportion estimator. The EB estimator has smaller variance and very narrow confidence intervals than sample theory estimators. ","Deconstructing a response rate into contact and cooperation rates can provide insight into the nature of nonresponse. For example, the contact rate for the 14,818 Gulf War era veterans selected for the United States Military Health Survey was 70.9 percent. Among those who were contacted however, the cooperation rate was 82.4 percent. Combining these produces a 58.4 percent response rate which is more influenced by contact propensity than cooperation propensity. We used sequential weight adjustments for contact and cooperation based on the propensity of a success at each stage. This paper (1) describes the modeling methodology, (2) investigates the relationship between the contact propensity and cooperation propensity, and (3) compares estimates using weights from our two-stage methodology with estimates from weights constructed from the design weights and a single-stage adjustment. ","We propose a new approach to small area estimation by borrowing strength through a Bayesian prior. This new prior leads to a generalized Polya posterior, which works as a constrained Polya posterior estimation when auxiliary information is available. The Bayes like character of the generalized Polya posterior determines the admissibility of our estimator. Our new approach needs no assumption of linearity. It utilizes both the area-specific and element-specific auxiliary information from each small area and outperforms classical regression methods. Variability of prior parameters allow more flexibility in estimating. This approach could be applied to cluster sampling as well. An approximation to the new estimator by using importance sampling with polar coordinates is proposed. It also provides a general tool to generate random samples in a constrained high-dimensional convex polytope. ","Aggregated Relational Data (ARD), originally introduced by Killworth et al. (1998) as \"How many X's do you know\" survey questions, are a common tool for observing social networks indirectly. Previous methods for ARD estimate specifc network features, such as overdispersion. We suggest a more general approach to understanding social structure using ARD based on a latent space framework. We first show ARD contain information about latent structure by apply a primitive latent-space model to data from McCarty et al. (2001). This example also demonstrates the utility of these models for understanding the networks of individuals who are dicult to reach with traditional surveys. We then suggest using latent space models as a unied framework for inference with ARD by demonstrating that the network features estimated using previous methods can be represented as latent structure. ","The NSCG is a longitudinal survey that derived its current sample from the 2000 census long form. With the long form being replaced by the American Community Survey (ACS), we are evaluating the ACS as a sampling frame for the NSCG. As part of the evaluation, we examined design options for the 2010 NSCG decade and chose a rotating panel design. To implement this rotating panel design, the 2010 NSCG will use a dual frame design with sample from both an ACS frame and the 2000-decade NSCG sample. In subsequent survey cycles, the 2000-decade cases will rotate out and will be replaced by sample from more recent ACS years. Since the sample from the ACS and 2000-decade frames cover overlapping populations, determining the correct dual frame methodology to bring together sample from the two frames will be paramount in deriving 2010 NSCG estimates. This poster describes our ongoing research. ","Secondary data analysis of complex sample survey results is common among social scientists. Yet, the degree to which unbiased estimates and accurate inferences can be made from complex samples depends on the care researchers take when analyzing the data, including strategies for the treatment of missing data. Several studies have illustrated that the results of subpopulation analysis may diverge from those obtained through listwise deletion. However, given the paucity of simulation work in this area, it is not clear how frequently discernable discrepancies will arise. This Monte Carlo study focuses on the impact of listwise deletion versus a subpopulation analysis, when the data are MCAR and MAR, in the context of multiple regression analysis of complex sample data. Results are presented in terms of statistical bias in parameter estimates and both confidence interval width and coverage. ","We compared the results of a grocery survey using an ABS frame to the same survey using a RDD sampling frame of Texas households. Special attention is given to the response rate and coverage issues of demographics. ABS sampling provides some relief to coverage issues related to cell phone only households and households without a telephone. The coverage and non-response bias inherent in the RDD sampling frame is examined and compared to the ABS sampling frame. We also compare costs of both sampling frames. Lastly, the extra cost of the ABS frame is weighed against possible bias in the RDD sampling frame. ","Since 2002, the United States Office of Personnel Management (OPM) has conducted the Federal Human Capital Survey, a biennial study aimed at measuring federal employees' satisfaction on a variety of work environment topics. The survey also contains questions regarding one's retirement horizon and intent to leave. This paper compares survey estimates with personnel data for a small set of agencies in an attempt to evaluate the degree of concordance between employees' intent to leave and what consequently occurred. Comparisons to OPM's model-based retirement projections will also be presented. ","Mixed effect models are fundamental tools for the analysis of longitudinal data, panel data and cross-sectional data. However, the complex nature of these models have made variable selection and parameter estimation a challenging problem. In this paper, we propose a simple iterative procedure that estimates and selects fixed and random effects for linear mixed models. In particular, this method utilizes the partial consistency property of the random effects so it selects the group of random effect coefficients simultaneously via a data-oriented penalty function (the smoothly clipped absolute deviation function). Simulation studies and data analysis are conducted to empirically examine the performance of this procedure under different sample sizes. We show that the proposed method is a consistent variable selection procedure with Oracle properties. ","The use of agreement answer scales in survey instruments is widely accepted, but little is known about how to word the labels that are presented to respondents. In multilingual research, this matter is further complicated because answer scales must be translated. Using ISSP data from 2002, we compare results from more than 30 countries that used one type of answer scales: a close translation of the original (strongly agree, agree, neither/nor, disagree, strongly disagree) or an adaptation of it (strongly agree, somewhat agree, and so on). Using Graded Response Models we show that changes response category labels commonly found in multilingual research affect how respondents use answer scales and the usefulness of each of the 5 scale points to measure the underlying construct. These results are discussed in relation to existing literature on response styles differences across cultures. ","Multiple imputation algorithms using a sequence of regression models are commonly used to handle non-responses in complex survey studies. Although such algorithms have several advantages over joint modeling of all survey variables, they have a theoretical limitation that the specified conditional distributions could be incompatible and the underlying joint distribution of the survey variables may not exist. Although previous simulation studies show that imputation algorithms using incompatible conditional distributions work well for some cases, the performance of such algorithms for complex data needs to be studied. We focus on general multivariate data to assess the convergence properties of the imputation algorithms using various types of conditionally specified models. We also evaluate the impact of incompatible models on imputation results through simulation studies.  ","Panel studies that include unequal selection of individuals often also suffer from intermittent nonresponse. Probability weighting may be used to correct bias due to unequal inclusion of individuals and wave-specific nonresponse. Several weighting approaches were applied to models of change for repeated continuous outcomes with nonignorable missingness. Estimator properties for the weighting methods for models of change in the mixed (random effects) and marginal modeling frameworks were compared empirically using simulated data. Probability weighting that accounts for both the unequal selection of individuals into the study and intermittent dropout over time performs the best in a marginal model. Traditional panel weighting used with the sample of individuals observed at every time point also performs well with larger sample sizes compared to weighting observations in the mixed model. ","In demographic and health surveys (DHS) sometimes both women and men are interviewed in selected households allowing matching of partner information and analyses of couples. Typically individual data from such sample surveys have sampling weights which incorporate factors for both the probability of selection and non-response. For analyses of couple data neither the weights for females nor the weights for males is appropriate. We present formula for estimating the appropriate weights from public data and apply these to an example data set of couples (The Dominican Republic DHS of 1996). ","National surveys have been producing estimates of activity limitations associated with seeing and hearing for many years, yet, estimates across surveys are often different. National estimates of activity limitations associated with sensory functioning from the National Health Interview Survey, the Medical Expenditure Panel Survey, and the Medicare Current Beneficiary Survey are compared. We explore potential reasons for why differences in estimates exist. Differences may be due in part to how the questions are asked: e.g., whether asking about the amount of trouble one has or asking about difficulty in specific tasks. While efforts have been undertaken to recommend standardized measures, question differences still exist. Discussion will focus on the value of reasonable consistency among sensory activity questions across surveys and suggestions for facilitating that process. ","Three key variables that explain radio listening variation among panelists in Arbitron's radio surveys are age, race and gender. Users of Arbitron data, primarily radio broadcasters and advertising agencies, require Arbitron to produce a variety of estimates for certain breakouts of these three variables, an example being estimates of radio listening for Black Males 18--34. Because these three characteristics are so critical to producing, understanding, and using the survey data we conducted a study to determine if it was better to weight the survey data separately by these variables, or if some form of nesting during weighting would be best. We used a regression analysis and a mean square error analysis to assess the effectives of age, race and gender nesting for both broad and narrow demographic estimates of radio listening. ","In observational studies, investigators have no control over the treatment assignment. Thus, the estimates of treatment effects may be biased because of the differences in the distribution of covariates between treatment and control groups. Propensity score matching is often used to balance the distributions of covariates. The propensity score matching and the statistical test of the treatment effect become more complicated when complex survey data are used. Appropriate survey weights should be incorporated into the calculation of propensity scores and the single-level statistical analysis conducted after matching. This study compares the use of survey weights in calculating propensity scores and statistical analysis of treatment effect after propensity scores matching. Simulated and read survey data are used to demonstrate the essence of the accommodation of survey weights. ","Arbitron's current method for forming confidence intervals for radio rating estimates is based on the normal distribution. It is well-documented that with estimates of proportions from sample surveys the usual confidence interval method based on the normal distribution is generally lacking in some important situations. One of those situations is when the estimated proportions are very small and especially when sample sizes are also small. Because Arbitron ratings represent estimates of small proportions, we recently re-evaluated our confidence interval methodology, which was first implemented in the early 1980's when radio listening was less fragmented - ratings were generally larger - and the methodology was less likely to fail. This poster displays the results of an empirical study designed to compare the currently-used Wald confidence intervals to some alternatives. ","Non-response adjustment in the Israeli Social Survey (ISS) is based on the MAR assumption. Association of non-response with key socio-economic characteristics like income and religiosity, which do not correlate strongly with standard survey design and calibration variables, may corrupt the MAR assumption validity. We analyze survey and item non-response in the ISS by estimating nonparametric sharp bounds for the conditional mean of a variable. The width of the interval between the bounds is used as an estimator of non-response influence. We find significant departures from MAR assumption in the ISS data. The non-response propensity differs between various population groups assumed to be homogenous according to the survey design. We propose to utilize information about incomes and religiosity, available on the individual or neighborhood level, for improving the ISS design. ","When data are not missing at random, approaches to reduce nonresponse bias include subsampling nonresponding units and modeling. Information from nonrespondent subsamples may be used to develop models for bias adjustment. When information from a nonrespondent subsample is available, weighting methods for missing-at-random data may be modified to reduce bias in estimates of population totals. Propensity score methodology for nonignorable missingness is used with the post-stratification adjustment and with the Horvitz-Thompson estimator to account for the dependence between the outcome of interest and the response mechanism. The proposed techniques are applied to a binary outcome subject to nonignorable missingness from a complex survey of elk hunters. ","This study shows a tailored RDD survey design by using two types of data available regardless of response status. The first coming from survey operations exhibit the history of all telephone calls made to each sample and the indicators of survey design features, such as rigorous refusal conversion. The second is prepared by linking geographic identifiers of all sample telephone numbers to the external data. As regarding survey response behaviors as a stochastic process, we fit multilevel models to predict how response behaviors change given hypothetical design features. The model is applied to any sample with the same set of variables, and the response behavior of a new sample can be predicted before fielding the survey. Based on the predicted response behavior, the design features may be tailored for each case so as to maximize positive response behavior for fixed costs. ","In the present investigation, we consider the problem of estimation of population total using the well known Rao, Hartley and Cochran (1962), say the RHC scheme, in the presence of dubious random non-response. The proposed estimator has been compared with the usual estimator of the population total in the presence of random non-response. A new idea of \"Dubious Random Non-response (DRN)\" through transformations on the response probabilities has been introduced and studied. ","RDD telephone sampling methods have depended on a high coverage of the general population for their use in the survey research industry. While there has always been some level of non-coverage in the sampling frame, the increase of the percentage of the population who exclusive, or nearly exclusively, use cell phones instead of landlines has decreased the coverage of traditional RDD methods to below 80%. While cell phone samples are available, they are not cell phone exclusive or cell phone dominant in nature. Therefore, in order to get a representative sample of the population of interest, the options are costly. This paper proposes a methodology based on propensity score matching that identifies proxies in the traditional RDD sample and weights them accordingly to produce a representative sample at nearly the same cost structure as before. ","When we have data with missing values, the assumption that data are missing at random is very convenient. It is, however, sometimes questionable because some of missing values could be strongly related to the underlying true values. We recently introduced methods for nonignorable multivariate missing data, called a Latent-Class Selection Model (LCSM). In the LCSM, the missingness is assumed to be related to the variables in question, and to additional covariates, through a latent variable measured by the missingness indicators. The methodology developed here is useful for investigating the sensitivity of one's estimates to untestable assumptions about the missing data mechanism. A small simulation study is conducted to compare to its performance of the proposed method to that of MAR-based alternatives. ","Prior theoretical work on survey nonresponse adjustment has shown that bias and variance reduction of a respondent mean requires high correlation between an adjustment variable, survey participation, and the variables of interest (Y) (Little and Vartivarian 2005). In practice, multiple auxiliary variables are used whose association with participation and Y varies in strength and direction and may be correlated with each other. In this paper, we will use simulation to examine how multiple influences on response propensity and Y affect nonresponse bias of the unadjusted respondent mean of Y, the correlation between estimated propensity and Y, and bias and RMSE of adjusted means. The simulations also examine the efficacy of nonresponse adjustment when one auxiliary variable is omitted from the response propensity model. The results will be compared to findings from existing surveys. ","Small area estimation with area level models requires variance estimates of the direct survey point estimates being modeled. Small area direct variance estimates are likely to be unstable, suggesting modeling the variances to improve them. One aspect of such modeling would be to specify a probability distribution of variance estimators-Fay's successive difference replication variance estimator. We examine via simulations whether the variance estimator could be assumed to approximately follow a scaled chi-squared distribution, and if so, with what value of the degrees of freedom? We study these  for simple random samples of various sizes from various distributions (normal, Poisson, and Bernoulli). The motivation comes from county modeling of ACS poverty estimates by the Census Bureau's SAIPE program, as direct variances of the ACS poverty estimates are produced using Fay's estimator. ","Fahimi et al. (2008) concluded that traditional list-assisted RDD samples that use \"1 +\" (or working) 100 banks include only 80 percent of landline households while using a \"1+\" 1,000 bank sample would increase coverage of this group to approximately 90 percent. This paper presents results from a survey that used a Genesys RDD sample selected from \"1+\" 1,000 banks. A random subsample (50 percent) was processed to purge non-household numbers and flags were added identifying sampled numbers that would be included in a \"1+\" 100 bank frame. We examine the result of the Genesys processing and our calling results, to assess potential coverage bias, and discuss whether it would be more efficient (considering bias, variance, and cost) to rely only on 100 banks, solely on 1,000 bank sampling, or employ a strategy of oversampling the 100 banks.  ","A small area model that accounts for trends and state effects is used to obtain county-level vaccination coverage rates for a multi-year period. Model-based county-level estimates - derived using a prediction approach - are a weighted combination of multiple estimates. ","Designed-based variances in the American Community Survey (ACS) are estimated using a replicate weight methodology (Fay, 1995). In counties with small sample sizes, the variance estimates of poverty statistics show wide variation as a function of sample size. Generalized Variance Functions (GVF) can be used to smooth out the uncertainty of the design based variance. We propose incorporating GVFs with small area model techniques to smooth out the variability in counties where the precision of the design-based variance is lacking. These smoothed variances can then be used in the small area models for poverty estimates. ","When the cells of a contingency table are unplanned domains, small realized sample sizes can cause direct survey estimators to be unreliable. We develop a procedure that obtains more stable estimators of the cell totals and proportions in a two-way table under  an assumption that the margins of the table are well estimated. The method preserves the direct estimators of the marginal totals and incorporates information from a previous census. The procedure was developed for the Canadian Labour Force Survey as a way to improve occupational detail at the province level. In a simulation, the predictor achieves a smaller mean squared error than the direct  estimator. Due to variability in the direct estimators of the marginal totals, the reduction in the MSE is greater for proportions than for totals. Empirical coverages of nominal 95% prediction intervals are betweeen 93% and 96%. ","Researchers often attempt to compare the prevalence of an attribute between subgroups of a population or between time periods. To account for distributional differences, one solution is to examine the difference between standardized estimates. We will discuss creating standardized estimates via the predictive marginals approach with data from SAMHSA and DoD studies. The predictive marginal approach is a generalization of the direct standardization technique and can be useful when controlling for a large number of effects during the standardization process. Another alternative approach to standardized estimates is direct estimates that are produced using weights from a model based weight adjustment. This weight adjustment will force the sum of weights in each subgroup to match across the main effects in the model. ","A primary goal of the 2010 Demographic Surveys Sample Redesign is to consider switching to using a housing unit sampling frame based on the Master Address File (MAF) for current demographic household surveys if the frame provides an acceptable level of coverage. To measure coverage for a MAF-based frame, we selected a nationally representative sample of census blocks and sent them for field listing. By comparing the addresses collected during listing to the MAF addresses, we produce estimates of coverage at the national and regional levels. We also analyze selected subclasses, such as addresses in rural blocks, to identify strengths and weaknesses in the coverage provided by a MAF-based sampling frame. ","There has been much interest in the possible use of address-based lists from information resellers (IRs) as a potential frame for household surveys. In this study, we matched the InfoUSA file to the Census Bureau's Master Address File (MAF) for all 50 States and the District of Columbia. We use this matching to compare the coverage and content qualities of the InfoUSA file to the MAF. Additionally, we compared the InfoUSA file to a nationally representative sample of housing unit field enumerations. ","Area listing is usually considered to have the best quality among available methods of developing survey frames. However, there has been very little research done to investigate the errors of area listing. This paper will look into the results when two different field representatives canvass the same sample of blocks. How consistent are their results? Are some blocks or types of housing units harder to list than others? What characteristics are more likely to produce inconsistent listing results? ","One of the goals of the 2010 Demographic Surveys Sample Redesign is to consider using the continually updated Master Address File as the primary source to develop the sampling frames for current demographic household surveys. To support this goal, the Census Bureau conducted several coverage evaluations and found that coverage differences existed between the current frames and the MAF-based frame. This paper investigates how Current Population Survey estimates are impacted if current permit frame and area frame sample units not found on the MAF-based frame are excluded. ","Changes in resource allocation, population characteristics or inferential goals often lead survey designers to consider substantial changes in, e.g., the sample design, instrument, collection modes, field procedures, edit and imputation methods, and production systems. Transitions from the previous design to the new design often require careful management of related costs and risks. In this open roundtable, participants will be invited to respond to the following questions: 1) Describe some examples of survey transitions that you have encountered.; 2) For the transitions identified in (1), what were some important cost and risk factors?; 3) What were the primary steps that methodologists and survey managers took in managing the costs and risks identified in (2)?; 4) What were some \"success stories\" and \"lessons learned\" from the experiences covered in (1) through (3)? ","Large-scale surveys have relied on self-reported physical activity (PA) recall data to obtain information on energy expenditure (EE) and time spent in moderate to vigorous PA (MVPA). Because recall data are prone to measurement error, it may be possible to obtain better estimates of PA distributions if data from an objective reference measure are available on a portion of the sample. We will investigate the utility of this approach with a survey to collect PA data from a random sample of adults. Data will be obtained from two 24-hour PA recalls and from sophisticated armband monitors worn during the recall periods. We will investigate measurement error models for EE and MVPA using pilot data. PA models will be adapted from approaches developed for dietary components that are consumed daily and episodically. ","Misclassification occurs when there is a discrepancy in classification based on two different sources. Such an error could result in the loss of effective sample sizes in key domains within a race/ethnicity group. We assessed whether the misclassification of race/ethnicity occurred during the sampling frame construction, assuming that the data obtained from the respondents most closely represents the \"true values.\" For each true value, we calculated a proportion of cases misclassified into different categories. This proportion was calculated as the usual weighted survey estimate. Misclassification error can be reduced by use of weighting techniques such as post-stratification or raking adjustment. In this study, we investigated how misclassification in sampling variables affects survey estimates produced using weights that had been raked into three marginal population totals. ","The reinterview method is a powerful tool in nonsampling error analysis. The gross discrepancy rate (GDR) is the ratio of the sum of the two types of discrepancies to the total number of respondents in a yes/no type of question. The response variance is half of the GDR. Response bias is also estimated from the ratio of the net difference between the two discrepancies and the total. However the respondent's memory of the original response at the time of the reinterview may influence the respondent's answer. If the respondents are asked whether they remembered the original response and on that basis divide into two groups, the analysis of the resulting data helps us to estimate the response bias, response variance (simple and correlated).  ","This study investigates whether the type of telephone (landline versus mobile) can influence the response distribution of survey measures that are sensitive to the respondent's physical location. The mechanism posited is that if respondents are interviewed away from home, they may encounter different stimuli (and different response considerations) as compared to the stimuli they would encounter at home. Data come from a fully-randomized, repeated measures experiment. One group of subjects was interviewed first on a mobile phone and two months later on a landline. Another group of subjects received the same treatments in the reverse order. Special items were administered in both surveys in order to test for this effect. ","The Census Bureau regularly conducts a large number of major demographic surveys. It has an ongoing Quality Control (QC) program which relies on a reinterview program as the primary method to evaluate field work or monitor the work of the field representatives (FRs), with a special focus on identifying falsified and misclassified interviews. Currently only about 2% of the original interviews in the Current Population Survey (CPS) are assigned for QC reinterview, and the sampling of QC reinterviews does not rely on any information other than interviewer's experience level. To improve the ability to detect falsified cases, we fit a logistic regression model to predict the likelihood of falsification using the data from the original interviews as the predictor variables. The fitted model was then used to construct two alternative sampling schemes for the QC reinterview. ","The Quarterly Financial Report Survey (QFR) collects income and balance sheet data for most manufacturing corporations and for large mining, wholesale, and retail corporations. Unit non-respondents are imputed using a combination of ratio and mean imputation. In order to enhance the imputation process by eliminating influential cases from the base, we investigated an iterative regression approach of outlier detection. The approach utilizes a combination of two regression diagnostics, leverage and studentized residuals. We compared the effectiveness of the \"regression fits\" approach to the Hidiriglou-Berthelot method of outlier detection for several positive valued QFR items. To evaluate the effectiveness of the approaches, we created plots of inliers and outliers. The \"regression fits\" approach can also detect outliers for negative valued QFR items. ","For the 2002 Census of Agriculture (COA), the number of operations misclassified (either as farms or non-farms) in the COA was estimated. Operations in NASS' June Agricultural Survey (JAS) and the COA were matched and their answers compared. Misclassification estimates were based on the assumption that the JAS was truth. The misclassification rate was small but it was clear that the JAS assumption was not always justified. The 2007 Classification Error Survey focused on understanding why operations reported differently in the JAS and COA, rather than estimating misclassification. Operations in the 2007 COA and JAS were matched but neither report was assumed as truth. Instead, operations were reinterviewed and respondents asked to resolve discrepancies. Errors were related to respondents, enumerators and NASS procedures and show that a multipart solution will be required to address them. ","This paper develops two robust Bayesian predictive estimators of finite population distribution functions and associated quantiles for continuous variables in the setting of unequal probability sampling, where inferences are based on the posterior predictive distribution of the non-sampled values. The first method fits a multinomial ordinal probit regression model of the distribution function evaluated at multiple values on a penalized spline of the selection probabilities. The second method is to posit a smoothly-varying relationship between the outcome and the selection probabilities by modeling both the mean function and the variance function using penalized splines. Simulation studies show that both methods yield estimators that are more efficient with closer to the nominal level credible intervals than the design-based estimators. ","Random forests, a data mining technique which uses multiple classification or regression trees, is a popular algorithm used for prediction. Inference and goodness-of-fit assessment, however, may require an estimator of variability. When a modified random forest algorithm is used to model mixed-effects data, an estimator of the vector of variance components is also required for prediction. We propose two estimators of the vector of variance components for random forest regression that take advantage of byproducts of the algorithm. The first estimator is based on the residual sum of squares from a random forest fit and uses a bootstrap bias correction. The second estimator is a difference-based estimator that uses proximity measures as weights. The estimators are evaluated through Monte Carlo simulations. ","When analyzing survey data it is often helpful to collapse strata in order to allow approximation of design-based variance estimators in cases that include strata with only one selected PSU and to improve the stability of these estimators. This work proposes an adaptive method for collapsing strata. For this we use recursively partition the data using strata indicator variables to build a regression tree. Important features of the proposed method include: a simple function representation of the tree; design based estimation of the variance of the tree coefficients; and use of backward elimination to collapse cells. We apply this method to data from the 2006 National Automotive Sampling System as an illustration. ","Single and joint inclusion probabilities are generally known in complex surveys up to the point where survey weights are modified for nonresponse and population controls. Best practice by survey practitioners usually includes weight adjustment, first by calibration, ratio adjustment or raking to correct for nonresponse; next by further steps to impose population survey controls; and often also by final steps involving weight truncation or cell-collapsing to constrain the modified weights so that large and small weights do not differ by too much. These adjustments are made in successive stages, the order of which may differ from one survey to another. In this paper, generalized calibration is adapted to make all these weight adjustments in a single stage, after which a linearization-based large-sample variance formula is available. The method is illustrated on SIPP 1996 first wave data. ","The Variance Inflation Factor (VIF) is widely employed to assess the degree of variance inflation of the parameter estimate for the i_th independent variable by its collinearity with the other independent variables in a regression model. However, little research has been done to extend it for the analysis of complex survey data by incorporating complex survey design features, which may relate to the independent variables and affect the collinearity patterns in the model. In this paper, two adjustment coefficients, zeta_i and varrho_i are defined to adjust conventional VIF when computing collinearity diagnostics in the analysis of complex survey data. We present both the model-based and design-based theory to justify the methods. Extended statistics will be applied to the data from 1998 Survey of Mental Health Organizations (SMHO) in a case study. ","Dan Horvitz was a leader in the development of statistical theory for and the practice of survey research. While writing his dissertation, he developed what came to be known as the Horvitz-Thompson estimator, about which papers are still being written today. Horvitz received his PhD from Iowa State University in 1953 where he studied under Oscar Kempthorne. He taught at the University of Pittsburg, helped establish a statistics department in Rangoon Burma, and had positions of increasing responsibility at the Research Triangle Institute from 1962 to 1988. In addition he was active in ASA activities and instrumental in the development of the National Institute of Statistical Sciences. This session will review his work an accomplishments in these areas. ","Horvitz and Thompson (1952) develop a theory for PPS sampling and estimation when sampling is PPS without replacement. The estimation procedure weights each selected observation by the inverse of the unit's overall selection probability and the unit's weight does not depend on the particular sample in which it was selected. They also show that nonzero pairwise probabilities are required for unbiased variance estimation. This paper shows how the Horvitz-Thompson PPS without replacement estimator and its variance are related to the PPS with replacement estimator and its variance as presented by Hansen, Hurwitz, and Madow. It also shows how this generalization can be related to minimum replacement designs which allow sampling units with large relative size measures (greater than 1/n) to be selected more than once. ","The underlying principle of Horvitz-Thompson (HT) estimation is that estimates of totals are weighted at the unit level by the inverse of the probability that the unit appears as a respondent in the sample. Without nonresponse, this probability is the sampling probability. With nonresponse, this probability is the product of the probabilities of selection and response. Thus, there is an important theoretical connection between the origins of estimation from probability samples. This paper traces the early developments of HT estimation ignoring nonresponse, reviews how nonresponse affects weighted estimate, considers the importance of total survey design, and summarizes more recent work on efforts to measure response propensities to produce HT estimates with reduced bias due to nonresponse. ","In this presentation we further discuss the approach introduced at the last JSM. We assume a model for the outcome under complete response and a model for the response probability, which, when combined, define the model holding for the outcome of a responding unit. Estimation of the model parameters is carried out by combining maximization of the likelihood obtained for the responding units with calibration equations that use known totals of the covariates. We investigate the properties of the resulting estimators and develop a new test statistic for testing which covariates to include in the   model for the response probabilities. We also consider imputation of the missing outcomes and covariate values, and estimation of the population mean of the outcome, with appropriate variance estimators. The results are illustrated using simulated data and a real data set of household incomes ","Long-standing federal information policy has discouraged unnecessary use and retention of Social Security Numbers (SSNs) by federal agencies. In 2007, the U.S. Office of Management and Budget issued new policy requiring breach notification policies in all federal departments. Given the high risk of identity theft and other privacy invasions from SSN loss, in particular, the policy also required agencies to catalogue current use and retention of SSNs, and to develop a plan for reducing unnecessary use. Two primary uses of SSNs by federal statistical agencies are for record linkage, particularly of survey to administrative records data, as well as for respondent locating, particularly in longitudinal studies. Our paper will discuss the rationale behind the OMB policy, some statistical agency responses to date, and the potential changes that could be required under pending legislation. ","The National Center for Health Statistics (NCHS) conducts mortality follow-up for its survey participants, creating an essential data resource for health research and policymaking. NCHS current mortality linkage employs a probabilistic matching process that uses Social Security Number (SSN) and other personal identifying information to select death records for survey participants. NCHS recently changed its collection of SSN information to include only the last four-digits, requiring an evaluation of linkage outcomes using partial SSN. Using the linkage of the National Health Interview Survey to death records, we compare linkage outcomes and mortality rates obtained using different SSN information. Findings will demonstrate the accuracy of partial SSN in record linkage and the utility of collecting such information for federal statistical agencies. ","In 2008 OMB issued guidance to Federal agencies to minimize the use of SSNs. Historically, SSNs have been a major component in tracing survey respondents over time. To test the feasibility of tracing respondents without using SSNs, in the fall of 2008 NISS, on behalf of NCES, entered into confidential agreements with three third party vendors to trace public school principals who left the field in the spring or summer of 2007. The sample was split with intentional overlap to provide a basis for evaluating the quality of the tracing information. The identified cases were also contacted as part of a short exploratory survey of former principals. In addition to the survey results, the survey contacts provided a further evaluation of the tracing results. The study design and the results and evaluation of the tracing efforts are the focus of this paper. ","In Canada, two important research tools are derived from administrative data to shed light on current and emerging issues. The first source of information is the T1 Family File (T1FF) produced from individual income tax declarations and completed by imputing non-filing spouses and dependents to create families based on the census family concept. After processing, the T1 Family File represents 96% of the Canadian population and indeed is a rich source of revenue and demographic information at a very detailed geographical level. The second source of information is the longitudinal administrative databank (LAD) which consists of a 20% sample of tax filers from the T1 Family File. These two sources of information as well as some examples of research studies which have used these sources will be presented.     ","Public authorities begun to build registers more than 4 decades ago. First, registers were independent of each other but about two decades ago Statistics Finland started to match and merge registers and other administrative records together using unique identifiers such as personal identity code, codes for businesses, coordinates for houses, business and service buildings and geographic areas. An outcome was the register-based census in 1990. A user of register data meets errors and non-up-to-date figures, but data are basically complete. Hence well-combined data are excellent for government and other policymakers. The paper presents a number of successful stories; for example how government subsidies have been shared to local authorities. Less successful stories are also presented. Many of them are related to increasing mobility of people and businesses within and between countries. ","Cancer surveillance requires estimates of the prevalence of cancer risk factors and screening for small areas such as counties or states. Two popular data sources are the Behavioral Risk Factor Surveillance System (BRFSS), a telephone survey conducted by state agencies, and the National Health Interview Survey (NHIS), an area probability sample survey conducted via face-to-face interviews. The BRFSS is larger and collects data from almost every county; but it has lower response rates and does not include subjects who live in households with no telephones. On the other hand, the NHIS is smaller and does not collect data from the majority of counties; but it includes both telephone and non-telephone households and has higher response rates. This talk describes work on using Bayesian methods to combine information from both surveys to incorporate their strengths into small-area estimates. ","Meta-analysis is an essential methodology for synthesizing evidence, particularly in the biomedical and social sciences. Quantitative research synthesis is frequently used to accumulate evidence about questions of interest; it can also be used to investigate the generalizability of research results, and to investigate alternative explanations for observed associations. In this talk, we discuss the role of meta-analysis in the assessment of drug safety considerations and consider the strengths and weaknesses of evidence generated from research synthesis. As a case study to help illustrate some of these issues, we consider the evidence from a meta-analysis of randomized controlled trials that the FDA used to inform their decision to issue a black box warning concerning the risk of suicidality in children who use antidepressants. ","For applications like measuring associations between drugs and adverse events in databases, the number of drugs and the number of adverse event codes may be so large that any particular drug-event combination occurs too infrequently for accurate estimation. If a hierarchy is available placing drugs into drug classes, as well as an analogous hierarchy of a medical event vocabulary, then Bayesian models can be used to allow small counts to \"borrow strength\" from other counts nearby in the two hierarchies. Diverse examples will be given using data from spontaneous reports, from collections of clinical trials, and from medical health records databases. ","This paper focuses on the objectives of the 2010 Census Program for Evaluation and Experiments (CPEX). It provides an overview of the evaluations and experiments topics and research questions. Additionally, it describes the planned assessments of operations, challenges facing the program, and how the Census Bureau is addressing those challenges. The 2010 CPEX will inform us about the 2010 Census and lead the way for planning the 2020 Census, and will provide useful information for the American Community Survey (ACS). The CPEX is designed to measure the effectiveness and impact on data quality of the Census 2010 design, operations, systems, and processes. The research objectives of the 2010 CPEX are based on seven research topics. The evaluations will address specific research questions and there are four experiments that will examine content changes and new methods. ","This paper discusses the coverage measurement program for the 2010 U.S. Census. It describes the goals of the program and the underlying key issues. It also describes the operations, how the 2010 program differs from the 2000 program, and why those changes were made. ","Counting everyone in a country as large and diverse as the United States is a significant challenge. To do the job well requires reaching everyone and persuading them to answer the census, despite myriad forces working against cooperation. Barriers range from lack of understanding of the census to serious concerns about whether the government can be trusted with personal information. To minimize these barriers and maximize participation, the Census Bureau will conduct a massive integrated communications campaign that includes outreach by an extensive network of partners, paid advertising, public relations, and Census in Schools. Each component will work to effectively reach the three campaign goals: 1) increase mail response, 2)improve accuracy and reduce the differential undercount, 3) improve cooperation with enumerators. ","The 2010 Census is nearly indescribable---an operation that is both national in its scope, but intensely local in its execution. Thousands of operations and activities must be integrated over the course of a few years and throughout the United States. This paper describes the key operations comprising the 2010 Census, from the recently completely address canvassing operation to the final dissemination of census products. The paper also includes discussion on progress on and challenges facing the operations, as well as what risk mitigation and contingencies plans are in place to address the challenges. ","Social desirability bias (SDB) is a tendency in people to present themselves in a more socially acceptable light, when faced with sensitive questions. People with higher degree of SDB tend to give answers that may not be accurate. Randomized Response Techniques (RRT) are among several techniques used by researchers to circumvent SDB in personal interview surveys. In this talk we will focus only on those RRT models that are appropriate for quantitative responses. We will discuss a variety of quantitative RRT models including full, partial and optional RRT models but the focus will be mainly on optional RRT models. We will discuss advantages and disadvantages of using additive or multiplicative scrambling, particularly when there is interest in estimating the sensitivity level of a sensitive question. ","Survey mode effects refer to systematic variations in the data obtained using different modes of data collection. Mode effects have been widely reported in the literature, but few mode effects studies have focused on the military population, whose demographics differ from the general population. Furthermore, military surveys typically involve questions that are unique to the military and military families. As multimode surveys are becoming the norm, it is important to understand if and how survey mode may influence sample estimates. Two recent Army surveys of military populations adopted a mixed mode approach involving mail and Web-based data collections. This paper examines the mode effects in these two surveys. The analysis was conducted using cross-tabulations, chi-square tests, and logistic regression models.  ","In 2003, the National Survey of Recent College Graduates (NSRCG) data collection protocol was changed from interviewer administration (CATI) to multi-mode administration (mail, Web, and CATI). To assess the effects of the data collection protocol change on the changes of estimates over time, an experiment was designed in advance before the 2003 fielding began. The 2003 sample was randomly partitioned into two groups: a control sample of 2,000 graduates to whom the survey was administered via CATI, and a treatment sample of 16,000 graduates to whom the multi-mode survey was administered. Comparisons were made between two groups on unit and item response rates, item editing rates, and key survey estimates. Statistically significant differences will point to a need for further analyses to assess the impact of the multi-mode data collection protocol on the NSRCG time-series analyses.  ","In this paper, we suggest a new method to estimate the proportions of two non-overlapping sensitive attributes using the famous method of Franklin (1990:Communications in Statistics-Theory and Methods). The extension is based on the recent work of Singh and Chen (2009: Journal of the Statistical Planning and Inference) where they suggest to utilize higher order moments of the scrambling variable to estimate single proportion of a sensitive attribute in survey sampling. The most important point is that here we use information from one sample to estimate proportions of two sensitive attributes. An attempt has been made to extend the proposed estimators to Bayesian and Empirical Bayes estimation techniques. Simulation studies are also performed to see the performance of the proposed estimators with their competitors. ","In spite of having a broad range of applications in surveys with sensitive questions, randomized response (RR) approach is limited in several ways including (i) a lack of reproducibility; (ii) a lack of trust from interviewees; and (iii) higher cost due to the use of randomizing devices. This article compares the efficiency between the non-randomized response (NRR) triangular model and the randomized Warner model. In addition, we propose the concept of the degree of privacy protection between the triangular model and the Warner model to reflect the extent the privacy is protected. Our studies show that not only the NRR approach is free of the limitations of the RR approach but also it in fact increase the (v) relative efficiency and (vi) the degree of privacy protection. Thus, the NRR approach offers an attractive alternative to the RR approach. ","It is estimated that less than 2% of the U.S. population has a history of epilepsy; however, existing prevalence estimates are not consistent and range from 0.4% to 1.1%. Moreover, there are no prevalence estimates for this disorder among minority subpopulations. The authors conducted a population-based survey to produce current prevalence estimates of epilepsy, overall and among African Americans and Hispanics. Because of the coverage problems associated with the RDD sampling methodology, an address-based sampling method was used to reduce undercoverage and facilitate application of a multi-mode method of data collection. Various findings from this research will be presented, including field results, demographic comparisons of respondents opting for different modes of response, efficacy of incentives in increasing response rates, and preliminary estimates of the outcome measures. ","The IRS conducted a survey of taxpayers to better understand the pre-filing and filing burden of individual taxpayers. The sampling frame was taxpayers who filed a 2007 income tax return in 2008. The overall response rate was approximately 40% with roughly 45% of the responses via phone, 35% via web, and 20% via mail. In this paper we explore the differences in respondents by response mode, with particular interest in those who responded via the web as the rate for this mode was unexpectedly high. We will also address the non-response bias and explore ways to adjust for this bias when the researcher is interested in a vector of estimates, not just one point estimate. For this study, total burden is actually the sum of seven separate but correlated estimates. Since we are able to link the survey responses back to the tax return, we have an especially rich data set to analyze. ","The Health Information National Trends Survey (HINTS) is a recurring household survey that collects data about the sources and characteristics of health information from a representative sample of all adults in the United States. Data for the 2007 HINTS were collected by telephone from an RDD sample of phone numbers and by mail from a sample of addresses. This paper focuses on the address sample, which was selected from a sampling frame based on the U.S. Postal Service's Delivery Sequence File and supplemented with non-USPS sources in rural areas. We discuss available stratification variables, characteristics of the rural-area supplemental addresses, rates of non-delivery and response by type of address, the handling of multiple ways a household can receive mail, and calculation of household- and person-level sampling weights. ","Under-coverage is one of the most common problems of sampling frames. To reduce the impact of coverage error on survey estimates several frames can be combined in order to achieve a complete coverage of the target population. Multiple frame estimators have been developed to be used in the context of multiple frame surveys. Sampling frames may overlap which is the case when a single unit of the sampling frame is related with more than one element of the target population. Indirect sampling (Lavall\u00e9e,1995) is an alternative approach to classical sampling theory in dealing with the overlapping problem of sampling frames on survey estimates. In this paper a new class of estimators is presented which is the result from merging dual frames estimators with indirect sampling estimators in order to bring together in a single estimator the effect of several frames on survey estimates. ","Increasingly, survey practitioners are considering the use of address-based sampling frames for household surveys. These frames are being studied as alternatives to random digit dial (RDD) frames, to facilitate the use of mixed-mode methods, and to supplement or replace traditional listing of addresses. The recent consideration of these frames has been made possible by the availability of files based on the United States Postal Service (USPS) Delivery Sequence File (DSF) and changes in addressing, particularly those resulting from the development of Enhanced-911 systems. Previous studies have, collectively, yielded an abundance of information about the utility of address-based sampling frames, and it has been critical to continue evaluating these frames in light of the ongoing changes in addressing. This paper extends the earlier results by evaluating the USPS-based address frame. ","Due to the declining coverage of random-digit-dialing (RDD) sampling frames resulting from the increasing proportion of households without a landline telephone, the National Immunization Survey (NIS) conducted a pilot in spring 2009 using address-based sampling (ABS) with multimode (telephone and mail) interviewing. Traditionally, data are collected in the NIS in two phases. In the first phase, households with 19-35 month-old children are identified using a list-assisted RDD survey, and data on the children's socio-demographic characteristics are collected. At the end of the RDD interview, consent is requested to contact the children's vaccination providers, who are then mailed an immunization history survey. This paper describes the ABS methodology employed in the pilot and presents operational results, including preliminary response and yield rates. ","REACH US (Racial and Ethnic Approaches to Community Health Across the U.S.) is an umbrella of community-based programs aimed to eliminate health disparities among racial and ethnic groups. Five of the REACH US programs are based in the Greater Los Angeles areas with various overlapping geographies and target populations, and with different combinations of scientific interests in cardiovascular disease, diabetes mellitus, adult immunization, and breast and cervical cancer. We will explore in this paper the potential of making cross-community comparisons, and discuss some of the issues. In particular, we evaluate the performance of Lohr's and Rao's pseudo-maximum likelihood estimator and Mercatti's multiplicity estimator. ","ABS with multimode data collection is being pursued as a cost-effective alternative to random digit dialing (RDD) and face-to-face interviewing; ABS frames have good coverage properties together with the potential for high (weighted) response rates at reasonable cost. This approach is to be used in 27 communities in REACH survey. REACH survey presents particular challenges and opportunities. The 27 communities vary dramatically: in size, from several Census tracts to whole states; in urbanicity, from 0% to 100%; in (racial/ethnic) eligibility, from 5% to 88%; and in address type, from 50% to 100% city style addresses. In this presentation, we will discuss the implications of these community characteristics on the ABS design for selected communities in REACH survey. ","Pattern-mixture models (PMM) and selection models (SM) are two alternative approaches for statistical analysis with incomplete data and a nonignorable missing-data mechanism. Both models make empirically unverifiable assumptions and need additional constraints to identify the parameters. We introduce Bayesian parameterizations to identify the PMM depending on the types of outcome and then translate these to the SM approach. This provides for a unified and robust parameterization that can be used for sensitivity analysis under either approach. The new parameterizations are easy-to-use and have intuitive interpretation from both PMM and SM perspectives. These models can be fitted using software implementing Gibbs sampling.  ","In studies with longitudinal data, missing responses often depend on values of responses. When missingness is monotone in the sense that a subject having a missing response at time t do not have any observation after time t, Tang, Little, and Raghunathan (2003) developed a semiparametric likelihood method for the estimation of parameters of interest. In practice, however, missingness is often not monotone and a direct application of the method in Tang et al.(2003) discards observed data from subjects having nonmonotone missing values. We extend the idea in Tang et al. (2003) to nonmonotone missing data and construct a semiparametric likelihood that utilizes all observed data. Our proposed estimators may be much more efficient than those obtained by discarding data. An application is illustrated using the household income data from the Health and Retirement Study. ","We consider assessment of nonresponse bias for the mean of a binary survey variable Y subject to nonresponse. We assume that there are a set of covariates observed for nonrespondents and respondents. To reduce dimensionality and for simplicity we reduce the covariates to a continuous proxy variable X that has the highest correlation with Y, estimated from a probit regression analysis of respondent data. We extend our previously proposed proxy-pattern mixture analysis for continuous outcomes to the binary outcome using a latent variable approach, applying a pattern-mixture model for the joint distribution of the proxy X and the underlying latent variable for the outcome Y. Methods are demonstrated through simulation and with data from the third National Health and Nutrition Examination Survey (NHANES III). ","Parameter estimation with nonignorable missing data is a challenging problem in statistics. Fully parametric approach for joint modeling of the response model and the population model can produce results that are very sensitive against the failure of the assumed model. We consider a more robust approach of modeling by describing the model for the nonresponding part as a exponential tilting of the model for the responding part, which can be justified under the assumption that the response mechanism can be expressed as a logistic regression model.   The model for the responding part can be estimated using a nonparametric method. Thus, the overall model can be called semi-parametric.   In this paper, based on the exponential tilting model, we propose a fractional imputation method that can be a useful computational tool for missing data analysis with non-ignorable missing data.  ","Felix-Medina and Thompson (2004) proposed a variant of link-tracing sampling to sample hidden populations. In their variant, an initial sample of venues is selected and the people found in the sampled venues are asked to nominate other members of the population to be included in the sample. Those authors derived maximum likelihood estimators of the population size under the assumption that the probability that a person is nominated does not depend on the nominee. In this work we extend their research into the case of heterogeneous nomination probabilities and derive maximum likelihood estimators of the  population size. In addition, we propose both Wald and profile likelihood confidence intervals for the size of the population. The results of simulations studies carried out by us show that in presence of heterogeneous probabilities the proposed estimators perform reasonably well. ","In a single-stage sample where the units report on multiple occasions, it is typical for some units to drop out of the sampling frame over time or at least drop out of sample (the deaths); for new units to be added to the frame (the births); while the remaining units stay in the original frame and, if selected, in sample. In this paper, we discuss an approach to adding sample units from the frame of continuing and birth units to compensate for the deaths, and reweighting the modified sample so that it results in unbiased estimates in the sense that the expected weight of each unit in the modified frame over all possible samples is 1. Possible applications, including sample augmentation of the first-stage sample of the Producer Price Index, are discussed, as are alternative sampling procedures that focus on converting the augmented sampling process to the selection of a single PPS sample. ","The Federal Reserve System relies on surveys of banks to monitor the aggregate use of paper checks and other major noncash payment methods. In recent surveys, the bank population was stratified by type and by a universally available measure of size correlated with payments, transaction deposits. For the estimation of, say, the number of checks, the separate ratio estimator has many desirable features. However, questions arose as to which and how many auxiliary variables should be used. Also, due to varied and significant levels of item nonresponse and adding-up requirements, constrained imputation methods were used for estimation which created special challenges for constructing error measures using standard methods, e.g., multiple imputation. Despite the difficulties, we find that the conclusion that check usage is declining relative to electronic payment methods is robust. ","Bryant, Hartley and Jessen (1960) have provided a method of selecting a sample under two-way stratification when the sample size is smaller than the number of strata. We consider the selection of a sample from a population with three variables of stratification when the sample size is smaller than the number of strata. It is required that the sample provide representation to each category of each stratification variable. An application of the two-way stratification procedure in two phases to identify strata from which samples are selected is presented. The estimation of population mean and its conditional variance are considered. "," ","Statistical bureaus in many countries conduct periodic sample surveys to obtain socioeconomic data for use in developing indices of interest to policy-makers and public agencies. The fixed periodicity of these surveys is a relic of the days when the primary data collection method was a census. Surveys, we argue, should be done continuously (i.e., as often as is practical and cost-effective) with much smaller samples. The policymakers and agencies are interested in changes in indices, and when continuous sampling uncovers such changes, larger surveys can be done to confirm them with modified scope and allocation. This dynamic approach, in contrast to the static approach reminiscent of a census, provides challenges to survey practitioners but would be more useful to policymakers and other users of data. To illustrate the challenges and opportunities, we consider several examples. ","Randomized impact evaluations are increasingly conducted in international development to assess more rigorously which interventions best improve conditions in poor countries. Set against backdrops of diverse cultures, political instability, and difficult conditions, though, randomized studies face challenges that conspire to reduce carefully-calibrated selection methods and power calculations. This paper will examine how the Millennium Challenge Corporation, a United States foreign aid program focused on poverty reduction and economic growth, has overcome those challenges, examining randomized evaluations in Georgia, Benin, and Nicaragua. It will compare how their randomization procedures and power estimates were affected by shifting project conditions, and the mix of quantitative and methodological approaches used preserve statistical rigor to provide reliable evidence for analysis. ","Most panel surveys are performed repeatedly, so that the overlapped respondents could provide better estimates of the monthly/yearly change. But, as the repeated interviews with the same panel increase the burden, rotation sample design is used. Recent studies suggest AK-composite estimator for the level change, Y_(t)-Y_(t-1), in rotated panel survey, which was first introduced by Kumar and Lee (1983). In this study, we introduce a composite estimator for estimating the change rate, {Y_(t)-Y_(t-1)}/Y_{t-1}, not the change itself.  We compare MSEs of composite estimators and those of simple estimator with In-for-6 simulated data. ","While influential values are rare in economic surveys, they are problematic when they occur. An influential value is a value that is correct but whose weighted contribution has an excessive effect on an estimated total. Currently the U.S. Census Bureau uses weight trimming to address influential values. Mulry and Feldpausch (2007) showed that two methodologies, M-Estimation and Clarke Winsorization, showed potential for improvements in detection and treatment of influential values in the Monthly Retail Trade Survey, a program that uses the Hidiroglou-Berthelot algorithm to identify potential outliers on a flow basis prior to final identification of influential values. The simulation assesses the effectiveness of the two methods under controlled (but realistic) scenarios by examining the statistical properties of the treated estimates obtained using each method over repeated sampling. ","It is common in survey research to have multiple sections of a questionnaire, each of which contains multiple items that measure related aspects of the participants. Even with a modest number of sections, such data sets can have a large number of variables relative to the number of cases in the study, and even low missingness rates among the variables can result in a high proportion of incomplete cases. Here we present a new method for multiply imputing missing items in such survey data. Instead of attempting to model the covariance between each pair of items, we model the relationships among a small number of factors extracted from each section. The method is illustrated on a set of items from a large national survey on adolescent health. ","During the U.S. presidential race of 2008, there were hundreds of polls conducted in each state to determine which candidate is most likely to win the state. We conducted a Bayesian analysis of these polls to determine the probability of each candidate winning each state. Our model included four categories for each voter's intention: Obama, McCain, undecided, and all third-party candidates (lumped together). We then used these state-by-state probabilities, along with the dynamic programming algorithm of Kaplan and Barnett (2003), to determine the posterior distribution for the number of electoral votes, and from that, the probability of each candidate winning the election. Nearly every day during the two months prior to the election, we monitored the polls and recomputed the state-by-state probabilities and the posterior distribution for the number of electoral votes for each candidate. ","The 2009 Public Employment Survey uses a new multi-stage sample method which combines cut-off sampling based on unit size with stratified sampling to reduce sample size, save resources, and improve the precision of the estimates. In this paper, we propose fitting either two separate linear models within size-based strata or one overall, based on the result of a hypothesis test of equality of the model coefficients. We study the variance and other statistical properties of this estimation method. We use data from the 2007 Public Employment Census to compare our method to the previous regression method.  ","During the 2008 election, as in every recent election, we have been inundated with polling results. The focus of these has been on the potential winning candidate and who will or did vote for him or her. Seldom have there been surveys that asked voters about the voting experience itself. This paper discusses such a poll. Issues of design are covered, with sampling and especially nonsampling issues given prominence. There have been smaller efforts of this sort in 2004 and 2006. The effort was divided into three parts. A sample of early voter experience in Ohio, and two samples of election-day experience, one in Ohio and one in the Washington Metropolitan area For the Washington Sample a conventional exit polling example was also conducted. ","The National Center for Health Statistics conducts two surveys of ambulatory care provided in the United States:  the National Ambulatory Medical Care Survey (NAMCS) and the National Hospital Ambulatory Medical Care Survey (NHAMCS). NAMCS is a nationally representative survey of ambulatory visits to non-federal office-based physicians, while NHAMCS is its counterpart in emergency and outpatient departments of non-federal, short-stay or general hospitals. This paper highlights changes made to the surveys between 2004 and 2010. These include dual frame samples of community health center physicians starting in 2006, dual mode surveys of physicians in 2008 and 2009, expansion of NHAMCS's scope in 2009 to include ambulatory surgery centers, supplemental surveys to NAMCS and NHAMCS on a variety of topics, and expansion of NHAMCS to measure correlates of emergency department crowding. ","The viability of traditional landline random-digit-dial (RDD) telephone surveys is being challenged due to a declining response rates and a sharp increase in persons switching to cell phone services by eliminating landline services. Given the popularity of RDD surveys in government, academic, political and market research field, it is important to address these challenges and discuss how to minimize the impact of these challenges. The panel members in this session will represent some of the large-scale RDD surveys in the US: National Immunization Survey, Behavioral Risk Factor Surveillance System, National Household Education Survey, and California Health Interview Survey. The panel will discuss challenges that each survey faces, findings from studies examining challenges, and its future plans. This session will be of interest to those who practice RDD survey for data collection. ","Many methods for handling missing data have been proposed in the literature. In a large survey with a complex design involving stratification and clustering, what methods work best and why? There are several ideas that can be considered. What are the next steps that are needed to critically evaluate options in real applications? The roundtable discussion will concentrate on these questions to generate an exchange of ideas. ","The ASA is planning an Advocacy Day Thursday of JSM, where statisticians will meet with their representatives on Capitol Hill. We'll probably focus on two issues, one on funding (e.g. basic research funding for NIH &amp; NSF, or funding for federal statistical agencies) and one policy (e.g. the Census, incorporating statistics in No Child Left Behind, or Climate Change). Those interested are  invited to join a discussion in preparation for advocacy day, and about how to be an effective advocate in general. ","The U.S. Consumer Product Safety Commission staff conducted a national probability survey on residential fires, including fires not attended by fire departments. Information was collected from 916 households, who told interviewers they had one or more fires in the past 90 days. Inability to remember complete dates of fires and decreasing estimates of annual fires with longer recall periods suggested that respondents were unable to recall some fire incidents. We adapt a method by Warner, Schenker, Heinen and Fingerhut (2005), for selecting the length of the recall period to minimize the mean square error (MSE) of the estimated number of fires, where the MSE is calculated from the recall bias and the sampling and imputation variance.  The results suggest that respondents do not recall events for long, which has cost and design implications for retrospective surveys. ","The Census Bureau conducts a census of governments every five years and a sample survey in the intercensal period. For the 2006 annual survey, a macro edit procedure was developed to isolate suspicious data at the aggregate level. Simple parameters were established based on subject-matter expertise. For the 2007 Census, the Hidiroglou-Berthelot method of determining edit bounds (H-B method) was used as a macro editing procedure. Both methods result in the identification of data that require manual review and analysis. This paper compares the methods used in the two years and provides recommendations for future applications. ","The U.S. Bureau of Labor Statistics' Quarterly Census of Employment and Wages (QCEW) program currently uses each establishment's year-ago trend in imputing missing employment and wages data without using an established imputation method; therefore, an empirical evaluation of well established methods, namely ratio and nearest-neighbor, is undertaken. This paper presents the analytical evaluation of these methods using current trends in the data. The reported data is simulated for imputing employment and wages on a random sample from QCEW Longitudinal Database (LDB). Both methods utilize exclusion criterion for removing influential observations from the computations. Finally, we offer comparisons of both methods, at stratum and aggregate levels, percentage relative errors.  ","The National Hospital Discharge Survey (NHDS) is currently conducted annually by the National Center for Health Statistics. This survey covers discharges from non-institutional, non-Federal, short-stay and general hospitals in the 50 States and the District of Columbia. Only three variables, namely age, sex and length of stay, are imputed for item non-response in NHDS data files. A hot deck method is used to impute the missing values. It is not known how this data imputation affects the variance approximations for estimates involving imputed values because the variance due to the imputation has not been evaluated for NHDS. This paper discusses an application of the multiple imputation method to estimate the magnitude of imputation variance for the 2006 NHDS. ","The Public Libraries Survey (PLS) imputes for missing data. Although the unit response rate is very high, there is still item nonresponse that requires imputation. PLS uses a variety of imputation methods depending on the item that is missing data. The purpose of this study is to evaluate current imputation methods along with some new alternatives. The imputation methods being evaluated are ratio, cell mean, adjusted cell mean, hot-deck, and combinations thereof. Another objective is to minimize the number of imputation methods being used. The study also compares current methods of determining the imputation cell boundaries to new methods using the population variable. ","Address-based sampling (ABS), the use of a comprehensive address database for sampling residences, is viewed as a viable alternative to random digit dialing. In November 2008, the Nielsen TV Ratings Diary moved from a landline telephone to an ABS frame, becoming the first major survey to make this important transition. The move was designed to solve three critical problems: (1) increasing noncoverage bias due to exclusion of cell phone only households and unlisted landline numbers in zero listed telephone banks, (2) number portability, and (3) declining representation of key demographic groups, particularly younger adults. The pioneering ABS design used a multi mode data collection approach, including Web, mail, and telephone recruitment tools. More than 500,000 addresses were sampled for the November 2008 implementation. We assess the success of this transition. ","The 2008 American National Election Survey (ANES) is an in-person, national household (HH) survey designed to collect data on voting, public opinion, and political participation as a way to better understand the theoretical and empirical foundations of national election outcomes. Since its inception in 1948, the ANES has been based on a multi-stage area sampling design that used field enumeration to construct a sampling frame of the HH population. In 2008, cost savings motivated a change to an address-based sampling (ABS) frame. We describe how the change to an ABS frame influenced the design and implementation of the ANES. In particular, we estimate the coverage of the sampling frame, and we evaluate the effectiveness of using ABS to over-sample Latinos and African Americans. We also discuss the alignment of postal addresses with geographic segments based on Census Block Groups. ","Address-based sample frames have emerged as a solution to shrinking RDD frame coverage due to cell phone-only households. Knowledge Networks' probability-based online KnowledgePanel\u00ae has used RDD frames for telephone recruiting since 1999. In 2008, KN conducted a pilot study (n= 10,000) to recruit panelists via mail using an ABS frame. KN developed/tested recruitment materials (advance postcard/initial mailer/non-respondent mailer) and a post-mailing telephone follow-up protocol. Half the sample was mailed the advance postcard, half was not. Randomized portions of each half received a $5 cash incentive, $1, or $0. Non-respondents with a landline match received a recruitment telephone call; the rest were mailed a reminder letter. Response was allowed via mail, online, or toll-free number. Results show demographic response/mode response/nested incentive conditions/advance postcard effects. ","Arbitron has been investigating the feasibility of using address-based sample frames. First, in 2007-08, we fielded two studies assessing whether CPO households (CPOs) could be identified in such frames (n=20,094). Indeed, they could be; therefore, we then placed radio listening diaries in these CPOs. Performance rates and costs were akin to or better than landline and RDD CPO samples. Third, responding to the 13% of cell-phone-primary households (CPPs), per the 2007 NHIS, questions to classify CPPs were added to the mail survey used in the previous studies. In Winter 2008-09, surveys were mailed to ~21,000 households to evaluate the percentage of CPPs in the address sample and their demographics. The findings and insights from these studies are key when determining if address frames should be used and if they are a methodologically sound way to include CPOs/CPPs in survey research. ","Address databases, derived from the USPS Delivery Sequence File, can serve as frames for face-to-face area-probability samples as well as multi-mode address based samples. Working with the National Children's Study (NCS), we continue our work to understand the coverage properties of these databases in order to determine what households tend to be covered vs. missed.  Recent in-field listings by the NCS give us an opportunity to compare the coverage of the address databases with a frame created in the field. After matching the listed addresses with two versions of the DSF, we returned to those housing units that were missed by one or more sources to collect additional data.  ","In a series of studies (a lab-based experiment, 2 web-based vignette studies, and a mail survey), we manipulated the risk of disclosure associated with participation in a survey, along with other factors including the possible harm associated with disclosure, survey topic, mode of data collection, and incentive. We examine the effect of these factors on stated willingness to participate (WTP) (in the lab and web studies) and actual participation (in the mail survey). We find a consistent effect of topic sensitivity on WTP. The manipulation of disclosure risk (e.g., 1 in ten versus 1 in a million) generally had little effect on WTP or actual participation, except under extreme conditions unlikely to occur in real surveys. This paper summarizes the results of this program of research and discusses the implications for survey practice. ","The National Children's Study (NCS) is designed to investigate the effects of environmental exposures and gene-environment interactions on pregnancy outcomes, child health and development, and precursors of adult disease. Of particular interest are precursors to rare disorders, such as autism. This longitudinal cohort study uses a multicenter collection design to follow a nationally representative sample of approximately 100,000 children born in the United States. Children will be followed pre-conceptually until age 21. Environmental samples, bio-specimens, family history, ultrasounds and medical records, images, and direct assessment of health and development will be collected. This unprecedented array of sensitive and personally identifiable data requires an innovative, comprehensive, and evolving data access plan. Preliminary data access policies and procedures will be presented. ","The National Health and Nutrition Exam Survey (NHANES) is a program of periodic surveys conducted by the National Center for Health Statistics (NCHS) that provide national estimates of the health and nutritional status of the U.S. civilian noninstitutionalized population. DNA specimens were collected during the NHANES III (1991--1994) and NHANES 1999--2002, then added back to the NHANES survey starting in 2007. In 2001, NCHS approved a protocol that allowed researchers to use genotyping data linked to NHANES data, although this data would only be accessed in the Research Data Center (RDC). This paper describes some of the issues involved in administration and disclosure review of genetic data. ","There are two main approaches to masking microdata---adding independent noise and multiplying by independent noise. The truncated triangular distribution, investigated by Kim (2007), was proposed for masking microdata with multiplicative noise: the masking variable is centered at one and truncated symmetrically around one so as to exclude values considered to be too close to one, thereby enhancing confidentiality protection. This paper generalizes Kim's earlier efforts by introducing truncated distributions of correlated noise in both the additive and multiplicative cases. Our method for adding correlated noise enables us to reproduce the first two moments of the original variable. We applied copulas to generate correlated noise. Two examples illustrate the methodology. ","Statistical data releases from statistical agencies (e.g., U.S. Census Bureau) have two basic requirements; usefulness (often at a detailed level) and confidentiality protection. We discuss a few protection techniques that satisfy those requirements, albeit in different ways. Many of our examples will involve two methods used to protect (economic) magnitude data tables. These are cell suppression and a simple type of noise that is added to microdata values. Some of our examples will involve data swapping to protect (demographic) categorical data tables. The common theme is uncertainty creation. The agency needs to determine how much in inherent in the data (e.g., due to sample design) and how much needs to be added (if any) to protect data against clever data intruders. The latter requires estimation of prior data knowledge that the best informed data users can obtain from public sources ","Fence method (Jiang et al. 2008) is a recently developed model selection strategy that is particularly suitable for nonconventional problems such as mixed model selection. It has been assumed that there exists a true model among the candidate models. We extend the fence method so that such a requirement is no longer needed. Furthermore, we develop two predictive measures of lack-of-fit for fence methods that are suitable for prediction problems. The method is applied to small area estimation problems. Methodology developments are supported by simulation studies and real data analyses. ","A popular approach to incorporating spatial information in small area estimation models between area variability via a Simultaneous Autoregressive (SAR) random effects model. An alternative method that incorporates spatial information locally is M-quantile Geographically Weighted Regression (GWR), which fits a local model to the regression M-quantiles of the conditional distribution of the outcome variable. A more global approach uses spline approximations to fit nonparametric M-quantile regression models that reflect spatial variation in the data. In this presentation we contrast SAR, M-quantile GWR and M-quantile spline models in terms of their performance using data with different levels of geographical detail. Our analysis is illustrated using data from the Environmental Monitoring and Assessment Program of the EPA and the World Bank's Albanian Living and Standards Measurement Study. ","Small area estimation has been extensively studied under linear mixed models. In particular, empirical best linear unbiased prediction (EBLUP) estimators of small area means and associated estimators of mean squared prediction error (MSPE) that are nearly unbiased have been developed. However, EBLUP estimators can be sensitive to outliers. Sinha and Rao (2007) developed a robust EBLUP method and demonstrated its advantages over EBLUP under a unit level linear mixed model in the presence of outliers in the random small area effects and/or unit level errors. A bootstrap method of estimating MSPE of the robust EBLUP was also proposed. In this paper, we relax the assumption of linear regression for the fixed part of the model and replace it by a weaker assumption of a penalized spline regression and develop robust EBLUP estimators. Bootstrap estimators of MSPE are also developed.  ","In this talk, we introduce a design-based model consistent estimator for a population domain mean. We consider here the unit level model, popularly known as nested-error regression model. We develop a fully design-based method for estimating the model parameters as well as a design-based EBLUP small area estimator, and derive their statistical properties. ","When informative dropouts exist for longitudinal studies, ignoring the informative dropout will result in biased results. Joint modeling of the outcome and dropout time can take into account some information from informative dropouts and correct some biases. We introduce a random pattern mixture model in this talk to jointly model the longitudinal binary outcome and dropout time; the random pattern effects in this context are defined as the latent effects linking the dropout process and the longitudinal outcome. Conditional on the random pattern effects, longitudinal binary outcome and dropout time are assumed independent. An EM algorithm is used for estimation. The method is applied to a data set from the Prevention of Suicide in Primary Care Elderly Collaborative Trial (PROSPECT). ","There are numerous measures that assess the effect of an observation, group of observations, a variable, or variables and observations on the regression estimation. Incomplete data is a common difficulty in data analysis. We introduce a new measure which assesses the effect of a missing observation, a group of missing observations, an incomplete variable or any combination of these, on the overall estimation. We call this measurement \"outfluence.\" The outfluence is a measure that can point to an observation or a group of observations that exert a disproportionate influence on the outcome of an analysis. The outfluence measure can be used in a regression analysis context or any other parametric settings. We illustrate the major benefits of outfluence using data example. ","Missing data are ubiquitous in longitudinal studies. In this paper, we propose an imputation model to handle ignorable drop-outs in longitudinal studies. Under the monotone missing pattern, the imputation task can be carried out sequentially. Our model allows the missing observations to depend on not only the immediate previous state, but also the entire history of the same participant. Furthermore, the built-in model selection mechanism chooses between parametric model and nonparametric model to impute each missing observation. Unlike the classic model selection problems which aim to find one model that fits the entire data set well, our choices of models are allowed to be different to impute each missing observations, that is, to find the best model to predict each missing observation.   ","Data from biomarker studies can be left- or right-truncated due to limitations of the assay. Efficiency-adjusted estimates of relative quantification of RT-PCR data are often right-truncated due to a limit in the number of cycles run and low copy number in the sample. When multiple genes from the same sample are run using RT-PCR, we can observe right-truncation in the estimates on  several variables. We propose a multivariate method of imputing the missing data in the right tail of the distribution of the estimates, and show that this method is less biased in estimating the parameters of the underlying distribution and results in better test properties than imputing the truncated data separately for each marginal univariate right-truncated distribution. ","A semi-recursive stochastic approximation to an idealized algorithm for estimating the most parsimonious generalized linear model with missing data and multiple covariates is studied. Simulation studies reveal robust properties of the approach under the missing at random assumption. Additionally, the method appears adequate for detecting interaction terms. Analysis of baseline and longitudinal measurements of CD4 count, HIV-1 genotype, RNA level is provided. ","This paper examines census captures as measured by the Census 2000 Accuracy and Coverage Evaluation. The premise of the research is that an accurate census person enumeration usually requires three successful census collections: the housing unit, a household population roster, and individual listing; and that a census  non-capture can be understood as coming from a failure at one of these steps. A logistic model is developed to measure the three phases of capture, and some population groups known to show high non-capture rates are examined to understand which collection phase(s) are most important in causing the high rate. ","The Consumer Expenditure Survey (CE) provides data on the buying habits of American consumers. The data are widely used in economic research as well as for weights in the Consumer Price Index. CE collects extensive demographic and socioeconomic data, including income data. CE staff assesses the quality of the survey's data by comparing CE data to similar data from other sources. Comparisons of income estimates first became possible with 2004 data, the year for which CE began to impute missing income values. This paper analyzes changes in the ratio of post-imputation CE income estimates to income estimates from the 2004-2007 Current Population Survey. The paper also gauges the impact of imputation on CE income estimates, comparing income reports prior to imputation in 2002 and 2003 with post-imputation data from 2004 to 2007.    ","Data files from the Statistics of Income Corporation sample for two recent years are analyzed to identify incidents in which taxpayer reported data have been perfected through the use of a procedure known as allocation. The perfection involves moving amounts to more specific areas of the statistical record from where it is initially reported by the taxpayer on the tax return. The characteristics of tax returns in which use of this procedure is most prevalent are examined. The value added for tax policy analysis is discussed.  ","An estimated 20 million individual income taxpayers utilize bank products (Refund Anticipation Loans (RALs) and Refund Anticipation Checks/Cards (RACs)) to obtain their income tax refund. Analysts in IRS were asked to determine if the use of RALs and RACs may also lead to less compliant behavior on the part of taxpayers and perhaps the preparers who facilitate access to these bank products. The primary issue we faced in conducting this research was that the only available data addressing noncompliance was from operational audits. Returns are not randomly selected for audit and therefore this data has selection bias. To address this bias, we chose to use propensity scoring techniques. This paper discusses the methodology we used in scoring, matching, estimation and sensitivity analysis. We found that the use of RALs was correlated with noncompliance, however the use of RACs was not. ","We evaluated trends in national vaccination coverage from 2000--2007 among children aged 19--35 months for the 6 universally recommended vaccines and the standard vaccine series, and predicted vaccination coverage levels in 2008--2010 for those vaccines that have not yet reached the Healthy People 2010 targets. We used the most recently available National Immunization Survey 2000-2007 data. Vaccination coverage trends were analyzed with weighted least squares linear regression models. Weibull and logarithmic regression models were fitted and extrapolated to predict vaccination coverage levels for 4+DTaP, 1+Var, and the 4:3:1:3:3:1 series from 2008--2010. From 2000--2007, observed vaccination coverage reached the 90% target for 3+Polio, 1+MMR, 3+Hib, and 3+HepB. Both Weibull and logarithmic regression models predicted that coverage with 4+DTaP will fall short of the target at 86% in 2010. ","The USDA National Agricultural Statistics Service (NASS) conducts the annual Agricultural Resource Management Survey (ARMS). ARMS collects detailed economic data from a sample of US agricultural operations and suffers from relatively low response rates for a federal survey. To compensate for nonresponse bias, coverage and measurement errors, NASS uses calibration weighting in the ARMS. Preliminary research using 2002 Census of Agriculture data, available for 2005 ARMS respondents and nonrespondents, indicated that calibration decreased nonresponse bias. The present study replicates prior research for the 2006 ARMS. The difference between respondents and the full sample was compared for several key survey variables both with and without calibration weights. 2006 results were compared to 2005 showing that calibration continues to be an effective method for reducing nonresponse bias. ","The Consumer Expenditure Interview Survey is a nationwide survey conducted by the U.S. Bureau of Labor Statistics to estimate the expenditures made by American households. The response rate for the survey is between 75 and 80 percent. In 2006, the Office of Management and Budget (OMB) issued a directive calling for any household survey with a response rate below 80 percent to conduct a study determining whether nonresponse introduces bias into the survey estimates. This paper is a synthesis of four studies undertaken to respond to OMB's directive. These studies include comparisons of response rates among survey subgroups, comparisons of respondents to an external data source, and using proxy nonrespondents to estimate nonresponse bias. Collectively, the studies show no meaningful bias in the survey's estimates even though the nonresponse is not 'missing completely at random.' ","I present methods of assessing the presence and influence of nonresponse bias on survey estimates used by two economic programs conducted by the U.S. Census Bureau: the Quarterly Services Survey (QSS) and the Annual Capital Expenditures Survey (ACES). These surveys represent a cross-section of non-response adjustment methods: the QSS uses ratio imputation; and the ACES uses weight adjustment. Business surveys compute two types of response rate: unit response rates, the ratio of the unweighted number of responding units to the number of eligible sampled units; and total quantity response rates, the weighted proportion of key estimates reported by responding units and from equivalent quality sources. I describe how these measures were used to target areas of potential nonresponse bias, then present subsequent analyses that used available auxiliary data to confirm initial findings. ","One of the prominent features of the Current Employment Statistics (CES) survey conducted by the U.S. Bureau of Labor Statistics (BLS) is that the data collection is conducted monthly and the estimates are published in a very short time period. It is important to analyze the effect of the survey nonresponse on the quality of the estimates. Several months after the reference period, universe data become available through the Quarterly Census of Employment and Wages (QCEW) BLS Program. The availability of the census data makes it possible to separate sources of potential divergence of the CES estimates from the corresponding population totals. The sources include sampling variability; the potential for nonresponse bias; the differences in the data reported to the two programs, i.e. reporting error; and frame imperfections due to the appearance of new establishments (births). ","The National Agriculture Statistics Service (NASS) conducts a census of agricultural every five years. NASS maintains a list frame (referred to as the Census Mail List or simply the CML) and mails census forms to every name on the list. However, there are many farms in the U.S. that are not on this list, creating undercoverage. To alleviate the effect of undercoverage bias, NASS employs an area frame (AF) sample of farms. The AF contains all the land area for the contiguous states and Hawaii and thus has complete coverage of all farms in the U.S. (except Alaska which currently has no AF). NASS uses the AF sample to estimate the number of farms that are not on the CML. NASS uses this information to adjust the weights of the CML respondents. This paper explains the methodology used to make these adjustments. ","The census of agriculture is conducted every five years and relies heavily upon comprehensive project management to accomplish the tasks required to ensure the high data quality standards. These tasks begin with a thorough evaluation of the most recently conducted census of agriculture. This paper describes the major steps involved to conduct the census of agriculture focusing on building the census mail list and measures of incompleteness (not-on-mail list); marketing the census of agriculture; utilization of various data collection methods; data analysis; summarization; and dissemination of results. ","For editing data from the 2007 U.S. Census of Agriculture, USDA/NASS built upon the foundation it laid for the 2002 Ag Census, when it ventured for the first time into editing and imputation on a large scale. Institutional acceptance of automated data correction was hindered in 2002 by problems inherent in establishing an entirely new enterprise-level processing system. Changes for 2007 were thus focused on making the system more responsive and reliable. This paper is an overview of improvements facilitated by development of a new imputation subsystem. New tools were provided---within the framework of decision logic tables (DLTs) built by NASS---not only to make donor data available directly to DLTs, but also to improve donor selection, especially through stratification. A variety of SAS techniques were used to achieve better speed in identifying and retrieving nearest neighbors. ","Every five years the USDA National Agricultural Statistics Service (NASS) conducts a Census of Agriculture for the entire US. NASS strives to achieve the most accurate results through diligent data collection and use of the best methodology available. For the 2007 Census of Agriculture, the non-response adjustment methodology was changed to incorporate the use of classification trees for each US state using a set of input variables describing several factors including size and type of farm as well as demographics of the operator. The trees split records into different groups based on which variables defined the largest difference with respect to response rate in each group. End groups, or leafs were adjusted for non-response based on the response rate of their respective leaf. This paper will present details and results of using this methodology on the 2007 Census of Agriculture. ","The census of agriculture data includes farm counts for all types and sizes of farms, including counts of farms operated by females and by individuals belonging to different racial and ethnic groups. These data are critically important to policy makers in developing agricultural programs that effectively support minority and socially disadvantaged farmers and ranchers. In order to improve the quality of census data on minority farms, the National Agricultural Statistics Service conducted an extensive outreach program in preparation for the 2007 Census of Agriculture. The outreach initiatives were intended to improve response to the census and coverage of farms by the Census Mail List. This paper describes the outreach initiatives and documents the resulting non-response and coverage adjustments applied to the 2007 census data. ","The Canadian Health Measures Survey at Statistics Canada consists of an in-home health questionnaire and a visit to a clinic where direct measures of health indicators are taken from a nationally representative sample of Canadians. The sample was drawn from a multi-stage complex design that used a stratification based on auxiliary variables obtained from the most recent Census. Many challenges needed to be addressed such as deterioration of the frame over time as well as age groups of significantly different sizes and the specific objective of obtaining equal numbers of respondents in each age group. This paper will provide an overview of the CHMS with emphasis on the efficiency of the adopted strategy to obtain sufficient sample size in the target groups. Other issues will be discussed, such as the effect of the distance a respondent has to travel to get to the clinic. ","Financial data for Canadian local governments are collected through various quarterly and annual surveys, known collectively as Local Government Surveys. For the last ten years, these surveys have been mailing out questionnaires to the same sample of municipalities. Over the years this has led to an increase in response burden, especially for small municipalities. A redesign has been proposed where annually updated population counts of municipalities would be used to select a new yearly sample as follows: large municipalities with certainty, small municipalities with a probability based on their population size, and the exclusion of the smallest municipalities. This paper will briefly summarize the Local Government program and will mainly focus on the various methodological issues behind the sample redesign of the Local Government Surveys. ","Like most establishment surveys the Public Employment Survey data are highly skewed. Our goal was to reduce the number of small units included in the sample since they collectively account for a small percentage of the total and account for a disproportionate number of sample units. A first stage sample was selected via a probability proportional to size method within each state by type of government (city, county, township, special district, school district) using the sample sizes from the previous sample design. In the second stage, smallness was determined using the Cumulative Square Root of the Frequency method within selected states by type of government. After smallness was determined, a subsample of the stratum containing the small units was then taken to reduce the size while maintaining comparability with the previous sample. ","The Occupational Employment Statistics (OES) survey provides detailed occupational employment and wage estimates for all metropolitan and balance of state (MSA/BOS) areas in the U.S. To provide consistency in annual state budgets and workloads, the OES state sample sizes have been fixed since 1995. As a result the reliability among areas across states has been inconsistent, especially since some states' economies have grown over time and become more industrially diverse. This paper describes a methodology to remove the fixed state allocation and change to a national allocation among MSA/BOS areas. This will result in a more robust design that will treat all MSA/BOS areas consistently, taking into consideration the industrial diversity and employment in each area compared to the other areas in the U.S and will provide more consistent reliability among area estimates throughout the U.S. ","The primary goal of the Drug Abuse Warning Network (DAWN) survey is to estimate the number of US emergency department (ED) visits involving drugs. These estimates are reported at the national level as well as for a small number of metropolitan areas. Two main statistical methods are used to improve the precision of the survey: stratification and ratio estimation. For stratification, within metropolitan areas, hospitals are assigned to ownership (public vs. private) and size categories (based on the total number of ED visits). For ratio estimation, the denominator is always the total number of ED visits. In this presentation, the efficacy of these two methods will be assessed. An alternative stratification scheme will be examined, and standard errors generated using no auxiliary information will be compared to those generated using ratio estimation. ","In 2007, the Committee on National Statistics released a report that evaluated the state and local government surveys conducted by the Census Bureau. The panel recommended a complete redesign of the Quarterly Tax Survey. This paper discusses the progress made so far in light of the panel's recommendations. The survey has undergone a sample redesign for both the property tax and the local non-property tax components. Dual processing of the new and old property tax samples was started in January 2009, while the local non-property tax collection forms have been redesigned and have undergone cognitive testing. Additionally, all components of the survey have new editing procedures and techniques for dealing with unit and item nonresponse. ","The Current Population Survey (CPS) sample selection has been based on Decennial Census data, so sampling has been done once every ten years. With the introduction of the American Community Survey (ACS) and a semiannually updated Master Address File (MAF), and with greater access to administrative records, it has been proposed that CPS sample selection be done more frequently. In this paper we discuss a general method of selecting second-stage CPS sample annually, which maintains the longstanding 4-8-4 sampling scheme. We address the expected benefits as well as drawbacks of changing to annual sampling. Specifically, this will include a discussion of the expected effect on changes in between-month correlations on CPS estimates and the efficiency of CPS estimates. ","This paper reviews how basic descriptive estimates for persons with disabilities are constructed from the Current Population Survey (CPS), and also explores how various analytic analyses can be conducted. Particular attention is given to the household-based collection of the data, and how imputation and editing is performed for each household member. Variance estimation is also discussed. Analytic analyses that are covered include transitional flows across adjoining months, matching individuals from one year ago, and measurement error modeling of disability status and its potential impact on statistical analyses. Suggestions for those wishing to conduct their own analyses of these data will also be presented. ","In late 2008, the Current Population Survey (CPS) started collecting data on a monthly basis to identify individuals as disabled. This paper uses CPS data from several months to examine participation in the labor force by disabled individuals. The first part explores rates of employment, unemployment, and discouragement over job prospects, for those who are disabled and those who are not. The second part explores the quality of the jobs amongst those who are employed including comparisons of earnings, work hours, and occupations. The third part decomposes differences in employment between those who are disabled and those who are not into differences in demographic characteristics of the two groups and differences in employment within demographic groups. The paper concludes with a comparison of the flows between employment and unemployment for both disabled and not disabled individuals. ","Researchers and policy makers have long expressed the need for a timely measure of the employment status of persons with disabilities. Effective with the release of data for January 2009, the Current Population Survey (CPS) addressed that need through the addition of a set of disability questions to the monthly survey. The Annual Social and Economic Supplement (ASEC) to the CPS has included a work-disability question for many years. For lack of a better option, researchers have used this question to evaluate labor force trends for persons with disabilities over time. This paper will show that the work-disability question has serious limitations when used to evaluate the labor force situation of persons with disabilities. ","In accordance with Executive Order 13078, the U.S. Bureau of Labor Statistics (BLS), along with several other federal agencies, identified the goal of placing a small set of questions within the Current Population Survey (CPS) to measure the employment status of persons with a disability. Extensive research efforts in support of this goal resulted in the identification of a set of seven questions which were tested within the February 2006 CPS. Analysis of the February 2006 CPS data revealed that the additional questions did not adversely affect the response rates for the survey but the questions did function in a significantly different way than anticipated based on prior tests. In lieu of further testing of the same questions, the decision was made to adopt a set of questions that had been developed for use in the 2008 American Community Survey. ","The Energy Information Administration (EIA) conducts a quadrennial attribute census survey of retailers and resellers of specific petroleum products. Information is collected on sales volumes, type, and geographic location and used to construct the sampling frames for a number of other EIA petroleum surveys. In this paper, analysis of the data over multiple years is used to construct a view of how the downstream side of the petroleum industry has changed over time. The paper will address characteristics of the industry in general, as well as sub-populations, and explore possible implications on future data collections. ","The EIA weekly survey of retail gasoline prices collects prices using a Computer Assisted Telephone Interview system with interactive data editing embedded to assure data quality. Edit performance statistics, however, showed that the data editing criterion sometimes missed true outliers at one end of the price change distribution and falsely over flagged outliers at the other end of the distribution, especially during times of large price change seen in the last three years. In order to improve the efficiency of the data editing criterion, a new data editing criterion based on price change relative to market change was developed. In addition, a new post-collection data validation procedure for screening price and price change outliers by region and grade that makes use of all respondents collected was also implemented in the survey process to further assure data quality. ","The petroleum industry is constantly changing to keep abreast of technology, resources and regulations. Likewise, surveys and survey methodology that measure the supply of petroleum and petroleum products need to be updated to keep pace with those changes. This paper describes the work done examining the edits and the edit performance in a family of surveys that provide monthly estimates of product supply. Within the constraints of a standardized system, the edits were modified and updated using both a statistical basis and industry specialist knowledge to improve the efficiency of the edit process and the accuracy of the resulting estimates. ","The EIA collects monthly information on the balance between supply and disposition of crude oil and petroleum products through a family of surveys. The process requires all imputed values to be available before responses are received, but uses values for only select cells. Previous analysis led to the recommendation for some surveys to implement an imputation method using historical values obtained through exponential smoothing and trend adjustments from a weekly survey. One survey was particularly difficult to resolve because of more extensive dimensions of the survey, the many cases of zero values, and fewer comparable cells in the weekly survey for trend adjustment. To group cells, a regression tree method (CART) was used to obtain groups for which the same smoothing coefficient could be used. Simulation analyses were then conducted to identify an optimal coefficient for each group. ","This paper describes the sample design used at the Guatemalan National Police Archive (GNPA). The Archive contains millions of documents, which were initially found mixed together and in poor physical condition. Given the Archive size and lack of a traditional sample frame, we opted for a multi-stage random PPS sample using the Archive's topography for stages 1 and 2. For stages 3 and 4, frames were created on location. The sampling faced several challenges, including movement of the documents as they were being restored and organized, and uncertain resource availability. To manage these difficulties we drew iterative sample waves. After rounds of evaluation, we modified the sampling to reduce one stage, making the sampling more efficient. Over 2 years of sampling, we have selected 20,000 documents. Next, we may use adaptive sampling to search for documents of interest.  ","This paper describes the weighting procedures used for the sample of records from the Guatemalan National Police Archive.  The decisions made about sampling procedures to address the structure of the Archive resulted in a number of special problems when calculating the weights. First and foremost, the universe from which samples were selected is highly unusual and fluid, with sparse knowledge of measures of size. At more than one stage of selection, probabilities of selection were not known, requiring weights to be based on estimates of the probabilities of selection. There were difficulties stemming from operational issues, such as the movement of documents while the survey was conducted and empty space within containers. ","This paper offers a first statistical look at the Guatemalan National Police Archive (GPNA). Preliminary estimates are provided of the degree to which specific groups within the National Police were aware of violent acts reported in the institution's documents. Weighted estimates were calculated based on a multi-stage sample of documents from the GNPA.  As outlined in the previous papers, the physical structure of the Archive presented many challenges, resulting in complicated selection probabilities.  The statistical analysis assumed containers of documents (stage 2) as the primary sampling unit. The proportion of all documents in the Archive of which any specific group had knowledge was compared to the proportion of documents of interest of which a group had knowledge.  Approximately 20% of documents in the Archive are believed to contain information about acts of interest. ","Many survey data sets are large, with many variables as well as complex structure. We explore the use of data mining methods such as regression trees and random forest with survey data. The methods can provide more flexibility for modeling than parametric and local polynomial regression since they are well suited for exploring conditional relationships in the data. We propose inferential methods that account for clustering in the survey data, and show how random forest methods may be used for model-robust small area estimation.    ","Linear mixed effects models (LMEs) encompass many statistical methods and have found extensive use in estimation for complex surveys, particularly in small area estimation and in various extensions of generalized regression estimation (GREG), including nonparametric regression. They have also been used as a means of relaxing constraints in calibration estimation. The purpose of this work is to consider methods by which LMEs may be used at the design stage of a survey. We review the ideas of balanced sampling and suggest an implementation of the cube algorithm for which \"penalized balanced samples\" can be selected. Such samples have the property that Horvitz-Thompson estimators from penalized balanced samples behave like LME model-assisted regression estimators. Empirical results with \"nonparametric\" survey designs demonstrate the usefulness of the methodology. ","Nonparametric regression is the model-based sampler's method of choice when there is serious doubt about the suitability of a linear or other simple parametric model for the survey data at hand. It supersedes the need for use of design weights and standard design-based weights. Recognition of this is especially helpful in confronting problems in  sampling situations where design weights are missing or questionable. One example is the case where we have data from two (or more) samples from a given population. We discuss this case. ","Federal statistical agencies are a primary resource for information to support evidence-based policy making. As part of a highly decentralized statistical system, they face both opportunities and challenges in maintaining the credibility, relevance, accuracy, and timeliness of their data series at a cost that policymakers are willing to pay. Opportunities include increased use of administrative records and computer technology; challenges include barriers to recruitment/retention of technical staff, declining response rates, threats to independence, and budget constraints. A distinguished panel will discuss the most important opportunities and challenges for the agencies in the next 5--10 years. The panel's discussion will use the Committee on National Statistics (CNSTAT) white paper, Principles and Practices for a Federal Statistical Agency, as a framework for discussion. ","We describe ongoing efforts to develop public-use partially-synthetic microdata based on the US Census Bureau's Longitudinal Employer-Household Dynamics (LEHD) database. These are longitudinal linked data on employers and employees, constructed from a variety of administrative and survey data sources. Confidential characteristics of workers, firms, and jobs are replaced with synthetic values. The graph of employment relationships is preserved, to the extent that doing so does not compromise privacy of workers and firms. We describe methods used to generate the synthetic values, and provide a preliminary assessment of utility and disclosure risk in the partially synthetic data. ","Longitudinal business data are widely desired by researchers, but difficult to make available to the public because of confidentiality constraints. We discuss the generation and use of the soon-to-be (or recently) released synthetic public use data sets of the U.S. Census Bureau's Longitudinal Business Database. The data sets contain actual units with most variables replaced with multiple imputations, i.e., values generated from probability distributions fit using the confidential data. This can protect confidentiality, since attributes are synthetic rather than real. And, when the models describe the data well, broad-scale inferences from the synthetic data sets will be inferentially valid. ","In 2006 the German Institute for Employment Research (IAB) launched a research project to investigate the chances of releasing a scientific use file of its establishment survey protected by means of generating multiply imputed synthetic data sets. After several pre-tests and simulations a release of the last wave of the survey for research purposes is planned for early summer 2009. In this talk we will discuss some of the challenges of implementing this approach for a real data set, where the data is anything but normal, missing values and skip patterns complicate the modeling, logical constraints have to be considered and a disclosure review board has to be convinced that a release is save without being too technical when describing the methodology. ","Many users of synthetic data, or any data altered to protect confidentiality, are understandably skeptical that analyses done on synthetic data will yield reasonable results. In this talk, I present recent research on methods to improve the analytic validity of synthetic data. Specifically, I talk about nonparametric methods of data synthesis that have the potential to capture complex distributional relationships, and about methods for providing automated feedback to users about the quality of their synthetic data inferences. ","National Health Interview Survey (NHIS) data can be used to monitor access to health care and health insurance in the United States. The NHIS, conducted by the Centers for Disease Control and Prevention's National Center for Health Statistics, enables policymakers and researchers to examine trends in health coverage and emerging health care access issues for the U.S. non-institutionalized civilian population. This paper focuses on status changes in health insurance coverage and patterns of health care utilization to demonstrate the depth and versatility of the NHIS health insurance data. Examples of innovative ways to use NHIS data in examining health insurance policy issues at the state level are also presented. ","The NHANES have been conducted since the 1970s and are unique in that physical examination data are obtained. The 1999--06 NHANES was designed so that estimates of health and nutritional status, risk factors and health conditions could be estimated in age-gender specific domains of the non-Hispanic (NH) black, NH white, Mexican American, and low income populations. NHANES provides the unique opportunity to examine associations between self-reported interview data on health insurance and access to health care and objective physical examination and laboratory data. Therefore, the association of health insurance status and access to health care with undiagnosed conditions, diagnosed conditions and treated conditions can be evaluated. Conditions such as diabetes, high cholesterol, hypertension and other health measures such as environmental chemicals and immunization status will be examined. ","Standardized data that examine health care for children and adolescents can be used to inform policymakers, evaluate programs, and monitor disparities over time.  Two nationally representative surveys provide these data for children and children with special health care needs aged 0 to 17 years old:  the National Survey of Children with Special Health Care Needs (NS-CSHCN), and the National Survey of Children's Health (NSCH).  Both surveys were sponsored by the Federal Maternal &amp; Child Health Bureau, conducted by the National Center for Health Statistics using the State &amp; Local Area Integrated Telephone Survey (SLAITS) mechanism.  Several examples are presented to demonstrate the analytic utility of SLAITS health care access and utilization variables and how they can be used to support trend analysis on health care access and utilization for children at the national and state levels.  ","The National Health Care Surveys are a family of nationally-representative provider-based surveys that collect data about health-care providers, their patients, and their care. The surveys cover health care providers across a broad spectrum of ambulatory, hospital, and long-term care settings. The resulting data are used to monitor health care service delivery among different U.S. populations. The presentation will provide an overview of the surveys and the data that each collects. Examples will highlight use of provider-based data to inform our understanding of health care access, including associations between payment source and use of care in different settings; the impact of Medicaid payment policy on duration of care in physician office visits; and variation in emergency department waiting times by urban location and high poverty zip code. ","Multiple imputation of missing data is generally recognized to be better than the usual alternatives of complete-case analysis, available-case analysis, and mean imputation.  But multiple imputation requires a probability model (or, at least, a probabilistic algorithm).  In typical applications, little effort is made to check the fit of the model, either to data or to external knowledge that can be used to judge whether the imputations make sense.  We are working on graphical methods comparing observed to imputed data to diagnose problems with imputations.  The goal is to build confidence in multiple imputations that will be used for later analysis. ","Imputation is a convenient tool for the analysis of incomplete data set via complete data procedures. Checking the imputation quality is generally a very challenging task especially for the data users. The imputations are usually produced based on statistical models, for which the user has little or no knowledge. Therefore, no \"Gold standard\" is available to check against. In this talk, we cast this problem into the setting of two-group comparisons: observed data versus imputed data. Typically, the imputed data are highly dependent on the observed data. This makes it hard to assess how close is close to enough for a good fit. More precisely, the challenge lies in the variance calculation due to the high dependence between the observed and imputed data. ","Multiple imputation fills in missing data with posterior predictive draws  from imputation models. To assess the adequacy of imputation models, we can compare completed data to their replicates simulated under the imputation  model. We apply analyses of substantive interest to both datasets, and use  posterior predictive checks of the differences of these estimates to  quantify the evidence of model inadequacy. We can further integrate out the  imputed missing data and their replicates over the completed-data analyses  to reduce variance in the comparison. In many cases, the checking procedure can be easily implemented using standard imputation software by treating re-imputations under the model as posterior predictive replicates. Thus it can be applied for non-Bayesian imputation methods. We sketch several strategies for applying the method in the context of practical imputation analyses. ","Increasingly no single study can provide data to fully understand the role of biological, social and environmental factors on health. However, there are surveys/studies that collect data focusing on specific scientific phenomenon and others collect data for more general purposes with some common sets of variables. For example, suppose that three sources are: 1)A population based genetic study on genotypes(X) and disease(D); 2)A population-based epidemiologic study on environmental factors and biological markers(Y) and Genotypes(X); and 3) A general purpose health survey collecting risk factors(Y) and Disease(D). By concatenating these data sets, the missing pieces in each survey/study can be treated as missing data and handled through multiple imputation. This talk will use an example with several data sets to illustrate the methodology, discuss challenges, and evaluate the inferences. ","USDA's National Agricultural Statistics Service (NASS) surveys farmers and ranchers across the United States and Puerto Rico for the purpose of making estimates on crops, livestock, production practices, and farm economics. NASS has established statistical standards documented in a series called Policy and Standards Memorandums (PSM). The PSM series is used to establish basic Agency administration policies and procedures, delegate or amend organizational authorities, and establish Agency operational program policies and standards relating to agricultural statistics, surveys and census, statistical research, and reimbursable surveys. PSMs are approved for 5 years but are updated as required. NASS also conducts periodic Technical Reviews both of its Field Offices and programs. This is a procedure to monitor the implementation of the standards and determine additional standards needed. ","The National Center for Education Statistics (NCES) statistical standards are designed to ensure information quality before release to the public. NCES standards describe agency policy and procedures for data collection, processing, analysis and review. NCES adopted statistical standards in 1987. These standards were revised in 1992 and 2002. The 2002 standards included topics found in OMB's 2006 \"Statistical Standards and Guidelines for Statistical Surveys.\" NCES is now revising its 2002 standards in direct comparison to the 2006 OMB standards. Other standards will be revised to account for what the agency has learned during this review process and over the past 8 years. NCES also monitors the implementation of and compliance with its standards through a review process. The paper presents the history and implementation of NCES standards with information on current standard revisions. ","In 2002, the U.S. Energy Information Administration (EIA) revised its statistical standards as a result of the 2002 Information Quality Guidelines issued by the Office of Management and Budget. As EIA increased its use of the Internet for releasing information, issues arose that related to implementing information quality guidelines.  New technology also created new capabilities that were not available in the past.  EIA reviews its statistical standards annually and applies several criteria to determine whether a statistical standard adequately supports the information quality guidelines.  This paper discusses some changes to the agency's statistical standards over the past seven years that were needed to keep pace with changes in technology and business processes. ","As a replacement for the once-a-decade decennial long form sample estimates, the reliability of the American Community Survey (ACS) estimates is an important consideration.  This paper presents research that assesses the reliability of the ACS estimates.  The reliability of theoretical tract level estimates from the Census 2000 Long Form sample is used to determine the annual sample size and sampling rate necessary for the ACS 2011-2015 five-year estimates to achieve various levels of reliability.  In this approach, the levels of reliability for the ACS are described as a function of the Census 2000 Long Form reliability, as measured by the coefficient of variation for a fixed 10 percent population estimate of the poverty rate.  Note that the poverty rate estimate has been identified as a key measure for the ACS and has been shown, in general, to have relatively large standard errors. ","The American Community Survey selects its large group quarters (GQ) sample by selecting clusters of ten persons from the GQ sampling frame.  Cluster size influences both the variance of the estimates and survey costs.  Reducing the number of people in a GQ sample cluster will improve the reliability of the estimates while simultaneously increasing data collection costs.  In this paper we evaluate the variance reduction gains from decreasing the sample cluster size for large GQs and the expected impact on survey cost.  Additionally, we calculate a cost/benefit factor in order to select the cluster size that offers the most benefit for the increased cost. ","There are two distinct types of places in which people live, housing units (HU) and group quarters (GQ).  People who live in HUs live in houses and apartments, whereas GQ residents live in facilities including but not limited to college dormitories, correctional facilities, and nursing homes.  It has been noted that variances are higher for American Community Survey (ACS) estimates of the GQ population of substate geographies than they are for the HU population.  The research described in this paper has two goals.  The first is descriptive, to determine for which estimates the GQ variances are highest and what the effects of the higher GQ variances are on the estimates of the total resident population (people in GQs and HUs).  The second is analytic, to assess the relative contribution of certain previously identified reasons why the GQ population estimates have higher variances. ","The American Community Survey (ACS) began producing estimates of the group quarters (GQ) population, for the full implementation of the survey, in 2006. The ACS uses a successive difference replication (SDR) method to compute variances for these estimates. Part of the SDR method involves the assignment of a set of replicate factors to each sample GQ person. In 2006 and 2007, the ACS assigned these sets at the person level. This assignment method resulted in the underestimation of GQ population variances, as they did not take clustering of persons in GQs into account. An alternate assignment method more accurately reflects the clustering effect that is associated with GQ sample persons. Alternative variance estimation methods (jackknife, random group) provide benchmarks with which to compare the a preferred SDR method. ","The American Community Survey (ACS) uses various techniques to improve the quality of the 1-year and 3-year data products it releases based on a series of data release rules. The release rules use the size of an estimate's coefficient of variation (CV) as a quality/reliability indicator. Products with too many estimates having large CVs fail the release criteria and are not published. This research is in response to feedback from ACS users that the current release rules are too conservative, especially those applied to certain types of estimates. We look into alternatives to the current release rules as they are applied to ACS 1-year and 3-year medians, ratios and zero estimates.  ","Respondent-Driven Sampling is type of link-tracing network sampling used to study hard-to-reach populations.  Beginning with a convenience sample, each person sampled is given 2-3 uniquely identified coupons to distribute to other members of the target population, making them eligible for enrolment in the study. This is effective at collecting large diverse samples from many populations.    ","We present a unified approach to modelling dyadic relational data, namely that seen in social, biological and technological networks, without restriction to the binary format. The approach involves three principles: considering the marginal specification of any edge as the  fundamental unit, embedding as much dependence as possible in latent structural forms, and using distributional forms that favour high-throughput computational methods for their solution. We show that this approach allows for an extremely flexible and generalizable  way of describing the structural properties of relational systems. We demonstrate this on a network of doctors and their patients in a hospital community as a method of predicting future behaviour. ","Network models are widely used to represent relational information among interacting units and the structural implications of these relations.    ","Respondent-driven sampling (RDS) is a contact-tracing sampling design (introduced by Heckathorn) which is widely and increasingly being used to study hard-to-reach populations such as injection drug users and individuals at high risk of HIV infection. We compare model-based vs. design-based estimators in this setting, and study the bias-variance tradeoffs which result depending on what assumptions one is willing to make about the population of interest and about the network structure. ","Often the choice of the level of aggregation in small area (SA) modeling is not governed by adequacy of modeling assumptions but by user needs. This may have serious impact on validity of the underlying exchangeability of area-specific random effects in a LMM for means. The problem doesn't arise with unit level models which, however, are difficult for taking design into account. As a compromise, we propose a building-block (B) model such that random effects at B-level subareas are likely to be exchangeable, and model parameters are estimated at this level or after grouping of B-level areas to avoid the problem of zero sample sizes or zero direct estimates. Target SA estimates are obtained as BLUPs using a reduced model with target SA-level direct estimates. An illustrative example is presented using Canadian Labour Force Survey data with provincial economic regions as target small areas. ","Current methodologies in small area estimation are mostly either parametric or are heavily dependent on the assumed linearity of the estimators of the small area means.  We discuss an alternative empirical likelihood based Bayesian approach.  This approach neither requires a parametric likelihood nor needs to assume any linearity of the estimators.  Moreover, the proposed method can handle both discrete and continuous data in a unified manner.  Empirical likelihood for both area and unit level models are introduced. Performance of our method is illustrated through real datasets.    ","Different methods have been proposed in the small area estimation literature to deal with outliers in individual observations and in the area-level random effects. In this paper, we propose a new method based on a scale mixture of two normal distributions. Using a simulation study, we compare the performance of a few recently proposed robust small area estimators and our proposed estimator based on a mixture distribution. We then compare the proposed method with the existing methods to estimate monthly employment changes in the metropolitan statistical areas using data from the Current Employment Statistics Survey conducted by the U.S. Bureau of Labor Statistics (BLS). ","Small area estimation techniques are becoming increasingly used in survey applications to provide estimates for local areas of interest. These techniques combine direct survey estimates with auxiliary data to produce estimates for areas with very few sampled units. Most of the applications have been in social surveys where the areas of interest are geographical regions with fewer applications to business surveys, although tax information is a usually available as auxiliary data. Statistics Canada has been investigating small area estimation for its Survey of Employment, Payrolls and Hours (SEPH). Preliminary results have been encouraging but some issues with negative variance component estimates have been noted.  In this presentation, alternative methods to estimate the variance components will be investigated and evaluated using a population generated from SEPH data. ","In the SAIPE program at the U.S. Census Bureau, one of the challenges is the estimation of sampling variances of the direct survey weighted estimators for the counties.  Design-based methods can be highly unreliable primarily because of small sample sizes in the area.  GVF methods have been previously used in SAIPE to obtain smoothed variance estimates. In this paper, we consider a synthetic approach in which we select a working model by comparing several competing models at the state level where standard direct design-based variance estimates are considered to be reliable.  The working variance model relates replication based variance estimates to the corresponding estimates and to survey design variables.  It motivates an integrated multilevel model that can be used in the future to produce improved estimates of different parameters of interest simultaneously. ","In our study we are exploring the relationship between early childhood respiratory problems and air pollution exposure during pregnancy. This is modeled using data from air pollution measurements and birth certificates from every birth in LA County in 2003. In addition we have more detailed survey data from 4% mothers about risk factors during pregnancy and a subsequent survey on about half of these mothers on respiratory health of the children. The motivation of this research is the increasing missing data problem in these surveys. Our methodology works by dividing LA County into small geographic areas where women within each area are similar in terms of demographics and pollution exposure. Since our data includes only a few observations in each of these areas we use small area estimation techniques to estimate the prevalence of early childhood respiratory problems for all of LA County. ","Statistical agencies often need to coordinate the selection of samples drawn from overlapping populations so as to optimize (maximize or minimize) the expected overlap, subject to constraints determined by the survey designs. For instance, maximizing the expected overlap between repeated samples can increase the precision of estimates of change between two occasions and reduce the costs of first contacts. Minimizing the expected overlap can avoid overburdening respondents with multiple surveys. We would like to present two new methods for optimizing the overlap of two consecutive surveys, which preserve the first order inclusion probabilities (this ensures that the estimators are unbiased). Our methods attempt to control the size of the overlap on each selected sample. We also intend to quantify the impact of the size of the overlap on the variance of the estimators of change.  ","We have described a method (2001, 2003, 2004, 2009) for merging two independent samples using data fusion (also known as statistical matching).  One sample contains (X,Z) and the other contains (X,Y), both drawn from a common nonsingular normal (X,Y,Z) distribution.  Following Kadane (1978) and Rubin (1986), we employ regression in our approach.  We assess the uncertainty introduced during the merge that is due to the unobserved (Y,Z) relationship by repetition over a range of (Y,Z) values that are consistent with the observed data.  An essential part of our algorithm is to add random residuals to the regression estimates.  Our initial approach for estimating the residual variance could fail (be negative) because it used subtraction of estimates from both files.  Innovations due to D'Orazio, et al. (2006) and Kiesl and Rassler (2009) give improved results, yielding a robust algorithm. ","In fiscal year 2010 budget, the U.S. Congress funded U.S. Bureau of Labor Statistics to develop measures of jobs associated with  environmental activity, also known as \"green jobs\".    Two separate measures of environmental activity are desired--employment by industry; and employment and wages by occupations.  In this paper, we discuss the challenges associated with integrating the sample design for a new environmental industry of employment called Green Goods and Services Survey (GGS) with the existing Occupational Employment Statistics (OES) Survey.  Both the GGS and OES Surveys call for producing very detailed estimates at various levels of industry and geography that are vastly different for each survey.  Statistical issues such as level of stratification, sample rotation, amount of sample overlap within each survey and between the two surveys are discussed. ","Combining data sources is often seen as a panacea, having the potential to produce more to produce cost-effective, accurate, fine-level statistics for a lower cost. This paper clarifies conditions under which Official Statistics data sources, particularly surveys and censuses or surveys and administrative sources, should and should not be combined using statistical models based on mass imputation, spatial microsimulation, and small area and domain estimation. The theoretical links between these three techniques are explored. The wider research from which this paper is a report considers the relevant literature in depth, further develops existing statistical methods, considers their application in principle to set of case studies in sociology, economics, and business, and provides guidelines for use of the three techniques based on this research.  ","When the idea of using multiple imputation for privacy purpose was first introduced, the claim was that if we release completely imputed (synthetic) datasets then confidentiality is protected because the individuals in the microdata do not correspond to any of the respondents. Privacy disclosure was taken for granted and most of the work focused on showing that synthetic datasets could provide accurate inferences. Several researchers have now come to the conclusion that there are risks of identity and attribute disclosure even when the released datasets are completely synthetic. In this paper, we look at different methods of assessing the disclosure risk from synthetic datasets, and study the efficiency of this technique to protect the privacy of respondents.   ","Matrix sampling, sometimes referred to as a split-questionnaire, is a sampling design that involves dividing a questionnaire into subsets of questions, possibly overlapping, and then administering these subsets to different subsamples of an initial sample. This design reduces the data collection costs and addresses concerns related to response burden and data quality, but also reduces the number of sample units that are asked each question. For matrix sampling with overlapping subsets of questions, we propose an estimation method that exploits correlations among variables surveyed in the various subsamples in order to improve the precision of the survey estimates. The proposed method uses a suitable calibration scheme, which is equivalent to a generalized regression procedure based on the principle of best linear unbiased estimation, and is computationally very convenient. ","The Initial Housing Unit Matching and Followup operation of the Census Coverage Measurement (CCM) program consists of the following activities:  computer matching, preprocessing, Before Followup Clerical Matching, Initial Housing Unit Followup, and After Followup Clerical Matching.  In order to conduct Before Followup Clerical Matching and After Followup Clerical Matching, trained technicians and analysts use the Housing Unit Matching, Review, and Coding System (HU MaRCS) and the HU MaRCS Map Viewing System (MVS).  This report summarizes each of the Initial Housing Unit Matching activities, including use of computer matching, the quality control program, and a high-level overview of the clerical matching process. ","There is an increased need to find duplicates in very large files.  This paper details the current version of Bigmatch software (Yancey and Winkler 2007, 2009) that is sufficiently fast for processing 10^17 (=300 million x 300 million pairs) for the U.S. Decennial Census and even larger administrative-record situations with billions of records.  The software, via a nontrivial application of a set of blocking strategies, is known to find more than 97.5% of true matches with very small error of less than 0.5% (Winkler 2004, 1995).  It does detailed processing on 10^12 pairs using 40 cpus on an SGI Linux in 15 hours. The software is 40-50 times as fast as recent parallel software (Kim and Lee 2007; Kawai, Garcia-Molina, Benjelloun, Menestrina, Whang and Gong 2006). ","The identification of American Community Survey respondents allows survey responses to be linked to administrative record files for analysis of response error and other purposes. Currently the Census Bureau uses a record linkage technique that compares a weighted sum of several comparisons (e.g., first name, year-of-birth, etc.) to a match threshold, deeming those record pairs exceeding the threshold to be valid matches. While this technique allows over 90% identification of respondents, it can potentially be improved by taking advantage of the relative frequency of names, dates-of-birth, place-of-birth etc. to compute the likelihood of spurious agreement across fields. Knowing the likelihood of spurious matches allows a computation of an estimated likelihood that pairs of records from the two files that are being matched together truly represent the same individual.  ","The UK Labour Force Survey (LFS) is a quarterly survey based on a rotating panel design, which generates about 80% sample overlap between two successive quarters and can induce correlated sampling errors and rotation group bias (RGB). The standard time series analysis packages fail to account for the effect of the sampling error autocorrelation (SEA) and the use of these procedures may produce spurious trends. This paper represents the first attempt to set up a structural time series model for the UK Labour Force Survey series that accounts for the correlation between panel estimates. The proposed model is a first step towards the development of a state-space model to estimate the rotation group effects and discontinuities that may arise due to the transferring of the LFS to the Integrated Household Survey. ","The Occupational Employment Statistics (OES) survey conducted by the U.S. Bureau of Labor Statistics produces detailed occupational employment and wage estimates.  To reduce response burden, the OES form asks respondents to report the number of employees by wage interval.  Many of OES's larger respondents, such as the Federal Government, report data electronically, providing wage rates for each of their employees, or \"point data\", which OES then reclassifies into wage intervals for estimation.  A mean wage rate is calculated for each interval from a secondary source and given to all employees within the interval.  Capturing and integrating point-data wage distributions into estimation could increase the accuracy of estimates.  This paper describes the research and implementation of mean and percentile wage estimators, along with variances estimators, using available point data wage data. ","Several of the leading economic indicator programs at the U.S. Census Bureau provide estimates of monthly or quarterly change.   Different implementations are being used to test these change estimates for significant difference.  Test implementation is derived from the form of the null hypothesis used for the test.  The goal of the presented research project is to investigate the statistical properties of these alternative implementations over repeated samples.  We consider three variations: a direct comparison of the ratio of two concurrent estimates to 1; a repeated measures analysis of the estimates' difference using point estimates; and a repeated measures analysis that uses smoothed estimates of variance and autocorrelation in the construction of the test statistic.  Our research uses two simulated population datasets modeled on the Monthly Retail Trade Survey. ","The National Compensation Survey is an establishment survey that provides measures of occupational earnings and benefits. The private industry establishment sample is divided into five sample rotation panels, with the panels being fully replaced over a five-year period. Data are updated periodically for each selected establishment and occupation until the panel in which the establishment was selected is replaced. The current sampling method allows for establishments to appear in more than one rotation panel. This can increase the reporting burden if a new set of occupations is selected for an overlapping establishment. This research uses sample simulations to explore an alternative design in which establishments selected in a sample panel would not be eligible for selection in subsequent panels of the rotation and discusses potential pitfalls in employing such a sampling approach. ","Conducting representative telephone surveys of the general public in the U.S. has become very challenging in the past five years because of the growing prevalence of cell phone only and cell phone mostly households - now estimated to constitute more than 40% of all U.S. households. The American Association for Public Opinion Research (AAPOR) has reconstituted its Cell Phone Surveying Task Force and will issue the 2010 Task Force report in May.  The roundtable discussion will be led the chair of the AAPOR Task Force.  The topics of discussion at the roundtable pertaining to U.S. cell phone survey as of 2010 will include challenges related to (a) sampling and coverage, (b) nonresponse, (c) measurement, (d) weighting, (e) legal and ethical issues, (f) operations, and (g) costs. ","One of the largest issues facing those in the public and private sectors collecting survey and census data is the need to maintain confidentiality and deliver useful information.  Clearly public trust has declined substantially in the past few decades.  At the same time, the need to better understand the economy, our social structure, along with medical information for health purposes has increased dramatically.  The panelists will explore how confidentiality is maintained under various scenarios of data collection.  These will include the implications of data linking; the need to protect DNA information; analyzing small area data; federal statistical agency cooperation; and using secure remote access locations.    ","For survey data logistic regression, the model fitness has been assessed through a set of diagnostic statistics, borrowed from the logistic regression in generalized linear models. However, for survey data, these statistics may need to be reexamined. In practice, we have observed their irregular behaviors, which make their established statistical criterion suspect. This presentation reports our use of Pearson residuals normality graphs as graphical diagnostic statistics, to assess survey data logistic modeling, as in a recent NASS study. Statistical graphics summary may provide broader scope and more elaborated information than analytical summary. The statistical graphs of Pearson residuals showed their diagnostic ability, and their careful reading may reveal delicate diagnostic information on modeling effects. We illustrate the statistical graphical modeling process with our analysis. ","The National Immunization Survey (NIS) is designed to collect vaccination information for U.S. children between the ages of 19 and 35 months, but due to nonresponse roughly 30% of the vaccination data is unascertained. The current method to compensate for missing provider data involves weighting adjustments. Using data from the 2008 NIS public-use data file, this paper evaluates four alternative imputation techniques and comments on some advantages and disadvantages of specific software used. All four imputation methods yielded slightly lower vaccination rates, although differences are small in magnitude. Single imputation is shown to underestimate standard errors as compared to the current weighting method and the other three multiple imputation (M = 5) methods. Standard errors between the three multiple imputation methods were all similar and mirror those from the weighting method. ","Two more statistical software packages, MPLUS and WinBUGS, were tested as a continuation of a previous study of analysis method/software robustness in the analysis of clustered data. We want to evaluate robustness in the context of a randomized complete block design, where each \"plot\" is a small group of children at the same nursery school and a series of measurements of each child are made. We constructed a series of super populations in which the standard assumptions of hierarchical (mixed effects) linear models were violated. The results were compared with HLM, SUDAAN, PROC MIXED and a semi-parametrical analysis of variance procedure we tested before. ","The Representativity Indicators (or R-indicators) are statistics designed to supplement the traditional response rate when evaluating the potential for nonresponse bias from a survey. The response rate provides information on the \"amount\" of nonresponse in a survey and the R-indicators are designed to measure the similarity between respondents and the original sample or population. In this presentation, we discuss how to estimate the R-indicator using a Logistic Model to estimate response propensity and the Generalized Exponential Model that is used to compute weight adjustments in SUDAAN Release 10. The linearized variance estimate of the R-indicator is presented as well as several related statistics. The variance estimate will account for both the complex design and the model used to estimate response propensity.  Example estimates derived using both SAS and SUDAAN will be discussed. ","In non-randomized observational studies, selection bias may confound the relationship between treatment and outcome. Imputation is one method for addressing selection bias. With this approach, a potential outcome is imputed for each treatment level not received and the association between treatment and outcome is estimated using both reported and imputed outcomes. Multiple imputations may be used to account for the impact of the imputation process on variances. This paper analyzes data from a four-arm comparison study (Shadish et al., 2008) where students were first randomly assigned to a randomized experiment or an observational study. Using the randomized experiment as a benchmark, we examine treatment effects estimated by applying semi-parametric multiple imputation, ANCOVA, and propensity scoring methods to the observational study. Issues with variance estimation are discussed. ","The potential outcome framework for causal inference is fundamentally a missing data problem with a special \"file-matching\" pattern of missing data. This article uses the sequential regression or chained equation methodology to impute the potential outcomes based on the observed data. The causal inference parameters are formulated based on the models for the completed data and standard multiple imputation (MI) combining rules are applied to infer about the direct and mediated effects. MI framework is modified to incorporate constraints or prior information in terms of augmented complete-data. Given the ability of the multiple imputation framework to handle several types of variables, missing values in covariates and the availability of software for performing multiple imputations, this approach makes easier to perform causal inference from both observational and randomized studies. ","In the past several years the statistical literature has developed a wide range of methods for the construction of regression trees and other estimators based on the recursive partitioning of a sample.  Many prospective applications involve data collected through a complex sample design.  At present, however, relatively little is known regarding the properties of these methods under complex designs. This paper establishes sufficient conditions to guarantee asymptotic design consistency of regression trees as an estimator for the conditional mean of the population.  Conditions on the population distribution and survey design, for which the results are proved, are quite general.  Performance of this proposed nonparametric estimator is investigated through a simulation study based on wage data from the Occupation and Employment Survey (OES) of the Bureau of Labor Statistics (BLS).   ","Statistical models are often used to assist estimation of descriptive statistics from surveys.  Perhaps the most common estimator is the Generalized Regression Estimator (GREG) which is design consistent, uses a linear assisting model, and results in a set of calibrated weights.  However, when the variable of interest is binary, binomial, or multinomial, it may be more appropriate to use a logistic assisting model instead of the standard linear model.  In this paper we develop point and variance estimators for totals of finite population characteristics from a clustered sample assisted by a logistic regression model.  Using a national Public Use Microdata set we compare the design-based properties of the new estimator to the GREG and the Horvitz-Thompson estimator under two clustered sample designs.   ","It is shown how to model conditional probabilities subject to log-linear constraints and construct estimators that are more efficient than the Horvitz-Thompson weighted sum estimator for estimating population sizes in the context of a poststratified survey with unknown stratum sizes. Our approach rests on computing an ``hybrid\" predictor, similar to that of Pfeffermann et al. (1998), and the expansion of a specific parameterization for conditional probabilities restricted by log-linear constraints, as proposed by Thibaudeau (2003). This parameterization facilitates the computation of MLE's and makes it possible to apply the method of Laplace for estimating variances of MLE's. The method of Laplace is strictly model-based. To assess its accuracy we compare it with balanced repeated replication, a design-based method. ","Quantile regression has gained much popularity in recent years. No method, except for bootstrap, has been proposed to estimate the variance in quantile regression in complex survey setting. We propose a Bayesian approach for quantile regression in stratified sampling, using a hierarchical model. This approach is easily implemented using a Gibbs sampling algorithm. It yields estimate that is design-consistent. We also discuss extensions of our method to other sampling schemes such as PPS sampling.  ","Cognitive interviewing has become a predominant method of survey question pretesting. Despite the stature that cognitive interviewing enjoys as an established and respected pretesting method, there is little consensus on the ways in which the verbal reports should be handled or how the analysis should be conducted. Nor is there a widely accepted notion of what comprises \"evidence\" from a cognitive session. In many important ways, cognitive interviewing has evolved from and become different from the original think aloud and verbal protocol methodologies of cognitive psychology. We revisit the original think aloud and verbal protocol methodology and examine how current question pretesting practices align with the original goals and limitations of the think aloud method.  ","The overall design of the National Crime Victimization Survey (NCVS) has been largely stable for over 30 years. Households in sampled housing units are interviewed for 7 waves, each collecting data for the previous 6 months. Until recently, the 1st wave has been omitted from the published estimates to exclude reports outside of the intended 6-month reference period. Using publicly available data for 1998-2004, we report on a series of analyses to investigate more current effects of the bounding and find evidence of more general time-in-sample effects. We will also report on the effect of recency in the observed incident reports, where more crimes are reported in the 1st month preceding the interview date than each of the previous 5 months. These phenomena are important in considering a range of design options for the NCVS that would alter the reference period or panel design. ","According to the literature, it is still not clear to what extent stated time allocation in questionnaires reflects the actual behavior of people.   Using experience sampling data, we analyze the congruence of stated time use and reported behavior elicited through a novel experience sampling method.  Our comparisons indicate that rather long-lasting and outstanding activities like market work seem to be accurately measured by common survey questions. In contrast, for more short-term activities or those around which people do not tend to structure their time-such as errands- only small correlations can be revealed.   We conclude that activities with a long duration can be measured in a satisfactory manner by short survey questions whereas it is an open methodological question if experience sampling method or survey questions deliver more reliable and valid measures for short-term activities ","Previous research by Tucker et al. (2007), working with the Consumer Expenditure Interview Survey (CE), explores the efficacy of measurement error indicators such as:  interview length, extent and type of records used, the monthly patterns of reporting, and income question missing in a latent construct.  Later research by Tucker, Meekins, and Biemer (2008) extend this latent class model to include indicators of response behavior across multiple interviews in a panel.  This research develops a number of plausible models which possess the qualities of reliability and validity, in that they appear to accurately capture measurement error, but prove unable to explain a large amount of the variance associated with expenditure reports.  These expenditure reports are the main data collected in the CE.  This work extends past research by including the relatively recently recorded indicators from  ","Speech, voice, and question-answering behavior are verbal paradata; facets of the survey process that are not captured by default. Verbal paradata should reflect respondents' psychological states, which affect survey responses in turn. For 185 interviews from the Reuters/Univ of Michigan Surveys of Consumers, 4 questions per respondent were transcribed and coded. Voice pitch was extracted with Praat. Questions were selected for sensitivity and complexity producing four repeated measures; sensitive and complex, sensitive but noncomplex, nonsensitive but complex, nonsensitive and noncomplex. There is evidence of psychological resource and conversationality mechanisms, through which respondents reduce verbal paradata on psychologically demanding questions rather than increasing them as a sign of trouble. The use of paradata in understanding psychological response processes is discussed. ","We discuss two approaches of integrating data from the National Immunization Survey (NIS)-a nationwide, list-assisted RDD survey conducted by the NORC for the Centers for Disease Control and Prevention which monitors the vaccination rates of children between the ages of 19 and 35 months-and state immunization registries for purposes of assessing the differential quality of NIS estimates and registry data. One approach requests from the NIS sample consent to obtain immunization data from the state registry; the second draws an independent sample from the state registry and collects data under NIS interview process. In both approaches, state registry vaccination records are compared with those obtained from NIS providers. We compare vaccination up-to-date rates between these two approaches, as well as limitations and benefits of each approach, and implications for survey improvement. ","The usual Reinterview program consists reinterviewing a set of respondents and the data of the two interviews yields such measures as: Index of Inconsistency, Simple Response Variance, Gross Difference Rate, and Net Difference Rate etc.  However the underlying model has a potential weakness namely that the respondent may be influenced by their first response. This can be effectively eliminated by splitting the sample into two segments on whether they remembered initial response or not. The resulting two tables corresponding to the split samples yield similar measures as above but they bear some interest in relationships. In this paper we derive these relations.  ","The National Ambulatory Medical Care Survey (NAMCS) is an annual survey of office-based physicians and visits to their practices.  Since its inception, the survey has been conducted face-to-face, but in order to provide better estimates of physician adoption and use of electronic medical records (EMR), the original sample was augmented with a supplemental sample of physicians who reported EMR-based content from NAMCS through a mail questionnaire. Mail and face-to-face survey data from 2008 were combined for the first time to produce dual-mode estimates.  This paper outlines the survey sample design by mode. The paper also compares responses and item response by mode according to physician and practice characteristics. Although mail survey respondents were less likely to provide certain numeric responses than face-to-face respondents, most results were comparable across modes. ","In a mixed-mode experiment conducted as part of a mail survey of physicians for the U.S. News &amp; World Report America's Best Hospitals rankings, we found that providing physicians with the option of completing the survey online did not improve responses rates to a mailed survey (McFarlane et al. 2009) overall. In addition to exploring the effect on response rates, we are interested in exploring the effect of the mixed-mode experiment on nonresponse bias. In previous years of the survey females, older physicians, and certain specialists were less likely to respond creating a potential for nonresponse bias.  In this paper, we examine whether the mixed-mode experiment affected the response rate for these subgroups differently and whether survey responses differed by mode of respondent. ","Declining response rates in both cross-sectional surveys as well as panel surveys are a challenge for survey researchers (Groves &amp; Peytcheva, 2008). Increasingly, survey organizations use a mixed-mode design that switches the mode of administration in order to recruit nonrespondents to early survey requests (de Leeuw, 2005). We know that these sequential designs are highly effective at increasing response rates compared to single-mode surveys (Dillman et al., 2008). But little is known about the effect of sequential designs on response in a panel survey. Individuals in a panel may use experiences of previous waves to influence response behavior, or they may become more efficient respondents in later waves, requiring fewer mode switches. The propensity and timing of response and participation in a particular mode across waves are examined for a study of young women in Michigan. ","To manage large and complex survey sample, the sample is divided into batches and each batch is released based on the performance of previous batches. The accuracy of predicting the performance of the released sample affects the forecast of the future release amount. There are many factors such as call outcomes, case disposition, number of refusals and number of dials that affect the probability of a completed interview.  The National Immunization Survey (NIS) is a nationwide list-assisted RDD survey that collects immunization histories of children within the age range of 19-35 months. Using data from NIS, we constructed several models, e.g. simple linear regression, logistic regression, CHAID, survival analysis and neural networks, to measure the performance of the released sample and compare the accuracy and efficiency of different prediction methods.  ","In recent years, the cost of survey collection has grown significantly and nonresponse has increased. To counter these trends, strategies are being studied to optimize collection activities, resulting in a more time efficient and cost effective survey collection process. For example, recent initiatives for Computer Assisted Telephone Interviewing surveys at Statistics Canada include experimenting with time slices, limiting the number of calls, and establishing calling priorities. Field testing new procedures however has its drawbacks: it is costly and it is difficult to control, which can render the results difficult to interpret. To address these issues, we describe in this paper the creation of a microsimulation system of the collection process which uses paradata as input. We discuss characteristics of the model as well as results of simulation runs with various parameters. ","Suppose we have a sample of households. Each household contains at least one person. A survey of our interest is to interview multiple randomly selected eligible persons in each randomly selected eligible household in the sample. Define that a household completes the survey if every selected person of the household responds to his or her questionnaire. Extending the current theory of multivariate survival analysis of grouped censored data, we propose to model the survey completion time of households and the survey response time of persons simultaneously. More specifically, we describe group response outcomes and times as well as individual response outcomes and times in terms of within-group dependency structures of static and dynamic types. Our goal is to construct a statistical framework useful for characterizing and understanding contact histories in survey research. ","Monitoring the sample released and the response rates achieved during the survey lifecycle is key to maintaining an appropriate balance between survey requirements. It is an even more important task for any subsample of the primary survey as decisions made about the primary survey can greatly affect the results and outcomes for the subsample. This paper examines these special subsample issues using two surveys sponsored by the Centers for Disease Control and Prevention: the National Immunization Survey-Teen, a nationwide survey that monitors the vaccination rates of adolescents aged 13-17 years; and the National 2009 H1N1 Flu Survey, a nationwide survey that monitors influenza vaccination rates for all age ranges. This paper will investigate methods to forecast the number of completed surveys as well as numeric and visual ways to present sample monitoring data. ","Each year, the National Agricultural Statistics Service (NASS) obtains an estimate of the number of farms in the United States (US) based on the June Area Survey (JAS). In 2007, the JAS estimate of the number of farms was much lower than that from the quinquennial Census of Agriculture. The discrepancy was more than could be accounted for by sampling error. The JAS uses an area frame that is, by design, a complete frame of the population. Estimates incorporate sampling weights appropriate to the sample design and so are unbiased unless misclassification is present. In 2009, NASS conducted the Farm Numbers Research Project (FNRP) to study misclassification of farms and non-farms in JAS. An annual (modified) version of FNRP called the Annual Land Use Survey (ALUS) is proposed to adjust the JAS estimate of the number of farms.  ","The National Agricultural Statistics Service (NASS) conducts an annual area frame based survey, the June Area Survey (JAS). Also, the quinquennial Census of Agriculture is conducted in years ending in 2 and 7. The census has a dual frame: an independent list frame and the area frame from the JAS. Both surveys produce an indication of the number of farms, and it is expected that the farm/non-farm classification for operations should generally agree between the two sources. In 2007, an evaluation of differences in classification for specific farms across the two surveys revealed that most classification errors occurred within the JAS, not the census. The study was later expanded to evaluate all operations which were misclassified on either frame. The characteristics of the farms that were misclassified by either the JAS or the census are discussed.    ","In years ending in 2 and 7, the National Agricultural Statistics Service (NASS) conducts the Census of Agriculture. In addition, NASS conducts an annual area-frame-based survey called the June Area Survey (JAS). Both the Census of Agriculture and the JAS provide an estimate of the number of farms in the United States. In 2007, the difference between the two estimates could not be attributed to the error associated with the respective estimates. In this paper, non-response and misclassification in the JAS are considered as possible factors leading to the discrepancy. How to account for misclassification and non-response in the JAS-based estimate is discussed. ","Each year, the National Agricultural Statistics Service (NASS) conducts the June Area Survey (JAS), which is based on an area frame. The JAS provides information on U.S. agriculture, including an estimate of the number of farms in the U.S. NASS also conducts the Census of Agriculture every five years in years ending in 2 and 7. The census uses a list frame, and also produces an estimate of the number of farms. In 2007, the two estimates were further apart than could be attributed to sampling error alone. Using data from the 2007 JAS and the 2007 Census, misclassification of tracts as agricultural or non-agricultural can be identified. A model estimating the JAS undercount of the number of farms was developed and used to provide a revised estimate. The development of the model and its potential use for adjusting the JAS estimate for misclassification in non-census years are discussed. ","Protection of the sensitive values at high risk of disclosure or values of key identifiers is a crucial problem. Meanwhile, there usually exists missing data in the census. Multiple imputation has been a useful tool to protect data confidentiality and handle missing data. We propose a new approach to census microdata dissemination: sampling simultaneously with synthesis and missing data, which generates multiple imputed datasets that simultaneously handle missing data and disclosure limitation. The basic idea is to fill in the missing data first in the census to generate multiple complete populations, then replace the identifying or sensitive values in each population with multiple imputed values, and finally release samples from these multiple imputed populations. This article develops methods to obtain valid inferences from the new three-stage imputation process. ","The NCHS conducts several population-based health surveys. The success of the surveys depends on the participation of the respondents who provide data under the assurance that their information will be kept confidential. Survey data files contain micro-data records for each respondent and risk of disclosure depends on the sensitivity of each item. Most public-use files (PUF) include demographic, socioeconomic and health related data. Personal identifiers are excluded from all internal and external files and are used only for data collection or data linkage purposes under strict confidentiality agreements. Geographic identifiers increase the risk of disclosure, especially for respondents with rare characteristics from small areas. The NCHS has established RDCs to analyze internal data under a secured environment. This paper summarizes the procedures used to protect respondents' data. ","This paper describes the statistical disclosure avoidance techniques to be used for all U.S. Census 2010 and American Community Survey (ACS) five-year tabular data products.  Many of these tables are published for very small geographic areas.  The paper includes procedures for standard base tables, special tabulations, and a future online query system.  Procedures include data swapping, rounding, collapsing categories, applying thresholds, table suppression, and generation of synthetic data.     ","In an attempt to protect sensitive counts data (discrete data), independent rounding of tabular data cells has been proposed and has also been used by statistical agencies all over the world.  In this paper we demonstrate that such a practice not only results in to (1) degradation of tabular data quality and (2) produces non-additive tables, but the strategy also fails to provide adequate protection from statistical disclosure to low count tabular data cells. ","The National Children's Study (NCS) is designed to investigate the effects of environmental exposures and gene-environment interactions on pregnancy outcomes, child health and development, and precursors of adult disease. Environmental samples, bio-specimens, family history, ultrasounds and medical records, images, and direct assessment of health and development will be collected. This unprecedented array of sensitive, personally, and socially identifiable data requires an innovative, comprehensive, and evolving data access plan that responds to NCS principles of data integrity, data sharing, and protection of confidentiality in a changing scientific and regulatory climate. Data access policies and procedures will be presented, with special focus on the management of socially identifiable data, and sharing information with study communities, including Tribal Nations. ","A large amount of genome-wide molecular profiling data has been accumulated on mRNA expression, histone modification, DNA methylation and transcription factor (TF) binding to study drug addiction. In this study, we develop a novel statistical model to joint model multiple molecular profiling data for detection of important genes associated with cocaine addiction. Here, data from ChIP-chip experiments include genome-wide DNA methylation, histone modification, TF binding data with cocaine exposure and with saline treatment. Expression data will include mRNA expression datasets with and without cocaine treatment. A Bayesian hierarchical model, combined with a logistic regression model for multiple-level responses, is used to integrate information from the multiple data sources available. Both simulation studies and real data examples show the good performance of the proposed model.  ","Recent genomic studies have shown that significant chromosomal spatial correlation exists in gene expression of many organisms. Interestingly, co-expression has been observed among genes separated by a fixed interval in specific regions of a chromosome chain, which is likely caused by three-dimensional (3D) chromosome folding structures. Ignoring such correlation in statistical modeling can reduce the efficiency of estimation and the power of statistical inference. Further, modeling the spatial correlation explicitly leads to essential understandings of 3D chromosome structures and their roles in transcriptional regulation. In this study, we proposed a hierarchical Bayesian method to formally model and incorporate the correlation into the analysis of gene expression microarray data. It is the first study to quantify and infer 3D chromosome structures in vivo using expression microarray. ","In cancer genomic studies, marker identification from analysis of a single data source has suffered from lack of reliability and reproducibility. More and more genomic studies are being shifted towards data integration and integrative analysis. Most existing methods are ad hoc, and there is a lack of novel methods with sound statistical basis. In this study, we propose novel penalization methods for integrative analysis of cancer genomic data from multiple sources and marker identification. The proposed methods can automatically accommodate the heterogeneity across multiple sources and the joint effects of multiple markers. We rigorously establish asymptotic selection properties and conduct simulations. Analysis of data on multiple cancers shows satisfactory performance of the proposed methods. ","In applied analysis, it is often useful to estimate a linear trend in the data even though classical assumptions of linear regression (i.e., homoscedasticity and linear mean) do not hold. In the frequentist setting, estimating equations and the Huber-White sandwich covariance estimator give asymptotically valid inference, provided that sufficient care is taken to specify the target of inference and the covariate sampling frame. We present a new Bayesian approach that leads to uncertainty estimates with the same robustness properties as the sandwich estimator. Our derivation provides a compelling Bayesian justification for using this simple and popular tool, and it also clarifies what is being estimated when the data-generating mechanism is not linear. We demonstrate our approach using a simulation study and health care cost data from the Washington State Basic Health Plan. ","There are many methods in the literature for imputing missing responses in semi-parametric regression, but not many of these articles address efficient estimation. Recently, Muller (2009) suggests an efficient approach in a nonlinear regression setting.  The drawback of that approach is that it requires constructing an efficient estimator for the finite dimensional parameter, which can be quite involved.  Under the assumption of responses missing at random I will investigate full imputation methods for several scenarios, using the efficient method suggested by Muller (2009) as well as other possibly inefficient approaches.  I will analyze the variance of the estimators, both theoretically and with simulations. ","Increasing processor clock rates enabled increases in statistical computing complexity and enabled the processing of larger data sets for many years. Statistical software would just run faster on a new chip with a faster clock rate. This free ride is now over until we fully embrace the concurrency revolution and the development of scalable parallel algorithms. The last five years have seen stagnant clock rates and more processors. Why? There are fundamental reasons for this change and the future is full of multiple cores and specialized co-processors. Serial algorithms get little benefit from a second processor. Scalable parallel algorithms now get a free ride. Where will they come from and who should provide them? This poster presentation will attempt to answer some of these questions. The new computing landscape presents both challenges and opportunities for statistics. ","The time-consuming methods popular in modern statistics require efficient use of available computing resources. The potential benefits of parallel computing are well known but among statisticians are not widely realized in practice. I will present a generic framework for parallel statistical simulations, called snowFT, that overcomes several obstacles in parallel programming. It generates reproducible results, it is fault tolerant, it ensures load balancing and computation transparency. It is implemented as an R package and its ease of use makes parallel processing accessible even to non-programmers. I will demonstrate snowFT on several examples. ","The Wrights used the 1st wind tunnel balance in 1903. This paper shows a designed test in 2000 on a NASA wind tunnel balance calibration application. Previously NASA used the one factor test method to calibrate balances.  Both methods were tested on the same balance. Compared to the one-factor method, the designed test reduced costs by 85% and improved data quality. NASA patented this work, though they usually make innovations public domain.  The 3 Force and and 3 moment balance dimensions must be orthogonal because of interaction effects. This constraint was satisfied by crossing a 3-variable design with a 2-variable design.  In aersospace work, 2nd order models are often insufficient when there is lack of fit. A sequential design was needed whereby a 3rd order design was added to the 2nd order design. The ANOVA tables, 2nd and 3rd order test designs, and confounded terms will be shown. ","Many test statistics under null hypothesis don't have closed forms of finite-sample distributions and hence must reply on asymptotic distributions to find critical values or p-values. This often leads to inaccurate assessment of the hypothesis tests. One solution to this problem is to use response surface regression technique. It requires mass simulation to estimate quantiles of the finite-sample distribution or asymptotic approximation with replications for each of different sample sizes. In this poster we will show how to use R package Rmpi to implement this technique. Rmpi is a wrapper to      MPI, the most used parallel computing tool, and has its own interactive master and slaves environment. It can be run under Windows, Mac OS X, and Linux platforms. Jaque-Bera normality test will be used as an example.  ","Gaussian mixtures used for clustering continuous data imply that each cluster has an elliptical shape, an assumption often violated in real-world data sets.  A recently developed method addresses this issue by partitioning data according to whether they can be brought to the same mode via an ascending path, assuming the density function is in the form of a mixture.  The correspondence between a mixture component and a cluster no longer holds under the new paradigm.  To treat large-scale real-world data, we have developed a strategy to significantly speed up the mode based clustering method.  Leveraging this state-of-the-art clustering method, we have developed an interactive visualization system to enhance the presentation of curves.  Moreover, a summarization method is developed for  meteorology cloud maps used in weather prediction. ","We consider the problem of screening variables for regression where a large number of variables are given as potential predictors of a response of interest. To examine the relative merits and drawbacks of screening methods, we compare univariate screening on the basis of correlation between each variable and the response with multivariate screening via a penalized least squares method.    ","We propose a new classification method for longitudinal data based on a semiparametric approach. Our approach builds a classifier by taking advantage of modeling information between response and covariates for each class, and assigns a new subject to the class with the smallest quadratic distance. This enables one to overcome the difficulty in estimating covariance matrices while still incorporate correlation into the classifier. Extensive simulation studies and real data applications show that our approach outperforms support vector machine, the logistic regression and linear discriminant analysis for continuous outcomes, and outperforms the naive Bayes classifier, decision tree and logistic regression for discrete responses. We will also present its statistical learning theory including upper bound and normal approximation to the generalization error. ","The estimated test error of a learned classifier is the most commonly  reported measure of classifier performance.  However, constructing a  high quality point estimator of the test error has proved to be very  difficult.  Furthermore, common interval estimators (e.g. confidence  intervals) are based on the point estimator of the test error and thus inherit all the difficulties associated with the point estimation problem. As a result, these confidence intervals do not reliably deliver nominal coverage.  In contrast we directly construct the confidence interval by use of smooth data-dependent upper and lower bounds on the test error.  We prove that for linear classifiers, the proposed confidence interval automatically adapts to the non-smoothness of the test error, is consistent under fixed and local alternatives, and does  ","We introduce a parametrized family of energy functions useful for proximity analysis, nonlinear dimension reduction, and graph drawing. The functions are inspired by the physics intuitions of attractive and repulsive forces common in graph drawing. Their minimization generates low-dimensional configurations (embeddings, graph drawings) whose interpoint distances match input distances as best as possible.   The problem of selecting an energy/stress function is translated to a parameter selection problem which can be approached with a meta-criterion. Of particular interest is the tuning of a parameter associated with the notion of \"clustering strength\". Such tuning greatly helps identifying clusters.   ","Low-dimensional structures such as sparseness or homogenous subgroups of predictors are useful in high-dimensional regression analysis. In this article, we identify the grouping structure with respect to the size of regression coefficients, while achieving sparseness of a model, where predictors correspond to nodes over an undirected graph representing prior knowledge about grouping, and edges for two connecting nodes indicate that grouping is likely between them. This is motivated from gene networks, where genes tend to work in groups according to their biological functionalities. Through the method of non-convex regularization, we develop computational tools and study theoretical result for the proposed method. Both simulation studies and theoretical results show that the true underlying model can be recovered through our method. An application to gene network data will be discussed. ","Classification for high dimensional data is very important in modern  statistical methodology development. In this article, we propose a  penalized matrix classification method (PMCA), which can be viewed as a generalization of the conventional LDA method to the data with matrix predictors. To incorporate the predictors sparsity into the classification analysis, we regularized the $L_1$ norm of the  matrix classifiers and integrate the classification and feature selection together. We studied asymptotic properties of the  algorithm by both theoretical analyses and empirical examples, and showed its superior performances in comparison with existing methods by simulations. We also show the practical usage of PMCA on the classification of the volatile chemical toxicants. ","Biomarkers have been widely used to aid medical screening, diagnosis and prognosis of disease. Receiver operating characteristic (ROC) curve is a popular summary for evaluating the accuracy of such biomarkers. Pepe and Cai (2004) and Cai (2004) considered fitting a regression model for the distribution of one minus percentile values to make inference about ROC curves. Here we extend the percentile value based ROC regression model to censored event time outcome data. The additional dimension of time is incorporated when estimating the covariate-specific percentile values using semiparametric and nonparametric models. A weighted GLM form of ROC regression is proposed to accommodate censoring. Our new approaches are fairly easy to implement using standard software. The performance of proposed estimators is investigated by simulations and is illustrated using Seattle Heart Failure Study. ","The Barell Matrix (BM) is an ICD-9-CM injury classification system used in epidemiology. The BM does not classify all knee injuries (KI) commonly presenting among Active Duty US Army (ADA) Soldiers. To address this gap, we propose the Hill Matrix (HM) for identifying KI among ADA. ICD-9-CM diagnoses (first 4 positions) from two random months per year (2000-2007) were extracted from inpatient and outpatient clinical encounter (CE) records for ADA (n=7,792,914). Each CE was classified as identifying KI by BM, HM, or both; discordance was analyzed.HM and BM identified significantly different groups of CEs (McNemar's p &lt; 0.0001). For certain KI (contusions, crushes, dislocations and tears), HM includes all of the BM ICD-9-CM codes. HM includes more codes than BM to identify strains/sprains and fractures; only BM includes burns. Amputations, derangement, and bursitis are only identified in HM. ","We present a novel latent space representation of the relative propensity for a respondent to form ties with members of a particular social group, a quantity related to overdispersion.  In many applications collecting complete network data is financially or practically infeasible.  Instead, we use data where respondents are asked for the number of ties they have with members of various subpopulations or ``How many X's do you know?' data.  We connect this data with recent work using models which represent dependence in a fully observed network through distance in an unobservable ``social space,' known as latent space models (Hoff , Raftery and Handcock (2002)).  This yields a latent space representation of overdispersion, further elucidates how these data measure social structure indirectly, and suggests a latent space model for such data.   ","The drug development and evaluation of oncology requires efficient strategies to predict the patient population who will benefit from treatment as well as surrogate imaging outcome in early phase of treatment. As an example, Glioblastoma Multiforme (GBM) will be used in this paper. GBM is a grade IV glioma tumor, and are the most aggressive and lethal primary brain cancer. Not all patients have responded to the standard therapy. Considering the short life expectancy for patients with GBM, it is critically important to predict which patients will benefit from the treatment. Apparent Diffusion Coefficient (ADC) from Magnetic Resonance Imaging (MRI) shows the cellularity or cystic regions of GBM and Fluid Attenuation Inversion Recovery (FLAIR) shows the effect if fluid. We proposed new quantitative imaging \"biomarker\", which is derived from estimated mixture of Gaussian distribution. ","Imaging is an important tool used to assess and accelerate the process of therapeutic options in clinical medicine.  Sharp's criteria for rheumatoid arthritis form just 1 of 3 licensed imaging surrogate biomarkers approved by the US FDA.  Along with imaging used in oncology research, the Sharp score serves as a biomarker that reveals the extent and severity of disease using standard radiographs.  However, in response to drug development programs and advancements in imaging techniques, the FDA issued its critical path initiative in 2004.  Advancements in imaging techniques like MRI, PET, ultrasonagraphy, and optical spectral transmission, for example, have helped to promote biomarker evaluation with ongoing trials.  This poster gives an overview of imaging biomarker development since the precedent of Sharp's criteria in relation to the scope of the FDA's modernization initiative. ","The increase of cell-only and cell-mostly populations in the U.S. has led survey organizations to sample from a cell-phone frame as well as the traditional landline frame. With the dual-frame design, cell-mostly households can appear in both frames. Cell-mostly households are defined here as households with both landline and cell telephones that are somewhat unlikely or very unlikely to answer their landline telephone. For weighting purposes, it is critical to understand how this population differs from the cell-only and landline populations. This paper will examine three types of households - cell-only, cell-mostly, and landline households - sampled from dual frames by using The National 2009 H1N1 Flu Survey data. Household types will be compared with regard to their propensity to respond, characteristics related to key survey questions, and demographic characteristics. ","The Minnesota Health Access Survey (MNHA), a large-scale health insurance survey conducted jointly by the Minnesota Department of Health and the University of Minnesota, tracks health insurance coverage and access in Minnesota. The 2009 round of the MNHA is the first to use data collected from a cell phone sample as well as a random-digit-dial (RDD) sample. This paper explores the trade-offs between screening cell samples based on landline and cell phone usage patterns and accepting all cell phone users as respondents. After concluding that screening is the optimal solution, the paper then explores the optimal percentages of MNHA interviews that should be attempted from the landline and cell phone frames. ","In recent years, the increasing undercoverage of random-digit-dial (RDD) landline frames has driven surveys to employ landline plus cell phone dual-frame designs. The 2009 Minnesota Health Access Survey, a large-scale health insurance survey conducted jointly by the Minnesota Department of Health and the University of Minnesota, collected 9,811 landline interviews and 2,220 interviews via a cell phone sample (regardless of landline/cell phone usage). This paper compares weights produced under four different screening strategies (RDD-only, RDD plus cell-only, RDD plus cell-only/cell-mostly, and RDD plus cell-any) and five different weighting adjustments for combining landline and cell-phone interviews. Results show the lowest mean-squared errors result from using all interviews with an effective sample size adjustment factor. ","We use National Health Interview Survey (NHIS) data to derive small-area-level direct estimates for every six-month period from January 2007 to June 2009 for wireless-only, wireless-mostly, landline-mostly, and landline-only households for states and several metropolitan areas. Because the NHIS is not designed to produce unbiased state level or metropolitan level direct estimates, we use a small area model with covariates obtained from the American Community Survey (ACS). The model-based estimates are raked so they agree with the national-level direct unbiased estimates for the corresponding six-month periods from the NHIS, and the estimates are also raked so they agree with the complement of the \"no phone\" estimates obtained from the 2008 ACS. State-level and selected metropolitan-area-level estimates are obtained by appropriately aggregating the model-based small-area-level estimates. ","Paradata are automatic data collected about the survey  data collection process captured during computer assisted data  collection, and include call records, interviewer observations, time  stamps, keystroke data, travel and expense information, and other data  captured during the process. Increasingly such data are being used in  real time to both monitor and manage large scale data collection  processes. In this paper we use a statistical process control perspective  to describe how such data can be used to monitor the survey process.  Process control charts and statistical models can be used to identify  areas of concern during data collection, and can lead to further  investigation of the problem and (if necessary) intervention. We  describe the data and analyses that are available and present several  case studies of paradata use in different types of surveys and  organizations. ","Increasing use of cell telephones, and corresponding decrease in households with landline phones, has led to coverage concerns in RDD surveys. For Racial and Ethnic Approaches to Community Health (REACH) U.S., a set of CDC-sponsored community surveys, NORC interviews minority households, for which the landline coverage issue may be more acute. Recently the REACH U.S. Survey converted to an address-based sampling approach with multi-mode data collection. Applications of existing methodologies were challenging due to limited availability of paradata to guide design and operational planning. In this paper we discuss the address-based design, alternatives considered, and related assumptions regarding response rates, eligibility rates, and other factors affecting sample size calculations and costs. We also discuss design and operational adaptations made as we gained experience. ","Since its inception in 1971, the National Survey on Drug Use and Health (NSDUH) has experienced many changes, including the transfer of the survey between federal agencies; changes in government project officers and contractors; modifications to questionnaire content; major changes to the sample design and size; introduction of new modes of data collection and incentive payments to respondents; changes in the oversight and management of field staff; and introduction of new weighting, editing, and imputation methods. Some of these changes have, not surprisingly, resulted in both intended and unintended consequences, in some cases despite best efforts to control and quantify the effects of these changes. This paper uses examples to illustrate how prior experiences have influenced both ongoing practices and the current approach to redesigning the survey. ","The 2009 Canadian Census Test was officially held on May 12th. It is a key element in the planning of the 2011 Census. Among the innovations tested were a wave collection methodology designed to maximize Internet response, a voice message broadcasted to encourage responses, a process to identify unoccupied dwellings early in the census collection process and a computerized system for field activities management. The sample for the Test consisted in a set of 110,000 dwellings selected in two sites of Canada and a set of 25,000 dwellings selected randomly across the country. The Test sample showed a high Internet response, an improved coverage and a streamlined collection process. In this paper, we will describe some of those new collection methods and present details on the results of their evaluation. We will also give an outlook of how this will be used for the 2011 Census. ","NORC has integrated GIS and linear programming to automatically assign cases with the goal of minimizing distance between interviewers and cases in recent years. Traditionally, this assignment process had been performed manually by field managers using their local knowledge. While our new automatic assignment method took advantage of distances calculated in GIS to considerably improve efficiency, there remained room for improvement as factors other than distance to case were often salient in assigning cases to interviewers. In 2009, NORC enhanced its assignment making software to incorporate additional factors such as language of cases and interviewers, experience of interviewers and case history. This paper summarizes these improvements and introduces an off-shoot of the assignment making program which assigns entire target segments based on distance and number of cases in each segment. ","Over a year of outbound phone calls on thousands of phone survey research projects, what can we learn from analyzing the outcomes and disposition codes of all the calls put together?  How can the existing trends help us with innovative potential solutions for improvement in the future?   We explore the disposition outcomes searching for ways to improve sample utilization and connectivity.  Particular focus is placed on understanding the major categories of Interviews, Eligible (Non-Interviews), Non-Eligible and Unknown Eligibility (Non-Interview) and how they break down so that technological innovations can be used to improve the contacting process in the future.  How long do disconnects stay disconnected?  Is it possible to use past dispositions to predict best time to reach respondents, and is it legal or ethical?  What do we know about current trends and how can we use it to improve? ","Data falsification occurs when an interviewer intentionally deviates from established survey interviewing procedures.  Falsification may be influenced by several factors, such as interviewer tenure and workload, survey subject matter, location, and length of survey. While past research projects have focused on the impact of these factors individually, few studies have examined the joint association between these characteristics and data falsification.  The current research is an exploratory analysis that focuses on the relationship between the above-mentioned factors and data falsification.  Results from formal investigations of data falsification on the Census Bureau's major demographic surveys from 2005 to the present will be analyzed along with respondent traits.  Findings from this analysis will be used to generate a profile of falsified interviews.   ","Minimizing survey error requires adherence to the accepted principles and best practices of survey research.  Interviewers can be a significant source of error that is difficult to control.  Ensuring that interviewers execute their jobs properly requires that they be well-trained, monitored, and provided feedback in real time.    ","Markov latent class analysis (MLCA) is a modeling technique for panel or longitudinal data that can be used to estimate the classification error rates for categorical outcomes with categorical predictors when gold standard measurements are not available. Because panel surveys track respondents over time, explanatory variables can be either time varying or time invariant. Time varying grouping variables can be useful in explaining differences in the latent construct over time. However, they generate a large number of model parameters that can make model results unreliable. This paper discusses alternative coding schemes for time varying grouping variables and proposes a set of procedures for determining the best coding scheme for a particular set of data. This process is then illustrated using data from the National Crime Victimization Survey (NCVS).  ","The Grade of Membership (GoM) model is a heirarchical Bayesian mixed-membership model used recently to analyze latent class disability profiles in the National Long Term Care Survey (NLTCS).  Survey data often includes complexities such as informative sampling and dependencies induced by clustering.  Two modifications of the GoM model are introduced to incorporate these complexities.  The first changes the Dirichlet prior to a polytomous logistic mixed-effects prior to model the stratification and clustering in the sampling design.  The second adds probability weights using a method called Weighting based on the Estimated Parameter, or wEP.  To demonstrate the effectiveness of these modifications, we use simulated data to compare the Dirichlet prior to the mixed-effects and then apply the method to the NLTCS data. ","Generalized structured component analysis (GSCA) is a component-based approach to structural equation modeling. In practice, researchers may often be interested in examining the interaction effects of latent variables. However, GSCA has been geared only for the specification and testing of the main effects of variables. Thus, an extension of GSCA is proposed to deal with various types of interactions among latent variables. In the proposed method, a latent interaction is defined as a product of latent variables. As a result, this method does not require the construction of additional indicators for latent interactions. Moreover, it can easily accommodate both exogenous and endogenous latent interactions. An application is presented to demonstrate the usefulness of the proposed method.  ","The USDA National Agricultural Statistics Service (NASS) conducts the quinquennial Census of Agriculture in years ending in 2 and 7.  Also, NASS conducts an annual area frame based survey, the June Area Survey (JAS).  The census has a dual frame: an independent list frame and the area frame from the JAS.  The JAS is used to identify farming operations missed on the list frame.  In 2007, a full census questionnaire was sent to all JAS records that were not found on the census mail list.  Multiple clustering techniques were used to characterize farming operations missed during the census mail list building.  Hierarchical methods (average linkage, centroid, and Ward's method) and non-hierarchical k-means clustering were used to identify groupings.  Through cluster profiling, potential improvements to future list building efforts are discussed.  ","Logistical challenges and limited resources for field experimentation often stand as major impediments to testing improvements in survey methodology, but other approaches are available for introducing and evaluating change on a gradual basis. This case study describes how the principles of action research were used to introduce more standardized interviewing into a survey that had completely relied on a conversational interviewing approach for many years. The advantages and disadvantages of standardized vs. conversational interviewing are discussed in this context. ","The National Health and Nutrition Examination Survey (NHANES) is a program of periodic surveys conducted by the National Center for Health Statistics (NCHS) that provide national estimates of the health and nutritional status of the U.S. civilian noninstitutionalized population.  DNA specimens were collected during the NHANES III (1991-1994) and NHANES 1999-2002, then added back to the NHANES survey starting in 2007. In 2001, NCHS approved a protocol that allowed researchers to use genotyping data linked to NHANES data, although this data would only be accessed in the Research Data Center (RDC). In 2009, NCHS implemented an RDC subcommittee to enable researchers to use the linked data within the RDC.  This paper describes some of the issues involved in allowing researchers use of restricted genetic data. ","The National Hospital Discharge Survey (NHDS) is conducted to produce nationally representative estimates of hospital discharges.  Micro data files containing data collected in the NHDS are released to the public, but sampling design variables required for computing the NHDS variances are omitted from these files to maintain confidentiality of respondents' identities.  To enable data users to compute variances for estimates derived from the public use data files, generalized variance functions (GVFs) are supplied in the public use data file documentation. This paper discusses the quality of the GVF results.  It compares using GVFs to derive relative standard errors (RSEs) (aka coefficients of variation or CVs) from the public data use files with applying SUDAAN to in-house data files to obtain RSEs. ","Universal Soil Loss Equation is a model that predicts the long term average annual rate of erosion on a field. It is replaced by the recently released RUSLE2 model. It is necessary to impute USLE estimations after 2006 and RUSLE2 estimation before 2002. We present models to predict soil loss in one model using the variables in the other one. With the aid of cross validation, we analyze an Iowa data set on field erosion with three fitting methods: simple linear regression, multiple linear regression, and multivariate adaptive regression splines. We found that MARS has the best prediction power of RUSLE2 soil loss based on USLE variables, while MLR is the best choice for predicting USLE soil loss based on RUSLE2 variables. The coefficient of variation computed for each observational point and a county-level CV map is produced to identity abnormal counties and spatial pattern. ","Beginning in 2008, the National Center for Health Statistics supplemented the  core personal interview survey of physicians in the National Ambulatory Medical Care Survey (NAMCS) with a mail survey to measure use of electronic medical records (EMR) in non-Federal office based physician practices.  In 2008, subsamples of the non-respondents to the mail survey were included in follow-up telephone or personal interview surveys.  Preliminary estimates based only on the mail survey and final estimates based on combined data from both the mail and interview surveys were produced.  This paper discusses the survey design and methods for the EMR estimates and some changes made after 2008. ","This research is motivated by the need to extend conditional logistic regression (CLR) for complex survey data.  We considered scaling the survey weights to be integers and creating a pseudosample by replicating observations according the weights; then, we used CLR with the pseudosample.  However, intuition suggests that this approach may be biased, because large-scale replication might approximate inclusion of a fixed effect for each cluster in the logistic regression.  For the simple matched pairs design, we show that replicating observations k times and using CLR results in an estimator whose limit as k approaches infinity is the ordinary MLE with a fixed effect for each pair.  Simulations are also presented. ","The International Price Program of the Bureau of Labor Statistics estimates monthly indices on the changes in import and export prices for merchandise and services. The data are collected through a complex sample of establishments using monthly reported data. Consequently, in time series analyses of IPP data, there is potential interest in the variances and autocorrelation functions of both sampling and measurement errors, as well as the underlying true price-index series. This paper presents several analytic methods that one may use to estimate the parameters of these terms. ","The NHIS is a complex survey targeting the health of the U.S. population.  To allow for design-based standard errors via linearization, the public-use data provides limited design structures: strata, PSUs and a final survey weight. These structures are accommodated very well by popular complex-survey data analysis software.  However, the linearization approach imposes limitations on statistical functional forms, and furthermore, this approach often does not account for statistical variability due to weighting adjustments. The Fay-modified balanced repeated replication (Fay-BRR) method provides a flexible alternative to variance estimation which can correct for some of the linearization method's weaknesses.  In this paper the Fay-BRR method is implemented on the NHIS adult samples for the 1997-2005 NHIS design years, and the operating characteristics are discussed. ","Recent methodological research has addressed the important issue of sample size at each level when estimating multilevel models. Although several design factors have been investigated in these studies, differences between continuous and binary predictor variables have not been scrutinized (previous findings are based on models with continuous predictor variables). To help address this gap in the literature, this Monte Carlo study focused on the consequences of level-2 sparseness on the estimation of fixed and random effects coefficients in terms of model convergence and both point and interval parameter estimates. The 5760 conditions simulated in the Monte Carlo study varied in terms of level-1 sample size, number of level-2 units, proportion of singletons (level-2 units with one observation), type of predictor, collinearity, intraclass correlation, and model complexity.  ","To simulate the impact of the NHANES sample design, we create a finite population defined by combining 9 years of the National Health Interview Survey linked to (correlated, often missing) air quality data. Based on the NHANES sample design, we will draw samples of 5,000 Households from this finite population of approximately 360,000 households and evaluate different analytic methods for linked data files, with attention to the use of the standard survey weights and multi-level models. With the simulated samples we will apply design-based and model-based methods to estimate the relationship between health and air pollution, with and without control for standard and, when appropriate, scaled survey weights. We will assess the impact of correlated and missing exposure data, and of variation in survey design (number, size of PSUs) on inferences to provide guidance to users of linked files. ","In probability proportional to size sampling, sizes for non-sampled (NS) units are not required for the usual Horvitz-Thompson (HT) or Hajek estimates, and this information is rarely included in public use data files. However, Zheng and Little (2005 JOS) show that including the sizes of the NS units as predictors in a spline model can result in improved estimates of the finite population total. We use a penalized spline model to predict the outcome for the NS units based on the predicted sizes. We compare the precision of the HT and Bayesian estimates when the population sizes of NS units are (a) known, and (b) estimated, using a Bayesian Bootstrap model with weakly informative priors. The variance of the latter estimator can be decomposed into one component from estimating the NS sizes and one from the spline model, allowing us to assess the gain in information with available NS sizes. ","Description:  Some previous evidence have already showed that people of different ages may response differently to a weather survey in unreasonable local temperature conditions. For the data from 8 CBS News surveys from 2001 to 2007, I will use logistic regression analysis to show that the  temperature change can affect the response results, and then use  post-stratification domain analysis to adjust the sampling nonresponse bias.   ","The U.S. agency for international development (USAID) Neglected Tropical Disease(NTD)Control Program supports the implementation of Mass Drug Administration (MDA) in 12 developing countries to control five neglected tropical diseases: Lymphatic Filariasis, Onchocerciasis, Soil Transmitted Helminths, Schistosomiasis, and Trachoma. The objective of the Post MDA Survey is to validate the accuracy of the country reported MDA coverage rates. There are unique challenges to designing and implementing household surveys in developing countries. We will present the design effect and the precision of the estimated drug coverage rates of multi-stage surveys from one country: Niger. We will identify effective sampling methodology for program evaluation in developing countries. Specifically, we will focus on the stratification and clustering strategies and identify effective methods to reduce variance ","The bias of the treatment effect estimator in an observational study can be reduced by the propensity score (PS) adjustment. Rosenbaum and Rubin (1983, 1984) provide a theoretical framework to use equal frequency subclassification (EFS) on PS with equal weights (EW) that was introduced in Cochran (1968). Hullsiek and Louis (2002) propose equal variance subclassification (EVS) using inverse variance weights (IVW). We develop a theoretical framework to illustrate if higher variation occurs with larger bias among the subclasses in EFS, then an IVW estimator has smaller bias than the EW estimator; and it always has smaller variance. We show if the equal variance in the EVS approach is larger than the harmonic mean of the subclass-specific variances under EFS, then the IVW estimator using EFS has lower variance. Numerical verification of these results and a data application are also given. ","We studies food sampling plans for food inspection, used domestic and   abroad, in order to harmonize the domestic ones to world standard. We have studied the relation between lot size and the sample size and found the relation for plant quarantine and quality inspection defined by Japan agricultural standard (JAS) law. We further studied the OC curve to compare the international standard, which is AQL of 6.5. JAS law recommded the switching of sampling plans from loose, normal and strict depending on the history. This improves the sharpness of sampling plan near AQL and decrease the cost for inspection. We further studied US's sampling plan described in code of federal regulations.   ","Managing survey operations in an electronic environment provides  opportunities and challenges.  One such opportunity is the potential to collect paradata,or measures of survey progress,costs,and quality.This roundtable will focus on building real-time systems to manage these performance indicators. Some key questions to be considered include: What paradata are being collected and used in \"real time\"? How are these data being used? What technologies are being used for building,displaying and monitoring paradata?  What is the development process? Are these built on a database or streamed from multiple sources? In addition,we would like to discuss the potential for more sophisticated displays of paradata during data collection that might facilitate quick decisions during the data collection period,potentially including changes to questionnaires, samples, modes,or interviewing methods. ","One important aspect of physical activity research is the assessment of usual (i.e., long-term average) daily energy expenditure. Daily measurements of energy expenditure taken from a sample of individuals are prone to measurement errors and nuisance effects, which can lead to biased estimates of usual daily energy expenditure parameters. Fortunately, statistical models can be used to account and adjust for these errors in order to give more accurate estimates. In this paper we develop a method for estimating usual daily energy expenditure parameters from data collected using a self-report instrument and an objective monitoring device. Our method is an extension of existing methods that utilize measurement error models. We illustrate our method with preliminary data from the Physical Activity Measurement Survey (PAMS) collected using an armband monitor and a physical activity recall. ","In recent years, the Federal Reserve has surveyed banks on the number and value of check payments, and to track the adoption of new electronic check clearing methods that replace traditional paper-based methods.  The data requested from each respondent consists of a hierarchy of nested totals and subtotals.  The ability of banks to report data to this voluntary survey tends to vary, however, which leads to a complex pattern of item nonresponse.   In light of this, independent estimation of aggregates by ratio estimator using only the reported data created violations of adding-up and other logical constraints among survey items.  To overcome this, we investigated various item imputation methods.  But these methods tended to overestimate the proportions of banks in the population that had adopted the new clearing technologies.  Accordingly, we adjusted the methods to account for that bias. ","The estimation of nonresponse bias and measurement error shares the problem of usually not having a criterion to assess the quality of the estimate.  Nonresponse bias analysis could use responders within the survey sample who are in some way similar to nonresponders to estimate the potential bias.  Measurement error studies have similar problems with estimating systematic error (e.g. re-interviews) to provide a measure of the quality of response.  Statistical matching uses responders or aggregate data from other sources to model the difference in estimates between sources.  The matching is done on respondent characteristics common to both sources.  In this study I will use several different sources to attempt to triangulate the differences.     ","Parameter estimation with non-ignorable missing data is considered. To avoid the identifiability problems in the model, we assume that surrogate information is available throughout the sample so that the response mechanism can be modeled as a function of surrogate variable and other covariates available in the sample. We use an exponential tilting model to derive an imputation model from the model for the respondents. Exponential tilting model uses minimum assumptions and thus is robust.     ","FACES follows children from their first year of Head Start through their kindergarten year. Child-level data are collected via child assessments, parent interviews, and teacher child reports. Teacher surveys are used to obtain information about FACES children's classrooms and teachers in Head Start and in kindergarten. During the one or two years that the sampled children are in a Head Start program, response rates among teachers are high; however, when the sampled children disperse to many kindergartens, response rates are lower. This paper (1) describes methods used to increase response rates, such as using prior information on where children will attend kindergarten and (2) explores whether nonresponse bias exists for the kindergarten teacher survey by examining a set of sociodemographic characteristics of the children and schools associated with responding and nonresponding teachers. ","The University of Michigan Dioxin Exposure Survey (UMDES) was conducted using a complex sample design to measure the association between environmental exposure and blood level of dioxin-like compounds.  A main interview, blood, soil and dust data were collected, however nonresponse bias was a concern.  At the conclusion of the main data collection (AAPOR RR4 rate 75%), non-complete cases were re-visited in a nonresponse follow-up study (NRFU) that 45% responded.  The NRFU data were added to the main data set, and three types of missing values were imputed. A sequential regression multiple imputation leads to slight improvements in precision. The fraction of missing information varies widely across estimates.  Study findings suggest that NRFU data be treated as a second phase sample of main data collection incomplete cases and combined with first phase sample data in a final data set. ","Estimation of response probabilities when the missing data are not missing at random can be done by postulating a parametric model for the distribution of the outcomes under full response and a model for the response probabilities. The two models define a parametric model for the joint distribution of the outcome and the response indicator, and therefore the parameters of this model can be estimated by maximization of the likelihood corresponding to this distribution. Modeling the distribution of the outcomes under full response, however, can be problematic since no data are available from this distribution.   Sverchkov (2008) proposed an approach that permits estimating the parameters of the model for the response probabilities without modelling the distribution of the outcomes under full response. The present paper extends the approach developed in Sverchkov (2008).   ","The National Agricultural Statistics Service (NASS) collaborates with the Economic Research Service (ERS) to conduct the Agricultural Resource Management Survey (ARMS), which provides a source of information for addressing issues relating to agriculture and the rural economy. ARMS is a multi-phase, multi-mode, and dual frame survey.  The third phase, ARMS III, collects data which is critical to assessing the relationship of the over-all financial health of the farm household and the farm operation with production practices.   Like many surveys, ARMS III is subject to item nonresponse and utilizes imputation to mitigate the effect of the missing information on statistical analysis.  Examination of the ARMS III data set and the current imputation method has revealed a rich potential for fruitful investigations into multivariate imputation techniques for a large, semi-continuous data set. ","The Agricultural Resource Management Survey (ARMS) is a high dimensional, complex economic survey which suffers from item non-response.  Here, we introduce methods of varying complexity for imputation in this survey.  The methods include stratified mean imputation, the approximate Bayesian bootstrap, and non-iterative and iterative sequential regression.  The iterative sequential regression is a form of Markov chain Monte Carlo (MCMC) that is unique in that it allows for flexible selection of conditional distributions while utilizing joint modeling.  Each of the regression procedures require data-driven transformations that allow for the implementation of a conditional multivariate normal model. ","This paper provides an assessment of imputations methods applicable to the Agricultural Resource Management Survey data.  We find that both iterative and noniterative regression procedures perform better than the current NASS method and an approximate Bayesian bootstrap method.  Both regression methods perform better when utilizing a log-skew-normal transformation over a log-normal transformation. ","Creating synthetic copies of a data set is useful for many reasons:  particularly,being able to conduct simulations that overcome the limitations posed by a small data set,and addressing concerns in external data sharing. A synthetic copy of a real data set suitable for analysis preserves the marginal distributions of each variable and their joint behavior.  Designing a general purpose method requires the estimation of the marginals to be data driven. Copulas are functions that combine univariate distributions to form a joint distribution with a specified dependence structure. Applications of copulas span the fields of business,actuarial science,engineering,biomedicine,and spatial science. The National Agricultural Statistics Service is investigating data driven estimation of marginals and using copulas to augment or simulate data sets with the integrity for valid statistical analysis. ","Using the 2008 National Health Interview Survey we examine how biased health surveys are when cell phone only households are omitted and explore whether post-stratification can reduce this bias. We are interested in how well the adjustment work for key subpopulations (young adults and minorities) and an array of health surveillance domains (health insurance coverage, access to care, smoking and drinking). Post-stratification reduces bias in all health related estimates for the total and non-elderly population. However, these adjustments work less well for Hispanics and Blacks and even worse for young adults (18-30). Reduction in bias is greatest for estimates of uninsurance and having no usual source of care and worse for estimates of drinking, smoking, and forgone or delayed care due to costs. We conclude that post-stratification adjustments may not do enough to reduce bias.  ","In 2008 the US Census Bureau changed how it measured whether a household had a working telephone.  The 2000 censuses, and the 2000-2007 American Community Survey asked specifically about the availability of telephone service, not simply the presence of a telephone.  The 2008 ACS, however, instructed respondents to include cell phone service as having a telephone; prior to 2008, this was not made explicit.  The result in the ACS was that estimate of households lacking telephone decreased dramatically.  For the country as a whole the decline was from 4.8% to 1.6%.  Among 18-34 year olds it decreased from 8.6% to 2.4% for American Indians went from 11.6% to 6.7%, Hispanics went from 6.9% to 2.8%, and those below 100 percent of FPL decreased from 10.9% to 4.5%.  We examine the implications for this change on the telephone surveys and especially those that use a \"Keeter\" adjustment. ","To cope with the declining population coverage of traditional RDD surveys, the National Immunization Survey (NIS) has conducted several experiments to test alternative sampling and data collection strategies and to gauge potential bias in current landline-based survey estimates.  These include a cell-telephone pilot study, an address-based sampling pilot study, and an experiment on the National Health Interview Survey where immunization information was collected from respondent's vaccination providers.  National vaccination estimates from the traditional NIS landline RDD sample, the combined RDD+cell sample, the address-based sample, and the area probability sample are presented. This paper will discuss these experiments in terms of their methodology, response rates, and survey estimates.  ","The National 2009 H1N1 Flu Survey (NHFS) is a random digit dial (RDD) landline and cellular telephone survey operating from October 2009 through June 2010.  Conducted by NORC for the Centers for Disease Control and Prevention, the NHFS tracks H1N1 and seasonal influenza vaccination coverage nationally on a weekly basis. This paper will examine results from the NHFS to detect differences between landline and cellular telephone respondents in vaccination rates, flu-related behaviors, reasons for not getting the vaccine, as well as demographic characteristics. The analysis will attempt to determine to what extent health behaviors differ between respondents from each sample type, accounting for demographics.  We also compare estimates from the landline interviews to the combined estimates to gauge the potential bias in landline-only estimates due to under-coverage.    ","The data collection of the 2006 Canadian Census of Population took   place between May and August 2006. An analysis was conducted to compare characteristics of responses by period of collection. The objectives were to have profiles of late responses from follow-ups as compared to responses from the first month of collection (self-reported) and to look at the impact of late responses on specific groups of the population. Differences for late and early responses were observed mostly for age, immigration status, visible minority groups and Aboriginal people. Overall, about 3% of responses were received in the last month of collection but for some specific groups, over 7% of responses were received in the last month   only. Findings from the analysis will be used to study possible change to the imputation strategy of the 2011 Census of Population. ","While even the random loss of respondents can create problems by eroding the power of analyses, many authors have noted that the most critical problem is the bias that the disproportionate loss of particular subgroups introduces into the analyses (Musick, Campbell, &amp; Ellison, 2001).  The size of the bias depends primarily on the relationship between the sources of nonresponse (e.g., demographic characteristics such as age) and their associated response probabilities, as well as the amount of nonresponse (Bethlehem, 2002).  Reducing this bias by increasing cooperation in the underrepresented subgroups will result in more accurate estimates.  This paper uses data mining techniques (specifically, decision tree analyses) to identify patterns of nonresponse in a Web survey of more than 60,000 U.S. Army Officers conducted by the U.S. Army Research Institute. ","The National Survey of College Graduates (NSCG) is a longitudinal survey that collects information on employment, educational, and demographic characteristics of the US college-educated science and engineering workforce. The NSCG refreshes its sample every decade. In the first year of the 2000 decade longitudinal panel, the NSCG experienced a 73% response rate. In response, we conducted a nonresponse bias study to assess the potential impact of nonresponse on the NSCG survey estimates. The study included a benchmark analysis, a comparison of response rates across subgroups, an evaluation of nonresponse weighting adjustments, and an evaluation of the impact data collection effort has on estimates. The goal of this study is to better understand the presence of nonresponse bias in the 2000 decade NSCG and to use the findings to assist in sample design planning for the 2010 NSCG and beyond. ","Unit nonresponse in a household survey may introduce substantial bias into estimates when (i) the number of nonrespondents is large relative to the sample size or (ii) the characteristics of nonrespondents differ greatly from those of respondents. Household surveys expend considerable effort and money to interview reluctant respondents. This additional effort is only worthwhile if it results in a set of respondents that is representative of the target population. In this study, logistic regression models are developed to compare characteristics of early respondents to those interviewed only with great effort. Data collected from the Omnibus Household Survey (OHS), a RDD survey on transportation issues, will be analyzed. In addition, because the sample size of the annual OHS is very small, multiple years of data will be used to augment sample size for this study. ","The 2007 SCF was designed as a continuation of a series of cross-sectional surveys on the financial condition of U.S. households. In light of the serious economic downturn that followed that survey, the Federal Reserve Board decided to pursue a second interview with the survey participants to understand how the aggregate changes played out across households. Ultimately, the survey achieved a re-interview rate of almost 89 percent and relatively low item nonresponse rates for such a complex survey. This paper uses the formal and informal paradata to examine key factors in survey response. If the nonrespondents to the re-interview are representative of marginal respondents in both years, there is an advantage in studying the group, because so much is known about them from their earlier interview and the process of obtaining that interview. ","A response rate is defined as the total number of participating units divided by the total number of eligible units in the sample. However, researchers often do not know the total number of eligible units in the sample as many sampled units go unresolved. Researchers are left to estimate the number of eligible units in the sample. AAPOR response rate equations account for this by recommending that all unresolved sample units be multiplied by 'e,' the eligibility rate among unobserved units, but make no recommendation for how to calculate 'e.'  Among multimode address-based samples, calculating 'e' is especially challenging - are non-responders vacant units; was the answering machine linked to the sampled address; can flags on the delivery sequence file help? We evaluate various methods of calculating 'e' to adjust for these unknowns and their ability to predict unobserved eligibility. ","The new Business R&amp;D and Innovation Survey, co-sponsored by the National Science Foundation and the Census Bureau, is conducted annually with a sample of about 40,000 US private sector companies. To maximize the success of the survey we conducted an experiment with historically problematic respondents and non-responders. In the control treatment, we sent a pre-survey letter to the respondent of record requesting updated contact information for the appropriate recipient of the survey. In the experimental treatment we sent the request to a company executive. We present the results of this experiment, assessing the effect of the experimental strategy on response rates and other outcomes of interest. ","Begun in 2007 as a part of the Global Tobacco Surveillance System, GATS is a growing collection of comparably designed national surveys on tobacco use behavior, currently in 14 countries around the world.  This system of periodic cross-sectional surveys has been developed as the main data source for planning and evaluating various efforts to reduce tobacco use in each country.  Our paper reviews the design rationale and the sampling-related challenges in developing this system.  We specifically emphasize common sample design features to each GATS survey and the statistical basis for GATS sample size requirements.  We also discuss the process used to compute sample weights, present a summary of some key sampling and recruitment outcomes from eight GATS countries that have released their survey findings, and comment on some of the lessons learned thus far. ","To overcome the problem of missed HU in area sampling, the HOI (half open interval) method is often used although its proper implementation remains elusive. As an alternative, we propose to use rejection sampling for drawing a random sample from each cluster with unknown list size for HU. The basic idea is to create subsegments or zones within a selected segment, the number being the cluster sample size, and then one unit is selected at random per subsegment. Using a reasonably sharp upper bound for each subsegment size, FI is provided with an ordered list of a few alternate random selections to use if an earlier selection does not correspond to an eligible HU. Other applications of the proposed methodology include substitution for ineligible HU selected from an initial cluster list in address-based area sampling, and substitution for nonresponding HU in any nonresponse follow-up survey. ","The U.S. Census seeks to determine duplicately listed individuals by searching across lists to identify records with the same name and birth date.  The question arises of how many of these agreeing records are random agreements, two different people with the same name and birth date.  To formally answer this question, we consider first the familiar Birthday Problem and then the more complicated Collision Problem.  For each of these problems we exhibit the explicit probability distributions from which we can compute means and variances for some parameter values.  We apply this result to voter registration lists for Oregon and Washington to estimate the number of \"false matches\" occur across these lists.   ","Since approval of the Cancer Registries Amendment Act in 1992, states have continued to improve the construction of databases containing demographic and clinical information on cancer patients treated within their jurisdictions. These databases can serve as excellent sources for sampling purposes. The National Cancer Institute Community Cancer Centers Pilot Program used cancer registries at 10 cancer centers to sample and analyze patient cancer care experiences. These registries were the basis for a multi-mode survey design that supported the cancer centers' technical constraints while minimizing burden to the patients. We present a cost-efficient sampling method to select probability patient samples without the use of personal identifying information. This work should be informative for other survey designs that sample patients from different types of disease   registries.   ","An innovation in the study of earnings is the availability of administrative records matched to individual responses in household surveys. These data are frequently used in empirical studies. A strong assumption about ignorability of sample selection is needed, though, as the subset of respondents who consented to have their earnings matched may be non-random; a counterfactual (administrative earnings of non-consenters) is generally not observed. We circumvent this obstacle by exploiting the differential timing of consent for respondents of the Health and Retirement Study; those who refused to consent in 1992, but later consented in 2004 form the counterfactual group for those who consented in 1992. Earnings, as well as their determinants, are similar between the two groups. Overall, there is little evidence of non-random selection from using the administrative data to study earnings.  ","High turnover among part-time interviewers is a serious problem for survey organizations with substantial effects on survey costs and data quality. Yet, the determinants of turnover remain largely unstudied (Bowers &amp; Clusen, 2000). Organizational researchers studying full-time employees have consistently found a relationship between job satisfaction and turnover. However, recent research with part-time workers suggests that the path from job satisfaction to turnover may be moderated by job involvement - an employee's level of psychological identification with the job (Martin &amp; Sinclair, 2007). Moreover, job involvement may influence the factors that drive job satisfaction. In this paper, we examine the relationship between job satisfaction determinants, job satisfaction itself, and turnover in light of the moderating role of job involvement in a large survey of telephone interviewers. ","A feature of address-based sampling (ABS) is versatility of the sample frame where many ancillary data can be appended to an address.  Commercial databases, e.g., Experian, infoUSA, Axiom are used to append observed and modeled information at various levels of aggregation.  This enables researchers to develop more efficient sample designs and broaden analytical possibilities with expanded sets of covariates.  While quality of ancillary data is of concern for researchers, the literature provides only anecdotal assessments on accuracy.  Relying on surveys and KnowledgePanel\u00ae recruitment samples that employ ABS, the authors present results of comparisons between an array of ancillary data and corresponding observed values collected directly from the responding households. Preliminary results suggest concordance for dichotomous variables range 96% to 15%, multi-level variables 84% to 0%.  ","A compelling case can be made that, for some individuals, substance use disorders may be akin to chronic conditions in which effective treatments may need to help patients when they are actually engaged in treatment (concurrent treatment effect). We develop methods to estimate concurrent effects from longitudinal data building on Marginal Structural Models (MSMs) with Inverse Probability of Treatment Weighting (IPTW). MSMs with IPTW provide a basis for posing scientific questions concerning the causal effects of time-varying treatments. But there is little literature on how best to estimate the IPTW weights and build models of the treatment assignment process necessary for estimating weights.  We discuss the estimation of concurrent treatment effect from a sample of over 2000 youth and describe methods for modeling treatment assignments and assessing the quality of the resulting weights. ","Large treatment effects are difficult to find when evaluating the effects of substance abuse treatment modalities among adolescent clients. It is hypothesized that client heterogeneity could explain this in so far as the effects of treatment may be greatest for a subgroup of patients. Most analyses examining moderation effects of treatment focus on moderation by time-invariant characteristics of patients measured at baseline. However, most clients in substance abuse treatment experience multiple treatment episodes over time, and time-varying moderators of treatment effects (e.g., severity of substance use problems after different treatment episodes) may exist. We employ the structural nested mean model to estimate the causal effect of additional treatment on longitudinal substance use outcomes, and to identify possible time-varying moderators of these effects. ","Group therapy for alcohol and other drug use disorders is often delivered under a rolling admissions policy, where new clients are continuously enrolled into the group as space permits. Despite the ubiquity of rolling admissions in practice, little guidance on the analysis of such data is available. We discuss the limitations of previously proposed approaches and improve upon them by fully modeling the interrelatedness of client outcomes using a hierarchical Bayesian model that assumes a conditionally autoregressive prior for session-level random effects. We demonstrate improved performance using our method with respect to coverage probabilities for parameter estimates and the enhanced ability to learn about the complex dynamics of rolling therapy groups. Our approach broadly applies to any group therapy setting where groups have changing client composition. ","Research on the sequential development of alcohol use can be challenging in part because the stage of drinking behavior at a certain time point may not be directly observable. In this context, a latent class analysis (LCA) can provide a set of principles for systematic identification of homogeneous subgroups. We propose an LCA approach, referred to as a latent class-profile analysis (LCPA), for investigating the stage-sequential patterns of drinking behavior. The LCPA characterizes different drinking patterns in terms of a small number of classes based on responses to items at each measurement occasion; and then it examines class sequencing over the entire time points so as to identify two or more homogeneous subgroups. Inferences about the model parameters are obtained by a combination of ML and Bayesian techniques, and their properties are investigated through a simulation study. ","Access and use of health care services are critical areas of interest for researchers and policymakers. Surveys used to analyze these issues have varying objectives and methodologies and can produce widely divergent estimates. Therefore analysts need to understand the data sources(s) being used and not presume that similar estimates or conclusions would result from alternative sources. The purpose of this paper is to illustrate the types of complexities/differences that arise when comparing estimates of ambulatory health care use from different sources. In particular, we compare 2007 data on health care use collected from several Federal sources (the Medical Expenditure Panel Survey, the National Health Interview Survey, the National Ambulatory Medial Care Survey, and the National Hospital Ambulatory Medical Care Survey).  ","The National Survey on Drug Use and Health (NSDUH), an annual survey of the civilian, noninstitutionalized population of the United States aged 12 years old or older, produces national and state-level estimates of the prevalence of use of illicit drugs, alcohol, and tobacco products as well as measures related to mental health. The survey also collects data on past year overnight hospitalization and emergency department (ED) visits for any reason and specifically for the treatment of substance use or mental health conditions. This paper compares NSDUH hospitalization and ED estimates with estimates of the same measures from the NHIS, MEPS, NHANES, BRFSS, DAWN, NHDS, NIS and NEDS and examines possible reasons for similarities and differences. Results from this comparison are expected to inform changes to the NSDUH questionnaire as the survey undergoes a redesign. ","Different methodologies exist for the collection of emergency department (ED) data, and these differences can produce varying estimates of ED utilization. For example, survey data are affected by factors like the sampling frame used, response rate, and question format, while administrative data obtained from patient encounter bills are subject to variations in billing practices. In this analysis we compare estimates of ED utilization in 2007 from the American Hospital Association (AHA) Annual Survey of Hospitals and the Healthcare Cost and Utilization Project (HCUP), which is administrative data based on patient encounter bills. For the 27 States included in the analysis, we find fairly broad concordance between estimates at the total and State levels and identify several hospital characteristics associated with differences at the institutional level. ","The National Hospital Ambulatory Medical Care Survey (NHAMCS) is designed to produce national-level estimates, but sample sizes are inadequate for direct county- and state-level estimation. To expand upon estimation capabilities, data from a sample of emergency departments (ED) from the NHAMCS were combined with universe county and hospital-level covariates to create small-area prediction models for estimating county- and state-level attributes of emergency department visits (rates of ambulance arrival and visits with asthma or injury). Effects of data clustering at the hospital, county or state levels were modeled by introducing random effects into a generalized linear model. Point estimates were calculated and compared when random effects were applied at different levels. A bootstrap approach to estimating mean squared errors was illustrated as well. ","The Medical Expenditure Panel Survey (MEPS) has detailed information on prescription drug use and expenditures for a nationally representative sample of the U.S. civilian, noninstitutionalized population. The data are collected from both household respondents and pharmacies. We evaluate drug use reported by household respondents and expenditures reported by pharmacies using a linked sample of 1,815 Medicare beneficiaries in MEPS and their Medicare administrative data. Our analyses assess the concordance between (1) the number of drugs and prescription drug fills reported by the household in MEPS versus the numbers in the administrative data, and (2) the out-of-pocket and Medicare expenditures reported by pharmacies in MEPS versus expenditures in Medicare claims. ","There are at least two reasons to calibrate survey weights: force estimators to be unbiased under a prediction model and adjust for the bias caused by unit nonresponse. Although a prediction-model justification is possible, Lundstr\u00f6rm and S\u00e4rndal (1999) argued that a unit's weight adjustment under calibration estimates the inverse of the unit's response probability. The functional form of the response model in their linear calibration adjustment is awkward and unlikely. We describe a nonlinear calibration procedure available in SUDAAN that includes a logistic response model, generalized raking, and bounds the weight adjustments limiting their inflationary impact on mean squared errors. Using this procedure provides double protection against nonresponse bias. If the linear prediction model or implied unit response model holds, the resulting estimator is asymptotically unbiased.  ","Statistical inference with missing data requires assumptions about the population or about the response probability. Doubly robust (DR) estimators use both relationships to estimate the parameters of interest, so that they are consistent even when one of the models is misspecified. In this paper, we propose a method of computing propensity scores  ","In its purest form, multiple imputation is a technique that compensates for item nonresponse using prediction modeling. Although developed in a Bayesian framework, its advocates claim the technique has good \"frequentist\" properties. With weighted survey data, however, this is generally true only when the item missingness is completely at random.  We present a way to conduct a weighted multiple imputation under which resulting estimates are doubly protected from nonresponse bias; that is to say, if either the assumed prediction model or the response (propensity) model is correct, the resulting estimator is nearly unbiased in some sense.  Unfortunately, the multiple-imputation-variance estimator will itself be nearly unbiased only when both models hold. Unlike multiple imputation, available imputation and variance-estimation techniques requiring only one of the two models to be true genera ","The notion of combining evidence from different studies has a long history, going back at least two centuries. This talk will provide some historical background to these approaches and describe the particular challenges faced with observational studies. The example of biomechanical exposures and low back pain (LBP) will illustrate the challenges, and methods to resolve the difficulties will be shown. The substantive example is an analysis of individual participant data (IPD) from 48 observational studies on this topic. Among the concerns of applying meta-analytic techniques to observational studies are difficulties of comparing exposures, outcomes, measures of effect, and adjustments for possible confounders across studies. Future directions will also be discussed, such as the application of these methods to harmonizing measures from prospective cohort studies. ","Non-randomized studies (NRS) do not use randomization for allocating to comparison groups. This includes studies where allocation occurs in the course of usual treatment decisions or peoples' choices. NRS include cohort, case-control, controlled before-and-after, interrupted-time-series studies and controlled trials with inappropriate randomization strategies.  The Meta-analysis Of Observational Studies in Epidemiology (MOOSE) and QUality Of Reporting Of Meta-analyses (QUOROM) guidelines provide a foundation for integrating data from observational studies.  For NRS reviews, the following will be considered: inclusion criteria, searching, selecting and collecting data, assessing quality and synthesis of data. Among the items to consider when integrating analysis from NRS are methods of handling missing data. Potential directions for handling missing data will be discussed. ","This research presents a framework for the meta-analysis of complex survey data using the sample design and a superpopulation model to quantitatively summarize different surveys that represent the same underlying population. Additionally, it will compare the classical design based methods using a superpopulation model approach.      ","During the spring through fall of 2009, the ICSP, comprised of the heads of 14 statistical agencies and chaired by the U.S. Chief Statistician, began an informal strategic planning process. This session will describe a little about that process, but give primary emphasis to the outcome of those discussions, i.e., the ICSP's vision of future directions for the Federal Statistical System. The vision emerging from that process involves building on the tradition of collaboration to achieve new levels of efficiency, data quality, and utility for data users through innovative efforts such as a system-wide approach to statistical uses of administrative records and more common data dissemination strategies and tools. Members of the ICSP will discuss these activities and how they hope to implement them in the coming years.  ","Cohen's kappa is the most widely used measure of agreement for categorical outcomes. We consider the use of kappa statistics as outcome measures in a group randomized trial (GRT). In GRTs, identifiable groups rather than individuals are randomized to study conditions, and observations within a group tend to be correlated. In addition, we consider a longitudinal GRT design with multiple rating pairs on the same subject within cluster over time. This work is motivated by the Detroit Middle School Asthma Project, an extended nested cohort GRT assessing two in-school interventions to enhance management of asthma. In this study, parents and children provide dichotomous reports of the child's experiences with asthma in school, and it is of interest to evaluate whether the intervention increases agreement of parent and child. ","Multiple imputation is frequently used to handle the missing data. However, there are currently no guidelines for the extension of commonly used variable selection methods for complete data to the setting of multiply imputed data. We proposed two variable selection methods for multiply-imputed data. The first method is a modification of the traditional stepwise variable selection method, implemented by obtaining combined p-values using Rubin's multiple imputation combining rule first and then selecting variables base on combined p-values in each step of selection. The second method is based on the regularized joint-likelihood estimation approach. The coefficients of the same variable across all imputed data are treated as a group, and the group lasso penalty is applied for the purpose of variable selection. We demonstrated our methods using simulation studies and a real example. ","The Aging, Demographics and Memory Study (ADAMS) was based on a stratified, probability subsample of 856 persons age 70 and older selected from the full panel of the Health and Retirement Study (HRS).  In addition to replicating the full battery of survey-based health and cognitive test measures employed in the biennial HRS, the ADAMS data collection involved detailed in-home psychometric testing, neuropsychological evaluations, and physical measures that were used by a panel of medical experts to assign disease classification (dementia, Alzheimer's Disease) to each consenting subject.  This paper investigates model-based methods for imputing dementia status to all HRS panel members age 70+ based on the ADAMS observations of the relationship of  between assigned disease classifications and the putative risk factors observed in both ADAMS and the full HRS panel interviews. ","In estimation of finite population quantities from survey samples, design-based methods focus on removing the bias while model-based methods seek to improve prediction of unobserved data.     ","The ESS is a biennial face-to-face survey of attitudes, opinions and beliefs in around 30 European countries. The target response rate is 70%, but in practice response rates are often lower and vary across countries. This paper presents an overview of the paradata that are collected in the ESS and of the purposes for which they are used. It also describes the different auxiliary variables that have been used to assess and adjust for nonresponse bias: population statistics, interviewer observations, data from doorstep questionnaires and follow-up studies, and paradata derived from the 'contact forms'. It also devotes attention to the balance between high national quality and optimal comparability across countries. ","In 2004, the U.S. Census Bureau introduced an automated instrument to  collect contact history paradata in personal-visit surveys.  Among  others, survey methodologists analyze these data to improve contact  strategies, predict survey nonresponse and evaluate nonresponse bias.  But while the paradata literature is growing, a critical question  remains - how accurate are the paradata themselves? We address this  question by analyzing contact history data collected by the same  instrument across three Federal surveys. We compare indicators of data  quality to assess level of consistency both across and within the  surveys. We also assess the degree of agreement between automated  contact history data (e.g., time/date stamps) and information entered  directly by the interviewer, such as attempt day and time, notes, and  assessments of respondent cooperation.   ","This paper discusses and evaluates methods proposed for nonresponse adjustments including paradata, for the Survey of Labour and Income Dynamics (SLID).  The proposed adjustments include variables such as  the number of calls to a household and time of call.  Methods of evaluation of the nonresponse adjustment include correlation of key auxiliary variables to SLID variables of interest, correlation of key auxiliary variables to response, evaluation of the stability of the weights, examination of pseudo-relative-bias as an indication of potential bias, impact on key estimates and standard errors, and validation of the proposed nonresponse models.  Results show both pros and cons for the use for paradata in the nonresponse adjustment for SLID, such as a trade-off of reduction in pseudo-relative bias for an increase in variance. ","The purpose of this paper is to identify factors affecting nonresponse of 12th graders in the National Assessment of Educational Progress, by using social isolation as a theoretical navigator.  We also evaluate the statistical impact of nonresponse bias on NAEP estimates by taking advantage of response propensity models built on a social isolation framework.  We use the 2000 NAEP science survey data and its contact history paradata, both of which are linked to the school administrative data from over 20,000 seniors in the 2000 High School Transcript Study whose sampling frame is identical to NAEP. We apply the final robust response propensity model to reweight NAEP estimates with additional covariates extracted from the HSTS administrative data.  We introduce the concept of \"pandata,\" the muiply-linked data including paradata and administrative data to improve nonresponse adjustment. ","Use of ABS frame for listing addresses for area sampling (AS) without making field visits is much cheaper, but it cannot be directly applied because some addresses, such as P.O. boxes and rural routes, cannot be linked to housing units (HU).  In contrast, cost savings in ABS with mail or telephone interviews (MI/TI) instead of field interviews (FI) in AS are offset by the problems of linking telephone numbers instead of addresses to HU.  We propose a new address-based area sampling (ABAS) design whereby a field prompter (FP) is dispatched to verify and update a selected list of ABS addresses, and then drop off short questionnaires to set up a longer follow-up by TI.  Ineligible addresses are substituted by rejection sampling techniques developed by Singh and Wolter (2010).  We show that ABAS with FP/TI may serve as a good compromise between ABS with MI/TI and AS with FI. ","In 2009, RTI conducted a field study for the National Survey on Drug Use and Health (NSDUH) aimed at investigating the cost implications and coverage properties of a sampling frame based on address-based sampling (ABS) in area-segments with adequate ABS coverage and field enumeration (FE) elsewhere. Data for this study were based on a probability sample of 200 segments and 3,878 eligible dwelling units from the NSDUH. We found that accurately predicting ABS coverage at the area-segment level prior to sample selection simultaneously lowers costs and improves frame coverage. ","This paper builds upon a previous evaluation of the United States Postal Service (USPS)-based address frames, conducted in the Vanguard Study locations of the National Children's Study. The prior research compared an address frame created by traditional listing (without additional missed units added during data collection) with a USPS-based address frame. We continue this research by reviewing addresses added as a result of a procedure to identify units missed in the original listing against the USPS-based lists. Using the earlier matching results, addresses added through the missed unit procedure, and household screening data for all addresses obtained by field listing, we examine and compare the household eligibility status for originally listed addresses and addresses added by the missed unit procedure, according to whether or not the address is on the USPS list. ","Research has shown the promise of using of the USPS Delivery Sequence file (DSF) as a replacement for traditional listing. The inclusion of non-city style addresses means that the DSF cannot be used as a sampling frame of housing units in rural areas. The point at which the DSF becomes insufficient for in-person surveys is unclear. The Residential Energy Consumption Survey (RECS) for the Energy Information Administration (EIA) used a hybrid approach to update the 2009 frame. Segments listed in 2005 were evaluated as to whether the DSF provided sufficient coverage by comparing counts to a number of controls. Listers updated the 2005 frames in the low-DSF segments during the fall of 2009. Results compare our a-priori assumptions with what was realized in the field. This research compliments existing work by exploring how to measure the point at which traditional listing is beneficial. ","The time and resources associated with traditional listing have led survey practitioners to consider the use of United States Postal Service (USPS) based address lists as an alternative. In our recent investigation, we found coverage of the USPS-based address lists to be generally good in urban areas, but possibly inadequate (after geocoding) in rural and high-growth areas. We develop a \"match rate\" (the proportion of traditionally listed addresses that would have been obtained from a USPS-based list) model that identifies areas where USPS-based lists could be used in place of traditional listing. We use multiple regression to predict match rate with respect to characteristics associated with the geographic areas of interest. In this paper, we also discuss identifying a match rate threshold that is used to make an a priori decision of when it is acceptable to rely on USPS-based lists. ","In-person surveys that use address-based sampling are often based on area segments defined by census geography rather than postal geography. Census geography enables more accurate inclusion of demographic information in the sample selection procedures and the use of frame supplementation methods to increase coverage. However, area frames based on census geography contain more frame error than frames based on postal geography because addresses must be allocated (i.e. geocoded) into area segments. When addresses are incorrectly geocoded into area segments, sampling inefficiencies occur. We examine data from the 2009 National Survey on Drug Use and Health to determine the extent of geocoding error in sampled segments and its implications on coverage and efficiency of area frame samples.  ","A common concern of survey researchers is whether the coverage properties of an address-based sampling (ABS) frame create outcome bias in the estimates from in-person surveys. This paper evaluates basic demographics and several drug use and mental health measures obtained from 1,725 respondents in a probability sample of 200 area segments from the National Survey on Drug Use and Health (NSDUH). The evaluation compares outcomes from respondents covered by the NSDUH's field enumerated (FE) frame to those covered by an ABS frame derived from the United States Postal Service Computerized Delivery Sequence (USPS CDS) file. After post-stratifying the weights to populations with known ABS undercoverage, we test for significant differences in outcomes between the two frames. ","In some surveys there is a requirement to have a certain minimum number of completes to meet the specified margin of error of the estimates. Therefore, it is necessary to replace the nonresponding units in the original sample. If the original sample is selected with probability proportional to size (PPS), the selection of replacement samples is not straightforward. If the replacement sample is also selected with PPS, then it is difficult to determine the overall probabilities of inclusion for the selected units. Also,the original requirement of keeping the probabilities proportional to size may no longer be possible. Determination of appropriate sampling weights is also problematic.  We look at two methods of selection of the replacement sample and examine the overall probabilities of selection and adjustment for nonresponse. ","Benchmarking is used often in establishment surveys to adjust sample weights to match the current distribution of population of interest.  In the National Compensation Survey, the weight of each establishment in the sample is adjusted to match the distribution of current employment by industry from the Quarterly Census of Employment and Wages program.  The process involves calculating a benchmark factor for each cell and multiplying the establishment weight by the calculated factor.  In cases where there are fewer than three responding sample establishments or the factor is larger than 4.00, two or more cells are collapsed.  The question is which cells should be collapsed so that the effect on the mean square error is minimized.  This paper presents the current collapse pattern and several other collapse patterns and evaluates their impact on mean square error of earnings estimates. ","Conducted by the National Center for Education Statistics, the National Assessment of Educational Progress (NAEP) is a periodic assessment of student academic achievement which produces estimates at both the national and state level. Subsamples of the selected students are assigned to subjects such as mathematics and reading, and weighted totals of each subsample estimate the size of the student population. Previous standard weighting procedures for NAEP resulted in minor discrepancies in distributions of weight totals across demographic subgroups, between subjects. Raking (Iterative Proportional Fitting) was implemented in 2009 to eliminate the discrepancies in the demographic distributions. Subject specific weights were raked to sample-based population estimates from the entire sample. An evaluation was conducted to investigate the impact on the assessment means and standard errors. ","National school surveys typically adopt post-stratification at the final stage of weighting.  As post-stratum cells are often defined in terms of school type (public versus non-public), grade, gender and race/ethnicity, some cells end up very small. Trimming is usually performed earlier in the weighting process prior to post-stratification; with trimming cells defined in terms of stratification rather than post-stratification variables, trimming is less effective because weights are not highly variable within these cells.  In the investigation reported here, we developed and tested an iterative procedure for the trimming and raking based on the same cells defined in terms of the factors above.  We applied the procedure to data from two national surveys. A comparison of the results shows gains in DEFFs obtained from the new alternate weighting over the established weighting procedures. ","A  household  Internet panel is containing  general information  of around 12000  households. From this quite exhaustive panel a representative sample is drown, for a survey on Work and schooling.  These data are merged to the demographic panel. Different weighting schemes are applied and compared. We use a generalization of maximum entropy weighting (MAXENT) to reweight a sample to match observed population moments.  We extend MAXENT to allow for solutions when the data are not full rank and, more importantly, to find the best solution possible when the population moments cannot be perfectly matched or when a perfect match would involve a large degree of extrapolation.  Methods to evaluate the degree to which the solution involves extrapolation (i.e., sharply unequal weights) are proposed.  Variance estimates are derived for when the weights are used to estimate regression coefficients. ","The National Health Interview Survey (NHIS) is a population-based survey which has collected health information from the U.S. civilian noninstitutionalized population since 1957. Major goals of the NHIS include the production of quality data as well as precise and reliable estimates of health conditions. This paper summarizes the results of alternative weighting techniques for NHIS data. The first approach applied different raking methods by age-sex-race-ethnicity, individually by education and income, as well as jointly by education and income with basic and full imputation. The second approach applied logistic regression to NHIS paradata using ten variables to predict the probability of response; the third approach applied recursive partitioning to the binary response variables. Raking was then applied to these nonresponse adjusted weights. ","Qualitative research methods, such as cognitive interviewing and focus groups, have become commonplace in survey research.  These methods are typically used to aid development of data collection instruments and contact strategies.  Findings using these methods may alert statisticians to potential nonsampling errors and may alert users to potential data discrepancies.  Thus, it is important that qualitative research findings be communicated effectively to statistical audiences. The purpose of this Round Table is to engage qualitative researchers, sampling statisticians, and statistical data users in a constructive dialogue focusing on effective communication of qualitative research findings as they relate to survey research.  Participants will take away an improved understanding of the underlying assumptions, interpretation, and report generation of qualitative research findings. ","In 2009, the Sexual Minority Research Assessment Team (SMART), working under a grant from the Ford Foundation, completed their report, \"Best Practices  For Asking Questions on Sexual Orientation on Surveys.\" This report seeks to increase the quantity and quality of data collected on gay, lesbian, and bisexual people, and, by extension, on heterosexual people. Over a five-year period, a multidisciplinary expert panel pooled decades of knowledge and experience, conducted new methodological research, and met with many survey specialists to identify the best scientific approaches to gathering data on sexual orientation. This document is the culmination of the work of this expert panel.  Dr. Elizabeth Saewyc, one of the report's authors, will lead a discussion highlighting the recommendations of this ground-breaking report. ","The Centers for Disease Control and Prevention and the U.S. Census Bureau have partnered to conduct an evaluation study of the National Immunization Survey (NIS). The NIS target population, households with children age 19-35 months, can be hard to identify. The NIS Evaluation Study approaches this challenge by using the American Community Survey (ACS) as the primary sampling frame, while drawing additional sample from an information reseller file in one locality where insufficient sample is available from the ACS. This paper discusses the procedures and challenges involved in creation of the sample universe, selection of the initial sample, and weighting of the results.  It also discusses prospective sample designs to maximize use of the somewhat ephemeral available sample of ACS-identified households with 19-35 month old children if the NIS becomes a continuing survey using the ACS. ","The National 2009 H1N1 Flu Survey, a RDD landline and cellular telephone survey that operated from October 2009 through June 2010 by NORC for the Centers for Disease Control and Prevention, tracked H1N1 (and seasonal influenza) vaccination coverage nationally on a weekly basis. National-level direct estimates for various socio-demographic groups and geographies were produced weekly based on a rolling sample of respondents. To reduce variability of estimates and trends, we estimated H1N1 vaccination rates using a survival analysis approach. In this paper, we consider  parametric and non-parametric models with date of vaccination either interval censored or imputed, and we evaluate our models to select the \"best\" model that fits the data. ","External causes of injury include pedestrians injured by motor vehicles, other traffic accidents, or accidental falls. The National Inpatient Sample (NIS) is utilized to ascertain injury patterns in the general population of people injured by a specific external cause. Five to eight million inpatient hospitalization records are included in the NIS annually, and this large sample size and survey design allows for statistical analyses that are not possible with smaller data sets. This talk will concentrate on how the patient population was identified through the use of external causes of injury ICD-9-CM codes. Discussion will not only cover examples of  varying external causes and resulting injury patterns, but also demonstrating how different factors, such as physical impairment or intoxication affect the prevalence of pedestrian accidents or injury outcomes.   ","The adverse effect of ambient air pollutants on health in the United States is a growing concern, and many studies have been done to collect data in order to explore this problem with mortality as a health endpoint.  However, results are varied between studies:  some analyses find a significant relationship between mortality and pollution levels, while others find no such relationship.  I will present a meta-analysis of current studies in an effort to reconcile the question of whether there does exist an association between mortality and pollution levels. ","IT MAY SEEM HARD TO BELIEVE BUT A DECADE HAS PASSED SINCE THE DISPUTED PRESIDENTIAL ELECTION OF 2000. MUCH OF THE COMMENTARY SINCE THEN HAS FOCUSED ON THE LEGAL NUANCES OF THE 5-4 DECISION BY THE US SUPREME COURT. THIS PAPER CONCENTRATES INSTEAD ON A REASSESSMENT OF THE STATISTICAL ISSUES RAISED IN THE CONTROVERSIAL COUNT AND RECOUNT IN FLORIDA- THIS WAS THE DECISIVE STATE AND IT WAS A WINNER TAKES ALL CONTEST. THE PRESENTATION WILL TAKE THE ATTENDEES BACK IN TIME TO THE 36 DAYS OF THE DISPUTED VERDICT IN LATE NOVEMBER AND EARLY DECEMBER 2000 AND ASSESS WHETHER A BETTER USE OF STATISTICAL TECHNIQUES COULD HAVE AVOIDED THE DRAWN-OUT RECOUNT BATTLE. THE PAPER WILL EXAMINE THE WAY THAT STATISTICAL INFORMATION WAS USED AND MISUSED BY THE COURTS AND MEDIA AND TRY TO PICTURE HOW WHAT WERE IN ESSENCE NUMERICAL ISSUES BECAME POLITICAL FOOTBALLS AND ENDED UP AT THE DOOR OF THE SUPREME COURT ","A survey is conducted in order to determine the demographic structure and job satisfaction of disabled people in working environment. The population is defined as disabled people who are working for the companies which are obliged to employ disabled people in Eskisehir, Turkey. It is thought that almost 1,000 disabled employees are working in private and public sector. 421 of those are reached and asked to complete a questionnaire. In this paper the demographic structure of disabled employees is given. Besides, logistic regression models are constructed; binary logistic regression model in order to find out whether or not they are meeting any handicaps at their working lives, ordinal logistic regression model for job satisfaction of disabled employees with their works and nominal logistic regression model for the task types of disabled people at work. The models are given and interpreted ","The U.S. Census Bureau's Small Area Income and Poverty Estimates (SAIPE) program produces model-based estimates for small geographic areas using survey data, administrative records, postcensal population estimates and decennial census data.  This paper proposes and evaluates a method for making year-to-year statistical comparisons of poverty at the county level.  The method uses aggregations of regression residuals in order to estimate the underlying serial correlation in SAIPE county-level estimates.  Three residual-based estimators for the model error correlation are considered, with alternative weights used for each.  The estimators are evaluated using simulations under the assumed error specification, and the effect of a heteroscedastic departure from these assumptions is evaluated.  Finally, empirical results are reported under two alternative error component specifications. ","The 2008 NSSRN is the latest in a series of national surveys of RNs carried out roughly every four years since the late 1970s. For the 2008 NSSRN a new sample design and weighting process were used. RNs were sampled independently from state listings of currently licensed RNs. Many RNs were licensed in several states, resulting in multiple chances of sample selection. Probabilistic matching and questionnaire responses were used to identify RNs appearing on multiple listings. Compositing factors based on the number of strata containing an RN were applied to nonresponse adjusted weights assigned to the sampled records of responding RNs. The nonresponse adjustment process was enhanced compared to prior surveys. This paper discusses the theory behind the estimation approach used, the processes of implementing the approach, and the corresponding impact on survey precision and accuracy. ","The Service Sector Statistics Division of the U.S. Census Bureau conducts seven surveys on an annual, quarterly, or monthly basis.  These surveys measure the nation's economic activity of the retail, wholesale, and service sectors.  Three of these surveys are principal economic indicators. Two of these surveys contribute to a fourth economic indicator.  To produce quality data about these industries in a reliable, timely and relevant manner, we must use a reliable sampling frame and have in place quality survey control procedures.   This paper will provide an overview of the surveys, describe in detail the initial frame construction and sample selection for these surveys, discuss how the frame and samples are maintained and periodically updated, and discuss the quinquennial sample revision activities for these surveys, following each Economic Census. ","One cohort of the German National Educational Panel Study consists of a sample of kindergarten children. No nationwide frame of kindergartens is available in Germany, contrary to the situation for primary schools. Following the works of Lavall\u00e9e, we present a solution by indirect sampling, using links between kindergartens and primary schools. In the first stage, primary schools are selected and all kindergartens are identified that have a link to the sampled schools. In the second stage, for every sampled school we take a random sample of kindergartens linked to this school. In the third stage, samples of children within the sampled kindergartens are selected. Using the links of the sampled kindergartens to all schools in the population, unbiased estimation for the population of children in kindergartens is possible. Our second stage sampling is in addition to the existing literature. ","A major redesign of the Survey of Household Spending, conducted annually by Statistics Canada, has been undertaken over the last few years. A new data collection model, combining the use of a recall interview and a diary, was developed. The length of the recall period for certain types of expenditures was reduced and data collection was spread through the year. The content of the previous Food Expenditure Survey was also integrated. A pilot survey was conducted to evaluate this new model. This paper presents the results of the analysis on pilot data collection and data quality, including the impact of nonresponse, and the results of evaluation of the new collection methods developed for this model.  ","The National Compensation Survey is conducted by the Bureau of Labor Statistics to measure employment cost levels and trends, incidence of employer-provided benefits, benefit plan provisions, occupational earnings by geographic area, and occupational pay comparisons between areas.  The current survey design uses a three stage sample design to select samples of areas, establishments, and jobs for which wage and benefit data are collected periodically over a five-year rotation.  In recent years, several potential changes to this design have been explored due to budget cuts, known issues with the current design, and an on-going effort to make the survey more efficient.  This paper will discuss the issues and alternative approaches to the current design that have been explored and presents some recommended changes to the general survey design. ","The Canadian Community Health Survey on Healthy Aging (CCHS-HA) was conducted by Statistics Canada between December 2008 and November 2009. The survey focused on the health of Canadians aged 45 and older by examining the various factors that impact healthy aging. A nationally representative sample of 30,865 respondents completed personal interviews which included an interactive component on cognition measures. The sample was selected using a multi-stage design that used the 2006 Census of Population as a frame in order to target the population of older Canadians. A number of sampling and estimation issues are presented in this paper including the use of the census as a frame, clustering, hard-to-reach populations and cognitive measures during interviewing. The effectiveness of the design is also examined using the results from the survey. ","Typically in designing samples of adults or households, an intermediate stage is the selection of areas such as blocks or groups of blocks. Census blocks are separated by a variety of features, including both visible (e.g., roads) and invisible (e.g., municipal) boundaries.  Invisible boundaries cause difficulties for field staff, potentially resulting in listing and interviewing error. When the issue is addressed after selection, the challenge is to ensure that the selection probability of each population element is determinable, including the probabilities for every sub-area (block or block-part) in the population.  Thus the boundary-adjusting method must lead to a consistent result regardless of which sub areas were selected.  We present an approach that greatly ameliorates this problem and show its application to the issue of segment boundaries in the National Children's Study (NCS). ","Longitudinal surveys are primarily interested in measures of change rather than measures of status. Yet many aspects of longitudinal survey design, including questioning techniques, are often optimised for measuring status. One effect of dependent interviewing can be to improve the accuracy of measures of change. Other questioning techniques may also help achieve this aim. This presentation reviews recent research into the measurement of change and attempts to identify the parameters that determine the best questioning technique for a particular situation. The suggestion is that the choice of technique should depend on factors such as the true rate of change, the interval between waves, and the task difficulty associated with understanding recall and reporting. Further needed research will be outlined. ","Panel conditioning is one of important sources of measurement error unique to panel surveys. It refers to changes to either respondents' true value or their report of the true value caused by being interviewed multiple times. Earlier studies on expenditures found that people interviewed for a second time reported significantly less expenditures than those interviewed for the first time. This paper examines panel conditioning effects in the Consumer Expenditure Interview Survey and found no evidence of panel conditioning in the total amount of expenditure reported and the different number of expenditure types reported. In addition, reluctant respondents and respondents with higher reporting burden did not seem to be more prone to panel conditioning effects than cooperative respondents or those with lower reporting burden.  ","The purpose of this study was to investigate the relationship between nonresponse and survey data quality in a longitudinal panel survey. Data from early waves of the Current Population Survey (CPS) were used to develop multivariate regression models predicting respondents' probability of response in later CPS waves, and the relationship between response propensity and indicators of CPS data quality was examined.  Results revealed that data quality decreased as the probability of nonresponse increased, with the strength of this relationship varying by data quality measure.  Statistically controlling for potential common cause variables did not weaken the relationship.  Implications of these findings for survey practitioners and for nonresponse and measurement error studies are discussed.  ","Survey often rely on respondents' reports based on their autobiographical memory. Therefore, survey data may suffer from measurement error due to memory failure that can affect models based on these data. The German panel survey PASS collects information on work histories using repeated interviews and retrospective reporting. The PASS data have been linked to administrative data on work histories from the German employment agency. The paper focuses on recall error with respect to recipiency of unemployment benefit/insurance payments. After describing distributions of different kinds of errors found in the data we focus on (1) seam effects and their causes, (2) the effect of length of the recall period on recall error, and (3) if likelihood of a recall error can be predicted by individual attributes and which effect it has on coefficients of time-to-event models of recipiency dynamics. ","The seam effect is an artifact of panel surveys in which respondents are interviewed every few months and asked for data about each of the intervening months. We used an experimental method to examine the seam effect for quantitative questions about dollar amounts. Our studies produced robust seam effects, accompanied by forgetting of correct amounts and constant responding (no change between months) within the reference periods. The results also showed decreases in the seam effect when questions about the same topic appeared in different parts of the interview. In some conditions, dependent interviewing also reduced the size of the seam effect. Neither manipulation, however, improved the overall accuracy of responses. Dependent feedback may encourage respondents to make less drastic changes at the seam but perpetuate incorrect responses from one reference period to the next.  ","Crossover designs are used often in clinical trials. It is not uncommon that subjects discontinue before completing all treatment periods. Despite availability of statistical methods utilizing all available data, naive approaches, such as the complete case (CC) analysis, which is only valid under missing completely at random, are still widely used in practice. We obtain the analytical form of the estimation bias of treatment effects with CC for 2- and 3-period crossover studies. We use simulation to examine the inflation of type-I error and efficiency loss in the inferences with CC. Invalidity and inefficiency of two other practical approaches for defining analyzed data in the presence of missing data -- data from at least two periods (2P) in multi-period crossover and available cases for a specific comparison of interest -- are also demonstrated through simulation studies. ","Dyadic data is common in social and behavior science. Members of dyads often influence each, leading to interdependence (or correlation) structure in the responses. A common problem of longitudinal dyadic data is the increasing number of dropouts. In this study we use mixed-effect hybrid models (MEHMs) to study the longitudinal dyadic data with informative dropout. We model repeated measures on the same subject by transition models and the correlation due to the dyadic structure by random effects. To model the missing data mechanism, we assume that the distribution of missingness depends on both the past history of the longitudinal process and the current outcome, but not on future observations. We use data from a breast cancer study to illustrate the models.  ","We consider testing of MCAR in regression data with nonresponse. Likelihood ratio tests have been discussed in the context of multivariate data with missing values but these tests require the specification of the joint distribution of all variables (Little, 1988). Subsequently Chen &amp; Little (1999), and Qu &amp; Song (2002) proposed a Wald-type test and a score test for generalized estimating equations with using the same fact that all sub-patterns share the same model parameters under MCAR. For regression analysis of data with nonresponse, we propose a Wald-type test for missing completely at random by comparing two sets of consistent estimators of regression parameters under MCAR instead of dealing with pattern-specific parameter estimates. This method can be applied in testing MCAR for longitudinal data with dropouts.  ","The Area Under the Receive Operating Characteristics (ROC) curve (AUC) is a widely used summary measure in ROC analysis. The estimate of AUC in observational studies of diagnostic tests is often complicated by missing values of biomarkers, which potentially leads to bias. We develop robust statistical methods to estimate the AUC using auxiliary variables. In the case of missing at random (MAR), our estimators is consistent if one correctly specifies, conditional on auxiliary variables, either the model for the probability of a missing biomarker or the model for the biomarker values. In the case of not missing at random (NMAR), we propose a sensitivity analysis to assess the impact of assumptions on missing mechanism. The finite sample behavior of the proposed estimators is evaluated in simulation studies and they are further illustrated using data from a psychiatric study. ","Missing data is not uncommon in clinical trials and is very often unavoidable. The presence of missing data often complicates the analysis of data. Methods have been proposed and developed to handle the analysis of clinical data with missing values; however, few of these methods are actually employed as the primary analysis in a clinical study for submission to the FDA. In this talk, current guidelines and strategies for addressing the missing data problem in various therapeutic areas will be reviewed and the difficulties encountered with using non-traditional analysis methodologies will be discussed. The role of sensitivity analyses and the use of multiple imputation as a sensitivity analysis tool will be discussed. ","Marginal imputation that consists of imputing items separately, generally leads to biased estimators of population coefficients of correlation. To overcome this problem Shao and Wang (2002) proposed a joint random regression imputation method that succeeds in preserving the relationships between two variables. One drawback of the Shao-Wang method is that it introduces an additional amount of variability (called the imputation variance) due to the random selection of residuals. As a result, it could lead to inefficient estimators. Following Chauvet, Deville and Haziza (2009), we propose a balanced joint random regression imputation that preserves the coefficient of correlation between two variables, while virtually eliminating the imputation variance. Results of a simulation study will be presented. ","We consider imputation methods for dealing with wave nonresponse in panel surveys. We have a good advantage in panel surveys unlike cross-sectional surveys that the previous data can be used to compensate for wave non-response. Some methods are presented and compared numerically through a simulation study based on Korean Welfare Panel data. Simulation results show that ratio and row-column imputation methods are much more effective in both predictive and estimation accuracy. Regression, longitudinal regression and carry-over imputation methods perform better in predictive accuracy, but nearest neighborhood, nearest neighbor regression and hot-deck imputation perform better in estimation accuracy. Finally, the mean imputation shows much lower performance in both criteria.    ","Goods and Services Tax (GST) data are used in many sub-annual business surveys at Statistics Canada. A study was done to evaluate existing imputation methods for late GST transactions by comparing imputed revenues to the reported values. In this paper, we will highlight some results obtained, especially the finding that imputing a new value each month only improves the quality for about half of the units. As a follow-up, we explored the possibility of imputing late values once and keep them at subsequent processing. Results showed that this approach reduces revisions to the data without imposing large differences at the macro level. In light of the findings, this strategy was implemented in the GST Processing System in September 2009. We will show the impact of this change using real data. Finally, we will give some recommendations to further improve the quality of imputation. ","The Internal Revenue Service has begun conducting annual studies of individual taxpayer compliance.  For these studies a stratified random sample of taxpayers is selected and audited.  However, not all audits are completed for a variety of reasons (i.e. the taxpayer was not locatable or did not show up for the audit) resulting in non-response.  Earlier work (Masken, JSM 2004) argued for treating non-response in these studies as missing at random, but did not come to a conclusion on the exact method that should be implemented.  Continuing that effort, we first explore various weighting and imputation techniques that we considered and then detail the sequential regression multivariate imputation (SRMI) method that we implemented.  Finally, we expect to demonstrate that the nonresponse bias is well adjusted and the standard errors of the estimates are reduced. ","In the presence of imputed data, Shao &amp; Sitter (1996) introduced a bootstrap method where the non-respondents in the bootstrap sample are reimputed with the same method, which leads to a valid variance estimator if the sampling fraction is small. We argue that this method can be justified by the reverse framework for variance estimation developed by Shao and Steel (1999). When the sampling fraction is small, this framework suggests a second valid bootstrap variance estimator that can be obtained using complete data bootstrap methods (e.g., the method of Rao, Wu and Yue, 1992) and does not require reimputing. Finally, we obtain a third variance estimator, valid even when the sampling fraction is not small, by modifying the weighting to account for non-response in the bootstrap weights approach and resampling the data and the response status independently rather than jointly. ","When designing surveys, survey organizations must consider numerous design features that may have a substantial and differential impact on both data quality and survey costs. They must recognize that surveys are inherently multipurpose and that a potentially long list of constraints (e.g., minimum sample sizes for domains) must be satisfied. A typical approach is to optimize an objective function subject to constraints on costs and quality. However, as the list of constraints lengthens and the cost and quality structures become more complex, finding a solution to this optimization problem (i.e., choosing the appropriate set of design features) while satisfying all of the constraints becomes increasingly challenging. This paper reviews the methods by which survey designers have attempted to satisfy multiple constraints while optimizing some function of data quality and survey costs.  ","This paper presents an alternative to existing procedures used when  sampling from rare populations. It is based on a two-phase sampling  scheme with a corresponding probability-proportional-to-size sampling  design. The sampling design proposed has the frequently used  conditional Poisson sampling design as a special case. The proposed  procedure is applied to a real life survey situation where real  estates with fishing rights in Sweden constitute the target  population. ","Typically, statisticians approach multivariate optimal allocation in one of two ways.  Either they set target CVs for the most important variables and minimize sample size with those targets as constraints, or they minimize a function of the CVs that implies some preference between variables constrained by a fixed total sample size.   This paper presents an alternative to those approaches.  A set of non-dominated trade-off solutions is produced using a multi-objective evolutionary algorithm.  A solution is non-dominated if no other solution exists that is better on one or more objectives and at least as good on all others.  By examining sets of non-dominated solutions, survey designers can see the trade-offs being made between them, rather than having to choose an arbitrary preference function prior to beginning sample allocation.  ","Classrooms are rated on the effectiveness of instructions given to students. A rater rates the effectiveness scores on n instruction items in a classroom. Cost constraint prevents each of available KB raters from rating all JB classes. Instead, K raters are randomly assigned to a block in which each rater rates J classrooms in the block. This design creates B incomplete blocks. This paper finds a design of an experiment that leads to an optimal choice for n, J, K and B to yield consistent ratings of classroom instructions. ","The Alcohol, Drug Abuse, and Mental Health Administration Reorganization Act of 1992 (P.L. 102-321) included an amendment (section 1926) requiring States to enact and enforce laws prohibiting the sale of tobacco products to minors. States must conduct \"annual, random, unannounced inspections\" of tobacco retailers.  Thirteen States conduct cluster samples.  We will use five years of data to investigate cluster trends within States and compare different cluster designs among States.  We will identify the most efficient cluster designs by analyzing the design effect.  We will investigate the most effective distribution patterns by the number of clusters selected, their geographical size, and their dispersion within the State.  Finally, we will characterize clusters that are related to high retail violation rates. ","The Alcohol, Drug Abuse, and Mental Health Administration Reorganization Act of 1992 (P.L. 102-321) included an amendment (section 1926) requiring States to enact and enforce laws prohibiting the sale of tobacco products to minors. States must conduct \"annual, random, unannounced inspections\" of tobacco retailers. Currently, 39 States use stratified random sampling to select tobacco retailers for inspection. We will identify States that may be able to reduce their standard errors through optimal allocation methods. We will also identify the strata within each State that would benefit most from increased enforcement efforts by examining stratum level retailer violation rates and standard errors, measuring the stability of any stratum effects over ten years of State data. Finally, we will identify any stratum characteristics that are related to the retail violation rate across all States. ","In attempting to contact households for a survey, it is necessary to determine the timing of each call. Often, average \"best\" times to call are used in order to determine when to place the first call(s). The timing of subsequent calls is then governed by very general rules. In fact, what we really want to know is the household-specific probability of contact at different times of the day and days of the week. Unfortunately, the amount of data we have varies from household to household. To address this issue, random-intercept models are fit with the household as a grouping factor. The probability of contact is estimated for each of several call \"windows\" and the window with the highest probability of contact is preferred for the next call. The estimates from these models are updated daily in order to guide the decision about timing the next call. The method is evaluated experimentally. ","For the 2010 Census, the U.S. Census Bureau will use Demographic Analysis along with results from a coverage survey to assess coverage. This paper discusses the methodology that will be used to develop the national Demographic Analysis estimates. The Demographic Analysis estimates have been traditionally produced by single year of age and sex for two race categories, Black, and non-Black. These estimates are developed by analyzing and aggregating various types of demographic data. Administrative records on births, deaths, and legal immigration combined with survey-based estimates of international migration are used to estimate the population under 65. Estimates of the population 65 and over are developed from data on enrollment in Medicare. These estimates will provide the first indications of coverage after the 2010 census. ","This paper discusses the estimation plans of evaluating the coverage of the 2010 U.S. Census based on the results of the coverage survey.  The U.S. Census Bureau will continue to estimate the net error of undercount or overcount based on the dual system estimation methodology.  For 2010, the survey evaluation has been given the new objective of estimating the components of census coverage that include erroneous enumerations and omissions.  This paper will provide more details on the estimation methodology being developed to produce these results.      ","For Canada's 2011 Census of Population, Statistics Canada will again use the Reverse Record Check methodology for estimating undercoverage. Estimates are based upon classification of a sample of persons who \"should\" be enumerated as being in-scope enumerated, in-scope missed or out-of scope. Overcoverage will be estimated using the Census Overcoverage Study, first implemented in 2006. This methodology is based upon matching the census database to itself using administrative records to aid in confirming the validity of matching pairs of enumerations.    ","It is accepted practice that an assessment of coverage should be part of the statistical operation of a traditional census. The UK is no exception, and the 2001 Census was the first real attempt to fully integrate the Census and coverage measurement processes, resulting in the development of the One Number Census (ONC) methodology. The ONC estimated the undercount to be about 6 per cent of the UK population, and the census database was adjusted by imputing 3 million individuals. For 2011, the strategy is to build on the 2001 framework, using the wealth of data that resulted from the ONC. This presentation will report on the research undertaken to explore potential improvements, including the design of the coverage survey, matching the coverage survey to the census, the estimation process, measurement of overcount and assessment of residual biases in the dual-system estimator. ","In comparing demographic groups on health outcomes, it is often of interest to examine the role that specified treatments play in determining those differences.  In particular, we may be interested in determining what part of differences in outcomes would remain even were treatment of the two groups equalized, and what part of the differences in outcome are due to specified differences in the way the groups are treated.  It is productive to consider these contrasts as the direct and indirect effects of the demographic group (even if we do not consider group membership as a manipulable variable).  We consider how to formalize the quantities of interest, identifying assumptions, and possible estimation methods.  To estimate these effects, we must control appropriately for confounders of the effect of the treatment.  ","We argue that collider-stratification bias (CSB) is a threat to validity when estimating covariate-adjusted disparities for demographic factors like sex and race. Using directed acyclic graphs (DAGs), we describe the pervasiveness of CSB, a type of induced selection bias, in disparities research. In modeling Black-White disparities, most potential covariates are (1) differentially distributed between Blacks and Whites and (2) differentially associated with unmeasured causal factors in Blacks and Whites.  If adjusted for, the covariates will be colliders and induce CSB. In estimating sex disparities, fewer potential covariates are colliders: adjusting for parental and residential characteristics uncorrelated with a respondent's sex should not necessarily induce bias. Using regression-based modeling to estimate adjusted disparities may be of limited utility in health disparities research. ","We categorize measures of health disparities by the questions addressed and data required.  This exercise clarifies the types of disparities revealed by various measures.  Next, we consider measures of local disparity in county-level stroke cases for the southeastern United States.  The small-area nature of the data prompts our use of spatially smoothed hierarchical models to borrow strength from local and global trends.  The framework allows model-based estimation of both relative and absolute disparity and reveals different patterns for each.   ","While the health status of Americans has improved over time, this improvement has been unevenly distributed. Designing a measure that tracks the resulting disparities remains a challenge. We propose a new measure of health disparities, the Symmetrized Theil Index (STI).The STI is symmetric; relates to the Pearson's chi-square test of independence for binary data, and to the F-test in one-way analysis-of-variance for continuous data; and is both a relative and a quasi-absolute measure of health disparities. We apply the STI to data on dental caries for children and adolescents from the National Health and Nutrition Examination Surveys (1988-1994 and 1999-2004). Although their oral health has generally improved, we find no change in the prevalence or severity of untreated tooth decay in children and adolescents between surveys in both the overall STI and its between-group component.  ","The National Center for Health Statistics recommends using health disparities indices (HDIs). Some authors claim all HDIs are prevalence dependent. Recent work varying prevalence with constant odds ratio (i.e. logit model) showed some HDIs depend on prevalence (Cheng et al. 2008). This investigation studied how sample size, effect size and prevalence affect HDIs conditional on prevalence difference (i.e. probit model). HDIs were estimated for the California Oral Health Needs Assessment 2004, a complex survey, to assess associations with untreated caries. Absolute measure, slope index of inequality, relative index of inequality(mean), and absolute concentration index were prevalence invariant; relative measures depended on prevalence. We also review the recent NCI/SEER HD*Calc freeware. Support: USDHHS NIH/NIDCR R03DE018116. ","Sequential probability minimum replacement sample designs provide a practical methodology for selecting PPS samples that satisfy the requirement of positive pairwise probabilities and nonnegative variance weights. The exact solutions for the variance weights can lead to some unacceptable variance estimates such as zero estimates regardless of the observed values. This paper explores some alternative approximate variance estimators that avoid this problem.  Although not strictly unbiased, the variance estimates from alternate estimators can be shown to be nearly unbiased and to have less variability than the unbiased variance estimators based on the exact variance weights.  Some comparisons to PPS systematic designs are also addressed with alternate variance weights. ","A relatively new aspect to small area estimation with area level models involves modeling direct survey variances for small areas to improve them. Here, as one aspect of this, we consider the distribution of some survey variance estimators- linearization,Fay's successive difference replication variance estimator, the jackknife, and the random group. We use simulations to examine whether the variance estimators might be assumed to approximately follow a scaled chi-squared distribution, and if so, with what value of the degrees of freedom? We do this for variances of estimated proportions from simple random samples of various sizes, with data generated from various distributions (Poisson, and Bernoulli),and from American Community Survey data. This builds on previous work where we considered Fay's successive difference replication variance estimator for means from simple random samples ","The Drug Abuse Warning Network (DAWN) was designed as a single stage probability sample of hospitals with a 100 percent second stage sample of the full daily census of all drug-related emergency department (ED) visits.  Recent introduction of sub-sampling of ED visits requires accounting for second stage variance.  We discuss three variance estimation methods:  a Taylor series expansion of the first and the second stages, Taylor series expansion in the first stage and half sample replication in the second stage and Taylor series expansion in the first stage and a multiple replication method in the second stage.  DAWN 2008 data will be used for evaluating these methods.  Recommendations will consider both theoretical and practical issues for implementing these second stage variance estimation methods. ","In some surveys, the number of primary sampling unit (PSU) selected is small but the number of ultimate sampling units is large. The usual consistent variance estimator based solely on the between-PSU variance is not stable because the number of degrees of freedom is small. One alternative is to use a variance estimator that estimates the within-PSU variance by treating the PSUs as strata. It has a larger number of degrees of freedom but underestimates the variance. Combining these two estimators, we can produce a variance estimator that is more stable than the usual consistent variance estimator and less biased than the within-PSU variance estimator. The performance of such a hybrid estimator is demonstrated by simulation using a population similar to the population for an actual survey where this hybrid estimator has been used. ","In the 2005 National Resources Inventory, a longitudinal survey of the land area of the United States, an experiment was conducted to calibrate a new method for measuring developed land against the standard survey protocol. Estimated Generalized Least Squares estimators based on a direct estimator of the covariance matrix proved to be unstable at small sample sizes. A related problem arises in the analysis of longitudinal data or complex survey data. The efficiencies of estimators of the regression coefficients in a linear model constructed with different estimators of the covariance matrix are compared through simulation. ","For the Census 2000 CCM, variance due to missing data imputation was incorporated into the jackknife variance estimates by the re-calculation of the missing data adjustments, such as for missing P-sample match status, for each jackknife replicate.  For each cell the weighted proportion of P-sample matches for resolved cases was used as the imputed probability of a match for unresolved cases in that cell. For the 2010 Census CCM, imputation will be done using logistic regression.  A possibility for variance estimation is re-computing the estimated logistic regression coefficients for each jackknife replicate to account for missing data variance.  This paper documents an empirical study using simulated data to evaluate the bias and variance properties of a jackknife variance estimate that includes variance due to a missing data imputation using logistic regression. ","The Office of Management and Budget directs survey programs to conduct nonresponse bias analysis when response rates fail to meet target values.  The literature focuses largely on nonresponse bias analysis methods for demographic surveys.  Such surveys are generally characterized by multi-stage designs with heterogeneous populations within selected clusters.  In contrast, business surveys are characterized by single-stage designs with highly skewed populations.  This paper examines nonresponse bias analysis methods for business surveys, including response rate analysis, examination of the response mechanism, benchmarking, and the use of frame data, illustrating each method with examples from ongoing economic programs conducted by the U.S. Census Bureau. ","In 1973 the Bureau of Justice Statistics (BJS) introduced the National Crime [Victimization] Survey (NCS and later NCVS), which is fielded by the US Census Bureau.  Data are collected twice a year from a nationally representative sample to obtain information about incidents of crime, victimization, and trends involving victims 12 years of age and older and their households.  Last year, NORC conducted a nonresponse bias study on the NCVS for BJS.  In this presentation, we will discuss the analytical methods that NORC used.  These methods include: Capture Recapture estimates, differential nonresponse by geo-socio-demo groups, comparison of results with Uniform Crime Reports, use of longitudinal data, use of paradata, logistic models and imputation. ","In 2006, the U.S. Office of Management and Budget revised and updated their Standards and Guidelines for Statistical Surveys, which document the professional principles and practices and the level of quality and effort expected in all Federal statistical activities.  The specific area that has received the greatest attention is the guidance to agencies to conduct nonresponse bias analyses if their survey response rates are less than 80 percent.  To help Federal agencies meet the OMB guidance, the Federal Committee on Statistical Methodology sponsored a one-day workshop on \"How to Do Nonresponse Bias Analyses in Household and Establishment Surveys\" that included examples of research done by Federal agencies to examine nonresponse bias.  This presentation will review the OMB guidance on response rates and nonresponse bias analyses and discuss the goals and results of the workshop. ","ARMS is a detailed survey administered every year by the NASS and the ERS. The ARMS survey provides the only comprehensive examination of the economic well-being of farm businesses and households and thus is an essential component of agricultural policy making as well as the information used by agribusinesses. As is true with most large-scale surveys, a considerable amount of data is missing. The current procedure for imputing missing data values involves the use of conditional means calculated from complete surveys for similar farms, where similarity is measured by sales class, geographic location, and commodity specialization. This talk reports on research that investigates the application of novel multivariate multiple imputation methods. Using simulated data as well as applications to the ARMS data, performance of proposed imputation methods are evaluated relative to the current use. ","Each June, the USDA's National Agricultural Statistics Service (NASS) provides an estimate of the number of farms in the United States, based primarily on the June Area Survey (JAS). Every 5 years, another indication (preliminary estimate) of the number of farms is obtained from the Census of Agriculture. In 2007, the indications from the JAS and the census were too far apart to be attributed entirely to chance. Initial reviews indicated that the differences were primarily a consequence of an undercount of small farms, especially those with minority owners. Using information from the 2007 Census and from the 2009 Farm Numbers Research Project (FNRP), misclassification in the JAS was determined to be a source of the undercount. An additional source is the treatment of incomplete responses. Possible revisions to the methodology of the JAS survey are suggested. ","Forecasting the end-of-year crop yield is critical for agricultural decision-making. Historically, a panel of specialists known as the Agricultural Statistics Board meets regularly to set estimates based on expert review of a combination of survey data and administrative/auxiliary information.  To make this process less subjective and more repeatable, a Bayesian hierarchical model (BHM) is developed that produces superior yield forecasts/estimates, while quantifying all sources of uncertainty. The proposed BHM naturally combines information from multiple monthly surveys, including a field measurement survey and two farmer opinion surveys. The dependence between the monthly updated surveys and serial dependence of annual yield are incorporated at different stages in the hierarchical model. The effectiveness of the model is demonstrated through simulation and application to USDA data. ","Statistical organizations often consider using data from administrative records to supplement or replace data from standard sample surveys.  This work with administrative data often can involve costs that are substantial and difficult to measure.      ","This roundtable discussion will aim to spark conversation on state-of-the-art (and available) computing methods for taking model-based approaches to the analysis of complex sample survey data. Topics for discussion include multilevel modeling of panel (longitudinal) survey data, programs for handling survey weights appropriately when fitting multilevel models to survey data, general software options for applied researchers working with survey data, published examples where participants might be willing to share code from the analyses, and how to make recommendations for non-statistical clientele.    ","Arbitron uses a hybrid sampling frame approach in all markets it surveys.  A Random Digit Dialing (RDD) landline sampling frame is used to cover households with one or more landline telephones, and an address frame is used to identify and cover cell phone only (CPO) households.   While including CPO households has helped Arbitron improve its coverage of the population, especially persons 18-34, recent research studies by companies that provide the RDD landline sampling frame have raised questions about the coverage of households with landline telephones.  Arbitron decided to conduct its own study to help assess the coverage of RDD landline sampling frames.  Results of that study will be included in the paper as well as the rationale for a hybrid sampling frame approach. ","The growth of cell-phone only households increases the likelihood of noncoverage bias affecting a variety of health estimates in traditional landline RDD surveys. Although recent data from the National Center for Health Statistics suggest the percent of children in cell-phone only households is on par with that of adults, noncoverage bias studies have focused on adults. In 2009 the California Health Interview Survey (CHIS 2009) expanded its cell phone sampling strategy to select children (age 0 to 11 by parental proxy) and adolescents (age 12 to 17), in addition to adults who had been sampled in CHIS 2007.  This study reports on the issues faced in interviewing parents of children and teens in cell-phone surveys, and presents some preliminary results to assess noncoverage bias in the CHIS 2009 child and teen landline RDD samples.  ","To determine whether exclusion of adults with cell-phone-only may bias estimates from a landline-based survey, a screening cell phone survey was conducted in parallel with the ongoing, monthly landline data collection in 18 states in 2008. The landline and cell phone samples were weighted at state level. Logistic models were developed for each of 16 health indicators to examine whether survey approach affected estimates after adjusting for the impact of demographic characteristics. The relative biases for estimates were calculated to estimate the potential biases in landline telephone surveys that exclude cell phones. The study found that as noncoverage rate for cell-phone-only adults continued to increase, the biases for 9 out of 16 health indicators resulting from their exclusion from landline-based health survey were significantly high and cannot be ignored.  ","As traditional telephone samples experience increasing challenges, practitioners are turning to alternative approaches. One such strategy is Address-Based Sampling (ABS) which involves sampling addresses and then collecting data via mail or telephone. In this trial the goal was to obtain 1,000 completed interviews using a short and relatively innocuous questionnaire. The address sample was matched using several vendors to find as many telephone numbers as possible and these were interviewed by phone. We contacted the remaining addresses by mail. In this presentation we report on the success rate of the matching process and the response rates for the telephone and male portions. We discuss using the household vs. individual as the unit of sampling and the resulting challenges for weighting. Finally, we compare this ABS survey with similar RDD studies with respect to cost and data quality. ","Address-based sampling (ABS) has emerged as a promising alternative to RDD telephone surveys. Little is known about the effectiveness of the various procedures for contacting the households and administering ABS surveys in reducing potential nonresponse bias. This paper is based on an ABS study involving two phases of data collection. Mail was the primary mode of collection for each phase, with limited telephone follow-up and several experiments designed to explore the effect on the response. Using the level-of-effort analysis, we examine how the response rates, demographic and socio-economic characteristics, and key survey estimates change with the screener follow-up attempts. The results compare the extent to which the screener follow-up strategies increased the effective coverage of the target subpopulation and affected the estimates in a two-phase survey setting. ","Although web panels are widely used in market research, there are persistent concerns about panel conditioning, panel attrition, and self-selection biases. We compared web panel samples to \"fresh\" random samples recruited via telephone across 3 medical specialties: neurology (n=167 web vs. n=97 phone), pulmonology (n=83 web vs. n=68 phone), and pediatrics (n=56 web vs. n=60 phone). All physicians, regardless of sample source, completed the surveys via the Internet. Comparative analyses were conducted on 163 measures of practice characteristics, treatment choices, attitudes and perceptions. Focus was placed on whether key research conclusions differed by sample source. Few significant differences emerged between the panel samples and the non-panel samples, and no systematic bias was manifested by the panel samples. Methodological limitations of this study are addressed. ","Response rates have declined significantly over the past two decades, while at the same time costs associated with conducting an RDD (Random Digit Dial) telephone survey have increased.  These trends have resulted in the development of techniques aimed at increasing response rates and/or decreasing costs.  Using RDD samples that are purged of business and nonworking telephone numbers is an important factor to consider in call productivity. Between 2006 and 2008 twenty-seven states/territories participating in the Behavioral Risk Factor Surveillance System (BRFSS) used the purging technique and 26 did not.  We will use detailed call history data from 2006-2008 BRFSS surveys for these 53 states and territories to assess the effect purging business numbers has on productivity, coverage error, and cost/benefits. The implications of the findings to BRFSS operations will be discussed. ","Collection cost represents a large portion of the budget allocated to carry out surveys using Computer Assisted Personal Interviewing (CAPI). To improve efficiency, the samples of three major ongoing Statistics Canada CAPI surveys using continuous collection are coordinated to ensure that the same geographical areas are targeted during the same collection period. Using this approach, we hope to reduce the interviewer travel time and distances and thus decrease collection costs. This paper focuses on the paradata quality issues encountered when we try to assess whether these expected savings are materialised.  The quality and \"fitness for use\" of the available paradata are evaluated. ","Editing all inconsistent data records is time consuming and costly. To save resources, alternative editing methods are sought by survey practitioners. In this paper, an editing procedure where the responses are selected for editing through Poisson sampling  according to their impact to final estimates is proposed. Probabilistic approach gives simple tools known from sampling theory to describe the effect of editing on the survey estimates. A two-phase design approach is applied for bias estimation, and  a bias corrected generalized regression (GREG) estimator and an  estimator of its variance are presented. The effectiveness of the  proposed editing procedure is illustrated using empirical data from  Statistics Sweden.    ","Surveys are useful in addressing health issues. Few studies have explored incentives to boost participation among US Veterans. The relationship between incentives, response rates and respondent representativeness was explored during the pilot phase of a multimode Department of Veterans Affairs health survey. Subjects (n=3000) were randomized into promised, prepaid and no cash incentive groups.  Excluding those who were never reached (n=561), the response rate was 26.3% (642 questionnaires completed); the rates by incentive type were: 21.1%, 27.9% and 29.8% for the 'None,' 'Promised' and 'Prepaid' groups respectively. Incentives increased response while shifting the distribution of military and personal characteristics compared to the sample distribution. Based on pilot results, a $10 prepaid incentive and baseline quality assurance and control measures were adopted for the main survey. ","Declining response rates for Random Digit Dial (RDD) household surveys have prompted an increase in the use of monetary incentives as a tool for converting reluctant respondents. But the question remains - is this money well-spent? Previous research demonstrates that respondents included as the result of a refusal-based incentive effort can differ from initially compliant respondents along such measures as education level, gender, race, income, geography, and household composition. We expand on this research, examining data from the 2007 National Survey of Children's Health - an RDD survey conducted through the State and Local Area Integrated Telephone Survey mechanism of the National Center for Health Statistics on behalf of the Maternal and Child Health Bureau. We focus on whether and how incentive cases differ from non-incentive cases - all while considering any impact on survey bias. ","The 2010 Census Coverage Measurement Program (CCM) will evaluate the coverage of the 2010 U.S. Census. The 2010 CCM will provide estimates of the components of census coverage error (erroneous enumerations and omissions) separately in addition to estimates of net coverage error. Evaluation studies are underway to examine the quality of the 2010 CCM estimates and provide information for improving census coverage measurement methodology. Synthesizing the results of all the CCM evaluations will aid in forecasting and optimizing tradeoffs among costs and errors for the 2020 census.  The current plan is to use a simulation approach in constructing the synthesis and to provide estimates of nonsampling bias in the estimated components of coverage error. This paper explores the use of the evaluation studies to yield estimates of nonsampling error for use in the simulation. ","The census provides benchmark information on the structure of the Canadian population. Population counts and estimates are required to determine electoral boundaries and the allocation of funds among regional and municipal governments. The primary objective of the 2011 Census is to provide high quality statistics that meet user needs. Achieving a high level of quality in such a complex realisation is the result of addressing, managing and balancing over time the various elements that constitute quality with due attention to the program objectives and costs. It is thus important to ensure an integrated and consistent approach to quality initiatives across the various census activities while assessing the right balance between quality, costs and timeliness. This paper describes the various processes, practices and procedures to monitor and manage the quality of the 2011 Census data. ","M-quantile regression is used to model the M-quantiles of the conditional distribution of an outcome variable given a set of covariates. An M-quantile random effects model allows one to account for data with hierarchical structure, e.g. longitudinal data, when modelling these M-quantiles. When the functional form of the relationship between the response variable and the covariates is unknown or is complicated, an approach based on use of a penalized spline approximation can offer signi?cant advantages compared with one based on a linear model. In this paper we extend the M-quantile random effects model to nonparametric regression, in the sense that the M-quantile regression functions are estimated from the data using penalised spline approximations. Maximum likelihood estimation for this model is described and then evaluated via simulation studies as well as by a real world application. ","Standard sampling methods require randomization.  Estimators typically rely on the inclusion probabilities inherent in the sample design.  We consider here a variant of adaptive sampling where, auxiliary information being available for all population units, sampling proceeds in stages, initially taking a partial sample to get a preliminary picture of the relationship between the variable of interest and the auxiliary, and using the information gained to decide where collecting further data would be most useful.  It is necessary to appraise the relative importance of competing determinants of where sample is needed, for example, indications of heteroscedasticity, of peaks or valleys, and of discontinuities.  Calculating inclusion probabilities in such a scheme is forbidding.  We aim at robust inference by employing estimators based on non-parametric regression.   ","A class of semiparametric marginal models, containing both linear and nonlinear additive components, are investigated for longitudinal surveys. We propose an extension of the parametric design-weighted generalized estimating equations (GEE) approach of Rao (1998) based on a spline approximation of the  nonparametric components. We apply the EF-Jackknife to estimate the variance for the case of complex survey data. We also develop a variable selection procedure to identify significant linear components using the smoothly clipped absolute deviation penalty (SCAD). A fast and efficient estimation algorithm is developed for the users to analyze complex longitudinal survey data within seconds. The results of Monte Carlo experiments confirm a good behavior of the proposed estimators with samples of moderate size. A survey example is used to illustrate the application of the proposed method. ","Post-stratification is used to improve the precision of survey estimators when categorical auxiliary information is available from external sources.  In natural resource surveys, such information may be obtained from remote sensing data classified into categories and displayed as maps. These maps may be based on classification models fitted to the sample data.  Post-stratification of the sample based on categories derived from the sample data (\"endogenous post-stratification\") violates the standard assumptions that observations are classified without error into post-strata, and post-stratum population counts are known. Properties of the endogenous post-stratification estimator (EPSE) are derived for the case of sample-fitted nonparametric models. Asymptotic and finite-sample properties of the nonparametric EPSE are investigated under both design and model frameworks.  ","Since the 1995 report of the NAS Panel on Poverty and Family Assistance on improving the poverty measure, the Census Bureau has focused its efforts on implementing the report's recommendations using the Current Population Survey. Meanwhile the Census Bureau has fully implemented the new ACS. Our paper will report on two research projects, one sponsored by the New York City Center for Economic Opportunity, another initiated by the Census Bureau, that explore how a NAS-style poverty measure can be implemented using the ACS. The paper will contrast the results of the these approaches to imputing data missing in the ACS (taxes, in-kind benefits, child care and medical expenses) and offer ideas as to how future research on imputation techniques should proceed. It will conclude with some thoughts about how the ACS questionnaire could be modified to provide more data on non-cash resources. ","This paper describes efforts to develop a more accurate, comprehensive, and up-to-date measure of poverty in Wisconsin. The new measure targets areas of greatest need and reflects the effects of anti-poverty policies and programs within Wisconsin by encompassing state-specific taxes and transfers as well as federal program effects. Based on input from state and national experts, the Wisconsin model uses American Community Survey data and imputations to measure the level, depth, and trends in poverty. This session will also assess the ways in which the Wisconsin measure affects the demographic composition of those living in poverty, how this model compares to efforts in other parts of the country, and how it can be used to simulate the effects of policies and programs on poverty in Wisconsin. ","The recommendations of a National Academy of Sciences panel in 1995 bring new resources and expenditures components into poverty measurement. The Center for Economic Opportunity (CEO) of New York City developed a new poverty measure for the city based on the recommendations. The new measure uses data from the American Community Survey (ACS) with simulations based on federal, state and city tax laws; estimates of expenditures using the Medical Expenditure Panel Survey (MEPS) and the Survey of Income and Program Participation (SIPP); and statistical matching to survey and administrative data bases. This paper presents methods used to account for the ACS sampling error and variance components associated with imputations for the CEO poverty measure derived from the MEPS and SIPP, and the statistical matching.  The variance estimates are partitioned into the contribution of each component. ","Benefit receipt in major household surveys is often under-reported. This mis-reporting has important implications for our understanding of the economic circumstances of disadvantaged populations,take-up rates,the distributional effects of government programs,and studies of other program effects.We use administrative data on Temporary Assistance to Needy Families and the Food Stamp Program matched to household survey data from the ACS,CPS and SIPP.We first assess the level of agreement between the administrative and survey data.We then assess how misreporting-both false negatives and false positives-vary with individual characteristics.We also examine the determinants of program receipt,the distributional effects of benefit programs, and calculate more accurate benefit take-up rates and impact on poverty.Finally, we examine how our conclusions differ from those obtained using survey data. ","Due to declines in response rates and coverage, alternatives to random digit dial sampling are being considered. One alternative is address-based sampling (ABS), but methods appropriate for rare populations have not been examined adequately. In the Fall of 2009, we conducted a pilot study to evaluate ABS, with mail as the primary mode of collection, for a rare population. The goal was to replace a periodic survey of preschoolers and school-age children that previously had been conducted using RDD.  This study included a screening phase to determine a household's eligibility, followed by a Topical survey administered in eligible households. In this paper, we present the results of the pilot study that included several experimental treatments, and discuss the implications for an approach that maximizes response in a two-phase survey setting. ","Address-based sampling has been used in response to the declining coverage and response rates of random digit dial surveys. Although mail surveys using address-based samples have emerged as a promising approach in general population studies, their usability for studying specific subpopulations is yet to be tested. This paper reports findings from a two-phase pilot study surveying Veterans as part of the National Survey of Veterans. The first phase was a mail screener sent to a nationally representative sample of 11,000 residential addresses selected from the US Postal Service address lists. The second phase was a topical questionnaire directed to the Veterans identified through the screening. The pilot study demonstrated the feasibility of applying two-phase mail survey design for Veterans and distinguished some factors associated with response rates. ","The Marine Recreational Fisheries Statistics Survey is a program consisting of the Coastal Household Telephone Survey (CHTS) to assess annual saltwater fishing effort and an access-point intercept survey to assess catch per unit effort.  The National Research Council critiqued the design of the CHTS, citing its high non-coverage rate (RDD landlines limited to costal communities)and inefficiency. To address these concerns, a pilot test of a dual frame, self-administered mail survey was fielded in 2009.  The design minimizes non-coverage of anglers by sampling from an address-based frame while maximizing efficiency by sampling from a saltwater fishing license frame. The paper reports the results of the field test, including response rates for both screener and extended interviews, data quality, frame deduplication, and estimation issues related to the use of overlapping frames.  ","An experiment in the California Health Interview Survey (CHIS) showed that the sample yield increased when interviewers were assigned either solely screening interviews or solely extended interviews, as compared with the standard practice of conducting screener and extended interviews on the same call (Edwards et al 2010). CHIS selects one adult from each household screened, and one child and one adolescent if present. This paper will review some further experimental outcomes of the 2-stage approach, including sequencing of the adult and child interviews and a $5 incentive mailed between the screener and adult interview. Based on the experimental results, 2-stage interviewing was phased in for the remaining CHIS sample; sample yield before and after 2-stage interviewing will be compared, and interviewer and respondent reactions discussed. ","The Statistics of Income division of the IRS started a panel sample of individual returns in tax year 1999. This panel sample is also used for cross-sectional estimations, with a small supplemental sample added to it each year. The base-year panel sample was a stratified sample, where stratum boundaries were formed using the return income. Because the income distribution is highly skewed, base weights vary dramatically. This poses a particular problem for returns whose income grows so dramatically that its associated base weight is no longer appropriate for cross-sectional estimations. In addition to stratum jumpers, high-income returns from both new filers and surviving filers in out-years are not well represented by weights based on selection probabilities. In this paper, we consider a calibration approach to adjusting return weights, including trimming for influential stratum jumpers. ","Statistics of Income has published annual income statistics since 1913.  While these publications continually prove to be invaluable to users, older publications use a different income definition, for sampling purposes, and groups.  In order to compare the income statistics across years, one would have to reconstruct these tables to eliminate these differences.  Because SOI only has the cross-sectional individual return sampled micro data for years since 1960, attempts at updating prior year tables becomes difficult at best.  This paper will discuss the need and feasibly of reproducing tables using new income groups by way of models and income distributions built from current micro data.  More specifically, this paper will look at the possiblity of using 2007 microdata to reproduce both 1997 and 1987 published tables.  ","Estimates for populations of interest for Statistics of Income (SOI) programs are produced by drawing stratified, random Bernoulli samples of tax and information returns as they are filed, over predetermined sampling periods that often span multiple years. While this methodology results in the inclusion of the majority of targeted returns, a small number of returns for each study are filed beyond the data collection period, potentially introducing non-response bias into the population estimates. For a given sampling period, the paper will analyze historical filing patterns to develop an approach for accounting for late-filed returns. This research will assess the weight adjustment approach currently used in SOI's estate tax study and will provide a basis for application of a similar approach in each of the exempt organizations and private foundations studies.   ","SOI's Tax Year 2006 Corporate sample is a stratified Bernoulli sample of approximately 110,000 corporate income tax returns. Raking adjustments are performed using the sample's design strata, related to the return's size of assets and income, and the primary industry,  based on collapsed categories of the six-digit NAICS code. We apply several alternatives to estimate the variance of national- and domain-level totals of several key variables of interest: ignoring raking, post-stratification, a Taylor series approximation, and the delete-a-group jackknife replication estimator (with 100 and 200 groups). Results demonstrate that the poststratified total had the highest variance estimates, while the linearization and Jackknife when implemented incorrectly produced variance estimates that were too small, despite large sample sizes. ","Extremely large survey weights, resulting from the inverse of the differential probabilities of selection, can often produce unreasonable point estimates of population means and totals without some form of adjustment. Considerable variation in the weights, which in some cases is dominated by a few extremely large weights, leads to less efficient estimators of totals.  Trimming or truncating these weights can substantially reduce the overall variability, leading to increased precision of the survey-based estimates, at the expense of introducing bias.  This study uses the Statistics of Income (SOI) Division's 2007 Form 1065 Partnership data of 3.5 million 1065 returns that were processed by the U.S. Internal Revenue Service between January and December 2008 to compare alternative model-based, design-based, and generalized design-based methods of weight trimming in simulations. ","In complex sample surveys like the European Social Survey (ESS), participating country is responsible for its sample to yield estimators of comparable precision (ESS, 2005b, p.1). The quality of an estimator calculated with data which have arisen from a srs differs from the quality of the same estimator calculated on the basis of a complex sample, given the samples are of the same size.    ","Behavior codes can provide valuable auxiliary information with which to evaluate the survey-related behavior of both respondents and interviewers.  They have been recently also employed to investigate cultural variability in both interpreting and answering survey questions.  A potential limitation of the use of behavior coding, however, for making cross-cultural comparisons is that the behaviors they capture may themselves be culturally conditioned, making their utility as objective, etic indicators of social behavior less tenable.  We investigate this possibility by evaluating the frequency with which a set of individual behavior codes are exhibited by respondents from four race/ethnic groups who participated in a health-related survey.  Individual behavior code ratios (and 95% CIs) between pairs of race/ethnic groups will also be compared.  Our discussion will focus on the conclusions  ","In cross-national surveys, sample design often differs substantially between countries with respect to clustering, stratification and selection probabilities. Estimation of standard errors of between-country differences typically fails to take this into account. Common approaches are, a) to estimate standard errors for country means but then make informal comparisons, or b) to  test differences formally but under an assumption (usually unrealistic) that the sample design is common across countries. Using data from the EU SILC survey, we develop design-based estimates of standard errors of between-country differences in means. We compare these estimates with estimates of type b) above, where only design weights are taken into account. Resultant misspecification effects are presented to demonstrate the importance of correctly estimating standard errors for between-country differences. ","Monitoring quality in any survey can be challenging. Monitoring quality in a cross-national survey involves additional layers of complexity which may include considerable organizational, methodological, and operational barriers. This presentation will describe a framework for monitoring quality across the survey lifecycle in a cross-national study. The framework addresses seven dimensions of quality, including total survey error as well as critical dimensions such as relevance, timeliness and comparability. The presentation will also address how costs, design constraints and other features of the survey interact in the framework. Concrete recommendations will be made on how to monitor quality through the collection of study level, country level and question level metadata, process or paradata, technical developments, and greater transparency through quality profiles.  ","Random hot deck imputation is often applied to survey data with nonresponse. One of the popular methods for variance estimation without nonresponse is the random group method, which has to be adjusted when it is applied to imputed data. One kind of adjustment is re-imputing nonresponses in each random group. We show that the random group method with re-imputation produces asymptotically unbiased and consistent variance estimators for both linear and non-linear estimators under random hot deck imputation and nearest neighbor imputation. We also show how to apply a shortcut random group method, which reduces the computational complexity due to re-imputation, and establish the asymptotic unbiasedness and consistency of the resulting variance estimators.  ","Model-based prediction theory for finite population sampling and inference (Valliant et al., 2000) largely assumes that auxiliary variables are available for all units in the target population. These auxiliary variables play many important roles in prediction theory, including selection of balanced samples, model fitting, and ultimately prediction of values for non-sample cases and subsequent estimation of population parameters. Auxiliary variables are generally assumed to be measured without error, and very little work has considered the impact of this error on the performance of model-based estimators of finite population totals. This paper uses simulations to examine this impact, and contrasts the simulation results with theoretical expectations under no measurement error. Samples with weighted balance and minimal model estimators are found to perform best in the presence of error. ","Disseminating microdata to the public that provide a high level of data utility while at the same time guaranteeing the confidentiality of the survey respondent is a difficult task. Generating multiply imputed synthetic datasets is an innovative statistical disclosure limitation technique with the potential of enabling the data disseminating agency to achieve this twofold goal. In this talk we present the first successful implementation of this approach outside the U.S.: The generation of partially synthetic datasets for a German establishment survey, the IAB Establishment Panel. We describe the evolution of the project from the imputation of the missing values in the original survey and the discussions which variables to synthesize to the final synthesis. We also present our disclosure risk evaluations and provide some first results on the data utility of the generated datasets. ","A variation of the correspondence analysis is proposed taking into account jointly the distribution and the magnitude of the occurrence-rates of some type of events in order to compare populations. The analysis is based in graphical representations of the data's departures from a model in which all the populations have the same occurrence-rates for each type of event, named as the equirates model. In the graphical display proposed of the departures from the equirates model one can identify straight lines along which the populations that have the same distribution of rates but differ in magnitude are located.  In order to illustrate the method, we present an application to the study of external-cause mortality in the 35 largest Colombian municipalities. ","The Current Population Survey (CPS) produces demographic and economic estimates of people, householders and households. The householder is the person or one of the persons who owns or rents the unit occupied by the household. The estimated numbers of households and householders should agree by definition, but there has long been a discrepancy in these estimates. Other surveys at the Census Bureau now use a family equalization adjustment that reduces this discrepancy. This adjustment also produces consistent estimates of householders and partners for married and unmarried couple households. This paper discusses the application of this adjustment in the basic CPS.  I examine estimates of household, householder and population characteristics with the current CPS weighting method and the family equalization method, including variances of the estimates from these methods. ","Each month, the Current Population Survey (CPS) interviews housing units (HUs) from eight rotating panels (rotation groups). Each panel is interviewed for four consecutive months, followed by eight months of not being in sample, followed by four more months of interviewing. One of the eight panels is new to the survey in any given month, replacing an outgoing panel selected from the same second-stage cluster of HUs. All panels in all months within a design are selected from the same primary sampling units, which are groups of counties. This approach to sampling results in a complex pattern of correlations among panel totals within and across months. Furthermore, the approach we use to construct estimation weights also affects correlations. We model the effect of different stages of sampling and weighting adjustments on CPS correlations, and in turn, on variances and their decomposition. ","The Current Population Survey is a monthly sample survey of housing units that produces reliable estimates of labor force statistics for the working-age population of the United States. In order to maintain the efficiency of the survey's design, the sampling frame has historically been updated every decade with the most recent census data. A new CPS sample design is transitioned in from the old over a 16-month phase-in/phase-out period. We will measure correlations of estimates for key labor force characteristics for months both inside and outside the transition period. ","Major research projects are being conducted at the Census Bureau to redesign the  Demographic Surveys for the 2010s. One project includes research to obtain a  method for stratifying the primary sampling units (PSUs). In this paper we revisit the  Friedman-Rubin's clustering algorithm that has been used in the last three redesigns  for stratification. This clustering algorithm attempts to optimize a criterion function  for a fixed number of strata. The most commonly used criteria functions are  Minimum variance, Wilks' lambda and Hotelling's trace. These criteria along with  the criterion (2000 criterion) used in the 2000 redesign are being studied empirically  and their performances and comparisons under these criteria are also being discussed.  Results suggested by these criteria show that the minimum variance criterion  provides the best stratification for labor force characteristics. ","The Current Population Survey (CPS) conducts reinterviews (RI) to measure the reliability of labor force (LBFR) status. Whether or not latent class analysis (LCA) can be used instead of RI, these models are able to identify sources and causes of unreliability as well as errors in its estimation. LCA uses substantially increasing the sample size without conducting RI. Agreed with RI results, we have been using LCA in parallel with RI to estimate the LBFR response error in CPS. We began with a first-order latent Markov model, but the challenge is it assumes unobserved homogeneity that is not always true. Mover-Stayer (MS) model solves the homogeneity problem, and outperformed the first-order model in estimating CPS response error, but if the dependency (second-order) goes beyond the previous state, we may be missing information. This paper presents different error models with second-order. ","Surveys are increasingly becoming more culturally inclusive, in that they either extend to groups not previously surveyed, engage in comparative analyses that require cross-cultural measurement equivalence, or attempt to improve methods for surveying respondents from a range of cultural and linguistic minorities.  Qualitative methodologists have therefore begun to conduct research striving towards 'best practices' in this area.  I will review the major issues involved in this area, and present recent work that touches upon each relevant facet. In particular, I will focus on questionnaire-related issues, including design, pretesting, and interviewer-administration, across a number of cultural and language groups, and including buth U.S. and European contexts.     ","Central to the development of multilingual questionnaires is a sound translation of the source questionnaire into target languages. This study aims to determine the types of translation issues that can lead to measurement errors in cross-cultural studies. Based on two multilingual projects conducted to cognitively test the translations of the 2010 U.S. Census questionnaire in four languages as well as the English original (Chinese, Korean, Russian, and Vietnamese) and the American Community Survey questionnaire in two languages (Chinese and Korean), we developed a coding scheme guided by sociolinguistic theories to systematically analyze translation issues. This paper discusses how the coding scheme can be useful in the development of multilingual questionnaires and suggests feasible solutions to translation issues, so as to ensure translation quality. ","This research explores the linguistic behaviors of Chinese-speaking respondents and their impact on the quality of data collected in survey interview settings. This study examines whether respondents provided sufficient, relevant, or contrary-to-face value (CTFV) responses to the proposition of a survey question asking respondents their intention to participation in a future survey. It compares the responses of Chinese speakers to those of English interviewees and explores how social factors such as educational attainment and dialect preference relate to the linguistic behaviors of Chinese-speaking respondents. Findings from this study are significant and insightful for our understanding of the characteristics of Chinese institutional discourse in comparison with other language groups and how these characteristics impact the quality of their responses in structured survey interviews.   ","A challenge for question design is to account for differing cultures, languages and socio-economic conditions that can undermine comparability. Understanding the degree of interpretive variation across groups of respondents is an important step for ensuring valid data. This paper describes a mixed-method approach used for a UNESCAP/Washington Group project to develop a disability measure. First, in-depth interviews were conducted in 6 Asian countries to identify the various interpretive patterns used by respondents to formulate answers. Then, to determine prevalence as well as to compare those patterns across groups, a structured field test questionnaire was developed from the qualitative findings and then fielded in those countries. This paper presents data from the two methods and illustrates how they were used together to portray a more complete picture of measurement comparability. ","After the completion of a survey with a given design, often one wishes to modify the design, based on the data collected, so as to increase the efficiency of the design for future use. In multi-stage samples one often uses the concept of a design effect to summarize the efficiency of a design for a particular survey estimator. For estimators of totals this concept is difficult to define, since for a simple random sample, the basis for comparison when calculating design effects, the population size is known. We present an approach for evaluating potential revisions to a two-stage design, where the sample estimators of primary interest are population and subgroup totals. An example is shown for a design to sample emergency room visits from hospitals. The approach uses the fact that estimators of population totals and subgroup proportions often have small correlations. ","The Francisco-Fuller (1991) approach yields unsatisfactory results when applied to data from multistage unequal probability samples. There have been problems encountered in practical application of the Francisco-Fuller method: it requires evaluation of bounds at many points, the resulting limits are, at times, not monotonic, and the stated coverage is not very accurate at the boundaries. An adjustment to the empirical distribution is used to achieve internal consistency and reduce coverage error. Hypothetical populations will be generated using normal as well as some skewed distributions and 1000 simulation samples will be drawn from each population. Multiple confidence intervals with varying confidence levels will be generated for several percentiles for each of the sample by two methods. The coverage rates obtained from the proposed method and the Francisco-Fuller will be compared. ","Undoubtedly, a growing number of households are foregoing landline services and relying entirely on wireless devices for their telecommunication needs.  Currently, results from the National Health Interview Survey (NHIS) are used to estimate the number of so-called cell-only households.  However, the NHIS-based estimates are available only at the national level, with state-level estimates that are at least two years in lag.  This creates challenges for survey research designs that employ random digit dial (RDD) sampling methodology, since RDD designs require reliable estimates for the number of households reachable via landline and cellular phones.  More importantly, such estimates are needed at various levels of geography for surveys with geographic domains below the nation.  A proposed solution to these problems is to combine survey results with reported data from industry.  This hybri ","In simple random sampling, the ratio method of estimation is a well-known technique for estimating the population mean of a study variable when the population mean of the auxiliary variable is known. In this study, a novel ratio estimator based on order statistics is introduced to increase the efficiency of the traditional ratio estimator. It is shown that the proposed estimator is considerably more efficient than the traditional ratio estimator over a very wide family of symmetric distributions. The robustness properties of the proposed estimator are also studied via simulations. ","The method of propensity scoring adjustment is commonly used to handle selection bias in many applications, including unit nonresponse and undercoverage in sample surveys. The propensity score is computed using auxiliary variables observed throughout the sample. We discuss some asymptotic properties of the propensity score adjustment estimator and derive an optimal propensity score estimator. The proposed optimal estimator is closely related with the prediction estimator based on an unconditional exponential tilting model. Variance estimation is discussed and the results from two simulation studies are presented.   ","In 2009 Pre-sampling Inference and it application to survey sampling were introduced but restrictions that imposed a unique model were required. This paper generalizes last year's by removing most restrictions through use of generalized inverse matrices but retains the reassuring impartiality of randomization and the inferential power of models.  Under the 2009 restrictions the sampling distribution was irrelevant to the Pre-sampling inference; this year's generalization requires the sampling distribution to deal with lessened model specificity resulting from this generality.   Inference flows from both the Pre-sampling model based BLUE and the Sampling Distribution's selection probabilities while achieving large mean squared error reductions compared to design based estimators.   The advantages of both model and design based inference are retained while eliminating their shortcomings. ","We propose a dynamic approach to determining the sample size in successive survey rounds, rather than a static approach with fixed sample size. Observed data in past rounds is used to update the sample size in the next round. Suppose that in each of n successive surveys, estimates have been made of a population parameter and estimates have also been made of the standard error of the parameter estimates. We consider two scenarios: (i) the estimates and sample sizes are approximately constant up to time n - 1, the sample size at time n is the same as in the past, but the parameter estimate and/or its estimated standard error changes at time n; and (ii) the estimates behave in a regular manner up to time n - 1, the sample size is approximately constant up to time n, and an autoregressive model with white noise matches the sample estimates up to time n -1 but fails at time n. ","Research involving elusive populations often relies on convenience samples, which are susceptible to potential biases.  Although any single dataset is likely to be unreliable, statistical methods that combine multiple data sources, such as multiple systems estimation, may still yield informative results. HRDAG has used MSE to count the number of homicides in a number of settings.  This paper presents our most recent application of the method, to sixteen different data sources in Casanare.  Due to the intractably large number of models that could represent the interactions between the data sources and the divergent results of models of comparable quality, we rely on Bayesian model averaging over all models applied to all partitions of the datasets. We also demonstrate how reliance on any one of these  data sets would paint a highly inaccurate picture of lethal violations in Casanare. ","Elusive populations are generally very rare populations that are difficult to locate and identify with standard survey procedures. Examples of elusive populations include refugees, the homeless, illegal immigrants, abused children or elderly persons, injection drug users, and persons with HIV/AIDS. This paper reviews the challenges in sampling elusive populations using standard survey sampling methods for rare populations, including large-scale screening, disproportionate stratified sampling, two-phase sampling, multiple frames, and network sampling. It also discusses the use of location sampling and various forms of link-tracing sample designs based on social networks, including snowball sampling and respondent driven sampling. ","Existing quantitative claims about enforced disappearances in Punjab (India) between 1984 and 1996 are not based on defensible statistical methods. We present preliminary results from a retrospective mortality survey in the rural region of Amritsar District designed to measure lethal counterinsurgency violence. The survey used a hybrid sampling design which combines probability-based random sampling with referral-based sampling. We explore the dynamics of referral-based sampling through a presentation of classical survey estimation methods and social network analysis metrics of the referral chains. ","The demand for small area estimation has been steadily increasing. We present how small area procedures were applied to a business survey, a health survey, and a household survey. Custom made small area techniques as well as computer programs were used. Since there is a need for small area estimates on across many surveys, we are developing a flexible small area system that will begin to address the needs of producing small area estimates in production. Basic requirements of such a system includes: the type of procedure used, area level or unit level based; incorporating the sampling design in the estimation of both primary statistics and secondary statistics; smoothing within and between variance components (part of the small area procedures); ensuring that estimates of totals add up to reliable higher level estimates; and developing diagnostic tools to test the adequacy of model. ","Models are developed for the correlation effect of block clusters and housing units on the capture/recapture model of Census coverage measurement using data from 2006 Census test. The purpose of understanding this possible correlation is two-fold: 1) as noted by Malec and Maples (2005) a model for within small area variability is needed because design based estimates may be imprecise. Also, using 2000 census coverage information, Keller (2007) has shown evidence of both variable census capture rates and correct enumeration rates between block cluster and 2) the heterogeneity effects of between correlation due to block clusters and housing units on estimates of coverage have not been thoroughly evaluated. Random effects and the choice of a random effects model will be evaluated as a tool for measuring variability and correlation at these small levels. ","In this paper, we consider an empirical Bayes method for combining multiple data sources in producing end-of-year estimates of crop harvested yield at the county level in the US. The method employs an area level model consisting of two submodels - one for the sampling  error of direct survey estimates and the other relating true yields  to a set of county specific predictor variables. Unlike standard variance component estimation methods such as maximum likelihood and residual maximum likelihood, the new methodology ensures strictly positive consistent estimation of model variance. In order to produce uncertainty measures of the proposed estimators, we introduce a parametric bootstrap method that incorporates all sources of uncertainty. Results of a study evaluating the empirical Bayes method for soybean yield in seven midwestern states are presented. ","Recent external studies find that the Bureau of Labor Statistics (BLS) Survey of Occupational Injuries and Illnesses (SOII), based on employers' OSHA logs, substantially underestimates the US total number of workplace injuries and illnesses. This paper describes four dimensions of the undercount allegations and summarizes the recent studies, which have largely compared aggregate or case-level data from various sources, augmented in the case-level studies with capture-recapture analysis. The paper provides an overview of on-going research conducted and funded by BLS to better understand the nature of the undercount findings. These studies include counting work-related amputations and carpal tunnel syndrome cases with multiple data sources; matching SOII to workers' compensation records; and, interviewing employers about reporting practices on OSHA logs and to workers' compensation. ","Recent work comparing the Bureau of Labor Statistics' Survey of Occupational Injuries and Illnesses (SOII) to Workers' Compensation (WC) claims databases concludes that the SOII substantially undercounts cases. We use linked WC-SOII data from Wisconsin to describe which cases are more likely to be accurately captured in the SOII. The SOII capture rate is higher for relatively acute injuries such as fractures, and is lower for injuries that are less readily identifiable as work-related such as inflammation or carpal tunnel, or for injuries in which WC claims were made substantially after injury incidence or after the year of injury.  These findings further our understanding of the strengths and weaknesses in both the methodology used to measure the undercount and in the current data collection methods used by the SOII. ","Recent studies have cited discrepancies between the BLS Survey of Occupational Injuries and Illnesses (SOII) and State Workers' Compensation claims to support the assertion that the SOII undercounts injuries and illnesses among the American workforce. To explore reasons for possible discrepancies, we conduct qualitative interviews with employers responding to the SOII, in the Washington DC metropolitan area. Our in-person interviews focus on possible measurement errors associated with establishment record keeping systems and understanding of the survey request. Results suggest that the varying roles of respondents in SOII and WC reporting, understanding of reporting rules, survey timing, and injury and illness case complexity all may play a role in the discrepancies.   ","Recent efforts to improve surveillance of workplace injuries and illnesses have focused on utilizing multiple data sources for the estimation or enumeration of cases.  In a feasibility study of this multi-data source approach, we attempt to identify cases from the Washington State-based workers' compensation database, hospital discharge data, trauma registry data, and the U.S. Bureau of Labor Statistics Survey of Occupational Injuries and Illness.  This approach at state-based surveillance using multiple data sources is being piloted with two different health outcomes:  work-related amputations and carpal tunnel syndrome.  This paper describes the unique qualities of each data source, challenges faced by the utilization of multiple data sources, and preliminary findings. ","Inverse probability weighting is a widely-used tool in estimation under selection. In survey applications, it is used to handle unequal probability sampling and non-response. This paper considers its use when the selection of different units may be dependent because of clustering. The main motivation is adjustment for nonresponse which exhibits clustering. It is assumed that nonresponse follows a logistic regression model which includes a cluster effect, which would conventionally be taken as random. The question addressed in the paper is how to 'estimate' this cluster effect when constructing inverse probability weights. Approaches considered include a marginal approach (in which the effect is integrated out), BLUP prediction of the random effect and a fixed effect approach.  ","In a survey of WIC participants, a telephone survey was conducted and fifty percent of the respondents were asked to participate in an in-home survey.  A number of respondents to the telephone survey declined to participate in the in-home survey.  The presence of a number of variables in the telephone survey lends itself to weight adjustments using either propensity weights or propensity categories.  Variables associated with the in-home survey variables were used to predict responding to the survey.  Two methods of adjustment were compared with results not adjusting for nonresponse.  In addition, a variable from the telephone survey was used to simulate an in-home response so as to compare estimates using the two procedures with actual estimates from the telephone survey. ","The number of call attempts can be used in response propensity models to reduce nonignorable nonresponse bias in sample surveys.  In applying these models, a number of issues arise from the definition of process variables like \"call attempt,\" \"contact/noncontact,\", and \"nonresponse\" to callback censoring, unrecorded call attempts, and other data errors.  As will be demonstrated, these problems can reduce the fit and utility of the models.  Despite these problems, callback models can substantially reduce the nonignorable nonresponse bias, particularly when data on nonrespondents are quite limited.  This presentation will review some approaches for employing callback models for nonresponse bias adjustment and analysis.  Some of the problems in using call-back data in nonresponse adjustments will be described and the potential of the approach will be demonstrated for a large field survey.   ","In the NSDUH weighting process, age-groups (12-17, 18-25,., 65+) are used for the nonresponse and postratification adjustments. Using 2004-2006 NSDUH data we found that the response rate decreases as age increases in the range of 12-25. Also the prevalence rates for drugs, alcohol and tobacco use increase almost linearly between ages 12-21, reaching a peak at age 21. Since the response rates and drug use prevalence rates change dramatically between the ages of 12 and 21, use of single years of age instead of age groups for both nonresponse and poststratification adjustments should reduce nonresponse bias as well as the variance of estimates. This paper explores the use of single years of ages between 12 and 25 in the 2006 NSDUH weighting process and discusses the prevalence rates and standard errors produced using the new set of weights.    ","Nonresponse bias occurs when respondents and nonrespondents differ with respect to characteristics of interest. A common way of reducing the nonresponse bias is to adjust the design weight of the respondents. A popular weight adjustment procedure consists of dividing the design weight of respondents by their estimated response probability, which can be estimated using parametric methods (e.g., logistic regression model) or nonparametric methods. Nonparametric methods include kernel smoothing methods and local polynomial regression (Da Silva &amp; Opsomer, 2006, 2009) or weight adjustment procedures applied within weighting classes (which is commonly used in practice). In this talk, results of a Monte Carlo simulation study comparing different nonparametric methods in terms of bias and mean square error will be presented. ","Obtaining a representative sample of completed interviews is a key goal of all survey efforts. With the advent of address-based sampling (ABS) -- sampling addresses from a database with near universal coverage - it is critical that researchers develop survey methodologies which work in tandem with sample designs to achieve this goal. Using a large ABS mixed-mode study of 150,000 respondents conducted in 2009, we explore three key questions: (1) What are the advantages provided by an ABS design to help in targeting hard-to-reach groups (younger adults, Hispanic, Black, and cell phone only)? (2) What are the challenges in reaching these groups when an ABS design is used? and, (3) How successful have efforts been in achieving representative samples? The findings further both our understanding of the strengths and limits of an ABS survey design and highlight key areas of continued research.   ","Motivated by the high costs associated with traditional listing, survey organizations have looked to the United States Postal Service Delivery Sequence File (DSF) as a replacement for traditional listing, particularly in urban areas.  However, several different vendors offer DSF-derived datasets and we do not know how they compare with respect to coverage.  Working with the National Children's Study (NCS), we continue our work to understand the coverage properties of these databases.  After matching traditionally-listed addresses with three versions of the DSF, we returned to housing units excluded from one or more source to collect additional data.  We use GPS-enabled photography and NCS screening and interview data to uncover \"patterns of missingness\" describing the types of housing units and households that may be included or excluded by traditional or DSF-based lists.   ","A feature of address-based sampling (ABS) is versatility of the sample frame where many ancillary data can be appended to an address. Commercial databases (e.g., Experian, infoUSA, Acxiom) are used to append observed and modeled information at various levels of aggregation. This enables researchers to develop more efficient sample designs and broaden analytical possibilities with expanded sets of covariates. While quality of ancillary data is of concern for researchers, the literature provides only anecdotal assessments on accuracy. Relying on surveys and KnowledgePanel recruitment samples that employ ABS, the authors present results of comparisons between an array of ancillary data and corresponding observed values collected directly from the responding households. ","Cost savings are the primary advantage of using address-based sampling (ABS) over field enumeration (FE) for in-person surveys of the civilian, non-institutionalized population. These cost savings are tempered by research which indicates that FE provides more complete coverage than ABS, especially in rural areas. We developed and piloted a candidate sampling frame for the National Survey on Drug Use and Health (NSDUH) that uses ABS supplemented with a frame-linking procedure in area segments where we anticipate adequate ABS coverage and FE in segments where we anticipate poor ABS coverage. The objective of the candidate frame is to lower costs without sacrificing coverage levels of the current NSDUH sampling frame which is based solely on FE. We report on the trade-offs between coverage and cost savings as area segments are shifted from FE to ABS. ","Arbitron uses a combination of RDD landline phone sample and address-based sample (ABS) to recruit households for its radio ratings service. For RDD sample we attempt to match addresses, whereas for the ABS we attempt to match phone numbers. In theory, the sample for which neither address nor phone matches are available come from the same population (the same commercial matching databases are used). To test this hypothesis, the unmatched households in both frames - i.e., no-address RDD and no-phone ABS - were invited to participate in our diary-based radio ratings. Demographic characteristics, survey performance, and media habits of both groups give insight as to whether these two sampling approaches are reaching the same population.  Given the increasing noncoverage of RDD landline frames and the growing interest in ABS, these findings are key to sound research and survey sampling. ","The basic theory for survey sampling is well established and is presented in a number of textbooks, but most texts cover the material at an intermediate level. The proposed course is designed for researchers and practitioners interested in advanced techniques and the theory underlying those techniques. The topics include asymptotic theory in survey sampling, optimality in estimation and design, and use of models with survey samples. Understanding advanced theory in these areas is important for the design and analysis of complex large-scale surveys and for development of rigorous statistical theory relevant to sample surveys. Participants should have a background in survey sampling and in statistical theory. Recent graduates working in survey sampling and graduate students are especially encouraged to attend.","In December of 2010 the U. S. Census Bureau released the first set of small area estimates from the American Community Survey (ACS).  The ACS is the new source of detailed demographic, social, economic, and housing data for areas as small as census tracts and block groups.  Similar data were released after Census 2000 from the long form sample.  The Census Bureau will release ACS data annually in the form of 1-year estimates (for the largest geographic areas), 3-year estimates (for smaller geographic areas) and 5-year estimates (for all areas).  This short course is designed to familiarize statisticians with the ACS and its many data products.  The course will provide an overview of the basic design of the ACS and the methods used to produce survey estimates.  We will demonstrate how to access and use ACS data and will provide specific examples of how to access, download and manipulate both ACS summary tabulations from the American FactFinder and ACS public use microdata sample files.  We will discuss some of the issues associated with the use and interpretation of ACS multiyear estimates.Target audienceThe target audience includes analysts and statisticians from federal agencies and state and local governments, the media, researchers, survey methodologists, and private sector statisticians with a basic understanding of the ACS and an interest in improving their skills in using and interpreting ACS data.  This course is aimed at the statistical community to provide an overview of the information available from the ACS and the types of analyses that are possible. Material will be presented at an introductory level but will include information of interest to more advanced ACS data users.","  The federal funding landscape for statistics will be discussed by key  panelists from NSF and NIH, the primary funding agencies for statistical research. The objective of the session is to discuss opportunities and engage the broader statistics community on current issues related to funding of statistics research.     ","Comparative effectiveness research is taking on a very important role in US health care reform. The basic idea of this approach is to take advantage of existing data sets, which are primarily observational in nature, to infer the relative effectiveness of medical interventions. These data sets, however, typically have severe limitations for drawing reliable causal inferences about medical interventions. For example, administrative hospital insurance claims data only include billable charges for patients who are in hospital and exclude patients who are not in hospital -- how can such data be used to compare outcomes for patients taking one drug versus another, even if the data for in-hospital patients are perfectly accurate? For another example, is it reasonable to assume that if a medical condition is not noted, it is absent? And of course, observational data, even when accurately record ","Address-based studies require high quality address sources for frame construction. The 2009 Residential Energy Consumption Survey (RECS) for the Energy Information Administration (EIA) employed an address frame based on the USPS Delivery Sequence file (DSF) supplemented by traditional and enhanced listing. The purpose of this research was to understand differences in coverage among the DSF, traditional listing, and enhanced listing in areas of questionable coverage. This research set out to: 1) match the addresses in segments that were used in both the 2005 and 2009 RECS in order to understand changes over time and changes in method, and 2) examine the coverage of the DSF in 2009 RECS segments that were proximate to the threshold where we could have used any of the methods. ","The National Children's Study (NCS) is a large and complex longitudinal study, the initial stage of which is the enrolment of women of childbearing age.  The NCS data collection depends on a sample of addresses within selected segments.  Addresses could originate from a number of sources, depending on the nature of the environment: traditional listings, enhanced listings, or licensed address lists based on the USPS delivery-sequence file (DSF).  The literature suggests that each method has different coverage properties in particular situations.  At question are the characteristics of housing units, households, and ultimately women and children included using each method.      Our analysis compares the characteristics of households that were present on the original DSF vs. those that were added through listing.  We do so using questionnaire data from a subset of sites to understand the types of women, babies, and potential health outcomes whose inclusion can be influenced by frame type.   ","The extended pilot phase of the National Children's Study (NCS) provides a unique opportunity to compare and contrast alternative sampling strategies. The NCS is currently undergoing data collection in 30 primary sampling units across the United States in an effort to examine a wide variety of environmental factors on the health of children from before birth until the age of 21. This research will compare data collected through three distinct designs: 1) in-person visits resulting from address based sampling, 2) a mailing and call-in approach using address based sampling, and 3) medical provider recruitment efforts. The study objectives are the same across all three methodologies, and we will examine which method most closely represents the expected population of a given county. Close attention will be given to expected number of pregnancies/births as compared to reported pregnancies/births, as well as how well a variety of demographic variables reported by survey respondents compares to Census and American Community Survey data. The research will compare data collected across the three methods to determine which appears to best represent the population. ","As concerns about declining representativeness of landline random digit dial (RDD) surveys have increased in recent years, many survey organizations have begun to explore alternative methods such as address-based sampling (ABS). Use of telephone as a primary interviewing mode can increase efficiency of ABS designs if addresses are reliably matched to telephone numbers. Several matching strategies were evaluated as part of the 2009 National Immunization Survey - Address-Based Sampling (NIS-ABS) Experiment. Addresses drawn from the USPS Delivery Sequence File were matched to telephone numbers by two sample vendors. Results indicate that the inclusion of \"inexact\" matches (i.e., matches to the street address, but not to the unit or building number) can increase match rates dramatically, especially in multi-unit buildings. However, the quality of these matches was shown to be relatively low. With the tradeoff between quantity and quality of matches, an approach that exclusively uses \"exact\" matches is more efficient in terms of cost and interviewing time than using inexact matches. ","Statistical agencies rely on well-designed questionnaires to collect good data, minimize costs and control response burden. We view a questionnaire as a directed graph, with questions as nodes and flows as paths. In this set-up, we would like to evaluate the structure of a questionnaire and improve on its design. The coverage of a question is a verbal description of the sub-population that is asked that question, and is very important to the analyst. The graph structure should point to questions where the coverage is hard to verify. A node with a high in-degree (several paths converge into it) corresponds to a question asked of several subpopulations following different paths, which eventually converge into this node. As it is hard to test the completeness of the coverage of this question, we should attempt to place it earlier in the questionnaire to minimize the problem. The link between information theory and questionnaires viewed as graphs (Picard 1980) would give us insight into the ordering and formulation of questions so as to maximize the amount of information collected and minimize the response burden. We illustrate these ideas with examples from a complex survey. ","In preparation for the 2012 Commodity Flow Survey (CFS), an advance survey of select establishments was conducted in early 2011.  This advance survey, also known as a precanvass, was initiated to obtain updated contact information for selected businesses, verify their shipping status and shipping address, and capture more accurate and complete information on the value of shipped commodities.  This information was an important initial step in the construction of the 2012 CFS sample frame, and helped to improve the efficiency of the sample design, as well as promote higher response rates for the actual survey.  This paper describes the methodology and preliminary results of the 2012 CFS precanvass survey, including lessons learned from the previous precanvass conducted in advance of the 2007 CFS.  In addition, notable improvements are described including the inclusion of an electronic reporting option, expanded establishment coverage, and collection of additional address information.  The CFS is a survey of shipping establishments in the United States and is conducted as a joint partnership between the Bureau of Transportation Statistics and the Census Bureau. ","Average Outgoing Quality Limit (AOQL) methodology may be applied, as it was for 2010 Census data capture, in quality control procedures to ensure the proportion of errors does not exceed a specified limit. For each batch of production units, we seek the set of sampling plan parameters (batch size, sample size, and acceptance number) that both ensures effectiveness in limiting errors and optimizes efficiency by minimizing workloads. When production operations do not allow strict control over these parameters, a sampling plan strategy is needed to find sampling plans fitting to each batch that ensure such effectiveness and efficiency over all batches. The best operational procedures to do so will capitalize on the mathematical relationships among statistics in the methodology. Computer simulations of output effectiveness, measured by the AOQL, and efficiency, measured by the Inspection Per ","Approximately every five years, the US Census Bureau selects new samples for its wholesale, retail, and service sector surveys, including the Service Annual Survey (SAS). The SAS provides data on service industries in the United States, giving estimates for total revenue and total expenses along with detailed data, such as product lines, for selected industries. The sampling procedure employs design-based sampling variance formulas to simultaneously meet requirements for coefficients of variation (CV) at many industry levels. In previous sample revisions, relatively little consideration was given to the properties and reliability of the CV estimates. This paper describes a simulation study exploring two approaches to improving those properties for the 2012 Business Sample Revision (BSR-12). The first is a review of the variance estimation method currently used, and the second is a method ","In this study we have tried to understand how Indian PES (Post Enumeration Survey) methodology works and how much difference between coverage errors estimated by Indian PES design and some other possible designs.We have initiated constructing a hierarchical model to capture coverage error in census and built an artificial population in that hierarchical setup following exactly the administrative structure of Indian census.We have repeated the experiment for small and large populations and perform Dual System Estimation(DSE) by artificial Census and PES operations independently on those populations.In present study,along with systematic sampling(used in Indian PES,2001),we have done SRSWOR within each of three strata- city,non-city urban and rural.In our model,we assumed inclusion probabilities are varying uniformly around a specified value,each for Census &amp; PES.It was found in other studies that DSE underestimates the total population.However,in our simulation study,we have not observed this fact for small population. There is no standard pattern about the performance of systematic and SRSWOR and \"systematic\" showing more fluctuations around true although it is better than SRSWOR. ","The Connecticut Collaboration for Fall Prevention (CCFP) recently demonstrated, by comparing an intervention region to a matched region where usual care was proceeding, a significant reduction in the collective number of emergency department (ED) and hospital admissions for fall related injuries among older persons. The units of analysis in CCFP were zip code tabulation areas (ZCTAs), i.e., bounded regions, subject to spatial correlation. Here we extend the CCFP analyses with a multivariate spatial model that simultaneously considers the spatial and cross correlation of ZCTA-based rates of two distinct categories of fall-related injuries (FRI) in both regions of the CCFP. The two categories of FRI are those leading to treatment and release from the ED, and those leading to hospital admission. We then combine state and federal estimates of the mean/median costs for each category with their spatially modeled rates to estimate acute healthcare savings attributable to CCFP. This work shows how sophisticated spatial models may be used to more accurately estimate healthcare savings from interventions designed to prevent injurious falls among older persons. ","This article develops methods for sampling patients on a flow basis as they enter a clinical setting: facilities, hospitals or physician offices. The methods involve selecting time periods coupled with locations within the sample facilities. We develop the sampling framework for two diverse studies where patients are sampled. The first study is a survey of HIV patients in selected facilities. The second study is a survey of adolescents in one urban health center. In both cases, we develop probability sampling methods that provide maximum coverage, representative samples and high participation rates. We also discuss the practical constraints and implications of the method. The method involves sampling from a frame that arrays eligible time periods for each entry point in each facility. Sampling with probabilities proportional to size (PPS) methods is possible by compiling patient flow dat ","In this paper, we present our recent research on hospital re-admission, in particular re-admission within 30 days of last discharge. The dataset is one year hospital discharge administrative records from a US hospital system. The dataset contains about 70,000 discharges, of which 22% are re-admitted, and about 40% of re-admissions are re-admitted with 30 days of last discharge. We will present the characteristics of the data, admission and re-admission of various diseases; challenges of the problem; comparison of multiple techniques used to predict future re-admission, and their prediction results. We will also discuss policy implications from the study and possible future work. ","Many health care finance mechanisms transfer health insurance risks to health care providers. Global capitation is the best known example but bundled and episode payments, Diagnosis Related Groups payments, the Medicare/Medicaid Prospective Payment Systems for Physicians, hospitals, nursing homes, and home health agencies, and agreements between health care providers and other third party payers, also transfer such risks.    ","In 2009, 16 states have some form of mandate for private insurance plans to cover diagnostic and treatment procedures related to autism spectrum disorder (ASD). In order to investigate the impact of the legislation, cost and utilization measures were compared between these 16 states and 33 states (Hawaii excluded) without mandated coverage. Using Thomson Reuters' MarketScan claims database and related products, measures such as insurance premium impact, covered expense per ASD claimant, and ASD claimants per 10,000 covered lives (i.e. ASD rate) were constructed for each state for confirmed and unconfirmed ASD populations. A T-Test analysis yielded that the insurance premium impact for procedures related to ASD was significantly higher (p&lt; =0.05) for the mandated coverage group. The confirmed ASD population covered expense per ASD claimant was significantly higher, while for the unconfirmed ASD population the ASD rate was significantly higher. A regression model created an equation to predict insurance premium impact based on ASD rates. A 1-way balanced ANOVA was used to examine four years of ASD rates for each population, which yielded significant increases. ","Interest in accurately estimating the cost and utilization of healthcare and the impact of specific treatments continue to increase. Observational real-world data have limitations and often are not appropriate for determining costs, use and the optimal treatment of conditions, since they focus on: routine care, use of broader outcomes and heterogeneous populations with co-morbidities. Insurance claims data can be used to understand the characteristics of treatments and behaviors of patients using specific treatments. It is expected that patient behavior in treatment; costs and utilization which are great determinants of effective health care programs, are modifiable by the consumption of information therapy. As people become more knowledgeable about health and care implications, it is expected that only a few patients would incur extremely high costs, or have longer hospital stays. A critical analytical issue is accounting for a large set of patients not incurring costs or services. We use Bayesian analysis of generalized linear models to estimate costs and utilization characteristics for health-care services from 2004 to 2008 of a mid-western city employer, utilizing claims data. ","Interviewer turnover and concerns about interviewer job dissatisfaction have plagued survey organizations for years (MRIA, 2008; Wiggins, 1999). The costs associated with continually recruiting, hiring and training new interviewers are significant (Harding, Yost, &amp; Knittle, 2007). While interviewers who leave drive up survey costs, dissatisfied interviewers who stay may exert less effort to obtain cooperation and may fail to apply appropriate interviewing techniques. Such poor performance can lead to nonresponse error and measurement error (Groves et al., 2009). Despite these serious consequences, no research has identified the attitudinal predictors of turnover and job performance. We surveyed interviewers in three centralized telephone interviewing facilities and tracked their subsequent job performance and turnover. With these data, we test a model that links a series of attitudinal states to the intent to quit and ultimately turnover. Additionally, we assess the effect of job satisfaction on interviewers' job performance in gaining cooperation and in applying standardized interviewing techniques. ","Obviously, when an interviewer administers a survey questionnaire, the collected data will be used to learn about the surveyed population. What is not so obvious is that survey respondents learn from the experience of being surveyed. The National Health Interview Survey (NHIS) is an in-person household survey conducted by the National Center for Health Statistics (NCHS). NHIS interviewers, who are employees of the Census Bureau under contract to NCHS, repeatedly ask for more information about the survey, e.g., why certain survey questions are asked, how the data will be used, and how participating in the survey is beneficial. Interviewers want more information, both for themselves and to convey to respondents. Accordingly, NHIS interviewer training aims to arm interviewers with information and techniques to improve their interviewing and elicit respondent cooperation and accurate responses. This paper describes how NHIS interviewing has recently been improved and how NHIS respondents learn, from participating in the survey, about their own and their family's health, public health, the science of surveys, and government involvement in public health. ","Although interviewer-related variance and potential biases that arise when interviewers administer a questionnaire has long been studied, the role that interviewers play in obtaining contact and gaining cooperation is increasingly being explored. In this paper, we investigate the relationship between interviewer travel distance and contact rates, response rates, calls per complete, and hours per interview in two studies, the National Survey of Family Growth and the Health and Retirement Study.  Using call record paradata that have been aggregated to interviewer-day levels, we examine the number of trips and number of call attempts interviewers make to sampled segments and the distance interviewers travel to segments.  ","One of the main increasing challenges for Statistics Canada is to collect cost-effective data while maintaining a high level of quality. Paradata research has been useful in improving the current data collection processes and practices. The research carried out with paradata suggested that collection resources are currently not always optimally allocated with respect to the assigned workload and the corresponding expected productivity for computer-assisted telephone interview (CATI) surveys.  In this paper, models to predict the probability that a telephone call would result in a completed questionnaire as a function of time of day, and resources spent to date were developed. The parameters estimated from these models are used as input to optimize call scheduling. The potential cost savings of this approach is illustrated by applying the theory to a large scale household survey. ","Paradata research materialized at Statistics Canada in the fall of 2009 with the implementation of a Responsive Collection Design (RCD) strategy for two Computer-Assisted Telephone Interview (CATI) surveys: Households and the Environment Survey (HES) and Survey of Labour and Income Dynamics (SLID). RCD is an adaptive approach to survey data collection that uses information available prior to and during data collection to adjust the collection strategy for the remaining in-progress cases. RCD objectives are to monitor and analyse collection progress against a pre-determined set of indicators to identify critical data collection milestones that require significant changes to collection approach and to adjust collection strategies to make the most efficient use of remaining available resources. This paper provides an overview of the RCD used and describes the results obtained along with les ","In complex sampling designs a large part of existing bootstrap methods use simple random sampling with replacement in order to provide an unbiased estimator of the variance. When these methods do not take the sampling design into account, they provide biased variance-estimator. Resampled units usually need to be rescaled or weighted to correct this bias. Another set of methods consists of constructing artificial populations and to resample from them. These methods are very often time-consuming and have rounding problems. Moreover, when the sampling design is made of several phases, the implementation of these bootstrap methods becomes very difficult. We present new resampling method for two-phase designs that consists of resampling only a subsample of the units in the second phase. This subsample is selected randomly in such a way that it directly reproduces the appropriate variance, without rescaling or artificial population. The main advantage of the method is its simplicity as the after treatments like calibration or imputation for nonresponse can be directly applied on the bootstrap samples. That is why the proposed method is particularly worthwhile in real applications. ","Jackknife replication provides consistent variance estimators for finite population estimators only with careful adjustments to the jackknife replicate weights to incorporate finite population corrections (FPCs). This particular application from the National Assessment of Educational Progress has two stages of selection. We wish to incorporate an FPC for the first-stage of selection (schools), but not for the second-stage of selection (students).   A method is developed for doing this in a way which is simple, allowing for its use by analysts without the need for explicit factors attached to sums of squares in variance estimation (the necessary adjustments are fully incorporated into the weights). The approximation is conservative in that it slightly overestimates the true variance. The paper develops this approximation and applies it to State NAEP data from the year 2000.   ","Smith et al. (JRSS-D 2003) recognised that assessing change is one of the most important challenges in survey statistics. The primary interest of many users is often in trends rather than cross sectional estimates. Samples at different waves are not necessarily completely overlapping sets of units, because repeated surveys often use rotating samples which consist in selecting for each wave new units to replace old units that have been in the sample for a specified number of waves (Nordberg, J. Off. Statist. 2000). Moreover, surveys are usually stratified and units can be selected with unequal probabilities. In this paper, we propose a novel approach to estimate trends and its variance taking into account the effect of rotations, unequal probabilities, stratification and unequal probabilities. ","Fay and Train (1995) present a method called successive difference replication that can be used to estimate the variance of an estimated total from a systematic random sample from an ordered list. The estimator uses the general form of a replication variance estimator, where the replicate factors are constructed such that the estimator mimics the successive difference estimator. In the paper, we establish the conditions for successive difference replication to be equivalent to successive difference estimator. We also discuss the impact of using a subset of the replicates as an estimator instead of the full set of replicates that is needed for the successive difference replication estimator to be equivalent to the successive difference estimator. ","NAEP utilizes a two-stage design to select samples of school students in each state. Schools are selected with varying probabilities, via a stratified systematic sample. Students are selected with equal probability within schools. Jackknife replication is used for variance estimation, and the current procedure assumes that schools were selected with replacement. In many states the sampling fraction of schools is large, so that this approach overestimates the sampling error. We evaluated an approximate method for incorporating conservative fpc factors directly into the replicate weights. The evaluation of this approach involved three components: 1) implementation under tight deadlines; 2) the impact of the change, using historic data; 3) an evaluation of the biases and variances of the past and proposed approaches, using a population generated to simulate the NAEP grade 8 reading populati ","Administrative lists offer great opportunity for analyses that provide quantities for policy decisions. This is particularly true when groups of administrative lists are combined with survey and other data. To produce accurate analyses, data need to be cleaned and corrected according to valid subject matter rules. This paper describes methods and associated computational algorithms that, while often being easier to apply, are sometimes 40-100 times as fast as classical methods. This means that large administrative files can be cleaned (via modeling/edit/imputation) to eliminate contradictory or missing quantitative data to yield valid joint distributions, unduplicated within files, and matched and merged across files in a matter of days or weeks. ","This session's title is: Is New Emphasis on Administrative Data a Supplement to Survey Research or a 'Paradigm Shift'?    ","Administrative data have long been looked to as a way of sustaining or increasing the quality and interpretability of surveys and censuses, while controlling for the rising cost of data collection. In this paper we revisit this familiar topic proposing a \"pandata systems\" approach. Pandata are the purpose-driven data produced by integrating strengths of multiple data files that link administrative data to paradata and metadata. A modeling navigator is used which guides selection of variables and ties data fitness to purposes. We demonstrate the merits plus shortcomings of our pandata systems approach, with examples fit to purposes of public health, education and economic policy research, in striking a balance between cost and information quality. We assess indicators of information quality such as relevance, coverage, timeliness and comparability.   ","Administrative data such as patient records from hospitals, physician offices or insurance companies (eg. Medicare) have been used for decades in health services and clinical research. More recently these records - especially Medicare records - have been crucial in documenting differing care patterns and patient care outcomes.   With the challenges facing Federal large-scale health surveys, the use of health administrative data will have a greater role in health research. We have used health records to expand survey information from one point in time data into longitudinal data for applications including estimating the impact of risk factors on health expenditures and future disease. These records also provide the basis for comparative effectiveness research on treatment regimens.   These data sources will proliferate as electronic health records expand along with demand for population health data at states and communities. We'll explore how this proliferation of information and its multiple uses could influence the data collected by Federal surveys. And we'll also discuss new roles for data agencies in the creation, evaluation and harmonization of these data sources. ","Address-based sampling (ABS) surveys are gaining popularity relative to landline RDD surveys because they provide better coverage (or representation) of U.S. residential households and the people who live in them. ABS surveys typically entail a multi-mode data collection approach, and there is considerable flexibility to structure the approach. However, the use of multiple modes of data collection requires that the survey organization have a strict control on where sample cases end up. Paradata are data created during the data collection process and can be used to monitor sample performance, case flow, and efficiency of the survey design. This paper discusses the collection and the use of paradata (such as call records data and interviewer observations) to monitor and assess the performance of an ABS study targeting households with at least one child under the age of 13. ","Coverage is expected to improve greatly using ABS instead of RDD because addresses can be contacted via mail when a telephone number is not available, but differences by mode both in the characteristics of the population surveyed and in survey estimates are a potential result. This paper addresses these issues in the context of the Racial and Ethnic Approaches to Community Health across the U.S. (REACH U.S.) Risk Factor Survey. REACH U.S. is the cornerstone of CDC's efforts to eliminate racial/ethnic health disparities. It employs a multi-mode ABS approach using telephone, mail, and in-person interviewing to complete interviews during each year of the survey. Analysis of Year 1 data indicated that 34% of mail surveys were answered by cell phone only households. To further study the extent to which the population can be contacted by various data collection modes and examine potential mode effects, we will use Year 2 data to analyze how respondent demographic characteristics vary across modes and whether key health estimates differ by mode. Moreover, we will compare REACH U.S. health estimates to those of a benchmark survey such as the Behavioral Risk Factor Surveillance System. ","The 2010 Census Integrated Communication Program Evaluation (CICPE 2010) included three waves of data collection to understand attitudes, knowledge, and intent/participation for the 2010 Decennial Census. Each wave consisted of an address-based sample (ABS) in which we attempted to match all selected addresses to a phone number. The primary mode of data collection was by phone, but we also attempted an in-person interview for a subset of unmatched addresses as well as a subset of those addresses where the matched phone number did not lead to a completed interview. The design motivation was to save costs versus a full in-person survey while replacing the disadvantages of phone surveys with the advantages of in-person surveys on coverage and response rates.    ","Much discussion has occurred in recent years over the ability for ABS to improve coverage and, potentially, response rates. However, at what cost? And, how does ABS compare to telephone designs that include a cell sample? Much depends on a variety of factors such as response rates, target population, sample size, and modes of data collection. Here, we compare costs of the most common designs - ABS for Telephone, RDD-Landline, and RDD-Landline Plus Cell. We evaluate key variable and fixed costs with various assumptions made of the critical cost drivers. We seek to provide a base outline for which design is the most cost effective, while outlining other advantages and challenges of each design that should be evaluated with the cost. ","The 2009 Residential Energy Consumption Survey was unusual in that it required a massive expansion of the sample design while preserving the 2005 RECS design for future use. Many primary and secondary sampling units in the area probability design had two probabilities of selection, one from 2005 and another from 2009, but sufficient information to compute joint probabilities was not available. The probabilities and separate base weights were scaled and joined using a multiplicity adjustment to form consolidated base weights. Dummy housing unit records were introduced into the nonresponse adjustments to account for missing secondary sampling units. Defining cells for poststratification was a balance between sample size and variability in the ACS control totals. ","Survey research commonly encounters the problem of analyzing data that contains incomplete or missing information. Acock (2005) notes that this missing information can produce biased estimates, distort statistical power, and allow researchers to draw invalid conclusions. Fortunately, these risks can be effectively curbed in many situations with imputation. The imputation theory, developed by Rubin in 1987, has been proven to successfully estimate a parameter of interest, as well as accurately assess the variability of the estimate. The purpose of this paper is to test the use and applicability of the multiple imputation method PROC MI in SAS as a valid and robust missing data technique as compared to a multiple hot deck process. The Department of Defense Manpower Data Center (DMDC) applied these two imputation methods to the Federal Voting Assistance Program (FVAP) survey of Local Election Officials (LEOs) and analyzed their efficacy. The failure of the LEO survey data to meet the assumptions of PROC MI makes that method intractable. While the multiple hot deck method provides more plausible results, it struggles to incorporate complex logical relationships between questions. ","The Scientists and Engineers Statistical Data System is a comprehensive, integrated system of information about the characteristics of scientists and engineers in the United States. Hot-deck imputation is used to handle item nonresponse in each of the SESTAT component surveys. To maintain consistency across surveys, stepwise regression models are fit to determine control variables and the control variables are used to identify donors for hot-deck imputation. Many items in the SESTAT component surveys contain multiple, potentially-correlated questions, yet the current imputation protocol calls for the responses to these questions to be imputed separately, without accounting for the correlation structure in the outcomes. To evaluate the validity of this approach, we consider an alternative method for imputing multi-response items, where control variables are determined using the Sequential Regression Multivariate Imputation method, which sequentially fits regression models to account for correlations among the outcome variables. Imputed responses based on the current protocol are compared to those based on this alternative approach.  ","Inference based on data from surveys is often complicated due to incomplete data which can lead to bias in the results. Standard missing data procedures for regression models do not reflect possible interactions in regression relationships among variables. We consider a regression model with one fully observed covariate (either dichotomous or continuous) and one continuous variable whose values may be missing. We derive the conditional distribution of the missing covariate given the observed covariate and the outcome variable, and conduct a simulation study comparing the performance of imputation under this correct conditional distribution to that obtained using standard methods which assume multivariate normality of the covariates. Several imputation methods are compared in terms of confidence interval coverage, widths and bias, and the results suggest that imputing the interaction term without constraining it to be a product of covariates performs better than methods that preserve the relationship of interaction term as a product. ","To account for missing data, Rubin and Schenker (1986) describe a multiple imputation approach called Approximate Bayesian Bootstrap (ABB) Imputation, which is simpler and more direct computationally than Bayesian Bootstrap Imputation. Several Monte Carlo studies have investigated the properties of ABB and suggested improvements to the ABB procedure. This paper proposes an alternative to ABB for multiple imputation. We will empirically investigate the properties of the ABB alternatives and weighted sequential hot deck (WSHD) for multiple imputation when the missing data mechanism is ignorable and nonignorable. Two different approaches to WSHD will be explored. The first approach uses WSHD to multiply impute using the same donor pool. The second approach uses a two-stage process that selects, with replacement, a new donor pool from the original set of donors and then applies WHSD to the n ","Imputation for large scale study when data are missing not at random is generally difficult, especially when there are a large number of items with general missing patterns. This paper is aimed to investigate several pattern mixture models for such data. The patterns are determined by summarized latent information from response indicators assuming item response models. Both Bayesian models and sequential regression imputation methods were considered. Simulation studies based on such pattern mixture models were conducted for multivariate normally distributed data with different missing mechanisms. Performance of the pattern mixture models compared to models assuming data are missing at random was examined.  ","Economic programs conducted by the U.S. Census Bureau often use ratio imputation models to impute missing or erroneous values. These methods are designed to yield \"correct\" tabulation at the cost of failing to preserve the underlying distribution of the microdata. In this paper, we evaluate hot deck imputation for potential use in three economic programs. Hot deck imputation procedures classify units into disjoint groups based on variables assumed to be correlated with the missing values, then match donor values from respondents to nonrespondents within the classification group. We consider three different hot deck methods. To compare the statistical properties of the totals generated using each hot deck method, we conduct simulation studies with populations modeled from the sample data, contrasting the hot deck results to those obtained using each program's existing imputation methodolo ","This panel will explore reasons for the growing complexity and cost of conducting a census. It is intended to spark discussion about whether simplification is possible in a diverse country of over 300 million people in different types of geography. The discussion will address the following questions: What are potential operational designs for the 2020 Census and how could they reduce cost and complexity?; How does the number of response modes and operations increase the complexity of the census?; and Why is getting the last 5 percent enumerated so difficult, and is it necessary?. This panel will provide an overview of early planning of the 2020 Census, focusing on potential operational designs, including expanding self response modes, using administrative records for non-response follow-up, and moving to extreme automation and fewer operations. The cost versus benefit of minimizing versus ","Intra-interviewer correlation can arise when answers from survey respondents interviewed by the same interviewer are more similar to each other than answers from other respondents, decreasing the precision of survey estimates. Estimation of this parameter in practice, however, only uses respondent data. The potential contribution of variance in nonresponse errors between interviewers to this correlation has been largely ignored: responses within interviewers may appear correlated because the interviewers successfully obtain cooperation from different pools of respondents. This study attempts to fill this gap by analyzing a unique survey data set, which includes both true values and reported values for respondents and arises from a CATI sample assignment that approximates interpenetrated assignment of subsamples to interviewers. This data set enables the decomposition of interviewer variance in means of respondent reports into nonresponse error variance and measurement error variance across interviewers. We show that in cases where there is substantial interviewer variance in reported values, the interviewer variance may arise from nonresponse error variance across interviewers.  ","Commonly employed weighting methods to address nonresponse generally lead to reduced precision, spurring tradeoffs between bias and variance. No corrections for measurement error are commonly incorporated, while variances of survey estimates are typically penalized when ample auxiliary information is available to correct for nonresponse. Multiple imputation can be used for unit nonresponse, item nonresponse, as well as measurement error, making use of frame data, paradata, and survey data with varying levels of missingness. Unlike nonresponse weighting adjustments, it can reduce variances, thus also reducing mean squared error of estimates (MSE). This approach was applied to data from NSFG cycle 5, which includes a rich sampling frame, paradata, and replicate measures less prone to measurement error. Based on multiply-imputed data, measurement error bias was almost three times larger than nonresponse bias. Although the same conclusion was reached through weighting, multiple imputation yielded substantially lower variance estimates and estimates of MSE. ","The Programme for the International Assessment of Adult Competencies (PIAAC) is a multi-cycle international survey of assessment of adult skills and competencies sponsored by the Organization for Economic Cooperation and Development (OECD). PIAAC will collect background information and administer an assessment of cognitive skills to measure participants' general levels of literacy and numeracy. In PIAAC, as in any survey, it is a challenge to minimize potential survey errors, which may be due to such factors as the sample design or selection, the measurement instruments, data collection or processing problems, weighting and estimation difficulties, etc. Furthermore, in a multi-national survey such as PIAAC, there are challenges associated with the diversity of cultures, languages and other practices among countries. No single survey design will be effective for every country. Nevertheless, because of survey complexities and the possibility of different practices among countries, it is important to standardize the PIAAC survey procedures as much as practically possible. This paper presents PIAAC's standards and guidelines, and the preliminary results gathered through a field test. ","Statistical matching (also called data fusion) tries to combine information from different data sets by matching on those variables that are common to both files. Algorithms like nearest neighbour or Mahalanobis distance matching are routinely applied, but it is well known that they implicitly assume conditional independence of those variables that have not been jointly observed (called specific variables). In this paper, we discuss how to quantify the amount of uncertainty in the matching process by calculating bounds on distribution parameters of the specific variables. Data fusion might be viewed as a missing data problem, and we propose a regression imputation algorithm that creates different matched data sets with different feasible correlation matrices. Since several recent studies have used propensity score matching for combining different data sets, we will also discuss why propensity score matching is appropriate for the estimation of average treatment effects in the context of Rubin's causal model (where we have to deal with a different conditional independence assumption) but should not be applied in the data fusion setting. ","Multiple Imputation (Rubin 1978, 1987) is a generally accepted method for analyzing incomplete data sets. Missing values are 'filled-in' or imputed m&gt;1 times, thus creating m completed different data sets which are identical in the observed part, but vary over the imputed part. Most models and applications still focus on the imputation of continuous variables, and are usually based on normal distribution assumptions. However, survey data typically feature a large percentage of discrete data with non-normal distributions.  This work addresses the multiple imputation of such variables in missing-by-design patterns (e.g. data fusion), where 'blocks' of data are missing. We propose Predictive Mean Matching (Rubin 1986) for vectors of variables as described by Little (1988) in combination with a Bayesian Bootstrap to create multiple imputations.  An additional imputation step is needed, if the donor parts have missing values as well. This data situation can be seen as a mixture of missing-by-design with an overlaying ordinary item-nonresponse.   ","The structure of multiple imputation algorithms is well suited for incorporation in MCMC estimation algorithms providing the analysis of primary interest. This paper implements two approaches to approximate the full conditional distribution of missing values within a sequential regression setup. In the context of a panel data set of bone ages with missing data, simple parametric models are chosen to provide an approximation of the full conditional distribution. Robustness checks are provided documenting the adequacy of the proposed approach. The resulting imputation algorithm is adapted within a MCMC algorithm allowing inference incorporating the uncertainty of missing explaining factors. Alternatively, a non parametric approach is chosen to mimic the full conditional distribution for missing values within variables with nominal and ordinal scale. This approach is applied within a Binary Probit Model incorportating a measurement error for the dependent variable aiming at an analysis of unit non response. Out-of-sample forecast criteria are used to gauge adequacy of non nested model spefications. ","Using population representative survey data from the German Socio-Economic Panel and administrative pension records from the Statutory Pension Insurance, the authors compare three imputation techniques (hotdeck, regression-based, univariate imputation sampling) and Mahalanobis distance matching to complement survey information on net worth with social security wealth information from the administrative records. The unique properties of the linked data allow for a direct control of the quality of matches under each technique. Based on three evaluation criteria (correlation coefficient, average distance, kernel density plots) comparing the observed and matched benefit, we identify Mahalanobis distance matching to perform best. Exploiting the advantages of the newly assembled data, we include social security wealth in a wealth inequality analysis. Despite its quantitative relevance in Germany, social security wealth has been thus far omitted because of the lack of adequate micro data. The inclusion of social security wealth doubles the level of net worth and decreases inequality by almost 25 percent. Moreover, the results reveal striking differences along occupational lines. ","Data fusion techniques typically aim to achieve a complete data file from different sources which do not contain the same units. Traditionally, data fusion, in the US also addressed by  the term statistical matching, is done on the basis of variables common to all files. It is well known that those approaches establish conditional independence of the (specific) variables not jointly observed given the common variables, although they may be conditionally dependent in reality. However, if the common variables are (carefully) chosen in a way that already establishes conditional independence, then inference about the actually unobserved  association is valid. Unfortunately, this assumption is not testable yet. Hence, we treat the data fusion situation as a problem of missing data by design and suggest imputation approaches to multiply impute the specific variables using informative prior information to account for violations of the conditional independence assumption. In a simulation study it is also shown that multiple imputation techniques allows to efficiently and easily use auxiliary information. ","The goal of the National Children's Study is to improve the health and well-being of children and contribute to understanding the role various factors have on health and disease. Longitudinal in design, the Study will gather data on a wide range of environmental exposures believed to affect child health and development. Recently, an Alternate Recruitment Substudy was launched to assess the feasibility, acceptability and cost of study logistics and three recruitment models. Thirty sites have joined the original 7 sites to comprise this pilot, called the NCS Vanguard Study. This panel will provide a study update, relate experiences and lessons learned thus far, discuss the current Alternate Recruitment Substudy methods, and discuss data collection and analytic activities underway. A description of a unique data repository system designed to aggregate frequent deliveries of questionnaire, metadata, paradata and cost data for analytic and field monitoring purposes will be described. Specific challenges and potential solutions for linking and integrating aggregate and individual data from environmental, administrative and survey based data sources to the NCS survey respondents will also be discussed. ","We examined potential nonresponse bias in a new multi-mode national survey of mental health and substance abuse treatment facilities and its association with the response rate. We used data from the 2010 SAMHSA Survey of Revenues and Expenses (SSR&amp;E), which was linked to data from the census of substance abuse facilities in the 2010 National Survey of Substance Abuse Treatment Services (N-SSATS) and the census of mental health facilities in the 2010 National Mental Health Services Survey (N-MHSS). We compared a range of facility characteristics of respondents and nonrespondents to SSR&amp;E using chi-squared statistics. We also examined the nature and strength of the association between response rates and a set of independent characteristics using a multivariate logistic regression model. ","An increase in quality and detail of publicly available databases increases the risk of disclosure of sensitive personal information contained in such databases. The goal of Statistical Disclosure Control (SDC) is to provide information in such a way that individual information is sufficiently protected against recognition, while providing society with as much information as possible, and needed for valid statistical inference. One such SDC method is the Post Randomization Method (PRAM), where values of categorical variables are perturbed via some known probability mechanism, and only the perturbed data are being released thus raising issues regarding disclosure risk and data utility. A number of EM algorithms are proposed to obtain unbiased estimates of the logistic regression model after accounting for the effect of PRAM. The effect of the level of perturbation and sample size on the estimates will be evaluated, and relevant standard error estimates will be proposed. The  ideas will be extended to generalized linear models.    ","The status quo in survey sampling is publication and dissemination of survey results which are directly calculated as a function of the sample design and the survey inclusion probabilities. This process is appropriate for population parameter estimates for the case in which the only information known about the population of interest comes from the sample. In other cases, however, components of a system related to the population may be available from sources outside the survey sample. This external information may allow a deterministic inferential solution to the population parameters of interest which are targeted by the survey. For this reason, the survey results may be incongruous with external data and publishing the survey results leads to contradictory information. Such is the case with livestock inventory surveys collected by the National Agricultural Statistics Service (NASS) of the United States Department of Agriculture (USDA). This paper details a solution to this issue of incongruity through the application of a State-Space model system. ","The University of Minnesota's State Health Access Data Assistance Center (SHADAC) is investigating the feasibility of producing model-based uninsurance estimates for Minnesota counties. County level estimates of health insurance coverage are frequently requested data from the Minnesota Health Access Survey (MNHA), but the survey sample size is not large enough to support estimates for this small geographic area. We build on the work of producing small area estimates for Oklahoma (SHADAC 2009) and use research findings from the Small Area Health Insurance Estimates program to develop a methodology using Bayesian techniques to model Minnesota county level estimates of uninsurance. We explore using the American Community Survey, the MNHA, administrative data sources such as unemployment claims and Medicaid/MHCP enrollment data and correlations over time and geography of these sources for the model. We develop and test the model on the 2009 MNHA data with plans to implement with the 2011 data.  ","Small area estimation from stratified multilevel surveys is well known to be challenging because of extreme variability of survey weights and the high level of data clustering. These challenges complicate county- and state- level estimates of healthcare indicators such as proportions of visits with asthma and injury diagnoses at emergency departments (ED) from the National Hospital Ambulatory Medical Care Survey (NHAMCS). In this study, proportions of visits with asthma and injury diagnoses to hospital EDs were predicted by various multilevel logistic regression models and then aggregated to state level estimates. County level population covariates from the Area Resource File, hospital level covariates from Verispan Hospital Database and survey design information were used for modeling fixed effects. Aggregation of predicted hospital proportions to state level estimates utilizing the available number of ED visits to each hospital amounts to poststratification with cells defined at the state level. We evaluated models by comparing predictions with estimates based on administrative data from the Healthcare Cost and Utilization Project (HCUP) databases. ","In the Small Area Health Insurance Estimates program, we produce model based estimates of health insurance coverage for demographic groups within states and counties. In part of the model, we model survey estimates of proportions insured, conditional on the actual proportions. These survey estimates are bounded between zero and one, and have positive probabilities of being exactly zero and exactly one. Thus, assuming normality, or any continuous distribution is questionable. Many survey estimates are one, and some are zero because of high proportions insured and small sample sizes. To handle the boundedness and probability masses at zero and one, we have developed a \"three-part\" model. We model the probability that a survey estimate is zero, the probability that it is one, and its distribution conditional on not being zero or one. The models for probabilities of zero and one depend on th ","The National Health Interview Survey (NHIS), conducted by  the National Center for Health Statistics (NCHS), is  designed to  provide reliable design-based estimates for national and four major  geographical regions of the United States for a wide range of health  related variables.  However, state or sub-state level estimates are  likely to be unreliable since they are based on small sample sizes.  In this paper, we compare the efficiency of different area level  models in estimating smoking prevalence for the fifty U.S. states  and the District of Columbia using the 2008 NHIS survey data in  conjunction with a a number of potentially related auxiliary  variables obtained from the Area Resource File (ARF), a large county  level database compiled from different U.S. federal agencies. A  major portion of this study is devoted to the investigation of  various methods to estimate the sampling variances needed to  implement an area level model. In our data analysis, the  hierarchical Bayes method based on the random sampling variance  model appears to be have an edge over other area level models  considered in the paper. ","The National Health and Nutrition Examination Survey (NHANES) is an on-going survey of the United States population that collects health and nutrition information through medical examinations. The support of the sampled communities is beneficial in obtaining cooperation from sampled persons and setting up mobile examination centers in the area. To provide valuable information to the communities and extend the usefulness of the NHANES data, NCHS, the survey's sponsor, decided to support research to develop local area estimates. This paper describes the method used to produce estimates for the state of California. The method needed to address the challenges of producing state estimates from a national survey, combining multiple years of data from different sample designs, and allowing estimation of multiple characteristics of interest. To produce estimates for multiple characteristics, we developed an approach to produce weights for NHANES sample cases in CA during the period 1999-2006. We report on a quasi-design-based approach we developed to combine data from the non-self-representing PSUs that had been sampled under two different stratification designs.  ","High-quality state-level prevalence estimates of wireless-only, wireless-mostly, landline-mostly, landline-only and non-telephone households are essential when weighting data from random-digit-dial telephone surveys conducted at the state level. Currently these estimates are available from the National Health Interview Survey (NHIS) at only the national and regional level. We will present recent state and local model-based wireless and landline estimates, produced using a combination of January 2007-June 2010 NHIS data, 2006-2009 American Community Survey data, and 2007-2010 data on listed households from infoUSA.com. We discuss our methods and observed efficiency gains using our modeling approach, and we present graphs/choropleth maps indicating changes over time. Finally, we discuss limitations of our modeling approach and also possible future improvements. ","Direct estimates of the poverty level for more than 25% of the school-districts in the U.S., are generally not available from the American Community Survey (ACS) data that are compiled annually.  The Small Area Income and Poverty Estimates (SAIPE) program has depended on coalescing information from the decennial census, the Internal Revenue Service (IRS), from linking IRS records to Census geography, and from poverty estimates at the county level, to produce poverty estimates for every school district. Income data from the decennial census in particular played an important role, but are no longer available starting from the 2010 census.  This research is part of the Census Bureau's effort to consider alternative estimation procedures.  We consider Hierarchical Bayes models that combine information from the ACS, and from the IRS, to obtain annual estimates of most current and most relevan ","We propose and evaluate a Bayesian beta regression model for U.S. county poverty rates. Such a rate model could be an improvement to the Census Bureau's current small-area poverty approach of linearly modeling the logarithm of poverty levels. For small areas, some of which may have estimates of no poverty or all poverty, a zero-one inflated rate model can usefully account for estimated rates of 0 or 1. Using Bayesian computation techniques, we estimate the parameters of a zero-one inflated beta regression model. We compare the results to the Census Bureau's current small-area model for county poverty estimation. ","Annual Survey of Employment and Payroll estimates the number of federal, state, and local government employees and their gross payrolls. In the past two years, we developed the decision-based method to estimate the survey total. In this paper, we discuss some small area challenges when we estimate the survey total at the functional level of government units such as airport, public welfare, hospitals, etc. First, we introduce the synthetic estimation and modified direct estimators. Then, we modified the composite estimation as a weighted average between modified direct estimation and synthetic estimation. Finally, we evaluate these methods by using the 2007 Census of Governments: Employment Component. ","Accurate county (small-area) level estimation of crop and livestock items is an important priority for the USDA's National Agricultural Statistics Service (NASS). We consider an empirical best linear unbiased prediction (EBLUP) method for combining multiple data sources to estimate crop harvested area (and potentially other crop parameters) at the county level. This method assumes a linear mixed model that relates survey reported harvested area to both unit (farm) and area (county) level covariates, with variance components estimated using a technique which ensures strictly positive consistent estimation of the model variance. A parametric bootstrap method that incorporates all sources of uncertainty can be used to estimate variability parameters. Results of a study comparing the proposed EBLUP method with standard ratio and regression type estimators and a synthetic estimator for corn and soybeans in seven states in the Midwestern grain belt region of the US are discussed.   ","A small area estimation system is developed for the Behavioral Risk Factor Surveillance System.  The BRFSS is a state-based health survey but there is a need for county estimates.  The 2005-2009 American Community Survey PUMS is used to create a \"population\" of adults for each county.  Iterative probability adjustment is used to ensure that the county estimates agree with the direct tabulated \"official\" estimates from the survey for key state and sub-state domains. ","Unit or person-level ARC models with linear, logistic, and log-linear marginal mean functions are developed for small area estimation. ARC models take the form of a first order Taylor series approximation to the associated general linear mixed model. The area-level random coefficient vectors specify effects for demographic groups. Protection against nonignorable sample designs is provided by a hybrid solution that combines the marginal [probability (P) sampling plus ARC model (?) ] distribution of the fixed regression coefficients with the MCMC simulated Bayes posterior distributions for the small area specific random coefficient vectors. Survey weighted estimating equations are employed in the solution for the fixed and random coefficients along with sample design consistent covariance matrix estimators. A generalized design effect matrix is used to stabilize the area-level covariance matrices for the random coefficients. A simulation study for the logistic ARC model contrasts the new method's performance with a nonlinear version of You and Rao's (2003) pseudo hierarchical Bayes solution that discounts the effect of nonignorable samples on the mean squared errors of estimates. ","In small area modeling, estimation of second order parameters (such as variance components or correlation coefficients in time series or spatial models) is often challenging because the estimates may turn out to be inadmissible or unreasonable. Estimated variance components may become very close to zero or negative (in which case it is truncated to zero or modified to get a positive estimate) due to model misspecifications or due to large sampling errors. The resulting SAEs tend to exhibit over-shrinkage to synthetic estimates and may be far from the direct estimator. We propose quasi-BLUP estimation in which suitably pre-specified values are used for variance components for computing SAE but the MSE estimates are adjusted for using working values which may not be consistent. Empirical results in the context of Canadian LFS show that such estimates have desirable properties. ","Value-added models are used by many states to assess contributions of individual teachers and schools to students' academic growth. There is concern, however, that missing data can bias those assessments. A joint, correlated random effects model is developed that extends the generalized persistence value-added model (Mariano et al., 2010) to include missingness as a function of teacher and student covariates and latent effects. Computational issues are discussed and the model is applied to data from calculus classes at a large public university. ","The competitive federal Race to the Top grant program relies on standards-based state assessments to measure student achievement and school performance. In a standards-based test, high-performing students may achieve the maximum possible score, suggesting lost information on student learning and potentially penalizing their teachers. We develop a multi-membership Tobit mixed model to explicitly account for score ceilings in value-added assessment of teacher and school effects. A Monte Carlo expectation-maximization (EM) algorithm is implemented for model estimation. Simulation and analysis of data from the Arizona standardized assessment are presented to demonstrate the practical utility of the proposed methods. ","In the United States, schools chronically experience a shortage of qualified teachers. Ingersoll (2001) argues that this teacher shortage does not come from insufficient numbers of qualified teachers in the general population but rather from higher employee turnover in education than in other professions. Attitudes toward job conditions, such as job satisfaction, are found to be positively related to job performance (Judge, Bono, Thoresen, &amp; Patton, 2001) and negatively associated with employee turnover (Harrison, Newman, &amp; Roth 2006). This study applied the Generalized Graded Unfolding Model, a unidimensional parametric Item Response Theory (IRT) based unfolding model, to the teacher data collected by the Schools and Staff Survey (SASS), sponsored by National Center for Education Statistics. An index of job satisfaction was developed for each teacher in the dataset. This index provides an estimate of teachers' job satisfaction, and can be used to predict teacher turnover.  ","Item response theory (IRT) models (e.g., Lord, 1980), which are latent structure models in which manifest variables are polytomous and the latent variable/vector is polytomous or continuous, are often applied to scores on questions/items on standardized achievement or aptitude tests (Junker, 1993). Despite their long history, it is often not obvious how to rigorously assess the discrepancies between such models and observed data (Hambleton &amp; Han, 2005). This situation exists despite the fact that Standard 3.9 of the Standards for Educational and Psychological Testing (American Educational Research Association, American Psychological Association, &amp; National Council for Measurement in Education, 1999) demands evidence of model fit when an IRT model is used to make inferences from a test data set. Generalized residuals are a tool employed in the analysis of contingency tables to examine goodness of fit. The essential feature of these residuals is that a linear combination of observed frequencies is compared to its estimated expected value under a proposed model. We apply these residuals to IRT models. Their use is illustrated with data from operational testing programs. ","An experimental-design study of the effects of the Science Writing Heuristic approach to providing elementary science instruction on student science content knowledge and critical thinking skills was implemented in 48 elementary school buildings in Iowa, with cluster random assignment of buildings to treatment and control groups based on percentage of students eligible for free and reduced lunch, enrollment in third through fifth grades, and private vs. public status. Confirmatory factor analysis of Level-1 (student) and Level-2 (building) characteristics for enhancing child outcomes undertaken using Mplus software shows statistically significant (p&lt; .05) direct effects on students' Iowa Tests of Basic Skills results in mathematics, science, and reading comprehension from race, sex, free and reduced lunch eligibility, English language learner status, gifted and talented status, special e ","By utilizing Promoting Power as a school-level contextual variable, this study examines the relationship between school rates of on-time promotion and student matriculation. Specifically, this study uses a two-level hierarchical general linear model (HGLM) analytic design to identify significant student- and school-based factors that predict college enrollment behavior among U.S. Department of Education, National Center for Education Statistics' Education Longitudinal Study (ELS) of 2002 participants. In both the two- and four-year enrollment HGLM models, Promoting Power was found to significantly increase the likelihood of student matriculation, with the effect more pronounced for two-year enrollment. ","The National Center for Health Statistics (NCHS) conducts the National Hospital Ambulatory Medical Care Survey (NHAMCS) to measure utilization of ambulatory medical care service provided in non-Federal, non-institutional general and short-stay hospitals located in the 50 states and the District of Columbia. From its beginning in 1991, NHAMCS has collected data about sample visits made to hospital emergency and outpatient departments. In 2009, the survey also began collecting data about visits made for ambulatory surgery. In 2010, the survey universe was further expanded to include visits made to freestanding (not-hospital based) ambulatory surgery centers. This paper discusses the sampling design for the current survey. ","by Scott Meyer and Edward J. Chen, Statistics Canada    ","The Early Childhood Longitudinal Study: Kindergarten Class of 2010-2011 (ECLS-K) is the second longitudinal study of kindergartners sponsored by the National Center for Education Statistics. As with the 1998-99 cohort study, it will provide national data on children's characteristics as they progress from kindergarten through the fifth grade, and information on key analytical issues such as school readiness and transition from kindergarten to subsequent grades. Unlike the 1998-99 study where data were collected every other year after first grade, the 2010-2011 study aims to study kindergartners at every grade after kindergarten. In this paper, we discuss the sample design, describe school sampling frames, present procedures adopted to improve the school coverage, and discuss deviations from the 1998-99 sample design. The difficulty of implementing the sample - the recruitment of schools  ","The National Crime Victimization Survey (NCVS) is a major crime survey for the United States. The survey collects data on several types of crimes, including the broad categories of violent crime and property crime. The 2010 redesign of the NCVS can potentially improve the efficiency of the survey if the level of crime can be predicted well by external data. Previously, we reported initial success in predicting the level of crime at the county level based on the Uniform Crime Reporting (UCR) System. A more fine-grained analysis shows, however, far greater success for property crime than for violent crime. This paper extends the previous results to examine the underlying associations more thoroughly. We find that the largest single component of violent crime in the UCR, aggravated assault, fails to add to the predictive accuracy of a regression equation including the other violent crime components of the UCR, rape and robbery. We believe this finding has implications for interpretation of the UCR. We extend the analysis to include demographic characteristics from the census and the ACS, and we examine the ability of tract-level characteristics to predict individual victimizations. ","The National Compensation Survey is conducted by the Bureau of Labor Statistics to compute measures of the pay and benefits for America's workers. The current survey uses a three-stage sample design to select samples of areas, establishments, and jobs for which wage and benefit data are collected periodically over a five-year rotation. In recent years, several potential changes to this design have been explored to increase survey efficiency, adjust to budget changes, reduce respondent burden, and reduce design complexity. Design areas that have been studied include sample rotation, allocation, sample frame preparation, establishment selection, and sample initiation scheduling. This paper will update the discussion of these issues, describe the alternative approaches that have been explored, present results from the recent design research, and present the recommended changes to the general survey design. The work in this paper updates and significantly expands upon the work presented in 2010 JSM Paper \"Evaluating Sample Design Issues in the National Compensation Survey\". ","Probability proportionate to size without replacement (PPS WOR) sampling methods are customarily applied in sample designs for establishment surveys that have skewed populations with readily available auxiliary data for unit measures. More often than otherwise, systematic PPS procedures are used because such methods are easy to implement as well as being relatively efficient. However, there are alternative PPS sampling methods that are as efficient and relatively easy to implement that also afford the use of the Yates-Grundy variance estimator. This is demonstrated using the sample design from the International Price Program (IPP) at BLS. ","The National Health and Nutrition Examination Survey (NHANES) is an ongoing annual survey with PSUs selected for four years at a time. This process minimizes overlap in annual samples and allows for adjusting the design overtime to address changes in the national health concerns. A major design change for the 2011-2014 sample reflected the decision to oversample the Asian population to produce adequate sample sizes for given subdomains. In addition, the stratification scheme for sampling the PSUs was re-designed to ensure PSUs comprising annual and multi-year samples are distributed evenly in terms of health level, geography, urban-rural distribution, and population characteristics. The re-designed PSU stratification scheme was based on grouping the states by health-related measures. Cluster and factor analysis were used to form state groups using a number of state-level health indicators. Within each state group, NSR PSUs were further subdivided into substrata by geography, urban-rural distribution of population, race/ethnicity density, and poverty level. Various options of strata formation methods were performed and compared in the sample design to reach the design goal.  ","The Consumer Assessments of Healthcare Providers and Systems (CAHPS) surveys of Medicare beneficiaries are conducted initially by mailout-mailback of a paper instrument, with telephone followup of nonrespondents. Differences between mean responses by mail and telephone can occur due to a combination of mode selection (those responding by telephone differ from those responding by mail) and mode response effects (responses by telephone differ from those that would be obtained from the same respondents by mail). We identified these effects using an embedded experiment in which a random subsample of beneficiaries was approached first by telephone and afterwards by mail. Our analysis was conducted in a principle stratification framework, decomposing the universe of potential respondents into mail-only respondents, phone-only respondents, and those who would respond by either mode. ","To improve coverage and response rates, many surveys employ designs where the survey instruments are administered using a mixed modes such as Mail, Web, Telephone and In-person. In some designs, the modes are changed as a part of refusal conversion process. What is the appropriate approach to analyze data from such mixed mode designs? One may decide to ignore the modes, if the measurement properties are similar across the modes but, what if there are mode differences? How should one construct inferences for the population quantities based on the data collected from such survey data? This paper proposes a causal inference framework where we generate potential populations under each mode and then combine these potential populations to form a single inference. A Bayesian framework is used develop inferences and thus fully account for differences in the mode effects.The data from a longitudinal survey that used single mode design in first wave and mixed mode design in subsequent two waves will be used to illustrate the methodology. Data will be useful to highlight issues where modes may affect differentially across variables. The methodology is evaluated using a simulation study. ","Two primary objectives of this research are:1)to evaluate if respondents can be motivated to complete the survey in any mode (web, mail, telephone), and 2) can we further motivate completion by the lowest cost strategy-the web questionnaire. An experimental frame work was used to evaluate the role and effectiveness of mixed mode survey implementation in combination with other survey strategies that can impact response. While web based surveys have become increasingly more common in usage and in research, this methodology has not been thoroughly investigated for agricultural populations and for USDA sponsored surveys. The other strategies included variation of benefit appeals used in letters, length of questionnaire, and visual design of question presentation screens on the web. This study also demonstrates the constraint of keeping question presentation similar across modes and evaluates the visual design consideration of one question per web screen versus multi questions per web screen on respondent completion and item non-response. Experiments were carried out on an initial sample of 13,000 and were evaluated through multivariate analysis of sample variables. ","\"Piggyback\" surveys have at least two parts: data collected from an initial sample are used to spawn another sample. Designing the second sample so that it \"piggybacks\" on the first is typically much more efficient than other sampling approaches. For many piggyback surveys, data from the two parts are collected in different modes (e.g., face-to-face for one, telephone for the other). This presents a number of design and operational challenges. The same questions may be asked in both surveys; they may be designed in one mode, then adapted for the other mode without considering mode effects. Elapsed time between the specific respondent's interview in Sample A and the interview with the Sample B respondent he or she identified may also be a concern. If too much time elapses, the link between the two may be broken. The paper will discuss lessons learned from a number of experiences with thes ","Since 2005, Mathematica has conducted the Kauffman Firm Survey (KFS) for the Ewing Marion Kauffman Foundation. The baseline KFS survey recruited a panel of U.S. businesses which were founded in the same calendar year (2004) using a multi-mode web/CATI design. This group of businesses (the KFS panel) has been contacted annually for follow-up data collection since 2006. The main goal of the KFS is to investigate how new businesses are structured and funded in their early years, and to measure the changes in business financing and productivity over this period. With the use of this multi-mode design, much of the data collection in the follow-surveys has been migrated from CATI to the web, reducing the costs and respondent burden associated with extensive telephone follow-up efforts. This paper will explore the experiences of recruiting a panel of establishments through a multi-mode survey, as well as the technological improvements made over the course of the study. The paper will also examine the cost effects of increasing web survey data collection in the follow-up surveys. ","Case-control genome-wide association studies provide a vast amount of genetic information that may be used to investigate secondary phenotypes. We study the situation in which the secondary phenotype and genetic markers are dichotomous. We first prove that with disease rate is known, the inverse-probability-of-sampling-weighted (IPW) regression method is exactly the maximum likelihood estimation method using the full disease model. Those two methods are the most robust methods in term of guarding the possibility of interaction effect of genetic variants and secondary phenotype on the disease. When there is no interaction effect, the maximum likelihood estimation method with the no interaction assumption is the most efficient method. To strike a balance of the above methods, we proposed an adaptively weighted method that combines the IPW and MLE with reduced disease model. Our adaptively weighted method is always unbiased and has reduced mean square error for estimation with a pre-specified gene and increase the power to discover a new association in a genome-wide study when non-zero interaction is possible. Case-control study with known population totals is also investigated rega ","Data measured over time on a grid of discrete values collectively define a functional observation. Studies examining the acute health effects of pollution exposure are frequently interested in the prediction of a functional health outcome from a functional exposure measurement, such as airborne particulate matter or black carbon. We develop a wavelet-based historical functional linear mixed model which allows for the modeling of repeated measures of a continuous functional response and a functional predictor. We employ a novel use of wavelet-packets in a Bayesian setting to regularize the regression coefficient surface and force the historical time constraint, so that future values of the covariate function are not used to predict current or past values of the response function.  ","Traffic congestion and parking difficulties, have become a major concern to members of the University of Lagos (UNILAG). UNILAG has witnessed unprecedented growth in student's enrolment, in the last ten years or so culminating in the current total enrolment of more than thirty-five thousand students, of which about twenty-five thousand are undergraduates. In order to study the worrisome traffic situation, independent, though similar, sample surveys of undergraduates of the eight faculties on the main campus were conducted in 2007. The purpose of the surveys was to collect data on undergraduates who owned or used motor vehicles on campus. Further, to investigate possible temporal trends, the surveys were repeated in 2009. The types of data obtained from the surveys provided avenue for the application of Empirical Bayes (EB) analysis to estimate the proportions of students of individual faculties who used motor vehicles on campus roads during the periods under reference. The EB technique, being a Bayesian method, combines prior information and sample information in a manner that \"shrinks\" an EB estimator towards the sample estimator if a vague prior is used. ","This working paper presents six methods for constructing a confidence interval for the ratio of two Poisson rates: Wald's, Delta and Fieller's methods under normal approximation, Bayesian method under gamma priors, and Bootstrap methods (Hall's and Bootstrap-t). These methods are evaluated on simulation data via confidence interval width and coverage probability. Some recommendations are provided under simulation scenarios. ","It has recently been proposed that variation in DNA methylation at specific genomic locations may play an important role in the development of complex diseases such as cancer. Here we develop one- and two-group multiple testing procedures for identifying and quantifying regions of DNA methylation variability. Our method is the first genome-wide statistical significance calculation for increased or differential variability, as opposed to the traditional approach of testing for mean changes. We apply these procedures to genome-wide methylation data obtained from biological and technical replicates  and provide the first statistical proof that variably methylated regions exist and are due to inter-individual variation. We also show that differentially variable regions in colon tumor and normal tissue show enrichment of gene regulating gene expression, cell morphogenesis, and development, supporting a biological role for DNA methylation variability in cancer. ","Genome-wide association (GWA) studies of asthma and associated traits have identified numerous genes. A substantial portion of the heritability of these traits remains unexplained. Some variants, not detectable via main effects GWA study may manifest themselves only in interaction with other variants. To search for interacting genes involved in regulation of asthma associated traits (total IgE, eosinophils, FEV1, FVC, FEV1/FVC) we performed GWA epistasis screening in two family groups of asthma patients:CAMP (Childhood Asthma Management Program:814 cases and 467 trios) and CARE (Childhood Asthma Research and Education:796 cases and 338 trios) [dbGaP accession number phs000166.v1.p1.c1]. Individuals were genotyped with the Aymetrix 6.0 array. After quality control 574922 and 575010 SNPs in CAMP and CARE respectively, were tested with FBAT. No main effects genome-wide significant associations were found. We prioritized candidate pairs of SNPs for MB-MDR epistasis screening using Biofilter leading to 7632 SNPs for CAMP and 7603 SNPs for CARE. The most significant pair-wise interaction was identified between SNPs from loci 7p21.1 and 12q23.3 influencing eosinophil level in asthmatics. ","Recent studies show that rare variants play an important role in complex disease etiology. New technologies now allow generating exome sequencing data to detect association between common disease and rare variant mutations. Methods for association studies using common SNPs have low power when being used to to identify rare disease-causing variants due to their low frequency. Most current strategies proposed for detecting rare variants either group or collapse the variants within a region, such as genes or pathways. Although such methods have reasonable power in detecting causal variants, we find that they often have biased control of the Type I error rate. In this paper, we propose a new method to analyze rare variant association data for complex traits.The control of the Type I error rate and the power of the proposed method will be compared to the existing methods for a variety of underlying genetic models. ","A widely used design strategy in the study of haplotype-based genetic association is case-control studies. Numerous methods have been proposed to infer haplotypes and investigate the role of genetic variants in common diseases under simple random sampling (SRS). It is becoming common that complex sampling, which usually involves complexities like differential weighting and clustering, is utilized in genetic association studies. In this article, we have formalized a two-step prospective approach that applies to case control studies with complex sampling. At first step, we develop a weighted EM (WEM) algorithm to infer haplotype frequencies from unphased genotype data. At second step, we study the association of haplotypes and diseases by weighted logistic regression. Monte Carlo simulation studies are used to evaluate the performance of proposed method. It has been found that methods developed under SRS give biased estimates and overstate the significance level. In contrast, our method performs well, produce consistent estimates and maintain its significance level. ","The use of paradata, or data collection process information, to enhance survey adminstration, survey estimation, and the monitoring of survey data quality has become almost ubiquitous with recent advances in data collection technology. Unfortunately, little is known about the error properties of the paradata themselves, and the implications of these error properties for survey operations and the quality of survey estimates. The objectives of this roundtable discussion will be to 1) discuss current uses of survey paradata, 2) brainstorm methods for quantifying the error in paradata collected using a variety of modes, 3) discuss study designs for analyzing the implications of the measurement error for survey operations and estimation, and 4) identify   meaningful research agendas in this area.   ","The relationship of cancer and smoking is complex and not very clear. Further more, there have not been many studies exploring the relationship of smoking and cancer. In the present paper we investigate the relationship of cancer (breast, ovarian, uterus and cervix cancer) and smoking history of patients using the National Health and Nutrition Examination Survey (NHANES) data. Apart from smoking history we also studied other important factors known to be associated with cancer: body mass index, age, alcohol consumption and exposure to second hand smoke. Since NHANES data set uses a complex survey design, hence we used generalized estimating equation method to account for the stratification, clustering and unequal weighting. The results of the analysis suggest that smoking is positively associated with risk of cancer. The odds of having cancer are much higher in women who currently smoke or are ex-heavy smokers compared to non-smokers. These results were adjusted for alcohol consumption and age. The results of the present study confirm our hypothesis that cigarette smoking is positively associated with cancer among women. ","Sibling recurrence risk (SRR) is a measure of familial aggregation of a disease and is often used in family-based studies in genetic epidemiology to indicate the existence of possible genes conferring susceptibility of disease. Estimating SRR requires information about the disease status of sibships of families with affected children.  Since family-based studies are not usually random samples, estimates of SRR derived from these studies may be biased.  Probability samples of individuals obtained in surveys such as the National Health Interview Survey (NHIS) can be used to obtained unbiased estimators of SRR and its related SRR ratio (SRR divided by the prevalence of disease).  Two methods of ascertaining sibships of affected families are described and illustrated for estimating SRR and SRR ratio for diabetes from the NHIS.  Estimators of standard errors of SRR and SRR ratio are provided along with consideration of reporting error to compare the ascertainment methods.   ","The SUDAAN CROSSTAB procedure provides an adaptation of the Mantel-Haenszel estimator of a common odds ratio for complex survey data. The estimator is consistent for large-strata limiting models, in which the number of strata remains fixed and the sample sizes within strata increase to infinity. However, we show via simulation and counterexamples that the estimator is inconsistent for sparse-data limiting models, in which the number of strata (more appropriately termed as clusters) increases to infinity but the sample sizes within cluster remain fixed. We also propose an alternative estimator that is consistent for sparse-data limiting models satisfying a positivity condition, but not for large-strata limiting models. We compare the estimators with each other and with recent adaptations of conditional logistic regression for complex survey data. ","Analysis of population-based case-control studies with complex sampling designs is challenging because the sample selection probabilities (and, therefore, the sample weights) depend on the response variable and covariates. In this paper we propose a new semi-parametric weighted estimator which incorporates modeling of the sample expectations of the weights into a design-based framework. The estimator has a theoretical basis and is robust to model misspecification. We describe also the sample pseudo maximum likelihood estimator for inference from case-control data. This approach generalizes the Breslow and Cain (1988) estimator and it can be applied to the data from complex sampling designs, including cluster samples. We discuss benefits and limitations of each of the two proposed estimators emphasizing efficiency and robustness. We compare the final sample properties of the two new estimators and the existing weighted estimators in simulations under different sampling plans. We apply the methods to the National Cancer Institute's U.S. Kidney Cancer Case-Control Study in order to identify risk factors for kidney cancer. ","A binational surveillance system for the U.S.-Mexico border does not exist and reliable estimates for cervical cancer screening are not available for the region. We used comparable 2006 data from the BRFSS, a state based telephone survey, and the ENSANut, a state representative area probability sample and face-to-face survey, to estimate the prevalence of cervical cancer screening, and to describe associated factors in women aged 20-70 years without previous hysterectomy living in the border. The two surveys are independent and the target subpopulation for this study is contained in the combination of the populations targeted for the surveys. We considered each survey target population as a super stratum and then the data from the surveys were pooled together and domain type analyses were performed using SUDAAN. Among women aged 20-70 years without previous hysterectomy, 47% (95%CI=44.5-50.2) reported having a cervical cancer screening within the last year. Health insurance (AOR=1.9, 95%CI=1.5-2.4), marriage (AOR=1.8; 95%CI=1.4-2.3) and living on the US side of the border (AOR=3.5, 95%CI=2.8-4.4) were positively associated with having a cervical cancer screening.  ","In Sierra Leone, UNICEF is implementing and evaluating Community Health Volunteers strategy to treat Malaria, Diarrhea, and Pneumonia in children&lt; 5 years of age. As part of this evaluation, a baseline survey was conducted in 2010. A follow-up using the same sampling design and methodology is planned for 2012. This paper describes the sample design and preliminary analysis results. The design is a household cluster survey in two intervention and two control districts that were chosen based on pre-determined criteria. A cluster sample of 3000 households was then selected from each district using a probability proportional to size sampling scheme. Population-based interviews were conducted in person. Both direct and indirect mortality rates were computed. The results indicate that the intervention and control districts did not differ significantly in socio-economic factors, disease prevalence or health seeking behaviors. Mortality rates were lower than in previous surveys indicating declining rates of childhood mortality in Sierra Leone. ","Sampling households (HHs) based on geographic location for surveys in countries where such information is unavailable can be challenging. The use of Geographic Information Systems can help overcome this challenge. As part of a UNICEF project in Sierra Leone, to implement and evaluate Community Health Volunteers strategy to treat Malaria, Diarrhea, and Pneumonia in children under five years of age, a baseline survey of 6000 HHs was conducted. Global Positioning System (GPS) enabled mobile devices were used to enumerate HHs using GPS sample. The HHs were then selected using a simple random sampling scheme which was coded into the mobile device (e.g., PDA). The skip patterns for the survey were also programmed into the PDA, vastly improving quality control of the data. These data were then uploaded into an ACCESS database, making them immediately available for analysis. Another advantage of using PDAs was to examine whether the interviews were actually conducted at the selected household, by comparing the GPS recorded location of the household at enumeration and at the time of the interview. The survey was highly successful with very high response rates and good quality data. ","The study of hard-to-reach or otherwise \"hidden\" populations presents many challenges to existing survey methodologies. Examples of such populations in a behavioral and social setting include injection drug users, men who have sex with men, and female sex workers. Examples in a broader economic setting include unregulated workers, migrants, homeless, displaced peoples.    ","During the past three years, the National Agricultural Statistics Service (NASS) has made an effort to address, quantify and adjust for an undercount in the number of farms indication on its annual June Area Survey (JAS), which is based on an area frame.  This undercount is a direct result of the misclassification of agricultural tracts as non-agricultural.  The 2007 Census of Agriculture mailing list (CML) was evaluated as a potential source to assess misclassification on the 2007 JAS.  This evaluation revealed that the CML was a rich source from which to quantify the undercount of farms on the JAS.  However, the CML is only available every five years and misclassification on the JAS should be assessed each year.  Independently of the area frame, NASS maintains a list of agricultural operators, referred to as the list frame.  Yearly list-based samples are selected from the list frame.  In addition, the list frame serves as the foundation for building the CML.  The list frame is updated on an on-going basis and operators are categorized as either active or inactive. This paper discusses the feasibility of using the list frame to assess misclassification on the JAS. ","In recent years, the National Agricultural Statistics Service (NASS) evaluated a variety of approaches to adjust for misclassification in its annual June Area Survey (JAS), which is based on an area frame. This misclassification is a direct cause of an undercount in the number of farms indication produced by the JAS. One approach to correct for this undercount is to use NASS's sampling list frame, which is independent of the area frame. However, recent studies showed that there are farm status inaccuracies on the list frame. These are active records that are not associated with farms. If the list frame farm status inaccuracies are not addressed, the adjusted JAS number of farms indication could become biased upwards. Using Classification and Regression Tree (CART) models, the probability that a list frame record is active can be obtained. This paper evaluates methods for classifying each ","When the finite population of interest is dynamic, a common way to estimate change is to fix a target population as that at one fixed time, often the first wave. However, if the population steadily accrues new individuals, this requires either disregarding some data or using fluctuating weights. If individuals also leave the actual population, complexities increase. An important case is the Survey of Doctorate Recipients, a panel survey conducted by the NSF, that collects data on doctoral holders in science, engineering, or health. The goal is to estimate socioeconomic changes of the doctorate holders and to assess these across fifteen years. The major difficulties are that the target population has changed during the period of interest, with the addition of new degree holders and with loss or deletion of other subjects. Hence the usual setting for finite population inference does not occur; rather there are a set of different (albeit overlapping) finite populations through time. A superpopulation formulation addresses these difficulties. A hypothetical model generates the different finite populations; and the quantities of interest are the parameters of this superpopulation model. ","Responsive designs use data collected about the sample to inform mid-survey decisions, or actions, affecting the error properties of the resulting statistics. A key element of these designs is the decision rule, a function of the collected data to an action. Before survey practitioners can choose actions that will positively affect the statistics' error properties, they must understand the relationships in the data so that they are more informed about tradeoffs when choosing among actions. Responsive designs can be utilized in many settings, e.g., split questionnaires. A responsive split questionnaire would entail administering subsets of questions of a full questionnaire to sample members based on the data collected about the sample during an initial phase. We use data from the Consumer Expenditure Survey, a panel survey collecting information on the spending habits of consumers, to explore the extent to which information from the first interview, or initial phase, is related to events, e.g., purchases, in later interviews. With this information, we develop decision rules regarding which subsets of questions to administer to sample members at subsequent phases of data collection. ","Asking sensitive questions in surveys is prone to mode effects. In a randomized experiment we compare a single mode CATI to a mixed mode survey where respondents can chose between a CATI and Web-based mode. Our main focus lies on the consequences of the additional Web-survey option on data quality. Indicators of data quality are the respondent`s tendency towards social desirable answers, satisficing behavior and item non-response. In both modes identical questionnaires are used covering sensitive items such as health behavior and self-reported illness and including auxiliary variables e.g. about item sensitivity and affinity to the internet. However differences in means of sensitive items between single and mixed mode surveys can also be due to differences in unit non-response induced by the Web-option. Therefore we propose a decomposition method. It utilizes statistical matching techniques to estimate even without external validation data the relative strength of selection and mode effects. As the data collection will start in April 2011 we will be able to provide preliminary results by July. The project is funded by the German Robert Koch Institute (FKZ 1362/1-978). ","For three decades, the US Department of Defense (USDOD) has collected health related information through the Survey of Health Related Behaviors (HRB)among Military Personnel. For two decades, the US Air Force (USAF) has conducted the Air Force Community Assessment (AFCA)Survey. Through both surveys, estimates of a variety of behavioral and health related outcomes have been obtained to support top level policy makings and assess military readiness. These estimates have been compared across different service types and between military and civilian populations, but seldom across different surveys in different data collection modes for the same military populations at the same time. This study examines the extents that alcohol and drug use and suicidality prevalence rates may differ in different surveys and whether these possible differences are systematic, using the survey data of the 28,000 service members (including 7,000 air force active duty personnel) across 64 military installations worldwide collected primarily through on-site group sessions and from 80,000 Air Force active duty members across all major air force commands globally collected via web survey.  ","In many countries the proportion of the population with cell phones is higher than that with Internet access. This is particularly true of the college population. Given the recent attention focused on surveys of cell phone users, and the preference for using the Internet to survey college students, it is useful to compare these two modes of data collection. This paper reports on a mode experiment conducted in the 2010 Time Use Survey of students at Dongguk University in South Korea. The frame of registered students contains both cell phone numbers and e-mail address. A sample of students was selected from the list and randomly assigned to a CATI or Web survey mode. Students were notified of the survey in both modes using both text messaging and e-mail messages. Findings show that the cell phone survey has a distinct advantage over the Web survey concerning response rates, coverage of domains, and item nonresponse. Substantive differences between the two modes were found for about half the survey questions. This suggests that cell phone surveys may be useful to surveys in populations with universal or near-universal coverage, and where cell use may be more popular than Internet use. ","The National Ambulatory Medical Care Survey (NAMCS) is an annual in-person survey of office-based physicians and visits to their practices. Since 2008, a supplemental sample of physicians has received a mail questionnaire with NAMCS questions on electronic medical record (EMR)/electronic health record (EHR) systems.  In both survey modes, respondents could be either physicians or office staff. This paper compares how mode (mail or in-person) and survey respondent (physician or office staff members) affected estimates of EHR use in the 2008 and 2009 surveys. In both 2008 and 2009, the proportion of physician respondents was higher in the mail survey than the in-person survey.  In 2009 only, reports of overall use were associated with survey mode and respondent type.  After controlling for mode and respondent type in addition to quarter of the year, practice size, physician specialty, and urban location, the association between mode and EHR use remained significant though the association between respondent type and EHR use did not.  Estimates of whether the physician had basic and fully functional EHR systems did not vary by respondent type or survey mode. ","Combining data from a survey with register data using record linkage (RL) can lead to missing data and potentially to biased estimates, if survey respondents have to consent to it. Missing data (MD) techniques can be used to correct for potential record linkage bias. Based upon a survey where participants were asked permission for RL the performance of different missing data techniques is compared. For respondents who refuse their permission I set their survey answers to missing, creating pseudo-missing data. To correct for potential bias, OLS Regression is performed using complete case analysis (MCAR), multiple imputation (MAR) and Heckman's sample selection model (MNAR), respectively. Their performance is compared to a benchmark regression that is based on the complete data set. Several missing data scenarios are compared. Results indicate that when RL-bias was small, all missing data techniques performed well. In contrast, when RL-bias was high, only multiple imputation was able to correct for the RL-bias, given that only independent variables had missing values. With high RL-bias and missing values in the dependent variable, none of the MD techniques eliminated the bias. ","Data fusion combines data items from various sources based on a common set of variables. Using the synthesized database, researchers can overcome the limitations of a single-source dataset and answer important questions that cannot be addressed otherwise. We propose an integrated adaptive imputation approach to data fusion method. The proposed method can handle a mixture of continuous, semicontinuous and discrete variables in a robust manner in that no parametric distributional assumption is required for any variable in the data. Therefore the method is applicable to any distributional shapes and can adaptively and automatically generate suitable distributions for any variables to be fused. A simulation study is conducted and shows superior performance of the method as compared with prior approaches. We then apply it to a survey study on counterfeit. The analysis demonstrates that the proposed method can increase the efficiency and validity of data fusion and make data fusion more powerful.  ","The Millennium Challenge Corporation funded a development project in Mali to improve the Niono Goma Coura road. The U.S. Census Bureau has been enlisted to provide statistical support in evaluating the economic impact of this project. A road corridor in a similar area was chosen as the control to measure the economic benefits of the road improvement project. A paired sample design was developed for housing units within the respective areas being the sampling units for a survey on household finances. This paper addresses sample design aspects of the survey, including calculations of anticipated sample sizes, power, cluster sizes, village and household pairing, sort, and stratification. A particular focus of the paper is the contrast of designing and conducting surveys in developing countries such as Mali with surveys in the U.S. ","Group Quarters (GQs) are one of several types of living arrangements sampled in demographic surveys. They include college dormitories, group homes, and religious quarters. For first-stage sampling, individuals that reside in GQs are converted into Housing Unit Equivalents within a block to ensure that a unit of sample selected in a GQ corresponds to an average household (which in 2000 was 2.59 individuals). First-stage sampling is done at the PSU (groups of counties) level. Several demographic surveys are undergoing redesign to address new and continuing data needs. For this redesign, research was conducted to determine whether bed-level sampling should replace measure-level sampling by examining whether bed-level sampling would decrease the level of clustering within the sample while avoiding a significant increase in advance listing procedures in the field.  ","Rare and hard-to-reach populations pose significant challenges to the design and implementation of cost-efficient sample surveys. To find and enumerate these populations, multi-stage surveys are often used to avoid the construction of a sampling frame for the entire target population, and primary sampling units (PSUs) are selected with probability proportional to a size measure related to the population sizes in the PSUs. When multiple populations are of interest, composite size measures are used that are based on the population counts in the PSUs. Some composite size measures were described by Folsom, Potter and Williams (1987) and by Fahimi and Judkins (1991). The purpose of this paper is to discuss the advantages and disadvantages of these methods for various study populations and when to use these algorithms. We will demonstrate these size measures in surveys of students with disabilities, of persons receiving unemployment insurance compensation, and of persons in households receiving Supplemental Nutrition Assistance Program (SNAP, formerly called the Food Stamps program) payments and low income households not receiving SNAP payments. ","Historically for the demographic surveys, we selected a decade worth of sample after each decennial census and divided it into quarterly or monthly samples for interviewing. This raises the issue of how to capture housing units which are built after the sample is selected. In the past, these new construction units were captured by selecting \"placeholder\" units which were later matched to actual growth to determine which units would be in sample. We refer to this method as skeleton sampling. As we move to sampling from the Master Address File, we have an alternative method available, which is to directly sample new construction; this method of sampling mimics the sampling of the original units. In this paper we provide an overview of the two methods for sampling new construction, discuss the benefits and drawbacks, and evaluate each method's impact on variances and operations.  ","There are situations when a study requires a fixed sample size, either for contractual reasons or because the cost of collecting data for too many cases is prohibitive. This makes the preferred practice of oversampling and then adjusting for nonresponse impractical. Under certain conditions a simple random sample can be obtained by randomly sorting the frame and selecting the first n in the random order. This yields a fixed initial sample size, but a variable respondent sample. In a case where potential respondents beyond the targeted number of completes can be approached in sequential order (exhausting contact attempts before going to the next unit), the sampling process can continue until the desired number of completes is obtained. Nonresponse adjustments can then be made as if the combined set of respondents and nonrespondents constituted an initial sample. A similar approach to the  ","To gain insight into how characteristics of an establishment affect nonresponse,  a recursive partitioning algorithm is applied to the Occupational Employment Statistics May 2006 survey data   to build a regression tree.  The tree models an establishment's propensity to respond to the survey given certain establishment characteristics.  It provides mutually exclusive cells based on the characteristics with homogeneous response propensities.  This makes it easy to identify interpretable associations between the characteristic variables and an establishment's propensity to respond;   something not easily done using a logistic regression propensity model.  A linear representation of the tree model is used to test the model   obtained using the May data against data from the November 2006 Occupational Employment Statistics survey.  This test, on a disjoint set of establishment data, gives compelling evidence that the tree model accurately  estimates the response rate of establishments.  This representation is then used along with frame-level administrative wage data linked to sample data  to investigate the possibility of nonresponse bias.  We show that there is a risk that the nonresp ","The National Health and Nutrition Examination Survey (NHANES) was recently linked to Centers for Medicare and Medicaid Services (CMS) Medicare data.  Non-response bias from refusal or insufficient information for linkage needs to be considered when analyzing these data.  We compared statistical weight adjustments for non-response, based on the 1999-2004 NHANES respondents who could be linked and had data from the 2007 Medicare Denominator file.  Different weights were calculated using SUDAAN's WTADJUST procedure. Summary statistics for the different weighting methods are presented in addition to the estimated percent of beneficiaries enrolled in Medicare Advantage, using the different weights.  The weights and estimates did not vary among weighting approaches.  However the comparison of the different weighting approaches may differ by analysis variables, and respondent characteristics.   ","National data on teachers are limited to periodic sample surveys or to simple counts at the district or school level. In response to the need for individual teacher-level data, the US Dept. of Education, NCES developed the Teacher Compensation Survey (TCS), an administrative records survey that collects total compensation, teacher status, and demographic data about individual teachers from multiple states. Approximately 1.6 million teachers are currently in the data set, representing 50% of teachers in the United States. In 2007, NCES launched the pilot TCS data collection, with seven states volunteering to provide administrative records for school year (SY) 2005-06. The TCS expanded to 17 states reporting SY 2006-07 data, 18 states reporting SY 2007-08 data, and 23 states reporting SY 2008-09 data. It is anticipated up to thirty-five states will volunteer to participate in the TCS from 2011 to 2013.  This session provides an overview of the TCS data collection, a comparison of state administrative records with other sources of data, data availability and quality, limitations, and advantages of the TCS. This session also presents findings and descriptive statistics. ","When working with data from surveys or administrative records, one often encounters special issues in the estimation of prevalence rates and means for rare subpopulations. Two notable issues arise when some data are subject to misclassification, and when one obtains limited data for some of the subpopulations of interest. This paper explores the extent to which one may adapt responsive-design methods to produce improved estimators of both prevalence rates and subpopulation means. In this paper we propose an adaptive design and methods for estimating prevalence rates and subpopulation means under this design. We then provide a detailed simulation study focused on evaluation of bias and mean squared error for the proposed estimators under specified sets of conditions. ","Estimation based on data with non-ignorable missing covariates is  considered in this paper. We first consider the case where the  missing mechanism is nonparametric and the relation between the  response and the covariates is of a parametric regression form. We  propose a consistent estimator to the regression coefficient when  the covariates have non-ignorable missing values. We then show that  this estimation procedure can be applied to the case even when the  response also has non-ignorable missing values. The identifiability  and the asymptotic normality are also derived. Simulation studies  are conducted to show the finite sample performance of the proposed  estimators. A real data set from the National Health and Nutrition  Examination Survey is analyzed. ","The propensity score adjustment method is commonly used to adjust the bias that is due to nonresponse. We consider the propensity score adjustment method under nonignorable nonresponse. The method we propose does not use a full parametric distributional assumption, but it leads to consistent estimation of the parameters with some moment assumptions. We used the generalized least squares method to combine the observed information and compute an optimal estimator. Variance estimation is discussed, and results from limited simulation studies are presented to show the performance of the proposed method. ","In RDD telephone surveys, direct approaches to assess non-response bias are typically not possible because little information is known about non-respondents. Information linked to landline telephone exchanges do not exist for cell phone samples. An indirect way to evaluate non-response bias in landline and cell phone surveys is a level of interviewing effort analysis, assuming that high-effort respondents may resemble non-respondents. For example, converted refusal cases can be compared to non-refusal cases and respondents can be compared by number of calls made. The National Immunization Survey (NIS)-a nationwide, list-assisted RDD survey, monitors the vaccination rates of children 19 through 35 months. A national cell phone sample for NIS was conducted, targeting households with age-eligible children that only or mainly used their cell phones. This paper uses level of effort analysis to assess the potential non-response bias in the estimates of vaccination coverage in the NIS cell sample, and compares the findings with similar analysis of the NIS landline sample. We will also evaluate whether appropriately weighting the sample can be effective in reducing the potential bias. ","The IRS National Research Program conducts annual studies of individual taxpayer compliance based on a stratified random sample. As with most studies, not all of the selected taxpayers participate in an audit, resulting in nonresponse that could be due to various reasons including missing returns, unlocatable taxpayers, or taxpayers who never respond to any IRS correspondence. Historically, the IRS has adjusted the respondents weights using assumptions about the missing data mechanism based on the reason for nonresponse. This approach deals with one important dimension and addresses total noncompliance, the key estimate from the survey. However, some single line item entries such as self-employed business income or charitable contributions are of great interest to analysts as well and the adjustment may be less effective for these items. In our paper, we review the rationale for treating all nonresponse as missing at random. We then explore ways to adjust for the nonresponse bias when analysts are interested in a vector of estimates, not just one point estimate. Finally, we demonstrate that raking to multiple variables is very effective as compared to the traditional approach. ","In 1999, the Statistics of Income (SOI) Division of the Internal Revenue Service began collecting individual tax returns for SOI's 1999 Individual Tax Return Panel.  Longitudinal tax data is essential to study how the tax system affects taxpayers over an extended period of time. However, as in all panels, SOI's 1999 Individual Tax Return Panel is impaired by panel attrition. Previous papers have evaluated the presence and motivation for attrition in prior SOI individual tax return panels, yet there has not been research that has measured the nonresponse error caused by the attrition. In this research, I use an exploratory approach to estimate the nonresponse error in specific tax-related variables collected from SOI's 1999 Individual Tax Return Panel from 2000 to 2003. Then I use a propensity score method of subclassification to investigate if the nonresponse bias can be removed. My results show that the nonresponse bias in the tax-related variables can be reduced with the use of propensity score adjustments. "," Nonresponse rates have been used as a proxy for survey quality since they indicate the relative potential for nonresponse bias. Recently the R-index (Schouten) has generated interest in an alternative approach that better represents the potential for bias by focusing more on coverage than nonresponse. The patterns of nonresponse rates (e.g.; seasonal, time in sample) and the R-index can provide insight into the usefulness of nonresponse rates and representativeness. The current study uses different measures of nonresponse bias, nonresponse rates, and the R-index to see if there are patterns for bias and representativeness which might be different than for response rates alone. Two surveys, the Current Population Survey (CPS), and the Consumer Expenditure Quarterly Survey (CEQ) will be used in this analysis. ","Reweighting a sample using weighting class adjustments is one approach to deal with nonresponse. This approach uses a response model defined as a set of assumptions about the true but unknown response distribution that corresponds to the weighting class. A reweighted estimator is unbiased if the model coincides with the response distribution. However, in most cases, the response model will differ from the true response distribution. In this paper we examine the effect of reweighting when the model fails in stratified designs. The majority of results on model failure in nonresponse in the literature assume a simple random sampling. We expand this to stratified designs and compare the results with other approaches such as nonresponse adjustments that ignore sampling weights. ","We propose a new method of handling missing values in longitudinal data under the linear  mixed model assumption. The new method combines the complete-data linear mixed model  with benchmark equations that involve both the complete and incomplete data. Simulation studies  show that the new method improve the efficiency of inference when a significant proportion  of the data are missing. ","Clustered binary data with a large number of covariates have become increasingly common in many scientific disciplines. We consider a generalized estimating equations (GEE) approach to analyzing such data when the number of covariates grows to infinity with the number of clusters. This approach only requires the specification of the first two marginal moment conditions. The likelihood function does not need to be specified or approximated. In the first part of the talk, we consider an extension of the classical theory of GEE to the large n, diverging p framework. We provide appropriate regularity conditions and establish the asymptotic properties of the GEE estimator. In particular, we show that the GEE estimator remains consistent and asymptotically normal, and that the large sample Wald test remains valid even when the working correlation matrix is misspecified. In the second part of the talk, we propose penalized GEE for simultaneous variable selection and estimation. The properties of the penalized GEE are investigated in the ``large n, diverging p\" setting which allows the possibility of p&gt;n. . Furthermore, we propose an effective iterative algorithm to solve the penalized GEE ","We seek to identify the threshold value at which a real valued  function takes off from its baseline level, under regression and multiple dose-response setting. This is relevant to a broad range of problems, e.g., estimating the minimum effective dose level in certain dose-response models in pharmacology, detecting tidal disruptions in dwarf spheroidal galaxies, advent of global warming etc. An important case involves the baseline set having the form [0, d], the unknown d being the threshold. On this set, the function stays at its baseline value (minima or maxima) and then takes off. The approach involves fitting stumps to p-values obtained from tests conducted at different points/bins under the hypothesis that the function is at its baseline level. This works well owing to the fact that the p-values exhibit a dichotomous behavior. This problem has natural connections to change point estimation. The procedure is consistent under minimal conditions, involves at most one tuning parameter and is computationally easy to implement. It also attains the optimal rate of convergence under certain assumptions. The asymptotic distribution are derived and subsampling is also shown to work. ","It is well known that cancer incidence rates are heterogeneous across geographical regions. The aim of this paper is to supplement the existing tools for analyzing cancer rates from the Surveillance, Epidemiology, and End Results (SEER) database that are able to find the change points over time. Subsequently, the model can cluster the geographical subregions based on the magnitude and direction of changes of the disease risk. The proposed model to find change-points over time and cluster spatial locations is based on Dirichlet process priors where we consider temporal functions as the random quantities arising from the Dirichlet process prior. Through the analysis of age adjusted lung cancer mortality rates from 1969 to 2006, the proposed model nicely characterized local data features, namely, the local change points, the rate of changes, and clusters of states that exhibited similar trends of cancer incidence rates. The procedure to extend this model to include covariates therefore enabling selection of meaningful covariates are also discussed.   ","Inferring unknown gene regulation networks is one of key questions in systems bi- ology with important applications such as understanding disease physiology and drug discovery. These applications require inferring multiple networks in order to reveal the differences among different conditions. The multiple networks can be in- ferred by Gaussian graphical models by introducing sparsity on the inverse covari- ance matrices via penalization either individually or jointly. We propose a class of nonconvex penalty functions for the joint estimation of multiple Gaussian graphical models. Our approach is capable of regularizing both common and condition spe- cific associations without explicit parametrization as well as has oracle property for both common and specific associations. We show the performance of our nonconvex penalty functions by simulation study and then apply it to real genomic dataset. ","Quantitative biomarkers are emerging to discriminate between two clinically useful conditions. The measured biomarker levels are often corrupted with a random error, which leads to an ROC curve lower than that of the true levels of biomarkers. A solution for correcting such random errors is to repeat the measurements. Due to the limit of detection (LoD) of the instruments, the biomarkers are deemed to be immeasurable when the measured level is below some threshold. Parametric ROC methods have been proposed to analyze repeated measurements of biomarkers. However, parametric methods rely on a strong assumption that the data follows a normal distribution. We investigated a semi-parametric ROC approach that relies on a much looser assumption: the data are related to normal distributions by an implicit monotonic transformation. Maximum likelihood estimation is used to estimate the error-corrected ROC parameters and quantify the amount of measurement error. Extensive simulations show that our method is robust across a broad spectrum of experimental conditions including large measurement errors, substantial LoD, and deviations from the normal distribution. ","It is common in applied research to have large numbers of variables with mixed data types (continuous, binary, ordinal or nomial) measures on a modest number of cases. Also, even a simple imputation model can be overparameterized when the number of variables is moderately large. Finding a joint model to accommodate multivariate data with mixed data types is challenging. Here we develop a joint multiple imputation model with multivariate normal components for continuous variables and latent-normal components for categorical variables. Following the strategy of Boscardin and Weiss (2003) and using Parameter-expanded Metropolis-Hastings estimation (Boscardin,Zhang and Belin 2008), we use a hierarchical prior for the covariance matrix centered around a parametric family. This not only substantially reduces the dimension of the parameter space but also allows the data to depart from a tightly defined structured covariance matrix. The method is compared in several simulation settings to available-case analysis and a rounding method. ","Self-reported physical activity assessment instruments are prone to error yet systematic investigations into the structure of this measurement error are lacking. We propose a measurement error model for physical activity assessment instruments using physical activity level (PAL), the ratio of total energy expenditure (TEE) to basal energy expenditure (BEE). A physical activity questionnaire (PAQ) was administered to 451 participants aged 40-70 y in the Observing Protein and Energy Nutrition (OPEN) Study. MET minutes from the PAQ were used to estimate PAL. The main objective is to relate the PAL measurement from the PAQ to true PAL. Although true PAL was not observed, TEE was measured using an unbiased biomarker conforming to a classical measurement error model and BEE was estimated from an equation. Because BEE is estimated from an equation, it is prone to Berkson error. Therefore, the non-questionnaire measure of PAL has a mix of classical (TEE) and Berkson error (BEE). We present a measurement error model for PAL that accommodates this mixture of errors and use it to establish the relationship between the PAQ measure of PAL and true PAL and its application from the OPEN study. "," Donald Rubin pioneered the use of Bayesian multiple imputation for analyzing a wide variety of incomplete data. Specifically, the general location model was proposed and used to impute entire data sets. Desai and Sen (2006, 2008) developed a frequentist method for analyzing randomly incomplete data without imputation by characterizing the underlying Fisher information appropriately. However, there are situations where imputation is necessary. In this paper, we propose and demonstrate the use of maximum-likelihood-based multiple imputation. After briefly outlining the theory, we present simulations and an example. ","Frequentist approaches to making inferences about the variances of random cluster effects in hierarchical generalized linear models (HGLMs) for non-normal variables have several limitations. This paper compares and contrasts alternative approaches to making a specific type of inference about the variance components in an HGLM, focusing on the difference in variance components between two independent groups of clusters. A Bayesian approach to making inferences about these types of differences is proposed that circumvents many of the problems associated with alternative frequentist approaches. The Bayesian approach and alternative frequentist approaches are applied to an analysis of real survey data collected in the Continuous National Survey of Family Growth (NSFG). The primary analytic question of interest concerns differences in the variances of random interviewer effects between two independent groups of interviewers, which may indicate that particular subsets of interviewers are having adverse effects on the quality of the survey data. Inferences regarding differences in interviewer variance components are shown to vary depending on the approach taken. ","This article describes the use of multiple imputation to combine information from multiple surveys of the same underlying population. The basic proposal is to simulate synthetic populations from which the respondents of each survey have been selected. In this process, different sampling designs of the multiple surveys will be taken into account.  Once we have the synthetic populations, we could treat them as simple random samples with no complex sampling design features and borrow information across surveys to adjust for nonsampling errors or fill in the variables that are lacking in one or more surveys. Then, we can analyze each synthetic population with standard complete-data software for simple random samples and obtain valid inference by combining the point and variance estimates first across synthetic populations within each survey using the existing combining rules for synthetic da ","Survey data are often used to fit linear regression models. The values of covariates used in modeling are not controlled as they might be in an experiment. Thus, collinearity among the covariates is an inevitable problem in the analysis of survey data. Although many books and articles have described the collinearity problem and proposed strategies to understand, assess and handle its presence, the survey literature has not provided appropriate diagnostic tools to evaluate its impact on regression estimation when the survey complexities are considered. We have developed variance inflation factors (VIFs) that measure the amount that variances of parameter estimators are increased due to having non-orthogonal predictors. The VIFs are appropriate for survey-weighted regression estimators and account for complex design features, e.g. weights, clusters, and strata. Illustrations of these methods are given using a probability sample from a household survey of health and nutrition. ","Demand for small area estimates is growing among a variety of stakeholders who use these data to study issues affecting local communities. Statistical agencies regularly collect data from small geographic areas but are prevented from releasing small area identifiers due to disclosure concerns. Several disclosure control methods are used to disseminate microdata, including summary tables, suppression of geographical details, and Research Data Centers, but none of these methods is ideal for meeting the growing demand for small area datasets. This research tests a new method for disseminating public-use microdata that contains more geographical details than are currently being released. Specifically, the method replaces the observed microdata with fully-synthetic, or imputed, microdata generated from a posterior predictive distribution. A hierarchical Bayesian model is used to preserve the small area inferences and simulate the synthetic data. Confidentiality protection is enhanced because no actual values are released. The synthetic data is evaluated by comparing inferences obtained from the synthetic data with observed data from the 2005-2007 American Community Survey. ","In this research we develop and apply new methods for handling not missing at random (NMAR)nonresponse. We assume a model for the outcome variable under complete response and a model for the response probability, which is allowed to depend on the outcome and auxiliary variables. The two models define the model holding for the outcomes observed for the responding units. Our methods utilize information on the population totals of some or all of the auxiliary variables in the two models, but we do not require that the auxiliary variables are observed for the nonresponding units. We develop an algorithm for estimating the parameters governing the two models and show how to estimate the distributions of the missing covariates and outcomes. We investigate conditions for the convergence of the algorithm for parameter estimation and develop conditions for the consistency and asymptotic normality of the estimators obtained by the application of this algorithm. We also consider several test statistics for testing the model fitted to the observed data and study their performance. The new developments are illustrated using real data set collected as part of the Household Expenditure Survey.  ","As landline telephone coverage declines nationally, surveys are supplementing landline random-digit-dial (RDD) samples with cell phone RDD samples to minimize potential bias from non-coverage of wireless-only households. This may increase respondent burden (through added screening questions designed to ensure respondent safety and confirm cell phone eligibility) and decrease participation rates. While initially gaining respondent cooperation may prove challenging, there is evidence to indicate that those cell respondents who do complete eligibility screening are more cooperative than their landline counterparts. By examining data from the cell component of a large population-based health survey we see that once cooperation is gained and initial screening completed, cell phone respondents require less time to complete the survey - potentially indicating they are more cooperative than landline respondents once trust is gained. We compare interview duration of cell phone and landline respondents, looking at timings before and after eligibility determination, to ascertain if the shorter interview duration observed in cell phone interviews may be attributed to cooperation propensity. ","Over a five day period at a major sporting event, spectators were invited to use their mobile phone to respond to a brief survey on their experience. Options for mobile web, interactive voice recording (IVR) and SMS text messaging were given so a respondent could choose which way to participate in the survey. Participation is tracked and data differences by questions are examined to determine mode affects.     ","Based on Perceived Risk Theory, we hypothesized that privacy and time risks of Web surveys as perceived by respondents would have negative impacts on response rates. However, no tool for measuring respondents' perceived risks of Web surveys was found. The purpose of this study was to develop a questionnaire which could measure respondents' perceived privacy and time risks of Web surveys. Privacy risk was defined as \"potential loss of control over personal information or the responses of Web surveys\", while time risk was defined as \" potential loss of time when replying a Web survey or spending more time on follow-up Web surveys after finishing the current one\". The Questionnaire of Web Survey Perceived Risk (QWSPR) was developed and comprised 10 items. Survey data consisting of 620 college students' responses to QWSPR were randomly separated into two groups for cross validation of score validity: group one was used for exploratory factor analysis and group two was used for confirmatory factor analysis. Due to cross-loading problems, one item was deleted. Results indicated that a two-factor (privacy and time risks) structural model was validated. ","Perceived Risk Theory (PRT) has been applied to explain Web survey respondents' reply intention. Prior research found that respondents who perceived higher level of time risk of Web survey (i.e., potential loss of time when replying a Web survey or spending more time on follow-up Web surveys after finishing the current one) were less likely to reply to academic Web surveys. No study to date has investigated why respondents perceived different levels of time risk. The purpose of this study was to examine whether gender differentiates perceived the time risk of Web surveys. A subscale (4 items) of Questionnaire of Web Survey Perceived Risk (QWSPR) was utilized to measure 620 college students' time risk perception in a survey deployed in fall 2010. A multiple-indicator multiple-cause (MIMIC) model was used to assess the difference between male and female students' perceived time risk. Results indicated that female students perceived a higher level of time risk than male students. Our findings raised the issue of how to develop an effective strategy which could decrease female respondents' time risk perception in order to enhance the Web survey response rate. ","Perceived Risk Theory (PRT) has been widely applied to predict consumers' online purchase intention. This study employed PRT to explain low response rates confronting by Web surveys. The Questionnaire of Web Survey Perceived Risk (QWSPR) comprised of 9 items was utilized to measure 620 college students' perceived privacy risk and time risk. Privacy risk was defined as \"potential loss of control over personal information or the responses of Web surveys\" (measured by 4 items), while time risk was defined as \"potential loss of time when replying a Web survey or spending more time on follow-up Web surveys after finishing the current one\" (measured by 5 items). In addition, students were asked to evaluate their reply intention if they received an academic Web survey via email. Student characteristics were also collected. After controlling for students' gender, age, parents' education, family income, and major, results showed as time risk perceived by students increased, Web survey reply intention decreased. However, no effect was found for perceived privacy risk on reply intention. Future study is needed to investigate how to decrease respondents' perceived time risk of Web surveys. ","Statistical agencies are constantly making efforts to control the response burden of their household and business survey respondents. Statistics Canada's Survey on Employment, Payroll and Hours is no exception. This monthly business survey, which produces estimates and determines the month-to-month changes for variables such as employment, earnings and hours at detailed industrial levels for Canada, the provinces and the territories, currently manages response burden by making use of administrative data and by having rules that prevent establishments from rotating in the sample too soon after being rotated out. Recently, two new ideas to decrease even more the response burden for respondents to this survey have been studied. The first is to control the overlap of the samples from one month to the next by the use of the microstrata method (Rivi\u00e8re (2001)) in the sample selection process. The second is the increased number of establishments in the take-none strata. This paper will present the studies that evaluated the pros and cons of implementing each of these new features in the survey. ","As survey response rates continue to decline, incentives are widely used as a way to motivate sample members to respond. However, rather than simply being satisfied with increased response rates, it is also important to determine whether the use of incentives affects the quality of the resulting survey data. Several indicators of data quality that are widely used in survey research have not often been analyzed in the context of incentive experiments. For example, little is known about the effect that incentives have on the prevalence of satisficing behaviors, such as nondifferentiation or response order effects. Furthermore, the effect of incentives on the reliability of survey responses is not well understood. Using data from an incentive experiment included in a recent mixed-mode survey, this presentation will explore the effect of prepaid cash incentives on these, and other, indicators of data quality. Differential effects of incentives on data quality across modes and demographic subgroups will also be discussed.  ","The U.S. Census Bureau has the responsibility to release high quality data products while maintaining the confidentiality promised to all respondents under Title 13 of the U.S. Code. This paper describes a Microdata Analysis System (MAS) that is currently under development, which will allow users to receive certain statistical analyses of Census Bureau data-such as cross-tabulations, regressions (with diagnostic plots), histograms and scatterplots-without ever having access to the data themselves. Such analyses must satisfy several statistical confidentiality constraints; those that fail these constraints will not be output to the user. In addition, all analyses are performed after application of the Drop q Rule, a data subsampling routine, and regressions involving categorical predictors sometimes require modification. We describe the system's capabilities, as well as the confidentiality protections and the major types of attacks they prevent, then conclude with a description of other approaches to creating a system of this sort, and some directions for future research. ","Statewide Longitudinal Data Systems (SLDS) are intended to enhance the ability of States to efficiently and accurately manage, analyze, and use longitudinal education data, including individual student records. Since 2005, 41 states and the District of Columbia have received grants to develop SLDS, currently in varying stages of completion.     ","The authors developed and tested a non-parametric method of estimating the risk of disclosing information about whether known population individuals have cancers. This method matches cancer patients diagnosed in 2000 in a research data file from the National Cancer Institute's Surveillance, Epidemiology, and End Results (SEER) Program with individuals in Census 2000 Public Use Microdata Sample (PUMS) by county of residence and several common demographic key variables and estimates the proportion of patients who are unique in both files. Older racial and ethnic minorities residing in less populous areas are at higher risks of being identified. Both imputation methods produce conservative risk estimate and the magnitudes of upwards bias are 3-4 times. The bias tends to be greater for areas with larger risks. This research is the first attempt to systematically evaluate the risk of disclosing the attribute of whether an individual has cancer, and it builds substantial foundation for establishing routine procedures for assessing such risks using yearly updated microdata from the American Community Survey (ACS) in the future.  ","Public release of spatially-referenced microdata can entail  significant risk that motivated intruders will be able to learn the  identities of respondents who provide sensitive data. To mitigate  this risk, it is standard to aggregate data over large geographic  areas, which can degrade the utility of the data for legitimate  researchers. As an alternative, we propose methods to produce  synthetic sets of areal identifiers. Our goal is to simulate multiple  sets of data that--on average--retain the statistical properties of  the observed data, while protecting respondents' anonymity. We  propose methods to simulate areal identifiers using a multinomial  probit model. Because this results in a model that (in typical  applications) will have hundreds or even thousands of response  categories, we propose a sparse structure for the multinomial model.  Further, we suggest a simplified, latent Potts model structure for the  regression coefficients, which can help to preserve spatial  relationships. We demonstrate our methods on simulated and genuine  data. ","Hyperlinked copy of this abstract is available at URL http://mysite.verizon.net/vze7w8vk/AbstractJSM2011.pdf    ","Recognizing the importance of research to find improved ways of conducting censuses and surveys, the U.S. Census Bureau has set up a new Research and Methodology Directorate. Roderick Little, Associate Director for Research and Methodology at the Census Bureau, will describe the organization of the directorate, some of the key statistical problems to be addressed, and exciting employment opportunities arising from this initiative. ","Response rate is often used as an indication of measuring the quality of the survey response. However, it only tells one side of the survey story; the other side about the association between respondents and non-respondents are unknown. Researchers continue to seek the tools to assess and compare the quality of the response to different surveys. For example, introduced by Schouten et al. (2009), R-indicators are used to measure how well a respondent set represents the sample or population from which it was drawn. This measure may be a better indicator of survey nonresponse bias than response rates for survey outcomes closely related to auxiliary variables used for R-indicator calculation. In this roundtable, we will lead a discussion of the use of alternatives to response rates in measuring survey quality. ","The Federal statistical community has understood the benefits of linking survey data and administrative records for many years and some federal agencies have been sharing and linking survey and administrative data for some time.  Yet, other agencies often cite difficulties with obtaining access to administrative data or permission to link data for statistical purposes. One barrier is informed consent. Current law, regulation and guidance contain requirements regarding what individuals should be told about the purposes and use of data being collected. However, there is considerably less guidance on what individuals must be told when data are linked with other data sources for statistical purposes.  This paper provides some examples of notice that effectively allow sharing of administrative records for statistical research purposes, with a focus on how notice is provided an the language used by agencies.  It also includes a discussion regarding how some agency practices may ease the challenges associated with sharing administrative data for statistical purposes. ","Data matching is a common practice used to reduce the response burden of respondents and to improve the quality of the information collected from respondents when the linkage method does not introduce bias. However, historical linkage, which consists in linking external records from previous years to the year of the initial wave of a survey, is relatively rare and, until now, had not been used at Statistics Canada. The present paper describes the method used to link the records from the Living in Canada Survey pilot to historical tax data on income and labour (T1 and T4 files). It presents the evolution of the linkage rate going back over time and compares earnings data collected from personal income tax returns with those collected from employer files. To illustrate the new possibilities of analysis offered by this type of linkage, the study concludes with an earnings profile by age and sex for different cohorts based on year of birth. ","Linking survey data with data from administrative records is of interest for both substantive researchers (because it enriches the data) and survey methodologists (because it enables research into measurement error and selectivity of the linked sample). The German sample of the Survey of Health, Ageing and Retirement in Europe (SHARE) asks respondents for permission to link their survey data to individual records from the German Pension Fund. In wave 3 (2008) all longitudinal sample members were asked for consent to linkage via their social security number. In wave 4 (2010) an additional three-quarters of the refresher sample was asked for consent to linkage. In addition, an interviewer questionnaire was administered collect data that might explain interviewer effects on consent propensity. Our analyses look into differences in the selectivity of the linked samples across the two waves and the role of interviewers thereupon. ","This paper describes the background and methodology of a Census Bureau program under development to improve the American Community Survey (ACS) estimates of the group quarters (GQ) population for small areas. What motivates this work is that while the ACS GQ sample was designed to produce estimates at the state-level, the estimates of the GQ population contribute to ACS estimates of the total resident population for substate areas such as counties and tracts. Consequently, there are small geographies which either do not have GQ sample or have GQ sample that is not representative of the area, which can lead to distorted estimates of characteristics and/or total population for these geographies. The approach taken is to impute whole person records (and weight them appropriately) to GQ facilities which appear on the sampling frame but were not selected into sample. ","Each year the American Community Survey collects data about roughly two million housing units, 4.5 million people in the household population, and 150,000 people in group quarters facilities. We have begun developing automated statistical process control methods to uncover potential errors in the data. Several methodologies are being used to investigate responses from all three data collection modes (mail, Computer Assisted Telephone Interview (CATI), and Computer Assisted Personal Interview (CAPI)) using traditional Shewhart charts. For mail, we compare the individual responses of each question at various levels of geography. For CATI, we use similar methods, but also compare data for each telephone center. For the CAPI data collection mode we concentrated our initial efforts on Field Representative (FR) item missing data rates and compare each FR to all FRs within clusters of counties. ","The American Community Survey (ACS) produced its first nationwide 5-year estimates in 2010, using sample data from 2005 through 2009. With five years' worth of sample, the combined sample size in some areas would be large enough that a finite population correction (FPC) factor might have a noticeable impact on variances. This paper will discuss the methodology used to incorporate an FPC factor into the 5-year ACS variance estimates, and how the method was adapted to account for the subsampling of nonrespondents . Results comparing the impact on the variance of using the FPC across a broad spectrum of estimates and geographic areas will also be presented. Preliminary work indicated improvements in the standard error estimates of between two and four percent could be achieved. ","The Public Use Microdata Sample (PUMS) files contain records for a subsample of the housing units and persons of the American Community Survey (ACS) annual sample. A weighting process was introduced for the 2009 PUMS that expanded the raking matrix to include more demographic controls and family equalization with the goal of forcing more consistency between the PUMS and the ACS full sample estimates. This paper discusses the preliminary research, the trade-offs of doing the weighting at the state versus PUMA levels, and some of the impact on estimates of the new weighting procedure. ","Maps are a frequently used tool to portray the Census Bureau's data and highlight spatial patterns that provide context and significance for the characteristics displayed. Casual users of maps of statistical data may not look past what is interesting visually to analyze the underlying data that a map depicts. However, that does not absolve the mapmaker of the responsibility for informing users of the statistical qualities associated with the mapped values. The Census Bureau set new standards for communicating the statistical qualities of estimates from the American Community Survey (ACS) by including information on the level of sampling error (specifically, margins of error) associated with every ACS estimate. Now, efforts are underway to develop an operational tool that will make it possible for geographic information systems (GIS) users to communicate this information through map products as well. ","Longitudinal Data Analysis is used on American Community Survey data to model PSU level 1-year price change in the Consumer Price Index. The American Community Survey (ACS) is a Census product that provides accurate and current demographic estimates every year for statistical agencies uses. Historical modeling methods from previous CPI revisions will be examined while highlighting the benefits of this longitudinal modeling method. Goodness-of-Fit statistics are calculated while Backward Elimination is used to determine a final model. The resulting model will be used to provide variables to stratify Core Based Statistical Areas in the next revision of the Consumer Price Index.  ","The Consumer Price Index (CPI) program at the Bureau of Labor Statistics (BLS) is actively considering streamlining its CPI Estimation System to produce a more efficient and more flexible overall operation. This New Estimation System will entail the elimination of the replicate structure, which currently provides the necessary replicate values for the CPI's Stratified Random Group (SRG) Variance System. In order for BLS to continue using its SRG methodology, it will become necessary to create these needed replicates \"dynamically\" (by random assignment) each month from full sample values. In this paper, we will investigate and compare as well as produce the results from the use of dynamically constructed replicate price changes and compare these variance results with the currently computed CPI variances. At least two random methodologies for selecting the replicate values will be analyzed ","The Longitudinal Database of Quarterly Census of Employment and Wages (QCEW) links quarterly reports of employment and total wages of all nonfarm establishments covered by State Unemployment Insurance. QCEW uses two types of linkage: deterministic and probabilistic. Deterministic linkage is performed by SESA IDs - a combination of state FIPS code, Unemployment Insurance Number, and Reported Unit Number. The remaining establishments, those not linked by SESA IDs, are linked by a probabilistic method (weighted matching), based on the Fellegi and Sunter theory. This paper performs and evaluates both deterministic and weighted linkage. Recommendations for weighted linkage are given.  ","Kenneth W. Robertson, Joshua Duffin, Jennifer Kim  U.S. Bureau of Labor Statistics, 2 Massachusetts Ave. N.E., Washington, D.C. 20212    ","United States (U.S.) private firms as well as state and local governments and the federal government offer employer sponsored pension plans to their employees. In 2006, sixty percent of the U.S.workforce worked for an employer who offered one or more pension plans. Employer sponsored pension plans fall into three categories: a traditional defined benefit plan, an individual contribution plan or a cash balance plan. The U.S. does not require employers to offer pension plans to their employees. U.S. employers set their own conditions on qualification for pension coverage. Results can be used to understand differences between workers who are eligible to participate and those who are not. Results also draws upon the different types of pension plans offered by employers. This paper uses the 2001 and 2004 Panels of the Survey of Income and Program Participation (SIPP) survey. The paper will ex ","Research indicates that children of criminal offenders are at higher risk of experiencing adverse social, economic, and developmental effects than their peers. They are six times more likely to be imprisoned, and one-tenth of them will be incarcerated prior to adulthood. Traditionally interventions to mitigate those effects have begun during the post-conviction phase. This underscores the need for data collection and analysis during the pretrial phase to determine whether providing services earlier would yield equal or greater benefits than post-conviction services. This paper outlines children's economic, social, and cultural rights to create a baseline for studying how well those rights are maintained from a parent's pretrial supervision phase to post-incarceration. It then proposes a combined framework of survey and focus group data from agencies or programs to assess the experiences  ","Interviewer Performance, defined in this paper as the ability of an interviewer to contact and convince respondents, is generally assessed by survey research call centres in using descriptive measures such as the number of completed interviews, -the number of completed interviews per hour, etc. Other more comprehensive performance indicators such as the cooperation rate at first contact and Net Contribution to Performance Index have been developed over the past few years. However many factors might impact interviewers? performance in a centralized call centre environment. In addition to the interviewer?s characteristics and environmental factors, the type and portfolio of cases called, the effort already put into these cases, the time the call is made and the general productivity of the survey at the moment at which the call is made are some of these potential influencing factors. This p ","Quality control is paramount to survey operations. Two methods currently used at the U.S. Census Bureau to measure data quality are the Performance and Data Analysis (PANDA) system and Quality Control (QC) Reinterview. PANDA uses CAPI trace files, data files, and other case information as indicators of cases or interviewers that might be at risk for lowering overall data quality. The QC Reinterview is a verification interview with respondents that asks questions about the interview experience to detect falsification. This paper explores whether these systems capture the same cases and interviewers. Both systems are used on the National Health Interview Survey, a nationwide face-to-face CAPI survey sponsored by the National Center for Health Statistics. Using data quality results from 2009 and 2010, we analyze which sample units and interviewers are identified by PANDA, by QC Reinterview, or by both. We use a multinomial logistic regression predicting identification by PANDA, Reinterview, both, or neither to see if any sample or caseload factors predict identification. Our results suggest independence of the systems with some qualifications. We discuss avenues for further research. ","This paper discusses research using statistical process control with paradata obtained during data collection for the National Health Interview Survey (NHIS). Statistical process control (SPC) involves using statistical techniques to measure and analyze variation in operational processes. The goal with this approach is not to simply monitor, but to improve the quality of the process over time. For this paper, we group interviewers into statistical clusters based on census tract level housing unit and respondent demographic characteristics and produce control charts which examine the variation of the process over time for each cluster. We address advanced SPC techniques such as multivariate charting. Indicators of data quality used in the paper are item nonresponse and interview duration. The charts are intended to demonstrate how survey managers can use paradata to monitor the data collection process using SPC principles and techniques.  ","Creating new methods for reducing survey costs and errors is an increasingly important issue as survey response rates continue to decline. To reduce survey costs on the Health and Retirement Study, we developed a logistic regression model that predicts the likelihood of a sampled address completing the screening interview.    ","In interview-based household surveys, effective interviewer calling behaviors are critical in achieving cooperation and reducing the likelihood of refusal. This paper aims to analyze best times to gain cooperation from sample members in a range of face-to-face surveys. Of particular interest is to what extend the best times may vary with the type of household, such as a single household or a household with children. We use an unusually rich dataset, the UK Census Link Study, which combines paradata from six UK surveys, including detailed call record data, interviewer observations about the household and information about the interviewer-household interaction, which was linked to information about the household from the UK Census. The data have a multilevel structure with households nested within a cross-classification of interviewers and areas. A multilevel multinomial logistic regression approach which jointly models the different types of outcomes at each call is used to predict the likelihood of interview or refusal. The findings may have important implications for survey practice for determining best times of interviewer calls.  ","Covariate measurement error and missing responses are two typical features in longitudinal data analysis. There has been extensive research on either covariate measurement error or missing responses, but relatively little work has been done to address both characteristics simultaneously. In this talk, we propose a simple method for the marginal analysis  of longitudinal data with time-varying covariates, some of which are measured with error, while the response is subject to missingness. The proposed method has a number of appealing properties: assumptions on the model are minimal, including no assumptions about the distribution of the mismeasured covariate; implementation is quite straightforward; and the  applicability of the proposed method is broad. We provide both theoretical justification and numerical results of our method.    ","We present a framework for generating multiple imputations when the missing data are assumed to be non-ignorably missing. Imputations are generated from more than one model in order to incorporate uncertainty regarding what is the \"correct\" imputation model. Parameter estimates based on the different imputation models are combined using the rules of nested multiple imputation. Through the use of simulation, we investigate the impact of imputation model uncertainty on post-imputation inferences and show that incorporating model uncertainty can improve the coverage of parameter estimates. We also apply our method to a longitudinal depression treatment study. Our method provides a simple approach for formalizing subjective notions regarding non-response so that they can be easily stated, communicated, and compared. ","In longitdinal studies, interest often lies in estimation of the  population-level relationship between the explanatory variables and  dependent variables. In this talk we propose an empirical likelihood-based method to incorporate population level information in a longitudinal study with drop-out. The population-level information is incorporated via constraints on functions of the parameters,  and non-random drop-out bias is corrected by using a weighted generalized estimating equations method. We provide a three-step estimation procedure that makes computation easier. Several methods that are often used in practice are compared in simulation studies, which demonstrate that our proposed method can correct the non-random drop-out bias and increase the estimation efficiency, especially for small sample size or when the missing proportion is high. Also, the proposed method is robust to misspecification of the working correlation matrix or the missing data model under the missing at random mechanism. Finally, we apply this method to an Alzheimer's disease study. This is a joint work with Dr. Baojing Chen and Gary Chan. ","In randomized clinical trials, it is common that patients may stop taking their assigned treatments and then start the standard treatment or completely dropout from the study. In addition, patients may miss scheduled visits even during the study, leading to intermittent missingness. In this paper, we develop a novel Bayesian method for jointly modeling longitudinal treatment measurements under various dropout scenarios. Specifically, we propose a multivariate normal mixed-effects model for repeated measurements from the assigned treatments and the standard treatment, a multivariate logistic regression model for those stopping the assigned treatments, logistic regression models for those starting a standard treatment off protocol, and a conditional multivariate logistic regression model for completely withdrawing from the study.  We assume that withdrawing from the study is non-ignorable but intermittent missingness is assumed to be at random. Various properties of the proposed model are examined. An efficient Markov chain Monte Carlo sampling algorithm is developed. A real data set from a clinical trial is analyzed in detail via the proposed method. ","In survey sampling, a sample unit`s study variables are expanded to population totals by probability design based (DB) or model based (MB) expansions that implicitly treat a unit`s study variables as totals over entities called atoms contained in a unit. Expansions would make little sense if applied to unit statistics other than atom totals, for example, industry surveys where units are businesses, atoms are employees, and unit study variables are establishment totals for hours, wages, and number of workers. Other examples are household, mail, and ecological surveys. Woodruff (2010, 2009), derived Pre-sampling Model Based Inference from this atom structure, a structure that depends on probability sampling of population units and of atoms that comprise each unit. It provides estimates that retain the best properties of both MB and DB inference and that eliminate the main shortcomings of e ","When using model-based inference in survey sampling, many practitioners are unsure as to whether sampling weights should be incorporated into the analysis. Weighted estimates can protect against informative sampling bias, however they have a larger variance that increases null results. A common rule of thumb is to compare the unweighted and weighted analyses. If the estimates match then the analysis is \"correct\" and you can use the more optimal unweighted analysis. If the estimates do not match, then you have non-ignorable design or model misspecification. In this case, more care is needed in model building. If a model cannot be found where the weighted and unweighted analyses match, then the weighted analysis should be used. While this advice is easy to give and implement, it is not always accurate. Using a set of simulations on survey weighted mixed-effect models; I provide counter-examples to this advice. I also provide guidance on when this advice may be correct, and when it may not be correct. ","This paper considers the joint estimation of population totals for different variables of interest in multi-purpose surveys that make use of stratified sampling designs. When the finite population has a hierarchical structure, different methods of unbiased estimation are  proposed. Based on Monte Carlo simulations, it is concluded that the proposed approach is better, in terms of relative efficiency, than other suitable methods such as the generalized weight share method. ","We propose a new method for evaluating the mean square error (mse) of a possibly biased estimator, or, rather, the class of estimators to which it belongs. The method uses confidence intervals c of a corresponding unbiased estimator and makes its assessment based on the extent to which c includes the (possibly biased) estimator of interest. The method does not require an estimate, implicit or explicit, of the bias of the estimator of interest, is indifferent to the bias/variance breakdown of its mse, and does not require surety of the model on which it is based. ","The negative hypergeometric distribution (NHD) is of interest in applications of inverse sampling without replacement from a finite population. Thus, sampling is performed by randomly choosing units sequentially one at a time until a specified number of one of the two types is selected. Assuming the total number of units in the population is known but the number of each type is not, we investigate the maximum likelihood estimator (MLE) and an unbiased estimator for the unknown parameter. We use Taylor's series to develop five approximations for the variance of the parameter estimators. We then propose five large sample confidence intervals (CIs) for the parameter. Based on these results, we simulated a large number of samples from various NHDs to investigate performance in terms of empirical probability of parameter coverage and CI length. The unbiased estimator is a better point estimator relative to the MLE as evidenced by empirical estimates of closeness to the true parameter. CIs based on the unbiased estimator tended to be shorter than two competitors because of its relatively small variance estimator but at a slight cost in terms of coverage probability. ","Two-phase sampling is often used in a wide variety of surveys. Variance estimation from a two-phase sample has been a subject of active research. The re-sampling method of variance estimation has been used for this problem. However, the method confronts a challenging problem when the first phase sampling fraction is high. In the extreme (but not uncommon) case some first-phase strata are take-all, but there is subsampling at the second phase. This issue is studied theoretically and using simulation.   ","Cover letters for mailed survey forms can differ in a variety of ways. Previous research suggests that visual design can impact response, and that the effects might even be negative. Therefore, the purpose of this study was to continue this line of research and investigate the impact of a unique visual design for a cover letter, while holding the information content constant. Whereas the possible types of visual design changes are numerous, this study looked at the impact of only one type of approach (Information Mapping\u00a9), which has been shown to improve the usability of written materials in a variety of written materials. A sample of 1,000 addresses was randomly assigned to either an experimental or control condition (500 in each), and the response rate was analyzed after one mailing attempt. Results showed no statistically significant difference between groups in response rates (overall response was 27.6 percent). A secondary objective was to determine how many respondents would opt to use a simple Web-reporting option when one was clearly offered in the cover letter. Only 2.6 percent chose the Web option, with ten times as many choosing to respond using the mail. ","The National Health Interview Survey (NHIS) is one of the major data collection programs of the National Center for Health Statistics (NCHS). This survey has a complex design that covers an approximately 10-year sample design period, and was redesigned most recently with the 2006 NHIS. The actual design features multiple stages of sampling and weighting adjustments. Simplified and user-friendly procedures have been developed for both in-house and public-use design-based analyses for use with linearization-based methods. In particular, for NHIS public-use data, a standardized design, modified to prevent identification of sampled geographical areas and simplified to consist of two sampled clusters per stratum, has been provided. For simplified procedures to be acceptable to the NHIS-user community, estimates of standard errors produced from simplified structures should be close to those pr ","Due to school nonresponse, three school variables-school level, school enrollment, and urbanicity-have about 10 percent missing values in the 1987-88, 1990-91, and 1993-94 School and Staffing Survey principal data files. To create fully imputed files to meeting reporting standards on key variables, a combination of imputation procedures were used for these variables based on the availability of auxiliary information. The first imputation method was to use existing sampling variables - the school stratum codes for public and private schools to extract school level values. The second method was a cold-deck imputation using universe data (Common Core Data school files and Private School Universe Survey data) to fill in school enrollment values when possible. Thirdly, as a last resort, a set of survey variables was selected to predict imputed variables and regression imputation was used to fill the rest of the missing values on school enrollment and urbanicity. The utility of the imputed data was evaluated by comparing the original data files with the imputed files on the weighted cell counts, standard errors, and pairwise correlations and multivariate associations. ","More than 30% of the population has zero probability of selection in most landline RDD surveys due to the exclusion of cell phone only households, zero banks, and households with no phone service. This coverage error is problematic since methodological research indicates that cell phone only respondents are very different than landline respondents; thereby, converting coverage error into coverage bias. To mitigate the coverage error many telephone surveys have moved to a dual-frame design consisting of both cell phone and landline numbers. For surveys that implement a repeated cross-sectional design, comparing estimates of population parameters before and after the change from a landline RDD to a dual-frame RDD can be problematic because differences in estimates might be a consequence of the different sampling frames. This paper presents methodology used to adjust for bias caused by the exclusion of cell phone only respondents; thereby, enabling the comparison of estimates from repeated cross-sectional studies that change methodology from exclusively sampling from landlines to sampling from both landlines and cell phones.  ","Estimates of health measures within a population can often be modeled by order-restricted population parameters. Population subdomains based on gender, race or a time period often define order relations for selected disease prevalence among the subdomains, for example, interventions have decreased smoking prevalence over time. With complex survey data such estimation and inferences are typically made using design-based (general) linear models or direct-estimate cell-mean models. These models are easy to implement, but have possible drawbacks including linear model distortion of the true monotonic nature or violation of hypothesized orderings by the direct estimate orderings when using a \"small sample\" cell-means model. As an alternative to the standard methods, isotonic regression techniques can be used as a basis for order-preserving estimation and inference. These techniques avoid the drawbacks of the standard methods mentioned. Data from the 1997-2006 National Health Interview Survey will be used to estimate parameters for hypothesized total and partial orderings for select health statistics. Comparisons of the standard and isotonic methods will be presented and discussed. ","Iraq is an extremely difficult environment for conducting survey research. Security concerns coupled with the lack of recent census population parameters present difficult challenges to sample design. Obviously, there is a need for accurate sampling in Iraq as policy-makers and analysts attempt to shape Iraq's future. D3 Systems is currently exploring innovative approaches in sample design and implementation to overcome these challenges in Iraq.     ","In household panels, typically all household members are surveyed. Because household composition changes over time, so-called following rules are implemented to decide whether to continue surveying household members who leave the household (e.g. former spouses/partners, grown children) in subsequent waves. Following rules have been largely ignored in the literature leaving panel designers unaware of the breadth of their options and forcing them to make ad hoc decisions. In particular, to what extent various following rules affect sample size over time is unknown. Such knowledge is important because sample size greatly affects costs. We find that household survey panels implement a wide variety of following rules but their effect on sample size is relatively limited. ","The New York City (NYC) Community Health Survey (CHS) is a telephone survey of adults conducted annually since 2002 by NYC's Department of Health and Mental Hygiene. The CHS collects approximately 9,500 interviews a year describing a wide variety of health outcomes and behaviors and is extensively used in public health planning and evaluation. A recent study of the items of the Kessler-6 non-specific psychological distress scale observed a distinct day-of-week pattern. Because the CHS is collected fairly evenly across days there is minimal impact on estimated trends. However, both variance calculations and measures of association with other variables may be affected if they also exhibit day-of-week patterns. To investigate this, we quantified day-of-week, time-of-year, and meteorological effects on various survey items including health status, physical activity, fruit and vegetable consumption, and drinking and sex behaviors. Though effects tend to be mild, they may alter estimates especially for smaller subpopulations suggesting a need to explicitly address the issue and this may have important ramifications for public-use microdata releases that do not include such information. ","The fraction of missing information (FMI) has recently been proposed as an alternative to the response rate for monitoring the quality of survey data. In order for the FMI to inform data collection decisions (e.g., in adaptive designs or for future survey waves), it must be decomposed into the relative contributions of individual auxiliary variables. We investigate these relative contributions in two ways. First, under some simplifying assumptions, we analytically decompose FMI to show the impact of each covariate. Secondly, we discuss several methods for estimating these relative contributions to FMI in practice, and contrast with alternate quality indicators. The methods are illustrated using data from the National Health Interview Survey (NHIS). ","Since 2002, the Oklahoma Dental Health Services have surveyed the dental health of the third grade population to assess the prevalence of caries and sealants. To compare different sampling methods, we simulate a population based on estimates from the previous 6 years of the Oral Health Needs Assessment and the current school percentage free and reduced lunch (FRL) report. Sampling methods include regional random sampling (current method), regional systematic sampling and state-wide systematic sampling. All samples will be of 36 schools with varying school sizes and selected after randomization by FRL percentage. For the two regional methods, the state is divided geographically into 6 regions and 6 schools selected from each region. Preliminary data indicate small differences from the population values when the complete sample is available. When the response rate decreases, both the estimates and variability are generally larger for the weighted estimates. Additional comparisons will focus on variations in the response rates and estimates obtained with and without weighting. Non-response adjustment will be explored. ","This study presents results from a new non-verbal response card method for obtaining more accurate responses to questions about sexual knowledge, attitudes and behavior in the context of interviewer-administered questionnaires. The effectiveness of the cards was tested in two random samples of young people ages 13-24 in southwest Ethiopia (n=201 and n=1,269) using a randomized control trial design in which one-half of the sample used the response cards and the other half of the sample provided verbal responses. The non-verbal response card method produces estimates of the prevalence of pre-marital and extra-marital sexual intercourse that are around twice as high as the estimates provided by the conventional verbal response method. We also found that estimates of the percentage of youth who knew where to obtain condoms were approximately 22 percent lower among youth who used the more private and confidential card method as compared to the verbal response method. Results from two rounds of a longitudinal survey of youth in the same area provide additional evidence of the internal reliability of the card method. The non-verbal response card provides a more private and confidential me ","In cross-national sample surveys like the ESS, a huge variety of sample designs is often applied in participating countries. In order to achieve estimates of comparable precision, the samples drawn according to these different sampling schemes must be of equivalent effective sample sizes, n_eff = n/deff, where n is the net sample size and deff is the design effect. As deff, among another parameter, depends on the average cluster size b, increasing the number of sampled clusters, ceteris paribus, decreases the design effect and hence increases n_eff. The presentation will show that, at a given linear cost structure (costs per interview and costs per sampled cluster), there exists an optimal number of clusters to sample so that a pre-defined effective sample size is exactly achieved - at minimum total costs. ","Composite measures of size can be used to select primary sampling units in a two-phase design such that multiple subdomains are self-weighting. This method can be generalized to situations where the PSU probabilities are prescribed so that the sample allocations to domains within PSUs are adjusted to achieve self-weighting domains. The method is illustrated for an area probability sample of housing units. ","After every decennial census, many surveys including the Consumer Price Index (CPI) and the Consumer Expenditure Survey (CE) redefine their primary sampling units (PSUs), which are sets of contiguous counties. Since the CE survey is used to weight the CPI, the two surveys use a common set of PSUs. There are two types of PSUs: self representing and non-self representing PSUs. Self representing PSUs are selected with certainty, whereas non-self representing PSUs are grouped into a stratification PSU and one PSU is randomly selected to represent the stratification PSU. To minimize survey variance, the stratification PSUs should be homogeneous and have approximately equal populations. This is a constrained clustering problem and is solved using heuristic algorithms. This paper presents a new heuristic solution procedure that uses a \"pseudo\" assignment algorithm to assign PSUs to a stratifica ","Dual frame RDD designs of landline and cell phone numbers to eliminate the coverage bias due to cell-only populations have been popular in many countries, but they are subject to the overlap and overrepresentation problem. One of the solutions to the problem is to use a compensatory weight for unequal probabilities of selection, but weighting is often inappropriate because of lack of information on actual phone usage. We consider a two-stage procedure, illustrated by Lepkowski and Kim (2005), different from a conventional within-household selection. First, with the assumption that both landline and cell numbers are 1) for the household, 2) shared, or 3) personal numbers, a few questions are used to identify such status from the informant for each number selected from the frame. Second, if the phone number is for the household or a shared number, one eligible person using the number is randomly chosen and asked to provide the information on other phone numbers that could have been reached to the person. This approach was successfully applied to a national survey in Korea. Several person-level weighting strategies based on phone ownership data from the survey are examined. ","Area household surveys conducted in the United States most often rely on data from the Census Bureau to calculate measures of size (MOS) of secondary sampling units (SSUs or segments). Yet, late in the decade demographic or housing data from the last decennial census are likely to be inaccurate in local areas with considerable growth or demographic shifts since the census taking and intercensal estimate are not available at the required level. Address lists available from the United States Postal Service (USPS) have been incorporated into survey sample designs in various capacities over the past decade: telephone surveys are using these lists as a first phase of selection; in-person area surveys are using them in place of the traditional address listing process; and mail surveys are becoming much more prevalent. In all these instances, the lists are being used as sampling frames. This pa ","A representative study sample drawn from a probability-based Web panel, after post-stratification weighting, will reliably generalize to the population of interest. Due to finite panel size, however, there are instances of too few panel members to meet sample size requirements. In such situations, a supplemental sample from a non-probability opt-in Internet panel may be added. When both samples are profiled with questions on early adopter (EA) behavior, opt-in samples tend to proportionally have more EA characteristics compared to probability samples. Taking advantage of these EA differences, this paper describes a statistical technique for calibrating opt-in cases blended with probability-based cases. Using data from attitudinal variables in a probability-based sample (n=611) and an opt-in sample (n=750), a reduction in the average mean squared error from 3.8 to 1.8 can be achieved with ","In this paper we consider the problem of how to allocate a sub-sample of non-respondents from an initial sample for follow-up, when we want to estimate totals or means for domains.  We consider different scenarios for what is known.  At one extreme, we may know the domain of every unit in the initial sample, whether it responds or not.  Alternatively, we may only know the domain of the respondent units, but have some auxiliary information such as the domain sizes.  We assume that follow-up is intensive, so that all of the non-respondents in the follow-up sub-sample will become respondents, although this is unrealistic in practice.  We then derive expressions for the variance which we use to find a follow-up allocation that satisfies CV criteria for domains of interest.  Finally, we show how the assumption of complete response to the follow-up can be relaxed. ","The National Survey of College Graduates (NSCG) is a source of detailed statistics on the nation's science and engineering labor force. The Census Bureau conducts the NSCG on behalf of the National Science Foundation (NSF). Historically, the NSCG selected its sample once a decade from the decennial census long form respondents. In 2010, the Census discontinued the long form, so the NSF switched to using the American Community Survey (ACS) as a sampling frame for the NSCG. This switch brought both opportunities and challenges. The ongoing data collection methods of the ACS offered the potential for improved coverage of the NSCG, and the inclusion of a new field of degree question on the ACS allowed for a more efficient sampling effort. Meanwhile, the ACS also introduced increased weight variation and complexity. This paper provides insight into our efforts to address the challenges of an ACS-based sampling frame by discussing the 2010 NSCG sample design issues, including the determination of an appropriate sample selection technique, the incorporation of an iterative sample allocation approach, and the identification and resolution of obstacles encountered during sample selection. ","The National Survey of College Graduates (NSCG) is the nation's only source of detailed statistics on the science and engineering labor force. Historically, the NSCG selected its sample once a decade from the decennial census long form respondents. In the 2010 NSCG survey cycle, the NSCG began using the American Community Survey (ACS) as the sampling frame for the NSCG. After considering numerous sample design options proposed by the NSCG survey sponsor, the National Science Foundation (NSF), and reviewed by the Committee on National Statistics (CNSTAT), the NSF approved the use of a rotating panel design for the 2010 decade of the NSCG. This rotating panel design allows the NSCG to address certain deficiencies of the previous long form-based design including the undercoverage of key interest groups. However, along with numerous improvements, the use of the ACS as a sampling frame for the NSCG and the implementation of the NSCG rotating panel design also introduced new challenges. This document summarizes the rotating panel design planned for the 2010 decade of the NSCG and discusses results from two research tasks related to NSCG estimation. ","The National Survey of College Graduates (NSCG) is the nation's leading source of detailed statistics on the science and engineering labor force. Beginning in the 2010 survey cycle, the NSCG will be constructed using multiple sampling frames. The NSCG had attempted a similar dual frame approach in the 2003 NSCG survey cycle, but differing population estimates between the frames led the survey sponsor, the National Science Foundation (NSF), to abandon the dual frame estimates for publication purposes in favor of the single frame estimates. New research into the 2003 NSCG dual frame design has presented an opportunity to reevaluate the 2003 estimates and the 2003 decision. This paper looks deeper into the 2003 NSCG dual frame design issues, including potential causes of the differing estimates and comparisons of single frame and dual estimates two frames for key estimates of interest. The  ","The 2010 National Survey of College Graduates (NSCG) selected its sample using respondents to the 2009 American Community Survey as the sampling frame. The sample design was two-phase which creates variance estimation complexities. This paper discusses the issues with two-phase variance estimation and describes a proposed estimator to produce approximately unbiased variance estimates.   The American Community Survey uses Successive Difference Replication (SDR) to estimate variances. This paper also discusses an evaluation plan to determine whether or not SDR best meets the NSCG estimation needs by comparing SDR variance estimates against delete-a-group jackknife and balanced-repeated replication variance estimates. Preliminary results of this comparison will be discussed and the plan for a simulation study will be described. ","Surveys are commonly affected by nonresponding units which can  produce biases if these missing values can not be regarded as missing  at random (MAR). As many papers examine the effect of nonresponse  in individual or household surveys, only less is done in case of business surveys. This paper analyses the missing values in the Ifo Business Survey, which most prominent result is the Ifo Business Climate Index, a leading indicator for the businss cycle development in Germany. Covering the period from 1994 to 2009 (with about 7,000 responding firms per month) a considerable number of observations is available to compare various imputation approaches for longitudinal data to evaluate the method which reflects the underlying latent data generating process best. After this, the imputed data sets will be aggregated as usual and compared with the original indices to evaluate their implications to the macro level. We analyse the indicators for different aggregation levels and show that the bias is minimal. ","In 2009, a re-interview with participants in the 2007 Survey of Consunmer Finances (SCF) was undertaken to provide information on the effects of the financial crisis on households. The panel questionnaire was designed to maximize comparability with the earlier data. The subject matter of the survey, wealth and related issues, is often considered sensitive or conceptually difficult. Consequently, editing and imputation of the data are very important considerations. Although the baseline data had already been edited and imputed cross-sectionally, they were re-edited along with the new panel data. Similarly, the data for both waves of the survey were imputed jointly. This paper has two goals: to examine the importance of the re-editing of the baseline data and to gauge the effects of the joint imputation of data from the two waves. ","We propose a method to handle monotone missing covariates in a generalized estimating equation (GEE) model for correlated binary outcomes, when the covariates are missing by design. The regression coefficients are obtained by solving an aggregate unbiased estimating function, and the variance of the regression coefficients is estimated using the one-step jackknife estimator (Lipsitz et al. 1994). The advantages of the proposed new method over the complete cases analysis and the inverse probability weighted estimating equation are demonstrated by simulation studies. The new method is used to study whether concentration of dioxin congeners in house perimeter soil is an important predictor for having a high concentration of dioxin congeners in household dust, where structurally similar and correlated congeners are present and a limited numbers of soil samples are measured.  ","Health disparities research in the Lesbian, Gay, Bisexual, and Transgender (LGBT) community has received national attention as part of the new goals for Healthy People 2020. Using data from the National Survey of Family Growth Cycle 6 (2002) and the 2002-2008 National Health and Nutrition Examination Survey we found that less educated and Hispanic respondents were systematically missing data on sexual identity measures. Cognitive testing results showed that less educated and Hispanic respondents had comprehension issues with some of the sexual identity terms which could help explain their higher rates of missing data on the sexual identity measures. Systematic missingness on sexual identity measures could lead to biased univariate estimates and regression analyses of the relationship between sexual identity and health outcomes such as access to health insurance or disease prevalence. We will compare alternative approaches for handling missing data including casewise deletion, single and multiple imputation, and maximum likelihood estimation to evaluate the sensitivity of univariate and multivariate estimates to these different approaches.  ","The National Ambulatory Care Survey is a national probability sample survey of visits to nonfederal office-based physicians in the United States. Of particular interest to many researchers in health care policy is the effect of race on health care usage and outcomes, but race was missing at relatively high rates, peaking in 2008 at 33%. Evaluating the effectiveness and utility of the results of such imputations in categorical data, especially data with small sub categories, can be challenging. The evaluation of race imputation in NAMCS is a case study in how to evaluate categorical imputation. Graphical methods were used to compare imputed vs. non-imputed data. In addition, race was set to missing for a subsample of known cases in order to evaluate whether the model correctly imputed these known cases. The differences in the accuracy of the model were interesting because some very small categories were imputed with far more accuracy than some larger sub groups. This paper discusses the methodology and the findings for the research. ","The Annual Social and Economic Supplement of the Current Population Survey (CPS ASEC) is a widely used source of data on health insurance coverage. About 10 percent of respondents do not answer any of the ASEC supplement questions and the entire supplement for these respondents is imputed. Davern et al. (2007) identified problems with the imputation of health insurance variables in the CPS ASEC. They found the full supplement imputations cases were less likely to have private coverage and more likely to be uninsured. This discrepancy may partially be caused by a misspecification of the hot-deck routine. Only members of a policy holder's nuclear family can be covered as dependents. This family restriction contrasts with the instrument itself. We collaborated with the U.S. Census Bureau and evaluated possible improvements to the imputation routine to fix the identified problems. The study team recommended switching the order of the hot-deck allocation matrices with public coverage first and removing the nuclear family restriction to the private coverage allocation routine. Using 2009 CPS ASEC data, making these modifications reduces the uninsured rate from 17.3% to 16.8%. ","Universal Soil Loss Equation is a model that predicts the long term average annual rate of soil erosion caused by rainfall. It will be replaced by the more sophisticated RUSLE2 (Revised USLE 2) model in 2006. Data for computing both USLE soil loss estimates and RUSLE2 estimates are collected between 2002 and 2006 for validation. To estimate long term soil erosion trend and maintain consistency as an input to other models, it is necessary to impute USLE estimates after 2006 and RUSLE2 estimates before 2002. We first present statistical models to predict soil loss estimates from one soil loss model using the variables from the other one. With the aid of cross validation, we assess three different fitting methods: simple linear regression, multiple linear regression (MLR), and multivariate adaptive regression splines (MARS). Next we investigate imputation of USLE estimates shortly after 2006 and RUSLE2 estimates shortly before 2002 using nearest neighbor, classification and regression tree, and random forest. Reliable imputation is achieved by exploiting the relationship among the changes between neighboring years.  ","In longitudinal studies investigators frequently have to assess and address potential biases introduced by missing data.  This paper proposes new methods for modeling longitudinal binary data with nonignorable dropout using maginalized transition models and shared parameter models.   Random effects are introduced for both serial dependence of outcomes and nonignorable missingness.  Fisher-scoring and Quasi-Newton algorithms are developed for parameter estimation. Methods are illustrated with a real data set. ","Incomplete data is common in longitudinal studies, owing to  subjects missing one or more follow up visits. When the missing-data  mechanism depends on the outcome of interest, inferences that only  use the complete data will no longer be valid. Shared random-effects  models that uses latent variables to link the binary longitudinal  response to the missing-data mechanism have been proposed, within a  Gaussian framework. However, such models only describe the  conditional model of the binary response given random effects and do  not yield a closed-form marginal model for either the binary  longitudinal response or the missing-data mechanism. We propose a  shared random-effects model that, unlike the existing models, allows  to preserve a logistic regression form for, at the same time, the  marginal probability of the binary response, the missing-data  indicator, and the conditional probability of the ","A common problem in the longitudinal data analysis is the missing data problem. Two types of missing patterns are generally considered in the statistical literatures: monotone and non-monotone missingness. Non-monotone missing data are generally caused by intermittent missed visits by study participants. Monontone missing data can be from discontinued participation, loss to follow-up and mortality.  Although many novel statistical approaches have been developed to handle missing data in recent years, few methods are available to provide inferences to handle both types of missing data simultaneously. In this research, a joint shared parameter model is proposed to analyze longitudinal outcome data with both monotone and non-monotone missingness.   To overcome the difficulty of high dimensional integration in the estimation of the shared parameter model, we propose to use adaptive quadrature for computational efficiency since it requires fewer quadrature points to achieve the same precision. Simulation study is carried out and a real data example from the Scleroderma lung study is used to demonstrate the effectiveness of this method.   ","In an Ecological Momentary Assessment of smoking, volunteer smokers were asked to record each cigarette smoked on a PDA over a 16-day period prior to a designated quit date. In addition, the PDA prompted smokers to answer questions regarding mood and environment at times of randomly sampled cigarettes and at times selected according to a probability-based sampling design. A mixed effects version of a point process model is constructed to describe the relationship between the repeated cigarette smoking events and the partially-observed time-varying covariates, allowing for variation among smokers in their responses to mood and environment. Estimating equations are proposed that yield consistent estimators of model parameters without making assumptions regarding the distribution of time-varying covariates. Moreover, statistical inference remains valid even when, conditional on the random effects, event times are not realized from a Poisson point process. ","We propose a joint model for longitudinal and survival data with time-varying covariates subject to detection limits and ignorable intermittent missingness. The model is motivated by data from the Multicenter Aids Cohort Study (MACS), in which HIV+ subjects have viral load and CD4 cell counts measured at repeated visits along with survival data. We model the longitudinal component of the joint model via a generalized linear mixed model (GLMM), predicting the trajectory of CD4 cell counts with viral load and other covariates. The viral load data is subject to both left-censoring due to detection limits (17\\%) and ignorable intermittent missingnes (27\\%). The survival component of the joint model looks at death due to AIDS, and is taken as a Cox proportional hazards model. The longitudinal and survival models are linked via the trajectory function of the GLMM, which is included in the survival component. A Bayesian analysis is conducted on the MACS data using the proposed model. ","Studies in mental health research often employ two-stage designs to assess combinations of pharmacotherapies and psychotherapies. Outcomes in such studies often consist of repeated measurements of scores such as the 24-item Hamilton Rating Scale for Depression (HRSD) over a fixed duration of time. Since treatment is given sequentially, the eligibility of receiving one treatment assignment depends on previous treatments and intermediate outcomes. The goal is to compare different treatment regimes in two-stage longitudinal studies to find the most beneficial one for each patient. The presence of missing data is a common phenomenon in longitudinal studies (drop-outs, withdrawals, etc.). In this paper, we show how to construct proper weights to account for monotone missing and apply them in the estimating equations to draw inference for treatment regimes from two-stage longitudinal studies. Specifically, we provide consistent estimators and their asymptotic variances of the effects of treatment regimes. Large-sample properties of the proposed estimators are provided analytically, and examined through simulations. We apply our methods to a depression dataset that motivated this study. ","Joint modeling is often interpreted in the narrow sense of modeling together a longitudinal and time-to-event outcome. However, the specific features of two (or more) outcomes recorded simultaneously, together with te phenomenon of unobservables, is very common, though disparate: informative cluster sizes; models for incomplete data; sequential trials; and time-to-event with censoring, to name a few. Starting from an extendes shared-parameter-model framework, we provide a comprehensive encompassing framework, within which we highlight communalities and differences. Connections with both likelihood inference and inverse probability weighting are brought to the forefront. ","Energy efficiency is defined differently for various purposes, making calculating end-user energy efficiency controversial in the context of demand-side management. One fundamental issue is that pure measurements of energy efficiency cannot be obtained without influence from confounding factors (i.e.: weather or equipment maintenance). Thus, several efficiency index decomposition analysis methods have been used to remedy the lack of uncontaminated data. However, most index decomposition analysis is done at an industrial level, which provides a better-controlled environment than that of the end user's level. Required information at an end user's level is much more difficult to collect making index decomposition analysis impractical. As the result, evaluation of energy efficiency associated with demand-side management programs becomes subjective. An assessment of whether the energy savings and actual peak load reduction varies by instituting demand-side management programs or not, and how different types of customers respond to them, is proposed in this study by using supplemental statistical analysis to reduce the influence of unwanted confounders. ","The term meta-regression often refers to a linear models of effect size with covariates. Unlike common versions of regression analyses, which include available record-level data on outcomes and covariates, meta-regressions may consist study-level covariates in addition to record-level covariates. Random effects meta-regression models account for non-systematic differences among study outcomes which cannot be explained by sampling variability alone. We demonstrate a new estimation method using a random effect meta-regression model along with other methods of meta-analysis to estimate effect size of interest and the heterogeneity component of the variance using a simulation study. In the simulation study, we draw samples from a stochastic model that closely emulates the design of health surveys. We select the estimators of effect size which are best in terms of minimizing mean-square-error and bias. We then apply these estimators to data from health surveys and explore the causes of heterogeneity in effect sizes by including covariates at the study-level, record-level or both. Finally, we compare statistical methods, and discuss the limitations and drawbacks of these methods. ","Since 2005, the Vera Institute of Justice has been contracted by the Department of Justice's Executive Office for Immigration Review (EOIR), the nation's immigration courts, to manage and evaluate its Legal Orientation Program which is a multi-site program providing legal rights information to non-citizens detained by the DHS. The evaluation of the LOP, aiming to assess its impact on case outcomes and the immigration courts, found that cases for LOP participants took fewer days to go through the immigration courts than those for detainees not participating in the program. As a follow-up to that finding, we conducted a history event analysis of the immigration court data and the LOP program data, taking into consideration the fact that analysis of completed cases alone would cause estimation bias. The results confirmed statistically that LOP accounted for shorter case processing times for detained cases.   Because of the immigration court system's relatively brief history, there was no rigorous prior research focused on case processing times of immigration court cases. Our study contributed to the development of the field by analysis beyond basic descriptive statistics. ","The Canadian Child Tax Benefit (CCTB) is a non-taxable amount paid monthly to help eligible Canadian families raise children until the age of 18. The list of applicants to the CCTB was used as a sampling frame for the first time in 2010, for Statistics Canada's Survey of Young Canadians (SYC). Data collection for this new cross-sectional survey, which includes both telephone interviews and personal visits, took place from November 2010 to February 2011. The presentation will briefly describe how the sampling frame was built from the CCTB database, and assess the quality of the contact information (names, birthdates, phone numbers and addresses) by presenting results of a small-scale test conducted in February 2010, as well as early results from the SYC data collection activities. The focus will be on reporting key collection paradata and survey results, and comparing some of these with results from projects using other types of frames. We will assess the accuracy of the frame information, patterns of non-response, and the success of tracing activities based on the multiple leads made available by the SYC frame. ","Statistics Canada's Quarterly Industry Revenue Indices (QIRI) provides sub-annual indicators of economic activity for selected business and consumer services. This program consists of a census where administrative and survey data are combined to measure changes in quarterly operating revenues. For simple structured enterprises, data are extracted from an administrative file. A questionnaire is used to collect information for enterprises with complex operational structures. A preliminary index is produced three months after the end of the reference quarter. A revision of this index is done three months later using the most up to date information. Investigations have shown that revised indices are usually, although not systematically, lower than preliminary numbers. It is known that some enterprises for which administrative data are used in preliminary indices are later identified as inactive (dead) by the time the indices are revised. Since this represents an important source of revisions, a study was done to quantify the various sources of revision and verify the overall impact of the inactivation issue on revision rates. Details and results of the study will be presented. ","The National Agricultural Statistics Service (NASS) is a statistical agency within the U.S. Department of Agriculture (USDA) that conducts hundreds of surveys every year and prepares reports covering virtually every facet of U.S. agriculture. NASS' traditional approach has been to perform a manual edit and review of all questionnaires for most surveys. As staff resources become more constrained, the agency has embraced technological advances. The goal of significance editing, defined as statistical data editing; selective editing; and outlier detection, is to (1) reduce the time and effort spent manually reviewing/correcting survey questionnaires, without damaging the quality of the resulting data, and (2) focus the manual effort on the accuracy of the survey questionnaires that strongly impact the overall results. During the survey process, the most influential records are identified by calculating a unit-level score based on the changes made by the automated statistical data edit.  This paper provides details on these unit scores as well as the implementation of the significance editing concepts. ","The National Health Interview Survey (NHIS) is a principal source of information on health in the U. S. NHIS data are linked to administrative records from several federal agencies. Records are linked by matching personally identifiable information (PII, e.g. name, birth date, Social Security Number (SSN) and Medicare health insurance claim number (HIC)) collected from NHIS participants to the PII on the administrative records. Health-related data from the linked administrative records are then merged with the NHIS files. Before 2007, participants refusing to provide SSN and HIC were considered to have refused linkage. The refusal rate has increased over time, reducing the number of NHIS participants eligible for linkage. Beginning in 2007, NHIS attempted to decrease linkage refusal rates by requesting only the last four digits of SSN and HIC, adding a short introduction before asking for SSN and HIC and asking participants for permission to link to administrative records if SSN or HIC was not provided. We examine the impact of these changes on record linkage refusal rates and explore the additional influence of participant sociodemographic and health characteristics on refusal. ","Sample Redesign is the program that designs, selects, and disseminates updated samples for major demographic surveys conducted by the U.S. Census Bureau. The surveys and redesign have a long history, fifty years since the first redesign was conducted following the 1960 Census. This paper highlights some of the key changes in the surveys and the redesigns over that time. Several major innovations in the current redesign are also highlighted, including the use of the Master Address File for the sampling frame and the use of administrative records in statistical research. This paper also looks at some of the challenges facing demographic surveys, and the research and innovations of other organizations to respond to those challenges. Innovative methods such as responsive design may be implemented in future redesigns of Census Bureau surveys. ","This paper gives a summary and interpretations of the results of a series of research conducted by the U.S. Census Bureau to support the development of a frame improvement system for demographic household surveys in the 2010 Demographic Surveys Sample Redesign. Coverage evaluations concluded that a household survey frame developed using the Master Address File as the sole source can provide a frame for producing comparable quality U.S. estimates and most state estimates. Coverage improvement will be needed to mitigate the risk of coverage bias in several states. The U.S. Census Bureau conducted subsequent research to identify the states that needing coverage improvements and the characteristics of the blocks that coverage improvements will be targeting. This paper will also describe the coverage improvement frame methodology for the 2010 Sample Redesign. ","The sample design of the household surveys is a two stage design. In the first stage, primary sampling units (PSUs) are defined, stratified, and selected. PSUs are made up of counties or groups of contiguous counties. In the second stage, address records within each PSU are sorted by geographic and demographic information and selected systematically survey-by-survey with previously selected samples removed and the sampling interval adjusted in the next survey. In this paper, we provide an overview of the sample design for the household surveys. We also highlight research and results on major methodological changes for the 2010 sample design, including the frequency of sampling, and the use of data from the American Community Survey and administrative records in sampling. ","Every 10 years the US Census Bureau uses the results from the Decennial Census to update the sample used for many of the household surveys, including CPS, SIPP, NCVS, NHIS, CE and AHS. Sample Redesigns provide an opportunity to improve operational instructions, improve coverage, and implement new technology. This paper discusses the improvements and changes to the existing processes used by the Regional Offices, Headquarters support staff, and the clerical operations supported by the US Census' National Processing Center.    ","The Bureau of Labor Statistics (BLS) Quarterly Census of Employment and Wages (QCEW) program has been using primary and complementary cell suppression techniques as its disclosure limitation methodology. The employment and wage levels of over 9 million establishments are collected under confidentiality pledges and other provisions that obligate BLS to protect the identities and characteristics of over 9 million establishments whose data are assembled into a multi-dimensional array of over 3.5 million populated data cells. The techniques have been providing an unsatisfactory balance between the protection provided to respondents and the usefulness of the data for economic and policy analysis uses, as over 2.1 million cells are suppressed. BLS is seeking to extend the basic random noise methods of Evans, Zayatz, and Slanta (EZS), to the characteristics of the QCEW program. BLS also is considering whether several of the protective mechanisms of the original EZS paper can be relaxed while still providing reasonable protection to respondents. ","Cell suppression is the most common disclosure limitation method that the U.S. Energy Information Administration (EIA) applies to the aggregate statistics that it publicly releases. Complementary cell suppression involves withholding the publication of non-sensitive cells in a table in order to protect the cells that were identified as sensitive to revealing company level information. EIA has two different automated suppression programs that it applies to tabular data. Each program follows a different methodology. The data protection levels for a table vary from applying different complementary cell suppression methodologies because the suppression patterns vary depending upon each methodology. If several tables within the same information product are related due to a high level of dimensions in the table designs, then the selection of complementary cells in each table becomes a more com ","An Economic Census of the United States is conducted every five years by the economic directorate of the U.S. Census Bureau. The main data products are additive magnitude data tables that typically involve NAICS categories as rows and geographic entities as columns (some tables have a 3rd dimension). The p% rule is used for determining which cells are sensitive, and how much protection each such cell requires. First, the sensitive cells are suppressed. Then a cell suppression program is run against a file with information about each cell including an identifier for each establishment that contributes to the cell value; its associated company, and the contributed value. This program calls an optimization routine for each sensitive cell in order to find the optimal set of additional cells that must be suppressed in order to find the set of cells with the minimum total value, that, when suppressed, lead to a protection of the sensitive cells at the company level. We discuss a number of complex aspects of the software and how each of these was modernized.  ","When a data cell in a table is suppressed by dropping its value based on a primary cell suppression rule, the value of that cell can still be determined if the table, subtable, or linked tables provide totals, marginal totals, or subtotals. Secondary cell suppression is therefore needed to avoid such disclosures. Two software packages are available to assist researchers with secondary cell suppression: Tau-Argus (Statistics Netherland 2009) and R-statistical package sdcTable (Meindl 2010). But even with this software, there is no simple way to perform secondary suppression for linked tables-that is, tables presenting data on the same cells that share some categories of at least one explanatory variable. Computation may not be trivial and may still require manual reviews, especially when dealing with a large number of linked tables. With an eye toward finding the most straightforward and  ","The first part of the talk is based on a paper published in JASA(2006; joint with P. Lahiri), in which we introduce a general methodology for producing a model-assisted empirical best predictor (EBP) of a finite population domain mean using data from a complex survey. Our method improves on the commonly used design-consistent survey estimator by using a suitable mixed model. Unlike a purely model-based EBP, the proposed model-assisted EBP converges in probability to the customary design-consistent estimator as the domain and sample sizes increase. The convergence is shown to hold with respect to the sampling design, irrespective of the assumed mixed model, a property commonly known as design-consistency.  The second part of the talk introduces a new approach to small area  estimation. We derive the best predictive estimator (BPE) of the fixed  parameters under the nested-error regression model. This leads to a new prediction procedure, called observed best prediction (OBP). We show that the OBP is more reasonable than the empirical best linear unbiased predictor (EBLUP) and can significantly outperform the latter when the underlying model is misspecified. ","The calibrated Bayesian approach to surveys basis inferences on the posterior predictive distribution from a Bayesian model, but seeks inferences that are frequency calibrated, in the sense of have good frequentist properties in repeated sampling. The calibrated aspect dictates that survey design features like clustering and weighting need to be appropriately included in the model, to protect against model misspecification. Examples are provided on how to do this, and calibrated Bayes inferences are shown by simulation to have better frequentist properties than standard design based approaches, including the model-assisted approach popular with current survey practitioners. ","Validation of a unit level model at the small area level may be difficult due to small sample size. In addition, the sample design may often be informative at this level and needs to be accounted for. Lastly, Bayesian inference requires a complete likelihood, necessitating the need for a detailed description of the small area likelihood. The consequences of knowing little about the sampling distribution beyond the first two moments, including the dependence between observations will be demonstrated.Criteria for choosing distributions will be discussed. ","In any survey an individual may respond to one or more items, and generally it is not possible to determine whether a nonrespondent differs from a respondent with the same traits. In the Slovenian Public Opinion Survey (SPOS) with the three categorical variables (secession, attendance, independence), there are eight patterns of missingness  (eight tables), and the emphasis was in determining the proportion of the population who would attend the plebiscite and vote for independence. One strategy is to center a nonignorable nonresponse model on an ignorable nonresponse model, thereby adding uncertainty to the ignorable nonresponse model. The categorical tables are model simultaneously with different parameters, and these parameters are independent and identically distributed. They differ by one or more sensitivity (or centering) parameters, whose prior specifications give the difference between the ignorable and nonignorable nonresponse models. With these specifications, a Bayesian approach is attractive, and the computation can be done using the Gibbs sampler. Thus, it is possible to study imprecision and ignorance to obtain an uncertainty interval. The SPOS data provide an illus ","The GSS has been tracking societal trends since 1972. It has measured changes in over 1,000 attitudinal, behavioral, and demographic variables. In carrying out this widespread social indicators monitoring, the GSS has developed both theoretical and applied approaches for maximizing the reliability and validity of trends and minimizing the occurrence of measurement artifacts. We will draw on specific GSS experience to illustrate the do's and don't of studying societal change. ","Paradata analysis was introduced as a powerful tool to reduce error in web surveys and interviewer-administered surveys (Kreuter, 2010), and researchers are now harnessing the power of paradata analysis in paper self-administered surveys as well. Features of the environment in which self-administered surveys are completed can be measured using respondent-provided paradata. Interview setting features such as the presence of others have been shown to affect reporting in interviewer-administered surveys (see Tourangeau &amp; Yan, 2007, for a review). We also expect that the survey setting may introduce measurement variability in paper self-administered surveys. Paradata including the presence of others, consultation with others in answering the questions, and location(s) of completion were collected from teachers and principals in selected schools as part of a self-administered survey. We assess whether features of the survey setting contribute to measurement error and nonresponse error as indicated by variation in the reporting of principals' and teachers' effectiveness, straightlining, reliability of multi-item scales, and missing data rates. ","The 2010 Census Coverage Measurement Program (CCM) plans to use logistic regression models for the correct enumeration rate, data-defined rate, and match rate that will be used in dual system estimation of the population size which then will be used to measure net census coverage error. Recent studies of the error structure of the logistic regression estimator for net error have described how various kinds of sampling and non-sampling errors affect the estimates of the net error as well as estimates of omissions and erroneous enumerations. In addition, the studies have provided decompositions of the sampling and non-sampling errors. These decompositions have been useful in designing a schematic plan for using a simulation methodology to synthesize the effect of the sources of error on the estimates of net coverage error as well as on the estimates of omissions and erroneous enumerations. ","Interviewer estimates of household characteristics have been used for nonresponse error adjustment (Kreuter et al., 2010) but rarely have interviewer observations of respondent behavior and attitudes related to the interview process been exploited for survey error adjustment (Tarnai &amp; Paxson, 2005). In addition to adjustment purposes, these paradata also can guide interviewer training efforts and case assignment based on interviewer skills. In panel designs, these observations from prior waves can be used for adjustment and management in future waves. We analyze interviewer assessments of respondent understanding of the questions and respondent attitude toward the interview from a rotating panel telephone survey with a battery of attitudinal items. Indicators of measurement error, including selection of the same response for most of a question series, socially desirable reporting, and reliability of multi-item scales, are modeled as a function of interviewer assessments of respondent understanding and attitude. We also examine whether interviewer assessments can explain indicators of nonresponse error, such as missing data rates and participation in a future survey wave. ","The Residential Energy Consumption Survey (RECS) is a quadrennial survey which collects energy data on a sample of U.S. households. The RECS questionnaire is technical, challenging respondents' capacity to report information on energy use, equipment, and related expenses. Householders who do not pay their energy bills directly, roughly 10% of the population, are more prone to sources of response error: difficulty understanding an energy question, not having access to information, and inability to produce an accurate response. EIA fills this quality gap by conducting a supplemental rental agent survey (RAS) for these households. We examine 2009 RECS data in households with both a household and a RAS interview to show patterns of response and nonresponse bias. We discuss the impact on key estimates and energy models, the utility of this approach in terms of the survey cost-error tradeoff,  ","We address an ongoing debate how to assess sensitive topics in telephone surveys. Examining three existing methods and implementing one new method, we developed a module to measure illicit work and tested this in two CATI studies (both conducted in 2010). In an experimental setting, we compare a double-list implementation of the Item Count Technique (ICT) with direct questioning as well as a forced-response implementation of the Randomized Response Technique (RRT) with direct questioning. In the first study (ICT; n=1.603), respondents were selected from the German general population. In the second study (RRT; n=3.211), respondents of two specific populations were sampled from a register: employed persons and those qualifying for basic income support in Germany. Goal of the studies is to evaluate which method elicits more socially undesirable answers in the context of illicit work and moonlighting, particularly with regard to the specific mode of data collection and different subpopulations. Furthermore, we developed a novel method which can be applied to the measurement of sensitive metric variables. This method requires no randomizer and can be easily administered in CATI surveys. ","Questionnaires often contain \"general - specific\" questions about the quality of community life in which a general question (G) either precedes (GS) or follows (SG) a series of specific items (S) related to the general question. One hypothesis is that the R2 is larger in a regression of G on the specific items in the SG conditions as respondents interpret G as a call to summarize. The magnitude of the regression coefficients can provide insight into the importance of specific items, but interpretation is difficult due to collinearity. An alternative approach is confirmatory factor models where each item is a manifestation of one or more latent variables. In this model, the same insight can be gained by comparing factor loadings in the GS and SG conditions. In addition, insight can be gained into the processes by which people answer questions. We hypothesize that respondents answer questions in an orderly sequence, represented as first order serial correlations between adjacent items. Our results support this hypothesis as evidenced by an improvement in the fit of the model over what one would expect if pairs of items were correlated at random. ","The NSF's Survey of Doctorate Recipients is conducted every 2 or 3 years and collects detailed information on individuals receiving PhDs in science and engineering in the U.S. and some others with PhDs from abroad in these areas. Survey weights adjust for oversampling and nonresponse on a cross sectional basis. A significant portion of the sample (e.g., 60% on 3 or more surveys from 1993-2006) appears in multiple survey years and can be linked across time. No longitudinal weight exists that would enable estimation of statistical models or comparison of finite population characteristics using data from multiple survey waves together. This paper explores calibration estimation for construction of such a longitudinal weight. Three requirements are considered when producing longitudinal weights. First, the weight needs to be calculable from existing data. Second, the weight needs to be usefu ","National Immunization Survey (NIS), a landline telephone survey, monitors vaccination coverage rates among children aged 19-35 months and adolescents aged 13-17 years. In 2009, the National Health Interview Survey (NHIS) attempted to obtain consent to contact vaccination providers of children with a completed interview. This NHIS Provider Record Check (NHIS-PRC) study followed the NIS provider data collection procedure for age-eligible children and adolescents with consent. The NHIS-PRC data collection operation covered both landline and non-landline telephone households. The increased number of inaccessible households in recent years due to non-landline telephone status widens the gap between the target and the sampled populations, raising a major concern regarding the representativeness of the NIS sample. We explore the impact of the potential nonrepresentativeness of the NIS sample on estimated vaccination coverage rates. Distribution of telephone status, household characteristics, and estimated vaccination coverage rates from the 2009 NHIS-PRC study are utilized to calibrate 2009 NIS estimates. We compare estimated vaccination coverage rates with and without calibrated weights. ","SUDAAN 10 (RTI 2008) introduced the WTADJUST procedure, which produces calibrated survey weights. By forcing the weighted  totals for a set of \"calibration variables\" to equal benchmark totals computed from either the whole sample, a larger sample, or the entire population, calibration weights can reduce or eliminate the potential for bias from unit nonresponse under a reasonable response model.  WTADJX, which will be available in SUDAAN 11 (RTI 2012) allows the set of model variables governing the response model to differ from the set of calibration variables. As a result, the new procedure  may be used to assess and perhaps even correct for unit nonresponse that is not missing at random, that is to say, when unit response is a function of variables known only for the respondents. There are other potential uses of WTADJX. These include producing calibrated weights asymptotically identical to so-called \"optimal\" ones and simplifying the computation of replicate weights when using a nonlinear calibration routine.  ","Statistical agencies invest considerable resources in the   scientific design of survey samples, one aspect of which   is the availability of explicit unit and joint selection   probabilities. In textbook theory, these selection probabilities   enable design-consistent estimation of the variance through   linearization formulae. In practice, however, the inverse selection-  probability weights are subject to multiple rounds of adjustments   to reflect population benchmarks and to compensate for nonresponse.   After these operations the weights no longer have a clear   interpretation. Then, formula-based design-consistent variance   estimation methods are often not longer easily available, and replication   techniques are used instead. We propose a comprehensive   optimization-based method for adjusting the initial survey weights  for population benchmarking and nonresponse and also accommodating   weight compression or smoothing, in a single operation. The benefit   is a unified theoretically supported weight adjustment with guaranteed   bounds on the weights, and which maintains design-consistency under   nonresponse or linear-model assumptions. ","Small area estimates are usually model based, and thus when aggregated, do not typically match the direct estimate for a large geographical area. This often causes concern since the direct estimate for a large geographical area is believed to be quite reliable. In order to address this concern, and possibly also to guard against potential model failure, small area estimates are often adjusted to match the direct estimate for the larger area after aggregation. This is usually referred to in the small area literature as benchmarking. The paper proposes two-stage Bayesian benchmarking with one single model.  A simple illustration is where one wants to benchmark the state estimates to the national estimate, and then the county estimates to the corresponding benchmarked state estimates.  ","The aim of this study was limited to determining various sample sizes (n) when estimating population proportion p. Tables were generated using a C++ program which depends on population size (N), degree of precision, or error level (E), and confidence level (Z). Nineteen different population sizes, five degrees of precision and three levels of confidence were utilized. The study found out that the larger the population size, (N), the smaller the degree of precision (E) and the higher the probability/confidence level (Z), the larger the sample size must be. Two values for the sample estimate of the population proportion, p were used in this study. Practical applications of randomly pulling appropriate number of samples from huge data sets were also discussed. ","The coverage of many Address Based Sampling Frames has been well documented; however, much less attention has been paid to the mechanisms generating coverage errors. A clearer understanding of the mechanisms generating undercoverage can help us more effectively target areas needing improvement or listing. To investigate correlates of Housing Unit undercoverage on Address Based Sampling Frames, we used preliminary results of the US Census Bureau's Address canvassing operation to assess the coverage of an Address Based Sampling Frame. We then explored the relationship between various block characteristics to the Housing Unit undercoverage rate. Our results provide insight into the distribution of omissions on an Address Based Sampling Frame. ","Many surveys have relied on data collection methods such as landline telephone interviews, Web surveys, and recently cell phone interviews. Much of the research on these modes of data collection comes from the USA or Western Europe. To what extent do the findings apply to other countries, especially those in Asia where both cell phone and Internet penetration is high? This study compares the USA and South Korea, both countries with high levels of cell phone and Internet penetration. We analyze data from comparable surveys on information and communication technology (ICT) adoption and use in the two countries, in particular examining whether the demographic correlates of access and use often found in the USA also hold for South Korea. We will compare the relative strength of covariates of technology use in the two countries, with the goal of identifying similarities and differences in technology adoption. We discuss the implications of these analyses for surveys conducted in the two countries using various modes of data collection, and especially in the extent to which methodological findings from one country can be generalized to others. ","Sampling the U.S. residential population using list-assisted random digit dialing (RDD) of landline telephone numbers has become problematic because of the increasing proportion of the population that is reachable only through cell phones. To address this coverage problem, round 6 of the Health Tracking Household Survey (HTHS6) employed an RDD dual-frame \"cell overlap design\": samples were selected from landline and cell frames, and interviews were attempted with all contacted households. Other approaches sometimes used to address the coverage issue include address-based sampling and dual-frame RDD designs in which the cell frame is screened for cell-only households. HTHS6 asked a series of questions about telephone usage from respondents in both the landline and cell sample frames. This paper will discuss contact and cooperation rates, along with number of calls per complete, by sample frame. In addition, this paper will provide information about landline and cell telephone usage by sample type and compare characteristics among the various telephone usage categories (cell only, cell mostly, some of each, landline mostly, landline only), including demographics, health status, and i ","Traditionally, large surveys use a single sampling frame from which  the sample is selected. As the population and methods used to  collect survey data change, single frame surveys may miss parts of  the population. In order to obtain better coverage of the population  of interest and cost less, a number of surveys employ dual frame  surveys, in which independent samples are taken from two overlapping  sampling frames. Some current surveys follow the same households at  regular time intervals so that longitudinal quantities such as  transitions in employment status can be studied. In this research,  statistical methods for analyzing longitudinal quantities from dual  frame surveys are reviewed. Goodness of fit (GOF) tests to assess  the model fit are developed. Statistical properties of the GOF tests  are investigated. Simulation studies and real data example are given  to illustrate the developed tests.   ","President Obama's Open Government Initiative outlined a new direction for the federal government to improve access to Federal data. It also supported the goal to expand the creative use of those data by encouraging innovative ideas and research. The Data.gov website is an important tool for coordinating the release of Federal data to the public. However, the collection of many datasets across federal agencies in a centralized location may increase the risk of disclosing the identity of participants in surveys. In addition, there are concerns that the cross-government cataloging and searching of data sets along with the development of related application programming interfaces associated with Data.gov may facilitate identifying relationships that may not otherwise be apparent. This paper discusses the processes that were developed to protect the confidentiality and privacy of the informat ","Privacy. Open Government. Transparency. Security. Functionality. Consolidation. Data Integrity.  While it may be difficult to include all these terms in a single sentence, Federal managers in the realm of statistical data must reconcile these multiple objectives and directives on a daily basis. The President's Open Government Directive introduces a dynamic which runs headlong into the ongoing buttressing of the security of data residing on Federal systems. It grows evermore difficult to assure survey respondents that we can maintain privacy of collected data while being directed to be more transparent. Consolidation of data center resources within departments, while offering enhancements to overall system security, also enhances the probability of internal breaches resulting from unintentional technical acts. The Statistical Community of Practice and Engagement (SCOPE), sanctioned by the Interagency Council for Statistical Policy within OMB, was formed to address these and other issues confronting US statistical agencies through collaboration and sharing of best practices.    ","Over the last decade, there has been an expansion in the amount of information on students and their schools reported to the general public. States now report student outcomes based on assessments of student achievement in specific subjects and grade levels. These reports have the challenge of meeting reporting requirements while also meeting legal requirements to protect each student's personally identifiable information (PII). States have adopted minimum group size reporting rules and various forms of suppression and top/bottom coding to protect PII. This paper provides a review and analysis of current practices to illustrate that some practices work better than others in protecting PII reported from student education records. This review has also led to the formulation of recommended reporting rules that are driven by the size of the reporting groups or subgroups. These reporting rules are intended to maximize the amount of detail that can be safely reported without allowing disclosures from student outcome measure categories that are based on small numbers of students. ","The Census Transportation Planning Products (CTPP) contains tables for 150,000 travel analysis zones (TAZs), other geographies and journey to work flows, which results in millions of tables involving dozens of variables. The data source is moving from the Census Long Form to the smaller American Community Survey (ACS) five-year sample. Under cell suppression, data loss at the TAZ level would be substantial on ACS data using Census Bureau Disclosure Review Board (DRB) disclosure rules. For this reason, initial research has been conducted to perturb ACS values in a feasible way that retains the usability of the data, satisfies the data users' analytical needs, and pass the DRB rules. Also, even though the CTPP will be available in tabular form only, it was necessary to consider the confidentiality protection of the ACS public use microdata. We report on an evaluation of three perturbation  ","Abstract: ICPSR is building and testing a data storage and dissemination system for confidential data, which obviates the need for users to build and secure their own computing environments. Recent advances in public utility (or \"cloud\") computing now makes it feasible to provision powerful, secure data analysis platforms on-demand. We will leverage these advances to build a system which collects \"system configuration\" information from analysts using a simple web interface, and then produces a custom computing environment for each confidential data contract holder. Each custom system will secure the data storage and usage environment in accordance with the confidentiality requirements of each data file. When the analysis has been completed, this custom system will be fed into a \"virtual shredder\" before final disposal. This prototype data dissemination system will be tested for (1) system functionality (i.e., does it remove the usual barriers to data access?); (2) storage and computing security (i.e., does it keep the data secure?); and (3) usability (i.e., is the entire system easier to use?).   ","During the 2010 Census planning cycle, the Census Bureau developed and tested a new listing operation to improve the coverage of Group Quarters (GQ) addresses. The new Group Quarter Validation (GQV) operation was the successor to the Address Canvassing operation which identified addresses as housing units or Other Living Quarters (OLQs). OLQs include addresses that were GQs, such as college dormitories, Transitory Locations, such as campgrounds, Assisted/Independent Living facilities, and any other living quarters not commonly recognized as a housing unit. During GQV, enumerators validated OLQ addresses as GQs, housing units or transitory locations. This paper will discuss the distribution of outcomes from all addresses in the 2010 Census GQV operation. ","The 2010 decennial census was the 23rd national census. Although other procedures were used in isolated areas, the majority of the country received a paper questionnaire delivered by the US Postal Service or by census employees. The questionnaire packets included a request for the recipient to complete the form for the household as of April 1, 2010 and return it by mail to a data collection center in a pre-paid envelope. For the addresses from which no mail response was received, the Census Bureau sent enumerators to verify the status of the address and conduct an interview based on the April 1 status. The data collection period was conducted between May 1 and July 30, 2010.    ","The U.S. Census Bureau conducted the 23rd decennial census of the country's population in 2010. A large percentage of the country's housing units received a census questionnaire in the mail or from a census enumerator and roughly two-thirds of those households returned their questionnaires by mail. The remaining households that didn't respond by mail received a personal visit by a census enumerator in order to capture their census information. This happens during the Nonresponse Followup (NRFU) operation. This presentation will discuss what happened in the 2010 NRFU operation, including characteristics of the enumerators' attempted contacts with each address, characteristics of the housing units that were visited, and the demographics of this subset of the U.S. population. ","The 2010 Nonresponse Followup Reinterview is designed to identify enumerators who intentionally or unintentionally do not follow data collection procedures. This is achieved by revisiting a sample of Nonresponse Followup cases and comparing data from the two interviews. The 2010 Census was the first full census where Nonresponse Followup Reinterview utilized the Matching, Review, and Coding System (MaRCS) - an innovative automated application which facilitated this data comparison through computer matching and clerical matching. The MaRCS reports also provided unprecedented insight and control of the Nonresponse Followup Reinterview program.    ","The 2010 Vacant/Delete Check operation verified the housing unit status of addresses classified as vacant or nonexistent (deletes) during the Nonresponse Followup operation. This operation also included a first time enumeration of addresses added to the Census universe and addresses that returned a blank mail questionnaire.     ","Census Transportation Planning Products (CTPP) is a set of tabulated data products designed for transportation planners. As the underlying data is moving from the Census Long Form sample to the smaller American Community Survey (ACS) five-year combined sample, disclosure risk becomes an avoidable concern, especially for small geographic areas such as Travel Analysis Zones. Three perturbation approaches were evaluated with respect to utility of the CTPP products, and conforming to Census Bureau's requirements for protecting against disclosure risk. This paper discusses the variance estimation on CTPP using perturbed ACS data from our research. ACS implements the Successive Differences Replication (SDR) method for variance estimation, with the advantage that the variance estimates can be computed regardless of the form of the statistics or the complexity of the design. However, the SDR estimator applied naively to the perturbed data does not account for the variance due to perturbation. As a remedy, an additional term was added to reduce the bias. The proposed estimators are compared with a few alternatives and evaluated through a simulation study. ","When intense redaction is needed to protect data subjects' confidentiality, statistical agencies can release synthetic data, in which identifying or sensitive values are replaced with draws from statistical models estimated from the confidential data. Specifying  accurate synthesis models can be a difficult and labor intensive task with standard parametric approaches. We describe and empirically evaluate four easy-to-implement, nonparametric synthesizers based on machine learning algorithms - classification and regression trees, bagging, random forests, and support vector machines - on their potential to preserve analytical validity and reduce disclosure risks. The results suggest that synthesizers based on regression  trees can provide high utility with low disclosure risks. ","The Quarterly Census of Employment and Wages (QCEW) program of the Bureau of Labor Statistics (BLS) publishes tabulations of monthly employment, quarterly wages, and number of establishments by industry and geography. In accordance with BLS policy, data provided to the Bureau in confidence should be used only for statistical purposes. In particular, the publication of data collected from BLS surveys should not lead to the identification of cooperating respondents. The BLS has been concerned about the current cell suppression method used with the QCEW. To address such concerns, BLS has conducted research about the random noise method as an alternative method.This paper provides an assessment of the BLS research to date. The paper begins with a review of the current cell suppression method in Section 2, focusing on the major disadvantages of the current method. Section 3 reviews the random ","MASSC (an acronym for Micro-Agglomeration, optimal probabilistic Substitution, optimal probabilistic Subsampling, and optimal weight Calibration) is a statistical disclosure limitation (SDL) methodology developed at RTI International for simultaneous confidentiality and analytic utility protection. In this paper, MASSC was compared with two other SDL methods by examining the degree to which these methods impact data quality and lower disclosure risk. The other SDL methods are Post Randomization Method (PRAM) and Random Swapping. The sample was taken from the 2006 and 2007 National Survey on Drug Use and Health (NSDUH) public use files (PUFs) as an initial data set for treatment, where the original PUFs were viewed as the \"population\" and the three methods were compared via simulations. For risk assessment, the matching probability was calculated to discover if a record in a treated sample can be correctly linked to the corresponding record in the \"population.\" For utility assessment, each treatment's impact on direct estimates and its impact on inference and on estimated regression-model parameters were compared. ","The Quarterly Census of Employment and Wages (QCEW) program of the Bureau of Labor Statistics publishes tabulations of certain attributes (employment, wages etc.) by industry and geography. The publication of data collected from BLS surveys (or census) should not lead to identification of cooperating respondents. The BLS has been concerned for some time about the current cell suppression method that can compromise the quality and utility of the QCEW data. To address such concerns, BLS has been conducting research on the random noise method toward extending that model for application to QCEW. The goal is to release significantly more data and to respond to new disclosure vulnerabilities. In this paper, we propose an alternative based on small area modeling techniques for output treatment for disclosure. In this new application of small area modeling we would like to exploit the built-in p ","A new optimal estimator of population proportion of potentially sensitive attributes in survey sampling is proposed and investigated. The proposed estimator makes use of known average values and known common variance of two scrambling variables at the data collection and estimation stages; more cooperation is expected from the respondents than in the Franklin (1989) model. The variance of the proposed estimator is minimized to determine the value of a constant which leads to an optimal estimator of the population proportion. The resulting optimal estimator has been found to be more efficient than the Franklin (1989) estimator, and the Singh and Chen (2009) estimator which suggest utilizing higher order moments of scrambling variables at the estimation stage. ","In this paper, a new estimator for estimating the finite population variance of a sensitive variable based on scrambled responses collected using a randomization device is introduced. The estimator is then improved by using known auxiliary information. The estimators due to Das and Tripathi (1978: Sankhya) and Isaki (1983: JASA) are shown to be special cases of the proposed estimator. Numerical simulations are performed to study the magnitude of the gain in efficiency when using the estimator with auxiliary information with respect to the estimator based only on the scrambled responses. An idea to extend the present work from SRSWOR design to more complex design is also given. ","The introduction of the American Community Survey (ACS) served two important functions - eliminating the need to collect detailed socioeconomic and housing data as part of the decennial enumeration and producing more up to date information for data users. Data that had historically been collected only once a decade from a sample of the population that received a \"long form\" are now collected continuously throughout the year by the ACS. While it was generally understood that eliminating the decennial census long form sample would simplify the 2010 Census of Population and Housing, it was also acknowledged that the critical need for detailed housing and socioeconomic data at small levels of geography still had to be met.   This paper will describe the transition from collecting and producing this information in a decennial census environment to collecting and producing annual survey estima ","This year Statistics Sweden face the challenge of conducting Sweden's first fully register-based census. We give an overview of how the available registers are to be used for the census, and how a novel register, the register of dwellings, is constructed. The register of dwellings will be matched with the population and the real properties registers to allow us to estimate, for example, distributions of variables such as the ratio of living space to the number of occupants. One issue is people without residence recorded on the register and dwellings without recorded occupants, which we treat with statistical matching. We also address quality measures and quality assessment of a register based census. Other methodological issues in a register-based census include disclosure control, 'unit editing' (i.e. whether the right units have been identified), and evaluation of model assumptions. ","Countries of the Caribbean region have jointly undertaken population and housing censuses since 1960. The regional effort was reinforced given disparities in population size, economic status and availability of financial resources across countries and territories.  Earlier censuses were done individually, some dating back to 1844.  With the passge of time the efficiency of census-taking within the Caribbean Region has been somewhat compromised due to factors akin to human resources, cultural nuances, financial constraints and methodological challenges.  This paper explores the prospect of more efficient models such as the long and short forms.  Using data from the 1990 Population and Housing Census, a pilot test was undertaken to compare sample estimates of population parameters with magnitudes derived from complete census counts.  Based upon a multistage sampling design targeting households as units of analysis, 98 parameters were estimated with only four yielding estimates that were at least 4 percentage points in excess of the magnitude observed on the basis of the complete census. The paper discussed the prospect of replicating this procedure in other Caribbean juridictions. ","To improve the value of the census data for research and analytical purposes, the Australian Bureau of Statistics (ABS) initiated a Census Data Enhancement (CDE) program in 2006. The program comprises a number of projects aimed at linking the census data, longitudinally, and to other data sources to maximise the value that is derived from combined datasets rather than the individual datasets alone. The datasets are combined using a probabilistic data linking method.     ","Respondent-driven sampling (RDS) is a link-tracing network sampling strategy to collect data from hard-to-reach populations (for example, injection drug users or individuals at high risk of being infected with HIV). Currently, the so-called Volz-Heckathorn (VH) estimator is used for RDS in practice, which uses very strong assumptions about underlying social network and RDS process. Via simulation, we study relative performance of the plain mean and VH estimator when assumptions of the latter are not satisfied, under different network types (including homophily and rich-gets-richer networks), participant referral patterns and varying number of coupons. The main question of interest is, given that the VH estimator is derived under strong simplifying assumptions, how strong a performance loss does it incur when RDS scenarios are more realistic? We convey the intuition using novel interactive visualizations of RDS process and simulation findings, created using the programming language for dynamic visualization called Processing. ","Recent education policies aim to raise academic expectations for all students. For example, there is an expectation that English Language Learners (ELLs) will move to rigorous main-stream classes as soon as they are ready to succeed in those classrooms. This paper uses signal detection theory with Michigan Merit Exam (MME) Reading proficient as an external criterion to examine the existing proficient cuts at grades 11 on Michigan English Language Proficiency Assessment (ELPA). Moreover, the resulting optimal cut on ELPA for grade 11 was used as the criterion to locate the optimal cut for grade 10, and so on and so forth, until we locate the optimal cut for Kindergarteners. The obtained suggested cuts were shown to be fitted by two separate polynomials of order 2 with regard to grade levels. Moreover, when the adjusted optimal cuts (after a quick fix to join two curves into one) were compared with the adjusted cuts (for fair comparison) set by standard setting panel, it seems that the ELPA standard setting panel was more lenient when setting the proficient cuts for higher grade levels, but was more stringent at the two lowest grade levels. ","Policy questions that often motivate transportation safety studies are causal in nature, yet they are typically answered using observational studies. This paper focuses on exploring the applicability of two causal modeling frameworks- Causal Bayesian Networks and Potential Outcomes - to elucidate causation from observational studies. The question addressed in this paper is that of the minimum levels of pavement marking retrore?ectivity on highways. Nighttime crash data for North Carolina, obtained from the Federal Highway Administration's Highway Safety Information System, appended to pavement marking data, are used. The causal effect of retrore?ectivity on safety of a road segment is estimated. The results of both modeling frameworks are generally consistent with each other. The effect of increased pavement marking retrore?ectivity is to generally reduce the risk of nighttime crashes. ","The clear association between population density and partisan preference in elections suggests that redistricting plans would be better aligned with principles of partisan fairness if there were a deliberate effort to balance population density across legislative districts. To balance population density without sacrificing geometric compactness, we define a density-variation/compactness (DVC) measure that can serve as a one-number summary of a proposed redistricting plan. We use data from California to guide the choice of a DVC measure and evaluate its performance using election data from Texas during the past decade. Using a political-science model relating legislative representation to the proportion of votes received, higher DVC scores corresponded to estimates of partisan bias with smaller magnitude across a range of redistricting scenarios; meanwhile, contrary to expectations that reduced partisan bias would be accompanied by reduced electoral responsiveness, there was no discernible pattern between DVC scores and estimated responsiveness. We conclude that the use of a DVC measure could provide a check on attempts to introduce partisan bias into the redistricting process. ","The Economic Census is one of the most important activities that the U.S. Census Bureau performs. It is critical for updating firm ownership structure and industry information for a large number of businesses in the Census Bureau's Business Register, impacting most other economic programs. We model the check-in rate for single-establishment firms by using a large number of variables that might be correlated with whether or not a firm returns a form in the Economic Census. These variables are broadly categorized as the characteristics of firms, measures of external factors, and the features of the survey design. We use the model for two purposes. First, by studying many of the factors that may be correlated with returns we aim to focus limited advertising and outreach resources to low-return segments of the population. Second, we use the model to investigate the efficacy of an unplanned i ","From the time the Census Bureau introduced an option to identify with multiple races on its survey forms, researchers within the Census Bureau have sought the best way to aggregate the possible responses into categories while preserving the information from an increasingly multiracial country. Classifying racial data into categories helps provide information to Census stakeholders so they can measure the Census Bureau's performance in identifying and correctly enumerating each population. As planning intensified for the 2010 Census Coverage Measurement study, research staff analyzed the Matching and Correct Enumeration rates of multiracial populations, in order to model the data. The paper details the techniques used to build models for Census Coverage data, by applying stepwise regression to the concept of CART modeling to partition the data into cells, and adding information criteria as a method of cross-validation. The paper also discusses: the specific issues inherent in modeling Dual-System Estimation data for this topic, and how they were addressed; the patterns of racial identification that were discovered; and the recommendations that were ultimately proposed. ","Rating scales such as Likert scales and semantic differential scales are very common in marketing research, customer satisfaction studies, psychometrics, opinion surveys, and numerous other fields. We  illustrate, review, and critique several forms of graphical presentation of results from studies using rating scales. These graphical forms include tables, bar charts of means, grouped bar charts, divided bar charts, ribbon charts, multiple pie charts, waffle plots, radar plots, and diverging stacked bar charts. We show the advantages of and recommend diverging stacked bar charts. We demonstrate how to create diverging stacked bar charts in R and Tableau. ","This paper presents the methods for the first assessment of classification error (measurement error for categorical data) in the National Crime Victimization Survey (NCVS). The NCVS is the only national survey that captures information on victimizations that are both reported and unreported to the authorities. To estimate reporting errors we use Markov latent class analysis (MLCA), a modeling technique that can be used to estimate classification error in panel data that does not require a gold-standard (error-free) measurement. This paper proposes a process by which an MLCA can be conducted on complex survey data, ensuring that all key assumptions are met or corrected for such that parameter estimates are valid. To conduct this analysis, we used a special longitudinal file containing all respondent waves from a sample of NCVS households. We determined that a model with fully constrained transition probabilities and partially constrained classification error probabilities fit the NCVS data the best. ","Previous research by Tucker et al. (2010), working with the Consumer Expenditure Survey (CE), explores the factor structure of measurement error indicators such as: interview length, extent and type of records used, the monthly reporting patterns, item missing , attempt information, and panel response behavior in a latent class model. Findings from this research, using cases from 2005 to 2009, yielded models with slightly poorer fit and less efficacy in predicting household expenditure than models using earlier data. While a number of recent revisions have been made to the CE that may have resulted in less measurement error overall, the differences in model fit and efficacy are worthy of investigation. In current research we add the use of the information booklet as an indicator of error. In addition, we examine the context of the model in much greater detail, examining subgroups where s ","This paper uses both the 2007-2009 Survey of Consumer Finances (SCF) panel surveys and the 2007 and 2010 SCF cross section surveys to investigate whether monetary incentives help data quality, conditional on responding to the survey, and help reduce time in the field. The 2007 SCF had a base incentive of $20, though many needed $50 before responding; the base incentive in 2009 and 2010 was $50.     ","Statistics Canada recently launched the Integrated Business Statistics Program (IBSP) project with the purpose of redesigning and integrating several annual and sub-annual business surveys. The objectives of the IBSP include improving efficiency and timeliness while ensuring high quality outputs. As part of the IBSP, a processing strategy that combines active collection management, editing, imputation, estimation and analysis is currently being defined. This strategy consists of periodically producing estimates and quality indicators based on available data, both reported and imputed. Quality indicators will be used to evaluate the level of quality achieved at different points of the collection process, to identify influential records requiring follow-up or micro-editing, and to serve as criteria to stop collection. In this presentation, the proposed processing strategy will be described ","The Bureau of Labor Statistics (BLS) intends to publish detailed economic statistics on industry employment in the green goods and services sector in the United States. This required BLS to adopt a definition of green goods and services. This paper reviews the process used by BLS to define green goods or services. This process began with an extensive review of definitions used in existing green data publications and studies from various agencies, including State governments, Eurostat, Statistics Canada, and private research institutions. With an understanding of the definitions from existing literature and after discussions with various environmental and government organizations, BLS reviewed each North American Industrial Classification System industry code to identify each industry's potential to produce green goods or services. This industry review led to the publication of a prelimin ","The Bureau of Labor Statistics (BLS) intends to publish detailed economic statistics on industry employment in the green goods and services sector in the United States. This paper discusses the methodology used by BLS to develop this new survey. The research methodology involved feasibility interviews, forms design, panel testing of the forms, follow-up interviews with panel respondents, and subsequent sample frame enrichment. The panels focused on the respondent burden when providing requested data items on the forms, the respondents' understanding of the BLS definition of green goods and services, response rates, prevalence of green activity at establishments, and an assessment of potential survey costs associated with the need for address refinement and potential non-response prompting. The outcome of this research was a proposed collection methodology, timing for the collection, the  ","In 2010, the U.S. Bureau of Labor Statistics (BLS) began preparations to embark on a \"green jobs\" initiative. The goals are to provide information for the U.S. economy on: 1) number of green jobs and trends over time; 2) industrial, occupational, and geographic distribution of these jobs; and 3) what these jobs pay. A new Green Goods and Services (GGS) Survey of 120,000 units is designed to measure the number of green jobs and trends over time as well as industrial and geographical distribution; the data collection began in 2011. The occupational distribution and what these jobs pay are being measured through BLS existing Occupational Employment Statistics (OES) Survey of 1.2 million units. For various reasons, both GGS and OES have independent sample designs--allocation, selection, data collection, and estimation. A major goal of this project is to maximize sample overlap between  ","The Bureau of Labor Statistics (BLS) is collecting and producing information on the occupational employment and wages of green jobs. BLS is using two approaches to measuring green jobs: the output approach, which identifies establishments that produce green goods and services, and the process approach, which identifies establishments that use environmentally friendly production technologies and practices. As part of the process approach, BLS has a special topic survey underway called the Green Technologies and Practices (GTP) survey. The GTP survey will measure jobs in which workers' duties involve making their establishment's production processes more environmentally friendly or use fewer natural resources. The GTP survey will first identify whether the establishment uses green technologies and practices and, if so, what workers they have, if any, whose duties are related to these practices.  Occupation and wage data will be collected for workers that spend more than half of their time working with green technologies or practices. ","The National Compensation Survey (NCS) is conducted by the Bureau of Labor Statistics (BLS) to compute measures of pay and benefits of America's workers. One output derived from the NCS data is the BLS Employer Costs for Employee Compensation (ECEC) series which produces quarterly estimates of employer costs for wages, salaries, and benefits. The ECEC weighting process consists of multiple steps including calculation of weights that reflect a 3-stage sampling design, nonresponse adjustment at the establishment level and occupational level, adjustment for sample rotation, and benchmarking. The weighting processes for the survey account for changes in employment totals due to establishments moving out of area, going out of business, or refusing to provide data, employment fluctuations, as well as occupations being abolished or not in scope for the survey. Data from a recent sample will dem ","The National Longitudinal Survey of Children and Youth is a longitudinal survey conducted by Statistics Canada. At cycle 7, there was a single non-response adjustment which was calculated using logistic regression models and RHGs (response homogeneous groups). Due to the difficulties encountered during collection and the low response rate at cycle 8 compared to previous cycles, several key changes were made to the weighting methodology with the goal of reducing the effect of higher non-response on the quality of the estimates. At cycle 8, two weight adjustments were made based on a sequential model to reflect the different aspects of non-response (refusals and non-contacts). The approach used to create RHGs was changed from a quantiles-based method at cycle 7 to cluster analysis at cycle 8, since the latter allows more flexibility. We also examined alternative weighting methods and compa ","In computing sampling weights for the Medical Expenditure Panel Survey (MEPS), base sampling weights are adjusted for nonresponse and under-coverage in various weighting steps including a final raking adjustment using selected demographic and socio-economic characteristics. The MEPS Full Year (FY) weight is released in two steps-a preliminary weight is released first with the FY Population Characteristics file, which is revised later for the final FY Consolidated file. The preliminary weight is produced without using poverty status in the raking adjustment as the poverty status variable is not available at that stage. The final weight is produced by adjusting for poverty status a few months later when the poverty status variable becomes available. This paper presents the results from research evaluating the impact of using education status of family reference person instead of family poverty status in the raking adjustment. Since education status is easy to derive and available early, the objective is to see if education status can be used to improve the preliminary FY weight or if education status can be used as an alternative to poverty status in producing the final FY weight. ","The U.S. Census Bureau publishes median estimates for several residential housing characteristics through the Survey of Construction (SOC), a national multistage probability sample that uses the modified half-sample (MHS) method of variance estimation. Currently, median estimates are produced using interpolation over data-dependent intervals (bins), after scaling by the 75th percentile. Thompson and Sigman (2000) demonstrated that this procedure has good statistical properties for medians computed from a highly skewed sample. This research explores decile estimation methods, with the constraints of maintaining reliable median estimates and using MHS replication. Using empirical data and simulated populations, we considered three different interpolation methods along with the traditional decile estimation method (no bins). Our simulation uses two finite populations to assess the statistical properties of each method via relative bias of the estimates, relative bias and stability of the variance estimates, and coverage rates. We found that a variant of the current procedure using the 95th percentile as a scaling factor produces decile estimates with the best statistical properties. "," In Probability proportional to size (PPS) sampling, the sizes for nonsampled units are not required for the usual Horvitz-Thompson or Hajek estimates, and this information is rarely included in public use data files. Previous studies have shown that incorporating information on the sizes of the nonsampled units through semiparamteric models can result in improved estimates. When the design variables that govern the selection mechanism, are missing, the sample design becomes informative and predictions need to be adjusted for the effect of selection. We present a general framework using Bayesian nonparametric mixture modeling with Dirichlet process priors for imputing the nonsampled size variables, when such information is not available to the statistician analyzing the data. ","A proportion of nonsampling error in telephone surveys is related to frameto-population linkage.   Individuals may have multiple phone numbers which might be included in the sample and/or multiple individuals may be using a single phone number, resulting in potential for errors association with frames using telephone numbers (Lessler and Kalsbeek, 1992). This research investigates the potential of frame-to-population linkage bias in the Behavioral Risk Factor Surveillance System (BRFSS), a large state-based survey conducted by each of the fifty states and coordinated by the Centers for Disease Control and Prevention, and discuss how the move to cell phones may alleviate such error. ","In sample surveys, it is often the case that there is insufficient sample size to obtain reliable direct estimates for a parameter of interest for certain domains.  Precision can be increased by introducing small area models which ``borrow strength' by connecting different areas and incrporating auxiliary covariate information.  This article considers multivariate generalized linear models for analyzing survey data, with special attention given to the mixed effect multinomial logistic regression model.  A comparison of the model where area-specific random effects are correlated is made with the model where area-specific random effects are assumed to have independent components.  A general theorem is presented which gives necessary and sufficient conditions for the propriety of the posterior.  An example is given where a simulated data set is analyzed using the model to estimate the proportion in different income levels for different demograph groups.  The results of this example indicate that we can have improved estimates over estimates based on a small area model for single components when we use area-specific random effects with correlated components. ","There are several notable distinctions between survey and market research practices. While survey researchers tend to focus on design and administration details, market researchers often pay less attention to the upfront activities and instead focus resources on the analysis and interpretation of the results. That is, survey researchers are typically design-heavy/analysis-light when in contrast market researchers are design-light/analysis-heavy. This historical divide, however, is not inevitable as it can be bridged by taking advantage of the best practices the two schools of research offer. Motivations for this collaboration include, on the one hand, the growing appreciation among survey researchers for more sophisticated analytical methods, and on the other, the increasing awareness among market researchers about the fact that such methods can fail to produce actionable intelligence when applied to survey data of questionable quality. The author will present examples of what additional information can be obtained from survey data when the employed analytics extend beyond what simple descriptive statistics may be able to reveal. In particular, this presentation will discuss ","This study explores multiple regression analysis with complex survey data. Four methods of multiple regression analysis, namely, ordinary least squares, weighted least squares, probability weighted least squares, and Quasi-Aitken probability weighted least squares are proposed for comparison by Monte Carlo approach to compare their efficiency based upon bias, variance, and MSE. The data from \"Taiwan Social Change Survey 2007\" collected under a stratified unequal probability sampling were used for empirical analysis to compare four proposed methods based upon the estimated regression coefficients and RMSE. The simulation results show that probability weighted least squares estimator and Quasi-Aitken weighted least square estimator perform better than others under the unequal probability design. The empirical results consist with the simulation results. The empirical results show that the education years of respondents in Taiwan has significant negative relationship with their age but has positive relationship with their parents' education years.  ","Estimating the number of factors to retain is one of the most important steps in exploratory factor analysis. The minimum eigenvalue method remains the dominant approach in applied research. Two other approaches that have shown promise are parallel analysis and the minimum-average-partial technique. The purpose of this study was to compare the effectiveness of these three methods in correctly identifying the number of factors when applied to a wide range of factor structures. A Monte Carlo simulation was conducted in which the communality range, number of factors, and variable-to-factor-ratio were varied. The three techniques were then applied to both the full correlation matrix and a reduced correlation matrix found by replacing the diagonal elements with squared multiple correlations. Results indicate that the accuracy of the investigated methods interacts substantially with factor structure changes, but parallel analysis of the reduced correlation matrix consistently performs quite well. ","The Linear Latent Structures (LLS) analysis assumes that the mutual correlations observed in survey variables reflect a hidden property of subjects that can be described by low-dimensional random vector. The statistical properties of LLS analysis, the algorithm for parameter estimates and its implementation, simulation studies, and application of LLS model to the National Long Term Care Survey (NLTCS) data are discussed. The results of analyses are compared numerically and analytically to predictions of the Latent Class and Grade of Membership analyses. Simulation studies demonstrate high quality of reconstruction of the major model components and demonstrate its potential to analyze survey datasets with 1000 or more questions. Applying the LLS model to the 1994 and 1999 NLTCS datasets (5,000+ individuals) with responses to over 200 questions on behavior factors, functional status, and c ","The focus of this talk will be on Bayesian approaches to two-stage cluster sampling. We will provide background information on the Polya posterior and Meeden's approach two stage cluster sampling. Also, we show Nandram and Sedransk's fully Bayesian approach when the parameter space is binary. Through simulation, we show how these two methods perform compared to classical design based estimates under a variety of assumptions on the known population. Through these simulations and literature review we propose extending further the fully Bayesian procedure to allow for constraints on total number of secondary sampling units. ","The Zenga Index is a recent inequality measure associated with a new inequality curve, the Zenga curve. The Zenga curve $Z(\\alpha)$ is the ratio of the mean income of the $100\\alpha \\%$ poorest to that of the $100(1-\\alpha)\\%$ richest. The Zenga index can also be expressed by means of the Lorenz Curve and some of its properties make it an interesting alternative to the Gini index. Like most other inequality measures, inference on the Zenga index is not straightforward. Some research on its properties and on estimation has already been conducted but inference in the sampling framework is still needed. In this paper, we propose an estimator and variance estimator for the Zenga index when estimated from a complex sampling design. The proposed variance estimator is based on linearization techniques and more specifically on the direct approach presented by Demnati and Rao. The quality of the resulting estimators are evaluated in Monte Carlo simulation studies on real sets of income data and robustness issues are briefly discussed. ","Survey data are increasingly collected through computer assisted modes. As a result, a new class of data - called paradata - is now available to survey methodologists. Typical examples are key-stroke files, capturing the navigation through the questionnaire, and time stamps, providing information such as date and time of each call attempt or the length of a question-answer sequence. Other examples are interviewer observations about a sampled household or neighborhood, recordings of vocal properties of the interviewer and respondent, information about interviewers and interviewing strategies. While the type of available paradata varies by mode, all share one feature--they are a by-product of the data collection process capturing information about that process.This course covers the great potential of paradata for social survey research. The course will give an introduction and overview of methodological issues involved in the collection and analysis of paradata. Research examples will be discussed, including but not limited to the use of paradata to monitor fieldwork activity, guide intervention decisions (e.g. through responsive design), and to address various total survey error components. Cases-studies will draw attention to the challenges in automated data capturing and modeling of the complex structure of paradata. ","The analysis of probability-based sample surveys requires specialized techniques that account for survey design elements, such as strata, clusters, and unequal weights. This workshop provides an overview of the basic functionality of the SAS/STAT\u00ae procedures that are developed specifically for selecting and analyzing probability samples for survey data. You will learn how to select probability samples with the SURVEYSELECT procedure, how to produce descriptive statistics with the SURVEYMEANS and SURVEYFREQ procedures, and how to build statistical models with the SURVEYREG, SURVEYLOGISTIC, and SURVEYPHREG procedures. Characteristics of different variance estimation techniques, including both Taylor series and replication methods, and domain (subpopulation) estimation techniques will also be discussed. The workshop is intended for a broad audience of statisticians who are interested in analyzing sample survey data. Familiarity with basic statistics, including regression analysis, is strongly recommended.","The Census Bureau has developed a new imputation-based methodology to improve the American Community Survey (ACS) estimates of the group quarters (GQ) population for small areas. The motivation for this work was that there are small geographies which either do not have GQ sample or have GQ sample that is not representative of the area, which could lead to distorted estimates of characteristics and/or total population. The new method imputes whole person records to GQ facilities which appear on the sampling frame but were not selected into sample. Previous evaluations have established the method's feasibility and allowed for its refinement. This evaluation aimed to establish that the new methodology improved the usability of estimates for census tracts. We applied the new methodology to the 2006-2010 ACS 5-year data and compared the imputation-based results with design-based results, using the 2010 Census as a benchmark. We found the imputation-based methods had a better distribution of GQ population by major type of GQ across tracts and a better distribution of demographic totals across tracts, as measured by the mean squared error. ","This study examined issues associated with the use of administrative data provided by health related facilities (HRF) and explored the reasons why records may have errors when used to complete census forms. Findings are based on focus group and unobtrusive ethnographic observations conducted during the 2010 Census in three HRF (including skilled nursing homes and hospice). Results from this study show that administrative listings maintained by the observed HRF were continually updated as a requirement for an accurate daily bill for services. However, pertinent demographic information required by the Census, such as race/ethnicity, was not always available in the administrative listings. Accurate enumeration was complicated by rapid population transitions with admissions, discharges and deaths. Suggestions such as the collection of additional information, such as admission and discharge date, and patients'last address would assist more accurate enumeration. ","Group Quarters (GQs) population data inform policies affecting institutionalized population in correctional and juvenile facilities and nursing facilities, as well as non-institutionalized population in college/university resident halls and military barracks. In this paper we examine practices and standards of identifying and classifying GQs facilities and enumerating the population residing in them. One size of standards does not fit all types of GQs and their complexities. We apply a census total coverage error and cost model to review and assess GQs census measurement tools and best practices. Census error and cost models for the enumeration of GQs are developed by benchmarking to total survey errors and cost models that integrate statistical and social science approaches (Groves, 1989; Groves, 2010). We use case studies to illustrate potential sources of census coverage errors and cost components appropriate for GQs. The intent is to demonstrate how the total survey error and cost paradigm might be extended to GQs for optimizing data quality within budgetary constraints. ","The American Community Survey is a widely used survey that is based on a complex sample. This paper analyzes the design effects (DE) of housing and economic variables. DE's measure the efficiency of the sample design relative to a hypothetical SRS of the same size. We find that the national DE's for personal earnings, health insurance, poverty and the percent of people living in rental housing are 2.0, 7.0, 10.0, and 21.7, respectively. These large DE's contrast with average DE's in sub-domains. Average state-level results range between 4.4 and 1.1. A potential cause for this result are outlying MSE's (the key component of the complex sample variance estimator). After excluding the top 5% of the MSE's the national DE's attenuate to expected levels. The national DE for health insurance is 2.6 versus a state average of 2.2. We use regression to identify cases that are likely to have outlying MSE's. Characteristics of interest include group-quarters, mode, demographics, and geography. We discuss implications for statistical inference in the 1, 3 and 5 year ACS files and speculate on why the published design factors (a variant of the DE measure) do not reflect the patterns we observe. ","Medical research involves discovery of basic principles while drug development involves the application of these principles to a product. Exploratory analyses of a drug development clinical trial data set are important, but should not be used in a licensing application as they often involve discovered endpoints or subgroups.   The drug regulation process is not like a final exam in which 'no help' is allowed; it is a process in which the study plans are evolved and iterated to get a better product to market (or to determine that it shouldn't be on the market).  Drugs are regulated by agencies. The FDA is the focus of this talk as I'm more familiar with procedures there. FDA has 3 main human product centers : Center for Biologics (CBER), Center for Devices and Radiologic Health (CDRH), Center for Drugs (CDER). The three centers have slightly different approval procedures. CDRH has the option of 'clearing' a device that is substantially similar to an already approved device.   It is vitally important that sponsors/statisticians understand the requirements of the FDA. The FDA publishes guidances to help clarify these issues. ","Scientific inquiry, public policy, and decision-making in industry are increasingly dependent on collection and interpretation of vast, complex data, and statisticians are uniquely qualified to lead these efforts. However, US students are not pursuing graduate training in statistics in the numbers required. Many still do not know enough about opportunities for statisticians, and there are far too few \"hands-on\" training programs available for graduate-level statistics students. Programs such as the NIH-sponsored Summer Institutes for Training in Biostatistics (SIBS), which expose US students to biostatistics and encourage them to attend graduate school; and opportunities such as NIH training grant programs and industry-sponsored internships are critical to achieving the US statistical workforce required; however, the current small number of such programs is insufficient. I will argue for a broad effort in which stakeholders from industry, government, and academia, come together to conceive of and support programs to increase the numbers of US students entering graduate programs in statistics and to provide them with essential practical training. ","Statistical, Mathematical and Computational Sciences (SMACS) have been invisible sciences in academia, industry and government, and are having an impeccable impact. Examples of the impact will be discussed. Also, the importance of being more visible, and proactively promoting the practice and profession of our disciplines will be discussed. Training of future problem solvers, with core, computational and communication skills in the era of data and observation and in a data-centric world, is a key to economic development and job creation. ","A fundamental component of the Nationwide Mutual Insurance Company management strategy is the use of analytics to create insights which serve as a rational basis for action to positively affect future business investment decisions. Nationwide has partnered with the Ohio State University to develop a problem formulation and data analysis strategy based on the application of the theory of chance cause systems of variation created by Dr. Walter Shewhart and championed by Dr. W. Edwards Deming. Dr. Shewhart gave us a new lens with which to formulate business problems involving complex data based on the application of his concept of rational subgrouping. His problem formulation and analytic methodology are not mathematically complex, but are profound in terms of creating practical customer and operational insights and predictions leading to more timely and effective actions. In this session we describe how the Nationwide Center for Advanced Customer Insights at the Ohio State University and Nationwide have used this methodology for the efficient creation of actionable insights from complex data, and we illustrate the approach in several business applications. ","Several important challenges are looming for U.S. Federal Statistical System: the costs of collecting data from U.S. Households and businesses are rising quickly because of the difficulties of contacting and addressing concerns of sample members; the demands for more timely and smaller area statistical information are growing; there are emerging new technologies that might be used to create new dats collection efficiencies; there are new digital data resources that can be used to improve statistical information; and Federal government budgets are likely to be flat or declining in the mid-run future.    ","Non-normally distributed data are common in sample surveys. We propose a robust Bayesian model-based estimator for finite population quantiles of non-normal survey data in probability-proportional-to-size sampling. We assume that the probability of inclusion is known for all the units in the finite population. The non-normal distribution of the continuous survey variable is approximated using a mixture of normal distributions, in which both the mean and the variance of the survey variable are modeled as a smooth function of the inclusion probabilities in each mixture component. A full Bayesian approach using the Markov chain Monte Carlo method is developed to obtain the posterior distribution of the finite population quantiles. We compare our proposed estimator with alternative estimators using simulations based on artificial data as well as a real finite population. ","We consider a Bayesian approach to study independence in a two-way contingency table which is obtained from a two-stage cluster sampling design with simple random sampling at both stages. For many large complex surveys, the Rao-Scott corrections to the standard chisquared  (or likelihood ratio) statistic is appropriate. If a procedure, based on simple random sampling rather than cluster sampling, is used to test for independence, the p-value can be too small resulting in significant evidence against the null hypothesis when there may be no  such evidence. For smaller surveys, the Rao-Scott corrections are not accurate, partly because the chi-squared test is inaccurate. In this article, we use a hierarchical Bayesian model to convert the cluster samples to simple random samples. This provides surrogates which can  be used to construct a distribution of the Bayes factor. We demonstrate our procedure on an example and also provide a simulation study which establishes our methodology as a viable alternative to the Rao-Scott approximation for relatively small two-stage cluster samples. ","We study Bayesian inference for the finite population total (T) in probability proportional to size (PPS) sampling. The sizes of nonsampled (NS) units are not required for the usual Horvitz-Thompson (HT) or Hajek estimates, and this information is rarely included in public use data files. Zheng and Little (JOS 2003) showed that including the sizes of the NS units as predictors in a spline model can result in improved point estimates of T, and later combine this with a Bayesian bootstrap (BB) model for the sizes, when they are only known for the sampled units. We further develop their methods by (a) including an unknown parameter to model heteroscedastic error variance in the spline model, an important modeling feature in the PPS setting; and (b) developing an improved Bayesian method for including summary information about the aggregate size of NS units. Simulation studies suggest that t ","We present a Bayesian method for making inferences about small area parameters using complex survey data. Although a number of Bayesian estimators have been suggested in the small area estimation literature, the methodological research on their design-based evaluation received relatively less attention. In this paper, we propose a new design-based evaluation criterion for comparing different priors and also other small area estimators. Using a real finite population and design-based simulations, we compare our Bayesian method with other small area estimation methods. ","The fraction of missing information (FMI) has recently been proposed as an alternative to the response rate for monitoring the quality of survey data and for tailoring survey design. In this paper we discuss the use of FMI as an indicator that can identify which nonrespondents should be targeted in order to reduce variability in estimates. Two-stage multiple imputation (Harel, 2007) is one method for estimating the relative contribution to FMI of individual observations. We investigate analytically and through simulation its properties as a method for identifying which nonrespondents have the largest impact on variability in outcomes. Since two-stage multiple imputation is computationally intensive, we also investigate the use of subject-level between-imputation variance as a proxy measure for contribution to FMI, a simpler measure to obtain. These methods are illustrated using data from the Ohio Family Health Survey (OFHS). ","We consider synthetic approaches to protecting data which are in the form of a bipartite graph, sometimes known in the context of social network data as two-mode or affiliation networks. Here the set of nodes is partitioned into two sets (individuals and \"groups\"). Edges in the graph represent an individual's association with a group. Such graphs are often characterized by very specific features (for example, almost all individuals have no more than two jobs, and most U.S. employers have fewer than 20 employees) and are typically accompanied by node-level attributes. The sparse but highly structured nature of this data make standard protection methods difficult to apply while maintaining data utility. We explore synthesis models which aim to preserve these important features while safeguarding confidentiality, with an application to the U.S. Census Bureau's Longitudinal Employer-Household Dynamics data. ","Access to realistic graph data is important for research, but at the  same time, there are privacy concerns in releasing the actual graph as  it may contain sensitive information. One solution is to release  synthetic graphs which satisfy both privacy and utility requirements.  In this work, we present algorithms to generate  synthetic graphs under the beta model. These algorithms satisfy the rigorous definition of edge differential privacy. We illustrate the usefulness of our algorithms by evaluating the utility of using synthetic data in estimating the parameters of the beta model for social networks. ","Statistical Disclosure Limitation and formal privacy modeling are applied to the interrelated, hierarchical, and temporal Quarterly Workforce Indicators. Synthetic data methods capture the essential features of confidential data while protecting the dependent data. Bootstrapping procedures are used to address the problem of dependent data that is not independent and identically distributed which is an issue for some differential privacy algorithms. Generalized Linear Mixed Models that handle time series and sparse counts are also investigated and improve on methods currently in use. Finally, probabilities of model failure are calculated over feasible ranges of protection. ","Statistical agencies and other organizations that disseminate data are obligated to protect data subjects' confidentiality. Even apparently safe \"anonymized\" datasets pose disclosure risks for individuals whose characteristics are sufficiently atypical. Hence, as part of their assessments of disclosure risks, many data stewards estimate the probabilities that sample uniques on sets of discrete key variables are also population uniques on those keys. This is typically done using log-linear modeling on the keys. However, log-linear models can yield biased estimates of cell probabilities for sparse contingency tables with many zero counts, which often occurs in databases with many keys. This bias can result in unreliable estimates of probabilities of uniqueness and, hence, misrepresentations of disclosure risks. We propose an alternative to log-linear models for datasets with sparse keys based on a Bayesian version of grade of membership (GoM) models. In our analyses, GoM models provide more accurate estimates of the total number of uniques in the samples, and they offer record-level predictions of uniqueness that dominate those based on log-linear models. ","Address lists originating from the Unites States Postal Service (USPS) have appeal as a possible alternative to the costly listing effort that has traditionally been used for multi-stage area household samples with in-person interviewing. A number of evaluations of the USPS lists have reached the consensus that the lists generally have excellent coverage in urban areas. However, these studies have consistently indicated poorer coverage in more rural areas, in areas with a large number of group quarters, and in fast-growing areas.    ","We propose a cost-efficient rejective sampling alternative to area sampling (for implementing two-phase multistage stratified cluster sampling designs) to satisfy three goals for reducing (1) nonresponse without field interviewing by making in-person contact for a leave-behind screener to selected HUs for eligibility and contact information for the telephone main interview;(2) undercoverage without counting and listing and without using half-open interval by adjusting selected HUs from the address frame; and(3) nonrespondent substitution bias without releasing additional replicates but by selecting alternate HUs at random in neighborhoods within each segment. The basic premise in rejective sampling for the proposed design is that it would be infeasible to sample directly from the domain subpopulation (such as all valid, eligible and responding HUs in a segment). ","When there is no sampling frame on the households, area sampling procedure provides a proper sampling frame and allows to sample household units in equal probability. Thus, we can obtain reliable estimates without sampling frame. Despite of these advantages, however, it has been hard for academic or non-academic researchers in Korea to conduct the surveys using area sampling because the list of enumeration districts is not open to the public, except for the samplers working in official statistics. But now area sampling in Korea, where information technology is highly advanced, is viable owing to the development of the Internet that enables researchers to get more precise information on the number of dwellings or structures, there location and address, etc. Based on this information, we can easily and correctly use area sampling procedure. This study shows how to conduct area sampling by using commercial maps, street view service and new address information map service offered on the Internet. We also present how to use the information on small areas available in the website of Statistics Korea. This methodology would lead to obtaining more reliable estimates. ","Drop points are a challenging component of the United States Postal Service's Delivery Sequence File (DSF) with respect to survey research. Drop points occur when there is a single mail receptacle serving multiple units, resulting in no designation between units. This lack of unit identifier impedes mail or phone contact. NORC conducted a limited field study of drop points in 2011, which showed that they are often geographically clustered. Additionally, buildings containing drop point units tend to be similar in structure to neighboring buildings without drop points.  At question is whether or not individuals living in drop points are different from residents at non-drop point addresses.  This paper attempts to determine if the presence of drop points on the DSF creates coverage bias and whom we would be missing if we eliminate drop points from the sampling frame. ","This paper describes the history of cell phone sample component of the California Health Interview Survey (CHIS). CHIS is a random digit dialing (RDD) telephone survey of California's population conducted since 2001. The objectives of CHIS are to examine issues in public health and health care and to monitor changes over time for Californians.     ","Surveys of cell-phone users targeting specific geographical areas are subject to inaccuracy in the estimated location of survey respondents. The county of cell-phone activation or \"wire center county\" on which many cell-phone sampling frames are stratified has an estimated county-level error rate of 62% and a state-level error rate of approximately 10%. Survey estimates based on sampling area are subject to bias due to the inclusion of outside residents, and survey estimates based on true area of residence with no screening are subject to increased sampling error when the probability of selection differs by sample area. In this paper we introduce a method for geographically screening cell-phone survey respondents, and describe the effects of alternative methods on sampling variability (design effects) and bias in survey estimates. We test our hypotheses using data collected by the March 2011 National Flu Survey, conducted by NORC at the University of Chicago on behalf of the Centers for Disease Control and Prevention. The survey utilized a dual-frame stratified sample designed to produce estimates of influenza vaccination rates nationally and among 20 sub-state sampling areas. ","Respondent driven sampling (RDS) is widely practiced for studies of hard-to-reach, hidden or elusive populations who cannot be easily sampled with traditional probability sampling approaches. By utilizing respondents as recruiters of subsequent respondents, RDS claims to produce representative samples after multiple waves. However, this claim requires meeting three critical assumptions based on incentive systems, first-order Markov chains, and network theory for modeling biased selections. Because it is difficult to test whether these assumptions are satisfied and because there is no viable external criterion to incorporate, error properties of RDS estimates have been under-examined. Moreover, most research data collected through RDS are not publicly available. Therefore, the survey methodology field has not seen an active assessment of RDS and its claimed representativeness. This study attempts to examine recruitment processes of RDS as a source of potential error using Sexual Acquisition and Transmission of HIV Cooperative Agreement Program, the only publicly available data source using RDS. ","Every census, when concluded, has some residual level of non-response and coverage error. This paper considers the case of a traditional census where enumeration at a usual place of residence takes place via direct contact with respondents using possibly multiple collection modes. The sources and patterns of these errors as well as strategies to minimize their occurrence and impact are both discussed. Commonality of causes and interactions between the errors are briefly considered. Means of measuring and adjusting for these errors are also noted. Although reference will be made to other censuses, the Canadian census will primarily be used illustrate the dicussion. ","One of the key goals for the 2011 UK Census was to produce high quality local authority population estimates. A number of innovations were developed to produce the best possible Census count, including the use of post-out with an improved address register, a flexible field force, and a wide reaching community liaison and engagement programme. Following the Census, the Census Coverage Survey re-enumerated a sample of the population with the aim of estimating the level of Census coverage through the use of a Dual System Estimator. The methodology was an improved version of the 2001 One-Number Census, this time including a method for measuring overcoverage and adjustments for observed correlation bias. This presentation will give an overview of the 2011 Census and the coverage assessment methodology, and then report on the outcomes of the Census fieldwork, the Census Coverage Survey and the estimation processes including some comparisons with the 2001 Census outcomes. This will provide some early results from, and analysis of, the 2011 Census processes. ","This paper shows the results of evaluating the coverage of the 2010 U.S. Census based on the Census Coverage Measurement (CCM) survey. The U.S. Census Bureau estimated the net error of undercount or overcount based on the dual system estimation methodology. The program also estimated the components of census coverage that included erroneous enumerations and omissions. This paper shows the estimated results of this evaluation of the 2010 Census. Results are shown for the population and housing unit coverage. The results shown focus on demographic, tenure and housing unit characteristics; governmental entities and census operations. ","Combining information from different source is an important practical problem. The source of information can come from a probability sampling with direct measurement, from another probability sampling with indirect measurement, or from auxiliary area level information. We consider the area-level model approach to small area estimation with at least two survey information. The way we combine information is based on the generalized least squares estimation from the measurement error model, where the sampling error of the survey estimates of direct measurement can be treated as the measurement error. Mean square estimation is also discussed. The proposed method is applied to the Korean labor force survey problem. ","Motivated by the problems of `quality filtering' of estimated counts in American Community Survey (ACS) tables, and of reporting small-domain coverage results from the Census Coverage Measurement (CCM) program, this paper studies methods for placing confidence bounds on proportions for cells and tables, estimated from complex surveys, in which the estimated counts are zeroes. While coefficients of variation are generally used in measuring the quality of estimated counts, they do not make sense for assessing validity of very small estimated counts. The problem is formulated here in terms of (upper) confidence bounds for unknown proportions. We discuss methods of creating confidence bounds from small-area models including logistic, beta-binomial, and variance-stabilized (arcsin square root transformed) linear models. The model-based confidence bounds are compared with single-cell bounds de ","Small area estimation often involves constructing predictions with an estimated model followed by a benchmarking step. In the benchmarking operation the predictions are modified so that weighted sums satisfy constraints. The most common constraint is the constraint that a weighted sum of the predictions is equal to the same weighted sum of the original observations. Augmented models as a method of imposing the constraint are investigated for both linear and nonlinear models. Variance estimators for benchmarked predictors are presented. ","We consider small area estimation problems when covariates are measured with error. A bias corrected profile likelihood approach is taken and an asymptotic expression for the mean squared errors of the small area estimators is provided. ","The U.S. Environmental Protection Agency produces national estimates of water quality using data from the National Aquatic Resources Surveys (NARS). The NARS program consists of four surveys conducted on a five-year cycle, each focusing on a different type of water body: lakes, wetlands, coastal waters, and rivers and streams. Individual states are interested in estimating water quality for their state and local areas; however, NARS is not designed to produce design-based estimates at that level of geography. Consequently, EPA is evaluating small area estimation approaches. Westat is assessing supplemental data sources for use with the Wadeable Streams Assessment (WSA) which collected water quality data from 1,400 sites. This presentation will report on progress in identifying data sources on all streams that might be used to produce small area estimates for phosphorus and nitrogen, which are of particular interest in environmental assessments. Different data sources are available at different hierarchical aggregations of streams which create additional complexity to designing approaches for SAE. These data sources will inform what SAE methodologies might be appropriate. ","Critical studies set forth unauthorized population estimates which are aggregate estimates derived by a residual technique rather than based on individual-level immigration status.  The population-level or census-level survey estimate of foreign-born persons (or aliens) is compared with estimates of the legally resident foreign-born (or alien) population based primarily on administrative data. The discussion summarizes the value for providing insights on the origins, demographic characteristics, geographic distribution of the undocumented alien population, and associated annual average measures of net unauthorized migration or population change due to unauthorized migration. These estimates have implications not only for evaluating immigration enforcement but also for projecting future workers and beneficiaries and evaluating finances for the Old-Age and Survivors Disability Insurance programs. ","We address the problem of performing a survey of a group of individuals present in a given population, when information about the complete list of the members is missing or partially unknown. This problem is particularly relevant in migration analysis, where many of the individuals are possibly unauthorised migrants and therefore not formally registered or accounted for in official statistics.   The sampling procedure consists in gathering additional information from a subset of individuals, which is then used to build suitable weights to re-proportion the sample. The bias introduced by the sampling procedure can be then corrected.   We assume that the experimenter has information about a number of \"aggregation centres\" that are regularly visited by the migrants and where the survey can be conducted.   If the experimenter can estimate the relative importance of each centre (possibly with re ","The United States is home to approximately 11 million unauthorized immigrants. Careful estimates at the state level are published regularly, but these do not describe the distribution of the population within each state. We have developed the first-ever estimates of the nation's unauthorized immigrant population by county and zip code. We use several datasets, relying primarily on newly generated and recently available data derived from IRS tax filings. Since 1996, unauthorized immigrants have been able to file tax returns using Individual Taxpayer Identification Numbers (ITINs) on their 1040 forms. The IRS has made counts of ITIN filers for each zip code publicly available for calendar years 2001-2009, and we have used these to estimate the geographical distribution of undocumented immigrants within states through 2008. This paper will extend the analysis to 2009, examining what impact the recent decline in the estimated population has had on its distribution within each state.    ","The health, effects and overall well-being of U.S. foreign-born residents by legal status are some of the most difficult demographic phenomena to measure for at least two reasons. First, migration is not a tangible biological event. Second, even when representative sample data include nativity and other variables of interest, they almost never include legal status (e.g., American Community Survey). However, since 1994 demographers, community-based organizations and statisticians have been collecting representative data that permit one to generate credible estimates of the number and characteristics of legal and unauthorized migrants. This paper explains and evaluates the community-based biodemographic survey methodology that has been employed to study legal and unauthorized Brazilian, Dominican and Mexican migrants residing in metropolitan Boston and Los Angeles. We also estimate how interviewer characteristics influence migrant responses to sensitive questions and requests for biological data; and whether estimated legal status differences in access to medical care and health from household survey data are supported by evidence available from administrative (Medicaid) records. ","This paper presents the latest estimates of the number of unauthorized immigrants living in the United States from the American Community Survey (2010) and the Current Population Survey (2011), population trends using consistently-estimated data, and information on selected socio- demographic characteristics of the group. Residual methods have been used to estimate the size of the unauthorized immigrant population in the US since the early 1980s based on comparisons with data from decennial censuses (1980-2000), the CPS (1995-), and the ACS (2005-).    ","In a survey of orphans and vulnerable children (OVCs) in Uganda designed to compare current status of OVCs after the termination of the Track 1 program across partners, the design effects vary substantially by partner and estimate. We examine the variances and design effects in an attempt to determine why they vary so substantially, and differently, by partner. The sample design is stratified by partner, but the ultimate effect is one of clustering. A primary goal of the study is to compare how well partners transitioned OVCs to new aid programs at the termination of the Track 1 program. The majority of estimates are proportions of OVCs receiving different types of aid after the termination of the program. Comparisons of these proportions by partner are hampered by large variances for selected measures. While mean and median design effects are similar for three partners, one partner has design effects of about double the other three, and one partner has design effects (and estimates) near zero. Even among the three partners with similar mean and median design effects, the pattern by estimate varies noticeably. ","Background:International Rescue Committee (IRC) surveyed two Afghan refugee camps in Pakistan before and after humanitarian aid. The objective of the project was to compare U5 mortality (P[Life ? 5 years]) at baseline (2007) and endline (2010).   Method: Nonparametric maximum likelihood and least squares were used to estimate survivor functions from current status data, because some ages at deaths were inadmissable. The Kaplan-Meier reliability estimator was used on admissable ages at deaths.   Conclusion: Survivor function estimates from current status and admissible ages at deaths agreed tolerably. The U5 estimates were 10% at baseline and 4% at endline. Pakistan U5 was 8.7% in 2009 (Wikipedia). Infant mortality at age one was almost 4% at baseline and endline. Pakistan infant mortality was 6.7% in 2010 (World Bank). ","Statistics-Without-Borders provided technical assistance to SciMetrica which conducted a survey in June 2010 of the economic impact of the January 2010 Haitian earthquake using a random digit dial (RDD) sample of mobile phone numbers. Based on the survey, Ashley, Fritz Scheuren (2011) discussed the potential of mobile phone interviews in a natural disaster setting and discussed about related methodological directions, including the computation of survey weights. We summarize the survey to report the magnitude of economic impact: 61.4% (69% in Port-au-Prince and 50.5% in other areas) of the residency buildings were partially or totally destroyed in the earthquake. Among those relocated, 28.4% of the adults were still living in tent or cabin. Only 5% relocated to a house more than one hour away from their original residence. Overall 49.4% of the households experienced the change in its members. Among such households, on average, 1.9 people (1.0 child) who had shared their cooking pot on January 12th were not living together anymore. In the early analysis, we are finding that the disintegration of family is more severe among families whose head had less than high school education. ","Various efforts to build statistical capacity broadly (with respect to health care and beyond) on three continents will be reviewed. Progress will be reported, future plans outlined, and successes, failures, difficulties, and unexpected outcomes will be discussed. ","Statisticians are aware of the fact that measures such as: mean, variance, Pearson correlation coefficient are disproportionately influenced by relatively few extremely large observations and, therefore, are unreliable as statistical measures in comparing overall quality of data with an extremely skewed distribution. Tabular data cells follow an extremely skewed distribution. In this paper we show that linear-programming-based controlled tabular adjustments (CTA), which generates synthetic tabular data (Dandekar2001), makes use of a least absolute difference linear regression model and is well-suited to control overall data quality on its own without additional steps proposed by quality preserving controlled tabular adjustments (QP-CTA) that has been heavily promoted to the statistical community since 2003.  ","A major responsibility of the Disclosure Review Board (DRB) at the National Center for Health Statistics (NCHS) is to protect the confidentiality and privacy of its survey respondents, persons or establishments. Before any public-use files (PUFs) can be released, the NCHS DRB must feel confident that measures have been taken to ensure disclosure avoidance where necessary. There are several surveys conducted by the NCHS and each has its own techniques for avoiding disclosure of identifiable data. This paper will discuss some of these disclosure avoidance techniques such as perturbation of variables, coarsening of data, swapping variables, suppression, and not releasing the data as a PUF. In the case where data are not released as a PUF, researchers could possibly analyze a restricted data set under a signed confidentiality agreement via the NCHS Research Data Center.  ","Matching with publicly available data bases was the first component of disclosure avoidance in NCES public-use data files. Typically matches are avoided by recoding variables, but at times perturbations are required. In 2002, the NCES DRB added the use of data swapping in NCES restricted-use and public-use data files as an additional means of disclosure avoidance. To facilitate this change, NCES supported the development of \"Dataswap\" software. Pre-swapped and swapped data were compared using different levels of swapping to examine the impact of swapping on data-quality. In 2010, a measure of the Hellinger Distance was added to the Dataswap software. This measure is compared across different swapping alternatives to identify an acceptable alternative that does the least \"harm\" to the distribution of the data, thus enhancing data quality. The most recent innovation involves the inclusion of a routine in Dataswap that identifies the variables and cases that are most at risk of disclosure, thus allowing for a more precise targeting of the data to be swapped. This is used to increase the level of data protection, while targeting the swapping where it is most effective. ","Disclosure in tabular data is associated with risky (sensitive) cells: in count data these are small counts, and in magnitude data cells where a total or net value is dominated by a few contributors. Cell suppression was the first technique developed for disclosure limitation in tabular data. Its use dates to the 1940s when only primary suppression-suppressing only risky cells-was performed. Additive relationships between cells renders primary suppression ineffective, thus complementary cell suppression (CCS) was developed. CCS involves suppressing additional, nonsensitive cells to thwart narrow estimation of sensitive values. CCS is an NP-hard mathematical problem-polynomial time algorithms for its solution are unlikely to exist. Nevertheless, sophisticated algorithms and software for CCS have been developed over the past 40 years. Unfortunately, internationally and within the US, many organizations employ primary suppression (only) or ad hoc CCS Moreover, user-developed techniques to analyze or work around suppressed data are not well understood. We report on a recent NISS workshop aimed at examining and improving suppression-based SDL and analysis. ","Recent developments in survey quality indicators may provide promising alternatives to the response rate. Survey quality indicators evaluate the representativeness of respondents, which reflects the quality of the survey inferences in terms of bias. We study the performance of these indicators through investigation of their sampling properties under different response rates and non-response mechanisms. NHANES data from 1999-2008 is used as the finite population for the simulation experiments. The experiments are conducted under two assumptions. First, all covariates associated with the true response probability are available and are included in the model. Second, auxiliary variables such as variables from a frame are known at the subject-level for both respondents and nonrespondents. A total of 72 scenarios from different combinations of experimental parameters are explored. The results  ","In late summer of 2011 the Bureau of Labor Statistics fielded one of two surveys designed to assess the number of \"green\" jobs in the United States, the distribution of these jobs by industry, occupation, and geographic area, and the wages of those working in \"green\" jobs. The survey will end data collection in early 2012 with a likely response rate of approximately 70% (or 65% of the sampled employment).1 The frame from which the sample of 120,000 was drawn (the Quarterly Census of Employment and Wages Longitudinal Database) contains information on establishment employment and age, industry, geographic location, as well as wage information and other economic information reported in the past. In addition, a total of 13,000 establishments were identified as likely producers of a green good or service. This work uses logistic regression to model response probability utilizing both auxiliary and design variables. It is suspected that many important estimates, such as the number of green jobs, produced from this survey are likely related to response - that is, they are not missing at random. We suggest simple procedures to test whether the response is missing at random or not. ","In the past, the Census Bureau has constructed noninterview adjustment cells by using deliberately chosen characteristics of the frame to define the cells. For the Beginning Teacher Longitudinal Survey (BTLS), the sponsor, the National Center for Education Statistics (NCES), used CHAID (Chi-Squared Automatic Interaction Detector) Analysis to develop the cells. This paper will focus on weighting one cycle of BTLS first using the CHAID analysis and consider the impact on bias analysis. That same cycle of BTLS was then run through the traditional weighting process and again considered the impact on bias analysis. A comparison of the two methods will be used to determine which of the two has a lower variance and a smaller bias. ","This paper seeks to evaluate the effectiveness of using metropolitan statistical area (MSA) geographic status as an adjustment variable in the cell-phone sample nonresponse adjustment for the National Immunization Survey (NIS) and investigates the potential to utilize the level-of-effort information collected during the telephone interview to improve the cell-phone sample nonresponse adjustment. We examined the viability of using two alternative measures of interviewing level of effort for cell-phone nonresponse adjustment: the number of call attempts to resolve a cell-phone number and soft refusal status which is an indicator of initial reluctance by the respondent to participate in the survey. The current approach of using MSA status in the NIS cell-phone sample nonresponse adjustment is compared to nonresponse adjustment utilizing the two alternative level-of-effort variables. We failed to find sufficient evidence to justify using the two new approaches or the current approach for the cell-phone nonresponse adjustment in the NIS. Consequently, eliminating the current MSA status cell-phone sample nonresponse adjustment is under consideration as we develop the future NIS weighting methodology.  ","Many sample surveys gather information on the accuracy of the data collection process.   For example, for questions related to salary in face--to--face interviews, the interviewer may record if the interviewee consulted the latest payment slip, an early payment slip or did not consulted any other source. Another example is when the interviewer records if a set of questions were answered very accurately, fairly accurately, not very accurately or not at all accurately. Although these types of data can be informative of the presence of measurement error, little is known how to use the accuracy data to produce corrected estimates of the parameters of interest.   In this paper, we propose a methodology that incorporates accuracy variables and allows for adjusting for measurement error. Statistical properties of the estimators are examined through a simulation study. The results demonstrate the adjustments outperform the simple estimator that ignores the measurement error. ","Total survey error (TSE) is the sum of errors that arise at every step of a survey. It includes both sampling and non-sampling errors (e.g., population coverage, nonresponse, and measurement error). In this study, we assess TSE in estimated vaccination coverage rates derived from the NIS, which is conducted annually by the Centers for Disease Control and Prevention. We utilize 2009-2010 NIS data for children 19-35 months and adolescents 13-17 years, specify a distribution function for each component of sampling and non-sampling error, and derive estimates of these component distributions from the best sources available such as the American Community Survey and the National Health Interview Survey. We apply a Monte Carlo simulation-based approach to combine the components of error into a total error distribution for each estimate examined. Findings suggest small biases exist (-0.2 to 2.1 percentage points) in the 2010 NIS vaccination coverage rate estimates for children in spite of escalating cell-phone-only households in the U.S. Bias in vaccination coverage rate estimates for adolescents was larger than that for children (over-estimation of 1.3 to 6.7 percentage points). ","Monitoring feelings of (un)safety in relation to victimization of crime is one of the underlying reasons of why victimization surveys were initially developed. To this day, the variable measuring feelings of (un)safety remains one of the most important indicators to set out a policy concerning safety.   Since 2008, the Dutch NCVS has a very complex design. It is a 2x2 mode survey that consists of: (a) a fixed part of about 20.000 respondents in order to ensure results at police-regional level and (b) a part with oversampling in areas that can change from year to year, depending on the community's needs. This complex and flexible design requests for a tool that monitors the main results from the survey and - when required - signals unusual outcomes. Visualizing the data helps, but in addition an EWMA control chart for global spatial statistics (Moran's I) is proposed here. In the event that a control chart signals, one can further examine the local spatial statistics to determine the source of the signal.   This paper shows the results of these analyses and indicates how they can be used to characterize the effects of shifts in the spatial dispersion across the country. ","This paper discusses the allocation of sample for the 2010 redesign of the Consumer Expenditures Surveys (CE). CE is a national survey of U.S. households that measures the expenditures made by households on different categories of items. The survey actually consists of two sub-surveys. Small expenditures are collected in the CE Diary Survey and larger expenditures are collected in the CE Quarterly Interview Survey. Both CE surveys use a two-stage sample design, where the first-stage is a sample of counties or groups of counties and the second-stage is a sample of households within the counties or groups of counties. In this paper we discuss methods for optimally allocating the survey's overall sample to the two sub-surveys where optimality is with respect to minimizing the survey's overall variance. Then, we compare several alternative methods for optimally allocating the sample to the s ","After every decennial census, many surveys including the Consumer Expenditure Survey (CE) redefine their primary sampling units (PSUs), which are small sets of adjacent counties.  CE conducts expenditure surveys in metropolitan, micropolitan, and rural areas in the United States.  For metropolitan and micropolitan areas, the PSUs are the U.S. Office of Management and Budget's \"core-based statistical areas\" (CBSAs).  Counties which are not in a metropolitan or micropolitan CBSA are rural and CE must group these counties into a PSU.  For CE, rural PSUs are small clusters of adjacent counties that are required to have a minimum population of 7,500 people and a maximum area of 3,000 square miles.  Unlike metropolitan and micropolitan CBSA's, rural PSU's are also required to be within a state boundary.  Using an adjacency matrix and zero-one integer linear programming, a \"first-cut\" assignment of rural counties to a PSU is made.  Since the algorithm does not account for geographical obstacles such as rivers and mountains, input from field representatives is used in the final assignment. ","The Internal Revenue Service periodically conducts complex surveys to measure the pre-filing and filing burden of individual taxpayers in response to the requirements of the U.S. federal tax system. The sample design for the survey needs to balance three major objectives. The first is to ensure a sufficient number of respondents within and across strata to meet the needs of the modeling of compliance burden. The second is that it must be efficient so that the estimates are reliable. The third is to facilitate the comparisons between the current year study and the previous study. An iterative procedure for a stratified random sample design is proposed to search for the optimal sample allocation. The proposed procedure utilizes the optimality in the Neyman allocation method, and incorporates the minimum sample size requirements for modeling and different nonresponse rates across strata.  ","The National Compensation Survey is conducted by the Bureau of Labor Statistics to compute measures of the pay and benefits for America's workers. Since the early 1990's, the survey design used a three-stage sample design to select samples of areas, establishments, and jobs for which wage and benefit data are collected periodically over a five-year rotation. In 2011, we presented a new two-stage sample design for private industry establishments that introduces a three-year rotation of establishment samples. This new design is a national design without any area sampling under which we select samples of establishments and jobs in the two stages of selection. This paper will explore design alternatives for selection of establishments in the State and local government sectors of the economy that are more consistent with the new private industry design. Design topics that are being studied and will be presented include sample rotation, allocation, sample frame preparation, establishment selection, and sample initiation scheduling. Recommendations for the design to be used when selecting future public sector samples will be presented.  ","The National Compensation Survey (NCS) conducted by the Bureau of Labor Statistics (BLS) is an establishment survey for which the sample selection methodology has been redesigned to reflect a change in the scope of the survey. The new sample design will be implemented in 2012. The main feature of the redesign is the change from a three-stage area based design to a new two-stage design. Under the new design, establishments are selected by the probability proportional to size (PPS) sampling method as the primary sampling unit (PSU) at the first-stage, and occupations again by the PPS method at the second-stage from the sampled establishments. For this new design, sample allocation needs to be done to meet the precision objectives of the survey. For this purpose, variance components are estimated, response rates are projected, and based on this information the national private industry samp ","The National Compensation Survey, conducted by the Bureau of Labor Statistics, is an establishment survey sampled yearly from a national frame using probability proportionate to establishment employment size. The national frame is developed from administrative files maintained quarterly by the States for the Unemployment Insurance program. Each establishment on the frame is assigned a measure of employment size equal to the employment in the third month of the frame quarter. In 2010, approximately 15% of the establishments in the frame, which includes seasonal businesses, reported an employment of zero, an employment that must be adjusted to ensure that the establishment has a chance of selection. In the past, establishments with zero employment have been assigned an employment equal to one employee, with some cases resulting in large weights for the occupations selected from these compa ","For over three decades HUD has conducted a series of national surveys examining the eligibility and rent determination processes in assisted-housing programs. While the sample designs for these surveys have been similar, there have been significant modifications over the three decades. The HUD Quality Control Studies have always involved three phases or stages -- geographically defined PSUs, assisted-housing projects (AHPs) of different types, and the selection of households within AHPs. However, the exact population, the estimates required, and the information contained in the frame files have changed over time. We explore some of the modifications that have been made over the years, including the change in PSUs from congressional districts to county-based clusters, the diverse structure of AHPs, the shift from record abstraction to tenant interviews and third-party verification, and the types of estimates for different assisted-housing programs over the years. ","Model-assisted regression estimators are popular in sample surveys for making use of auxiliary information and improving the Horvitz-Thompson estimators of population totals. In the presence of strata and unequal probability sampling, however, there are several ways to form model-assisted regression estimators, i.e., regression within each stratum or regression by combining all strata, and a separate ratio adjustment for population size, or a combined ratio adjustment, or no adjustment. In our paper, the asymptotic normalities of the estimators are established under two different asymptotic settings. In both cases, we consider variance estimation by applying substitution or the bootstrap, which is useful in large sample inference. The relative efficiencies among these six estimators are obtained based on the asymptotic properties under two settings. Some simulation results are presented to examine finite sample performances of regression estimators and their variance estimators. ","The Medical Expenditure Panel Survey Insurance Component is an annual survey of establishments and governments sponsored by the Agency for Healthcare Research and Quality and conducted by the U.S. Census Bureau. The survey collects data on employee characteristics and availability and options of employer provided health insurance. Primary information collected includes availability of employer sponsored insurance, types of insurance offered, uptake by employees, employee premiums, and employer contributions. Previous work on the impact of imputation on variance estimation has indicated that for a na\u00efve jackknife variance estimate, the underestimation of the variance of means and proportions is on the order of ten percent. However, in that work, the Rao-Shao type adjustment produced surprising and sometimes unrealistic results for some estimated proportions. It was observed that the adjusted replicate value could be out of range, i.e. negative, for a proportion.  The current work investigates this issue using simulations. ","The 2010 National Survey of College Graduates (NSCG) selected its sample from 2009 American Community Survey respondents, creating a two-phase sample design. This creates variance estimation complexities when using replication methods, as there are two sample designs to account for. One solution is to create a set of replicates for each sample design phase but this can be unwieldy as it leads to a large number of replicates. Another solution, which we pursued, is to use replicates from the first-phase sample and adjust them to account for the second-phase sample design. In particular, we used a Reweighted Expansion Estimator (REE) that post-stratifies the second-phase sampling weights back to the first-phase estimated totals within each of the second-phase sampling stratum. We conducted a simulation study to evaluate the performance of the REE estimator with different replication methods, including successive difference, grouped-jackknife, and balanced repeated replication. Initial results showed poor performance with some replicate variance estimates due to the inability of the replicates to capture the systematic sample selection used in the second-phase sample design. Accounting for the systematic second-phase sample with post-stratification resulted in good performance for the REE estimator with all the replication methods and the successive difference replication method was ultimately chosen as the 2010 NSCG production method. ","Multiple imputation has become one of the most popular and successful methods for dealing with missing data in statistical analyses.  Multiple imputation allows one to use observed data to model relationships among variables, represent uncertainty in missing values through multiple draws from conditional distributions, and produce both point estimates and variance estimates for parameters.  Variance estimates incorporate contributions to variance from both within and between completed data set analyses.  Despite the advantages of multiple imputation, it has been noted that multiple imputation variance estimators can be biased.  Bias is possible when, in the imputation model, survey weights are not used.  Calibration weighting and its familiar forms, including raking and post-stratification, are often used in sample surveys to adjust sample estimates to match control total values and reduce variance.  We explore the possibility of using calibration weighting in combination with multiple imputation to remove or reduce bias in multiple imputation variance estimation when survey weights are not used in the imputation model.  Methods could apply to both sample survey and more general study design contexts. ","Past research indicates that employment size, industry sector, multi-establishment status, and metropolitan area size, along with important interactions, have a significant impact on an establishment's propensity to respond to the Bureau of Labor Statistics Occupational Employment Statistics survey (OES). Using administrative wage data linked to the sample, we find that these establishment characteristics are related to wages; wage estimates are a major OES outcome variable. In this paper, we investigate the use of the administrative data for imputing missing data due to nonresponse. The multiple imputation method focuses on adjusting the OES wage estimates with this auxiliary data to reduce potential bias. ","The Survey of Income and Program Participation(SIPP)is a nationally representative longitudinal household survey, conducted every four months over a 48-month period. In order to improve data collection and to reduce costs, as well as the respondents' burden, the 2006 redesign initiative devised SIPP-EHC(Event History Calendar), of which one feature is to conduct a survey once a year as opposed to every four months. This paper compares the SIPP and SIPP-EHC survey instruments with respect to item nonresponses for a set of income and asset variables. Specifically, this study examines item nonresponse rate of the variables that measure SNAP eligibility: earned income, unearned income, and resources. Using logistic regression, we test two hypotheses:1)item non-responses are invariant to the type of survey instrument; and 2)the difference in the type of survey instruments is due to the improvements in the SIPP-EHC instrument. The regression analysis result shows that the instrument has a statistically significant effect on the item nonresponse rates. For example, the SIPP-EHC instrument reduced the percent of item nonresponses of Federal Supplemental Security Income about 20 percent ","In any large scale survey there are usually selected sample units that provide no responses,   and the missing information from these nonrespondents could have a negative impact on data quality.   Response propensity models estimate the likelihood that a given unit will respond to the survey based on known characteristics.   These models are important in understanding survey response   and the impact that intentional or unintentional treatments or actions will have on response.   These models usually incorporate data available from the sample frame,   as well as administrative data obtained during the attempts to collect the survey data.     ","Early methods to impute missing household panel survey data were designed to produce longitudinally consistent population estimates. Given the growing demand for panel data analysis, new imputation methods are needed to produce longitudinal consistency at the person level. This paper introduces an imputation technique that applies the estimates from models that generate conditional probabilities subject to log-linear constraints. Our approach rests on expanding a specific parameterization for conditional probabilities as proposed by Thibaudeau, Slud and Gottschalck (2011). The parameterization is manipulated so that it applies to the situation where some survey respondents skip waves in the context of a longitudinal survey. The general model we use to represent this situation is a first order Markov transition process between wave, with double transitions to represent the gaps. Our modeling approach is combined with hot-deck imputation to provide imputations for all the requested items. The technique is tested using the Survey of Income and Program Participation. ","There are numerous ways to address nonresponse bias adjustment in surveys; two such methods are calibration weighting and propensity score models. Calibration is a viable technique when good external benchmarks exist; however, good external benchmarks are not always available.  An alternative method to calibration is to use propensity scores to adjust for nonresponse.  There are at least three main modeling techniques used to create propensity scores, but little if any research has focused on which methods provide the best propensity scores in terms of nonresponse adjustment.  This paper compares calibration weights with three propensity score adjustment methods.  One propensity weight is based on logistic regression models; the other two are based on classification trees (using either a single or an ensemble tree approach). ","Address-based sampling (ABS) frames can support a wide variety of survey designs due to the information that can be appended to a mailing address. This information allows for a variety of contact and data collection modes, including mail, landline telephone, web, and in-person. As researchers experiment with and debate these methods, the amount of available data that can be appended to address files has increased and, in most cases, improved in its quality and completeness.     ","With the commercial availability of address-based sampling frames sourced from the U.S. Postal Service files, address-based sampling is offered as a higher coverage alternative to random digit dialing samples. Telephone contacts and data collection are less expensive than their in-person counterparts, however, so some vendors append telephone numbers to addresses where their sources indicate a match. The coverage of the addresses with telephone numbers is far from complete, leading to an increase in mixed mode surveys, where the alternative is often in-person. The greater the coverage of the telephone numbers, the lower the cost of the survey.     ","Arbitron implemented In-Person Recruitment (IPR) of survey respondents in July 2010. The purpose was to improve the response rate and sample representation for certain demographic groups that were difficult to engage using traditional RDD or mail recruitment methods. This mode of selecting panelists poses a question, however. Does the new method unduly influence the listening patterns of panelists selected via IPR with that of traditionally sampled panelists?    ","Racial and Ethnic Approaches to Community Health across the U.S. Risk Factor Survey (REACH U.S.) provides the Centers for Disease Control and Prevention (CDC) and the communities with quantitative data to track the progress and achievements of the community intervention programs to eliminate health disparities. For REACH U.S., NORC conducts multi-mode surveys using address-based sampling frames enhanced with race/ethnicity information. Specifically, REACH U.S. employs two sampling frames: 1- An address-based sampling (ABS) frame derived from the U.S. Postal Service (USPS) Delivery Sequence File (DSF), and 2- A race-enhanced targeted list. Also, REACH U.S. collects data mainly via two modes of data collection (telephone and mail interviews) to improve contact rates in the targeted subpopulations. This paper examines the REACH U.S. Year 3 achieved sample and investigates the association be ","The swift adoption of cell phones in the last few years has now left some random digit dial (RDD) surveys with a substantial problem of undercoverage. A likely solution has been to adopt dual frame designs, augmenting landline samples with cell phone numbers. The telephone survey environment continues to be in a state of rapid change.  Current dual frame designs fail to sufficiently incorporate changes in the environment, data collection efficiency, and variances. There are also fundamental limitations in available sample allocation formulae. As a result, dual frame surveys tend to oversample the landline frame. We challenge the current sample optimization methods by incorporating missing components. We argue that we are approaching the point at which the optimal design for a national RDD telephone survey is a single frame cell phone design. Additional benefits include substantially reduced complexity in sampling, management, and weighting.  We extrapolate current trends in telephone use, and employ survey data to evaluate the use of a single frame cell phone RDD design. We compare estimates of bias, variance, and MSE of key survey estimates across different sampling scenarios.   ","Cell phone surveys have become increasingly popular and researchers have noted major challenges in conducting cost-effective surveys while achieving high response rates. Previous work has shown that calling strategies that maximize both respondent contact and completed interviews for landline surveys may not be the most cost-effective for cell phone surveys. For example, Montgomery, et al. (2011) found important differences between landline and cell samples for best times to call and declines in contact rates after repeated dialing. Using paradata from the 2010 and 2011 National Flu Surveys (sponsored by the Centers for Disease Control and Prevention), we investigate differences in calling outcomes between landline and cell surveys. Specifically, we predict respondent contact and interview completion using logistic regression models that examine the impact of calling on particular days o ","The National Immunization Survey (NIS), a landline telephone survey moving to a dual landline and cell phone frame in 2012, collects vaccination information for U.S. children ages 19 to 35 months and 13 to 17 years. Starting in Q4/2010, NIS was augmented with cell phone samples. Data collection consists of two stages: first, a random-digit dial (RDD) telephone sample is fielded to screen for households with one or more age-eligible children; second, consent is sought to mail an immunization history questionnaire to the child's provider(s), to collect vaccination history including number of doses and type of vaccines.    ","Double protection means that if either the prediction model or response propensity model is correct we will get approximately unbiased results. Typically, the models proposed for predication and response propensity are some type of regression model. In this paper, we use the double protection concept for imputation where the models are based on classification and regression trees. We use one tree to create the imputation classes from the prediction model and another tree to create the imputation classes from the response propensity model. Once the two sets of imputation classes are created, we group all the observations, both respondents and nonrespondents, based on these cross-classified imputation classes. Within these cross-classified imputation classes, we use a hot deck methodology to impute. We employ a series of Monte Carlo simulations with various patterns of missingness to compare the empirical biases resulting from using, (1) the cross-classified imputation classes, (2) imputation classes derived from a prediction model only, (3) imputation classes derived from a response model only, and (4) imputation based on a linear model. ","Abstract:   In many data mining problems where the goal is to estimate a population proportion, the percentage of missing data can be quite high. The usual practice of ignoring missing data assumes a missing completely at random (MCAR) mechanism, which might be seriously violated in some applications. Judgment post-stratification (JPS) estimation of a population proportion has been shown to increase the precision over the commonly used simple random sample proportion estimator. We compare the JPS estimator with an estimator based on random forests (RF) outcomes assuming that missingness is related to either the response or explanatory variable, referred as Missing Not at Random (MNAR) or Missing at Random (MAR) respectively. In particular, we develop and analyze a random forests adapted JPS estimation method. We use a dataset collected by NASA's satellite remote sensing instruments MODIS and CloudSat/CALIPSO as a test bed to demonstrate the benefits of JPS, RF and RF adapted JPS for estimating a population proportion when missingness is not MCAR. ","Assumptions about the missingness mechanism often cannot be assessed empirically, which calls for the sensitivity analyses. However, few studies with missing values are subjected to such analyses due to the lack of clear guidelines on a systematic exploration of alternative assumptions as well as the difficulty of formulating plausible missing not at random (MNAR) models. We present graphical displays, based on the \"tipping-point\" analysis first introduced in Yan et al. (2009), that help us visualize the results of a set of sensitivity analyses for missing outcomes in studies that compare two treatments. The resulting \"enhanced tipping-point displays\" provide compact summaries of conclusions drawn from different alternative assumptions about the missingness mechanism simultaneously. A recent use of these enhanced displays in a medical device clinical trial has helped lead to FDA approval. ","The growth of nonresponse rates for social science surveys has led to increased concern about the risk of nonresponse bias. Unfortunately, the nonresponse rate is a poor indicator of when nonresponse bias is likely to occur.  We consider a set of alternative indicators - including the Fraction of Missing Information, R-Indicators, the coefficient of variation of subgroup response rates, and model fit statistics such as R-squared, pseudo R-squared, and the area under an ROC curve. A simulation study is used to explore how each of these indicators performs under a variety of circumstances. The simulations vary the missing data mechanism (MCAR, MAR, and NMAR), the strength of covariates in predicting response and survey outcome variables, and the impact of the misspecification of models.  Finally, we will discuss how these indicators can be used when creating a plausible account of the risks  ","Researchers are often faced with analyzing data sets that are not complete.Such data analysis requires the knowledge of the missing data mechanism. If data are missing completely at random (MCAR), many missing data analysis techniques leas to valid inference, and thus, it is desirable to test for MCAR. The package MissMech implements two tests for this purpose that were developed by Jamshidian and Jalal (2010). These tests can be run using a function called TestMCARNormality. One of the tests is valid if data are normally distributed, and the other test does not assume any distributional assumptions for the data. In addition to testing MCAR, in some special cases the function TestMCARNormality is also able to test whether data have a multivariate normal distribution. As a bonus, the functions in MissMech can also be used to perform several additional tasks including (1) Test of homoscedasticity for several groups when data are completely observed, (2) impute incomplete data sets assuming normality or non-normality, (3) Obtain ML estimated of mean and covariance for incomplete data,(4) perform $k$-sample test, and (5) perform Neyman test of goodness of fit. ","In an era of declining response rates, survey organizations are increasingly examining the effects of nonresponse bias on their survey estimates. As fewer people respond to surveys, the nonresponse bias increases if survey respondents and nonrespondents are systematically different from each other, even after we apply sophisticated weighting methods to account for survey nonresponse. To assess the prevalence of this bias in estimates from the Federal Voting Assistance Program Post-Election Voting Survey of Uniformed Service Members, we performed a non-response bias study by surveying a random sub-sample of survey nonrespondents. Because we interviewed the nonresponse sample by telephone and the production survey was a self-administered, Web instrument, we also selected a sub-sample of the production sample for phone interviews to test for mode effects. Results show that modest mode effec ","In survey data, an observation is considered influential if it is reported correctly and its weighted contribution has an excessive effect on a key estimate, such as an estimate of total or change. Influential observations occur infrequently in economic surveys but have a detrimental effect on the key estimate when they do appear.  In previous research with data from the U.S. Monthly Retail Trade Survey (MRTS), two methods, Clark Winsorization and weighted M-estimation, have shown potential to detect and adjust influential observations.  This paper discusses results of the application of an improved simulation methodology that generates more realistic population data.  The analyses consider several scenarios for the occurrence of influential observations in the MRTS and assess the performance of the two methods in detecting influential values under the different scenarios.   ","The National Academy of Sciences' Committee on National Statistics (CNSTAT) has concluded a 30-month study of the Commercial Buildings Energy Consumption Survey (CBECS) and the Residential Energy Consumption Survey (RECS). The Committee has considered \"how to improve data quality, geographic coverage, timeliness of data releases, and relevance of the data for meeting user needs for energy end use information\" in this decade and beyond. The RECS and CBECS currently rely on complex multiple-frame area probability sample designs, in-person Blaise-CAPI enumerations of building or housing unit energy and occupant characteristics, and follow-on multi-mode surveys with energy companies to collect consumption and expenditures for sampled units. The panel discusses topics of growing relevance to a JSM audience, including survey design, frequency, and scope alternatives, as well as survey practice and operations. This session brings broad perspectives and deep insights to survey methodology for complex topics such as energy, applicable to any program that must optimize multiple designs, frames, modes and tools in an increasingly challenging operational and budget framework. ","This presentation will summarize upcoming changes to the National Ambulatory Medical Care Survey (NAMCS) and the National Hospital Ambulatory Medical Care Survey (NHAMCS) that will be useful for monitoring the effects of ACA. Changes in NAMCS include increased sample sizes allowing state-level estimates of preventive care given in physician offices and community health centers for 34 states. Changes in NHAMCS will allow us to be able to look at the effect of ACA on crowding the emergency department in 5 of the most populous states. The new National Hospital Care Survey (NHCS) will also be described, including the potential to link data from NHCS to Medicare and Medicaid and death records. ","Some observers have expressed concern about the capacity of the existing healthcare workforce, especially the primary care workforce, to provide care to individuals expected to be newly insured due to implementation of provisions of the Affordable Care Act. This paper discusses the use of data from the National Ambulatory Medical Care Survey, 2011 Electronic Medical Records Supplement, to monitor healthcare workforce capacity as measured by the fraction of physicians accepting new patients. The percent of office-based physicians who accepted any new patients and new patients with particular sources of payment will be presented both nationally, and by state. Findings indicate that nearly 96% (95% confidence interval (CI) 95-97%) of office-based physicians accepted new patients in 2011, though the percent accepting new patients was lower for those in primary care specialties (about 93%, CI 92-95) compared to others (97%, CI 96-100). About 69% (CI 66-71) of physicians accepted new patients with Medicaid. This is lower than the percent accepting new self-pay patients (91%, CI 90-93), new Medicare patients (86%, CI 84-88), or new privately insured patients (83%, 81-85). ","The Patient Protection and Affordable Care Act (ACA) was signed into law in March 2010. It puts in place comprehensive health insurance reforms that will roll out over four years and beyond, with most changes taking place by 2014. Some changes have already begun. This paper describes how the National Health Interview Survey (NHIS), which is conducted by the National Center for Health Statistics (NCHS), is being used to monitor the population's health status, health insurance coverage, health care access, and health care utilization, which are all potentially affected by the ACA. The NHIS, which has been in the field since 1957, was already equipped with many relevant questions to yield data for that purpose. In addition, about 86 new specialized questions explicitly designed to yield enhanced information about changes potentially associated with the implementation of the ACA were added to the NHIS starting in 2010. Also, the NHIS sample size was increased starting in 2011 to enhance the precision of state-level estimates, which are important for monitoring changes in the health care system. Continued timely release of NHIS public use data will enable the public to perform its own analyses of the rich multivariate NHIS data. ","The Affordable Care Act (ACA) introduced changes in health care access, with some provisions implemented in 2010 and 2011. Monitoring the impacts of these changes requires not only establishing baseline information but collecting additional detail on their effects. One source of such information is the National Health Interview Survey (NHIS). Questions that have been on the NHIS for several years or more can be used to establish baseline estimates and to monitor the impacts of the ACA provisions as they are implemented. Questions added to the NHIS in 2011 provide more detail about topics in health care access and utilization addressed by the legislation. Using NHIS data, we examine four topics of interest: public perception of the financial burden of medical bills; expanded access to health insurance for young adults aged 19-25; reasons for emergency room use among adults; and state-level estimates of health insurance coverage. ","Many investigators and organizations view sharing research data with others as an integral part of the scientific process and their overall mission. Indeed, several federal funding agencies mandate that their grantees share research data with others to promote new discoveries and reproducible research. However, data providers also are ethically and often legally obligated to protect the confidentiality of data subjects' identities and sensitive attributes. This creates a tension: how to share research data while protecting confidentiality? The speakers in this panel session will offer their experiences with and solutions to sharing diverse types of confidential data, with the aim of providing ideas and options for statisticians involved with data sharing. The speakers cover a range of data and interests, coming from business, academia, and government; health sciences and social sciences; and Europe and the US. Speakers will allow ample time for questions from the audience. ","This session brings together senior election officials, statisticians, and election integrity advocates to discuss legislative issues, recent theoretical developments, and pilots of election auditing. One topic will be risk-limiting audits, which have a guaranteed minimum probability of correcting wrong electoral outcomes. California and Colorado have laws authorizing or requiring such audits and grants from the Election Assistance Commission to support pilots. The ASA has also endorsed Risk-Limiting Post Election Audits. ","As society is becoming more dependent on data and increasingly data rich, it is important to take a look at what data is being used for analysis and where it originates. This session, \"Challenges of Data Analysis in Transportation\", addresses some of the challenges researchers face as statistics adapt to serve an increasingly data reliant society. Four speakers from both the government and private sector will discuss some of these challenges with respect to the transportation industry. Discussions will focus on new challenges in transportation statistics, including the need to fully explain the large and sudden drop in automobile fatalities in the last five years, the necessity to improve seat belt use survey data quality, multiple imputation of linked data and assessing the quality of data with methods to overcome the difficulty of working with not-so-clean data. ","In conjunction with the theme of the 2012 conference 'Statistics - Growing to Serve a Data Dependent Society, the goal of this session is to highlight some of the innovative activities being done by Federal statistical agencies and to encourage new graduates to consider employment with the Federal government. Among the activities being showcased are: (1) the Census Bureau's On-the-Map, a geographic visualization with built in disclosure avoidance techniques, (2) the National Center for Science and Engineering Statistics' interactive graphics of various state data, (3) the Energy Information Administration's National Energy Modeling System (NEMS), an econometric modeling system designed to project long-term changes in energy supply, demand, and prices under user-specified assumptions, and (4) experiences with using hierarchical models at the National Agricultural Statistics Service. ","A wealth of data on the health of Veterans is available for those receiving care from the Veteran Health Administration (VHA) - there is less data available for the Veteran population as a whole. The Behavioral Risk Factor Surveillance System (BRFSS) is a valuable state-administered health survey that collects some Veteran information, including self-reported Veteran status. BRFSS data is weighted to be representative of the general state populations, but not to the Veteran state subpopulations. Previous research has shown that certain BRFSS data is relatively representative of the total general population for many health topics, but other research has shown this is not the case for the total Veteran population for the same health topics.     ","The main purpose of this paper is to analyze affordability of housing among owner- and renter occupied units in New Jersey, Maryland and West Virginia so as establish which state is most affordable for low-income households. This was done through analyses of variables from the 2009 Census (Public Use Microdata Set). Descriptive statistics show significant differences in income levels and availability of affordable housing among the three states. Six predictive models of the odds of affordability - two models for each state- were constructed, and their differences were assessed. To identify smaller subsets of important variables, we employed several methods: the LASSO method, elastic net and stepwise regressions. In general, the number of persons in the household, the number of rooms in the house, and educational level and the seniority status of the head-of-household are most important p ","Life tables for the years 1999-2001 for the fifty US states plus the District of Columbia will soon be published by the National Center for Health Statistics. Race and gender specific tables are calculated based on death certificate data filed through the National Vital Statistical System. Table indices are provided for death cohort-based single-year ages from birth to age of 109 years, and include probability of dying, survival population, and life expectancy. Statistical methods are used to smooth cohorts and to resolve problems like missing/misreported ages, few or zero deaths for some young ages in small populations, and highly variability in numbers of observed deaths across ages. In this paper we present statistical methods used in generating US state-level life tables, including age imputation, probability models based on historical data, and parametrical and non-parametrical smoothing. Issues of standard errors for estimates and criteria used for estimating reliabilities are also discussed. ","Current production models of poverty and income utilized by the Census Bureau for the Small Area Income and Poverty Estimates (SAIPE) program do not utilize labor market indicators. The mismatch between the geographic definition of a labor market versus residential-based poverty is the primary impediment to their utilization. This paper will use alternative methodologies to construct indicators for the characteristics of the labor market faced by each residential-based area. These indicators will include both overall employment base indicators, such as industry mix or long-term employment growth, as well as near-term stress indicators, such as continuing unemployment claims. The indicators will be utilized in county-level SAIPE poverty models to test their usefulness. ","It is well known that the standard chi-squared test for testing the homogeneity of multinomial proportions performs poorly in terms of maintaining the stipulated Type I error for sparse data. We will review several tests which have been proposed to remedy this situation, and also provide some new tests. A comparison of all the tests based on extensive simulations will be presented. As an application, we consider tabular decennial census data at the block or tract level, which can be quite sparse. This application arises from a larger goal of building a model that is suitable for generating a synthetic version of the decennial census microdata. ","A long-standing question within the Consumer Price Index (CPI) program has been how best to determine the contribution of geographic areas to the overall CPI variance using standard statistical inference tools. The CPI is constructed of higher-level AREA-ITEM aggregates that are built up from an initial set of AREA-ITEM cells at the basic Index-Area-Item-Stratum level. The CPI produces summary percent price changes for all of these aggregate levels. By utilizing the basic level price changes and their higher level price changes, we will proceed to construct an \"adaptive\" analysis of variance (ANOVA) using these basic level price changes as our initial set of observations. We then can use a standard two-way ANOVA with one observation per cell. The ANOVA results provide F statistics that demonstrate the significance (or not) of AREA and ITEM in the two-way model. For the time periods cover ","Poverty mapping that displays spatial distribution of various poverty  indices such as head count ratio, poverty gap, poverty severity are most  useful to policymakers and researchers when they are disaggregated into  small geographic units, such as cities, municipalities, regions or other  administrative partitions of a country. Typically, national household  surveys that contain welfare variables such as income and expenditures  provide limited or no data for small areas. In this paper, we develop  a Bayesian methodology for producing poverty maps and the associated  uncertainties. We compare our Bayesian method with other existing  methods using Monte Carlo simulations. ","Standard methods frequently produce zero estimates of dispersion parameters in the underlying linear mixed model. As a consequence, the EBLUP estimate of a small area mean reduces to a simple regression estimate. In this paper, we consider a class of generalized maximum likelihood estimators that covers the well-known profile maximum likelihood and the residual maximum likelihood estimators. The general class of estimators has a number of different estimators for the dispersion parameters that are strictly positive and enjoy good asymptotic properties. In addition, the mean squared error of the corresponding EBLUP estimator is asymptotically equivalent to those of the profile maximum likelihood and residual maximum likelihood estimators in the higher order asymptotic sense. However, the strictly positive generalized maximum likelihood estimators have an advantage over the standard methods in estimating the shrinkage parameters and in constructing the parametric bootstrap prediction intervals of the small area means. We shall illustrate our methodology using an example from the SAIPE program of the US Census Bureau. ","Due to the fact that they borrow strength, model-based estimates typically show a substantial improvement over direct estimates in terms of mean squared error (MSE). It is of particular interest to determine how much of this advantage is lost by constraining   the estimates through benchmarking. In our paper, we show that the increase due to   benchmarking is $O(m^{-1})$, where m is the number of small areas.     ","We propose a new approach to small area estimation based on joint modeling of means and variances. The proposed model and methodology not only improve small area estimators but also yield ``smoothed'' estimators of the true sampling variances. Maximum likelihood estimation of model parameters is carried out using EM algorithm due to the non-standard form of the likelihood function. Confidence intervals of small area parameters are derived using a more general decision theory approach, unlike the traditional way based on minimizing the squared error loss. Numerical properties of the proposed method are investigated via simulation studies and compared with other competitive methods in the literature. Theoretical justification for the effective performance of the resulting estimators and confidence intervals is also provided. ","Random effects models play important roles in model-based  small area estimation. Random effects account for lack of fit  of the regression of the population small area means on appropriate  explanatory variables. It is possible that these covariates explain variation of some of small area means reasonably well, but not so well for the other areas. As a flexible alternative to random effects model, we propose a mixture model. For small areas whose population means are adequately explained by covariates, the model does not add any random model error, and for the other areas, a random component is added to the regression. This is a flexible alternative where the data determine the extent of lack of fit of the regression model for a particular small area, and add a random effect if needed. We use this mixture model to estimate poverty counts for US counties. With simulated data in SAIPE setup, we evaluate the small area estimates from the mixture model in terms of average absolute relative bias and squared relative bias.  Measured in terms of these biases, our simulation study shows that the  new estimates are more accurate than the estimates resulting from  the standard Fay-Herriot model. ","In many evaluation studies of a causal agent (treatment), analysts use observational data in which the treatment and control conditions are not randomly assigned to participants. By using propensity score matching the analysts can balance observed covariates between treatment and control groups and hence reduce potential bias in estimated causal effects. Incomplete data is common in longitudinal studies due, e.g., to participants' death or withdrawal. Such drop-out is said to be non-ignorable when it depends on the participant's underlying rate of change in the outcome. Not taking into account non-ignorable drop-out may yield biased estimates of causal effects. In this paper we propose a method for estimating the average causal effect of a treatment, at baseline, on the trajectories of some longitudinal outcome under the presence of non-ignorable drop-out. We illustrate this method with an analysis of the causal effect of living alone on memory performance based on data from a large longitudinal study conducted in Ume\u00e5, Sweden. ","The paper proposes a method to impute missing values of predictors for the subsequent predictive models on large and distributed data sources using a Map-Reduce approach. Firstly, for each predictor that has missing values, imputation models based only on the target variable are built independently on different data sources and on different machines using the Map functions. During the step, validation samples are extracted randomly across all data sources and merged into one global validation sample along with the collection of imputation models using the Reduce function. Then all imputation models are evaluated based on the global validation sample in a distributed manner using another set of Map functions to select the top K models and form an ensemble model. Thirdly, the ensemble model is sent to each data source to impute missing values of predictors. Finally, the complete dataset can be used to build any models for prediction as well as discovery and interpretation of relationships between the target and a set of predictors.     ","As part of a four year study to test the effectiveness of a teaching technique called the Science Writing Heuristic (SWH) on 3rd-5th grade students, 48 school buildings accross Iowa have been split between treatment and control groups, where the treatment schools have adopted SWH and the control schools are teaching science as previously. To evaluate the effectiveness of SWH scores on the Iowa Test of Basic Skills (ITBS) and the Cornell Critically Thinking Test (CCTT) were collected. The CCTT was given to 5th graders and the being and end of the school year. For each student the answer they gave for each of the questions was recorded (including missing). Mean substitutions (whether by student, sub-set of students, question, sub-set of questions) are common for this type of missing data. For this study the above four mean substitutions as well as another substitution method are applied to CCTT and then compared by fitting various models to see how they deviate. ","The Hosmer-Lemeshow (H-L) test is widely used for evaluating goodness of fit in logistic regression models. The H-L test first creates groups based on deciles of the estimated probabilities and then compares observed and expected event rates within these groups. Multiple imputation (MI) is growing in popularity as a method for handling missing data, and how to apply the H-L test after MI is not straightforward. In this paper we discuss complexities involved in applying the H-L test to multiply imputed data, related to which variables have missingness. When covariates have been imputed, predicted probabilities vary across imputed data sets, and thus the boundaries of the predicted probability groupings vary as well. When the outcome has been imputed, both predicted probabilities and \"observed\" event rates change from one data set to the next. We then propose several different methods for using the H-L test with multiply imputed data, and compare the methods through simulation. ","Cluster analysis is an analysis technique to classify objects into several groups based on characteristic variables. Model-based cluster analysis assumes that data can be expressed as a finite mixture of distributions. The mixture of the multivariate normal distributions is commonly chosen for model-based clustering. However, when data include intermittent measurement error, the normal distribution may not fit well. Moreover, cluster analysis using standard statistical software assumes that data are fully observed. In this study, we consider model-based cluster analysis of incomplete continuous data with intermittent measurement errors. A finite mixture of multivariate normal distributions is extended to handle both missing values and intermittent errors. An MCEM algorithm is proposed to handle model-based cluster analysis with missing values and intermittent measurement errors by using a measurement error model. ","Extensive research documents incentives' ability to increase response rates; clearly, they convince some sample members to participate who otherwise would not have done so. If incentives influence behavior at the stage of deciding whether or not to participate, they may also affect the way that respondents act during the survey interview - in particular, their motivation to provide high quality responses. Existing research examining the effect of incentives on respondent effort has focused on a limited set of indicators - item nonresponse and responses to open questions - and has produced mixed results. This presentation aims to expand our knowledge of this topic by examining the effect of incentives on a wider variety of satisficing indicators, such as non-differentiation, acquiescence, and response order effects. The data come from a telephone survey in which some sample members received a prepaid cash incentive. I will discuss the effect that the incentive had on twelve indicators of satisficing, as well as whether or not the incentive's impact varied depending on respondent characteristics.  ","The effects of incentives on survey participation have been found in previous research to be dependent on both the survey context and respondent characteristics. In this study, we report the results of an experiment in which we expand the source of leverage to include other members of the household to assess whether adding incentives for completion by all eligible members of a sampled household has a substantial and differential effect on survey participation. The experiment was conducted among six communities that participate in the Racial and Ethnic Approaches to Community Health Across the U.S. (REACH U.S.) Risk Factor Survey (RFS). Addresses were randomly assigned to one of the following groups prior to data collection: (1) control with no incentive, (2) a group that receives a $5 prepaid incentive, or (3) a group that receives the $5 prepaid incentive with a promised $20 payment if all eligible members of the household respond within 7 days of initial contact. We examine key survey performance rates including the interview completion rate to assess whether the household level incentive has any additional impact on the willingness to complete a survey. ","Response rates from military surveys have steadily decreased over the last several years and across all military services and ranks. Survey fatigue, length of time to complete surveys, lack of interest in the subject matter, deployments and other factors have contributed to this decreasing response rate. The most difficult sub-group to survey within all of the services has been the lowest ranking military members which are typically the younger population. The Defense Manpower Data Center (DMDC) is responsible for conducting the surveys of military members when they include all services (Army, Navy, Marines and Air Force). Since military surveys are not mandatory and incentives are not allowed, DMDC has been actively looking at methods and modes which can improve response rates of their surveys within the constraints of a shrinking federal budget.     ","We discuss results and methods used to estimate anticipated survey response rates for a future landline or cell-phone survey. In order to estimate the anticipated survey response rates, we develop models for each of the components (resolution rate, working residential number rate, screener completion rate, interview completion rate) of the survey response rate. For each of the components, an extensive set of data from past surveys were modeled with the idea of using \"similar\" past surveys to estimate each of the components of the survey response rate for a future telephone survey. Moreover, in addition to using similar surveys, our approach takes into account state-level socio-demographic information from the American Community Survey, and survey administrative procedures (length of data collection period, number of call attempts, use of advance letter, incentives, etc.). Using our model-based estimates for each of the components, we can derive an estimate for the anticipated survey response rate. We also discuss limitations of our approach, and potential applications. ","The Beginning Teacher Longitudinal Study is an annual survey of a national sample of public school teachers about their job conditions, mobility and attrition. In order to boost response rates, study participants were given cash incentives in waves 3 and 4 before the survey was administrated. In wave 3, all cases were randomly assigned to two incentive groups ($10 vs. $20). The results showed that a larger incentive amount was associated a higher response rate. However, the cost analysis taking consideration of telephone follow-up cost showed that $20 group also had higher average cost per respondent. In wave 4, all cases were given $10 because of budget concerns and relatively low cost of follow-up effort in wave 3. The results showed that, for the people who received the same incentives in both waves, there was no significant change with their final response rates from wave 3 to wave 4. However, the response rate of people who received the reduced incentive in wave 4 was significantly lower than their response rate in wave 3. The average cost per respondent also increased as a result of having extended telephone follow-up period and dedicated staff to boost the response rate. ","Randomized response (RR) is an interview technique to sensitive questions that can protect respondent's privacy using a probability mechanism using randomization devices. Self-protection (SP) is introduced to evaluate the survey, describing the responses by participants who give the evasive answer without taking the result of the randomization device into account. In this study, we tackle two problems in RR using a Bayesian perspective: modeling RR sum score variables and estimation of the mean of the number of respondents who possessed a rare sensitive attribute. ","In this paper, we use the idea of post-stratification based on the respondents' choice of a particular randomization device in order to estimate the population mean of a sensitive quantitative variable. The proposed idea gives freedom to the respondents and is expected to result in greater cooperation from them as well as to provide some increase in the relative efficiency of the newly proposed estimator. ","We study the estimation of complex non-linear small area parameters using empirical best (EB) and hierarchical Bayes (HB) methods. Our methodology is generally applicable, but we focus on measures of poverty indicators, in particualr on the clas of poverty measures called FGT poverty measures used by the World Bank. Small area estimates of the FGT measures for several countries have been released by the World Bank, using methodology of Elbers, Lanjouv and Lanjouv (2003) that combines census and survey data and produces simulated censuses. Estimates for any desired small area are produced from the simulated censuses.We obtain EB and HB estimators under a nested error regression model, assuming normality on a transformed welfare variable. We use a parametric bootstrap method for estimating the mean squared error of the EB estimators. We obtain posterior means and posterior variances, using a grid method that avoids the use of MCMC methods. We also extend the EB and HB methods to skew normal nested error models. We present the results of a model based simualtion study on the relative performance of EB, HB and the World Bank methods and an application to data from a Spanish survey. ","In practice many applications of small area models use a `Normal-Normal-Linear' assumption, i.e., a normality assumption for the design-based survey estimates and for the area-level random effects and a linear regression function relating the true parameters to available covariates.  We compare the performance of rate models by slightly changing the assumptions and using internal and external checks.  when area sample sizes are in the hundreds, empirical analyses using a 'Normal-t-Linear' to protect against outliers, or a seemingly reasonable `Beta-logistic' assumption for rates, show no gain over the `Normal-Normal-Linear' type model.  However, the same type of analyses show additional benefit from including historical data through a cross-sectional and time series model.  We use Monte Carlo Markov Chain (MCMC) to implement the proposed models, posterior predictive checks, as well as external checks for model comparisons. ","A small area estimation technique for producing \"Poverty Maps\", developed by Elbers, Lanjouw and Lanjouw (2002, 2003), has seen application in a large number of countries. Opportunities to formally test small area estimation methods remain rare due to lack of appropriate detailed data. This paper compares a set of predicted welfare estimates based on a generalized ELL methodology against their true values, in a setting where these true values are known. The new approach accommodates Empirical Best estimation but does not make restrictive assumptions about the distribution functions of the errors. We will present a unique empirical application using population census data from Denmark and/or the state of Minas Gerais, Brazil, which include unit record income data. ","The national Youth Risk Behavior Survey (YRBS) is conducted by the Centers for Disease Control and Prevention (CDC) biennially to monitor six categories of priority health risk behaviors among a representative sample of high school students. The complex sample was not designed to be representative below the national level; however, researchers are often interested in the effects of higher-level characteristics (e.g., a state law or policy) on lower-level characteristics (e.g., student behaviors). To simply add higher-level characteristics to a logistic regression model with a student-level outcome may lead to na\u00efve or incorrect inference. The purpose of the study is to compare modeling procedures to analyze state-level characteristics on student-level outcomes using YRBS data. We explore the differences in multilevel models in M-Plus and single-level (i.e., non-multilevel) models using SUDAAN to analyze state tanning bed laws on student tanning bed use. We hypothesize the inference from each model may differ and aim to identify the simplest model that maintains inferential accuracy. ","Diagnostic criteria for childhood behavioral/emotional conditions require data from several informants. While the use of multiple informants presents a more complete picture of symptoms and impact, it creates the challenge of resolving discrepancies across informants. We will use a school-based longitudinal study of ADHD as an example. At the first follow-up interview, the DISC was administered to children and their parents to assess the prevalence of comorbid internalizing and externalizing conditions. There was a low level of agreement for both internalizing and externalizing conditions, suggesting the importance of melding child responses with parent report to understand the true prevalence of these conditions. Additionally, there are differences in agreement by age and child ADHD status, illustrating developmental differences in the presentation and recognition of behavioral/emotional condition symptoms. Given these types of discrepancies by informant, prudence and clinical judgment should be practiced when analyzing these data. We will discuss implications for study results and present strategies for handling such discrepancies, such as age-dependent heuristics. ","This paper discusses the methodology used in a study to estimate serious emotional disturbance (SED) in children aged 4-17, based on a 5-item mental health screener for parents of children (the short Strength and Difficulties Questionnaire (SDQ)). The National Health Interview Survey (NHIS), which contains the SDQ, was selected for this study. A subsample from the NHIS of about 614 parents of children aged 4 to 17 were administered the Child and Adolescent Psychiatric Assessment over the telephone to determine the presence or absence of SED in their child. The diagnosis of SED based on parent responses to the clinical interview will be modeled against their responses to the SDQ screener, and a cutoff value will be identified to determine SED based on SDQ scores. This paper will include references to the methodology used in the Mental Health Surveillance Study conducted by the Substance Abuse and Mental Health Services Administration (SAMHSA) to identify a cutoff value from adult mental health screeners to determine serious mental illness (SMI), the adult analog of SED. ","The National Children's Study will examine the effects of the environment, as broadly defined to include factors such as air, water, diet, sound, family dynamics, community and cultural influences, and genetics on the growth, development, and health of children across the United States, following them from before birth until the age of 21 years. This paper describes the national probability sample with counties at the first stage of sampling and two methods for the second stage of sampling, geographical segments and providers. Initial promising results testing an alternate provider-based recruitment strategy within the geographical segment method have led to recent explorations of designing a provider-based sampling method at the second stage in three study locations. We discuss the statistical, sampling, operational, and recruiting issues and implications of these alternative second stage sampling methods. While this paper is focused on these design and operational experiences, the NCS continues to explore additional design options. ","This session highlights the strategic contributions of CNSTAT in two areas. The first reflects on historical and current perspectives while the second focuses on challenging issues for the foreseeable future: 1) How CNSTAT and the Academies critically complement the work of governmental statistical agencies and the federal statistical system and 2) CNSTAT's policy research and institutional memory on the measurement of the social phenomena of poverty, race/ethnicity, and small geographic areas. This is in memory of CNSTAT past director Edwin Goldfield and in honor of Margaret Martin on her 100th birthday. ","Retention of respondents in panel studies has become increasingly difficult and costly, yet is critical for the study's long-term success. Understanding the factors that are associated with panel retention is important for targeting field interventions to improve retention and reduce costs and for assessing and accounting for attrition bias. We use data from the Health and Retirement Study (HRS), a panel survey of older Americans that began in 1992, to examine factors that are associated with continuous participation and with re-engagement of participants who have missed one or more of the survey waves. We pool the data across waves and estimate two logistic regression models: one predicting current wave response among previous wave non-respondents (the re-engagement model) and a second predicting current wave response among previous wave respondents (the retention model). Predictors include study design factors; prior wave effort, burden and response outcomes; respondent and interviewer characteristics. Findings will help identify areas for improvement in the administration of the HRS and may have implications for the design and administration of panel surveys more generally. ","An important development in survey research has been the use of Internet panels, where pre-recruited individuals agree to participate in online research studies. One methodological concern with Internet panels is whether members with certain characteristics leave sooner after recruitment than others. Past studies have provided snap-shot evidence of disproportionate panel attrition among single cohorts of respondents. However, the Internet and how individuals use it is ever evolving, and so too should panel attrition trends. We use data from the GfK-Knowledge Networks (GfK-KN) probability-based Internet panel to explore the dynamics of attrition across three cohorts of recruited panelists: 2008, 2009, and 2010. Preliminary results show that while only approximately 39% of new recruits left GfK-KN's Internet panel within six months of joining in 2008, approximately 53% left within six months of joining in 2010. While attrition has always been higher among younger adults (18-29), it increased more among older adults. Attrition also increased more among higher educated individuals. Conversely, the rate of attrition among Spanish language survey takers has decreased over time. ","Measuring Attrition in Long-term Longitudinal Surveys  Aref N. Dajani*, Toni M. Warner*, and Shawn Bucholtz**    ","Respondent-driven-sampling (RDS) has recently been utilized as a valid method in survey methodology for reaching hard-to-reach populations. However, the assessment of its efficacy in expanding sample size as well as its estimation methods has largely focused on the in-person implementation which includes an initial convenience seed. This implementation varies greatly from its sibling implementation in telephone survey sampling, which is most heavily reflected in the assumptions used for deriving estimates. Through the use of simulation to generate numerous different types of correlated networks, we compare RDS in a telephone setting in comparison to the in-person setting and assert that RDS is a robust sampling method. We also explore practical matters in regards to the implementation of RDS for telephone surveying such as the number of waves to reach equilibrium and obtaining an accurate parameter estimate.  ","The potential effect on respondent burden is a major consideration in the evaluation of survey design options, so the ability to quantify the burden associated with alternative designs would be a useful evaluation tool. Furthermore, the development of such a tool could facilitate more systematic examination of the association between burden and data quality. In this study, we explore the application of Partial Least Squares path modeling to construct a burden score. Our data come from a phone-based, modified version of the Consumer Expenditure Interview Survey in which respondents were asked post-survey assessment questions on dimensions thought to be related to burden - e.g., effort, survey length, and the frequency of survey requests (Bradburn, 1978).  These dimensions served as the latent constructs in our model. ","Panel conditioning is an important source of measurement error unique to panel surveys. It refers to the phenomenon where participation in repeated interviews changes respondents' true behaviour, attitudes, or knowledge, or their reporting of their true behaviour, attitudes, and knowledge. A major weakness of (and a challenge to) the existing research on panel conditioning is its inability to distinguish between true change and change in reporting behavior. Existing studies are heavily reliant on assumptions and models when studying panel conditioning because they have no external gold-standard data source. This paper examines panel conditioning effects using data from four waves of a large German panel survey on labour market outcomes (PASS). Because administrative data on employment and unemployment benefit receipt status are available for nearly all respondents, we are able to separate panel conditioning due to change in true status and panel conditioning due to change in self-report of the true status without depending on assumptions. Our results show that PASS respondents are more likely to change their true behavioural status the longer they stay in the panel. In addition, they are less likely to misreport their behavioural status the longer they stay in the panel. ","Lengthy surveys may be viewed as burdensome and yield both low data quality and high nonresponse. To address these concerns, we can reduce the length by eliminating items, but this means that some information does not get collected. An alternative is to divide a survey into subsets of items and administer each subset to subsamples of the full sample. This is called a split questionnaire (SQ) and has the benefit of collecting all original survey data. We identify a deficiency in the current set of SQ methods - incomplete use of prior information about the sample unit in the design. In applications of SQs, only characteristics of the survey items are used in the design; but, if joint consideration is given to characteristics of the items and sample unit, then there may be the potential to improve the SQ's efficiency. In this paper, we explore the extent to which jointly considering both sets of information will yield more efficient SQs. We propose methods for including prior information about the sample unit into the SQ using features of responsive design. We use concepts from survey design, experimental design, and epidemiology to evaluate the proposed new elements of our SQ design. ","We provide a novel and completely different approach to dimension reduction problems from the existing literature. We cast the dimension reduction problem in a semiparametric estimation framework and derive estimating equations. Viewing this problem from the new angle allows us to derive a rich class of estimators, and obtain the classical dimension reduction techniques as special cases in this class. The semiparametric approach also reveals that in the inverse regression context while keeping the estimation structure intact, the common assumption of linearity and/or constant variance on the covariates can be removed at the cost of performing additional nonparametric regression. The semiparametric estimators without these common assumptions are illustrated through simulation studies and a real data example. ","Bayesian MCMC calibration and uncertainty analysis for computationally expensive models is implemented using a radial basis function interpolator, also known as an emulator or meta-model, for the logarithm of the posterior density. The meta-model is a radial basis function interpolator. To prevent wasteful evaluations of the expensive model, the emulator is built only on a high posterior density region (HPDR) located by a global optimization algorithm. The set of points in the HPDR where the expensive model is evaluated is determined sequentially by the GRIMA algorithm developed by the authors. Enhancements of the GRIMA algorithm are introduced to improve efficiency. A case study uses an eight-parameter SWAT (Soil and Water Assessment Tool) model where daily stream flows and phosphorus concentrations are modeled for the Town Brook watershed, a part of the New York City water supply. An interesting feature of this example is that the posterior is bimodal. The HPDR comprises less than 1% (by volume) of the parameter space, so the GRIMA algorithm is much more efficient on this problem than previously available methods. ","Testing for observable structures (such as the shape of a mean function) has been well studied.  In this talk tests for more indirect quantities will be discussed.   Correlated survival data are often modeled via proportional hazards models with a shared frailty. A common assumption is that the frailty distribution is gamma. We construct tests for the null hypothesis of a gamma frailty distribution that are nonparametric in spirit and that are in the style of the order selection tests for the normality of the random effects in linear mixed models, using ideas of flexible modeling of random effects by series expansions.   A new class of frailty models is constructed that extend the gamma frailty model by using certain polynomial expansions that are orthogonal with respect to the gamma density.   Simulations and data examples illustrate the test's performance.  ","In analytic inference for a regression relationship in a complex survey, informative selection may lead to inconsistent estimation. A standard approach is to expand the covariates to account for design features, but these design covariates may be of no scientific interest. An approach to removing these design covariates using nonparametric regression is explored. The resulting predictor has variance that can be estimated using a combination of standard design-based variance estimation and model-based variance estimation. ","Census Coverage Measurement for the 2010 Census will provide estimates of net coverage error and components of census coverage for housing units and persons in housing units. The 2010 estimation process uses a dual system methodology in which a sample is independently enumerated and then compared to the census. In support of this process, five field data collection operations and three matching operations were conducted: Independent Listing created a list of housing units in the sample area; Initial Housing Unit Matching of the Independent Listing and the 2010 Census addresses; Initial Housing Unit Followup collected data to resolve issues from the matching (e.g., unresolved unit status); Person Interview created a roster of people in the selected housing units, Person Matching compared the Person Interview roster with the entire 2010 Census roster; Person Followup collected data to reso ","Census respondents often have their own ideas regarding where people should be counted, which can lead to people being enumerated in more than one place (duplicated). Problems in establishing a person's Census Day residence led to the 2000 coverage measurement program underestimating erroneous enumerations (many of which were duplicates). This paper will discuss methodology used by the 2010 Census Coverage Measurement (CCM) Program to improve measurement of Census Day residence and improve techniques to detect and resolve duplicate enumerations. CCM collected address information for other places people could have been counted on Census Day. We conducted computer and clerical searches for census duplicates and matches between the CCM and census near these additional addresses (on top of searching near the sample address). We also conducted a nationwide computer search that was less conser ","The goal of the Census Coverage Measurement (CCM) Program for the 2010 Census was to provide estimates of net coverage error and components of census coverage, for both housing units and persons living in housing units. To measure the coverage of housing units, the Independent Listing operation created a list of housing units or potential housing units that could exist as of Census Day or by the Person Interview timeframe in selected block clusters, completely independent from any census operations. Each interviewer made up to three attempts to contact a household member to ask if there were additional units at the sample address, such as garage or basement apartments, that may have been missed without talking to a household member. To measure coverage for persons, the Person Interview, which listed all household members living at the sample address any time between Census Day and the da ","The goal of the Census Coverage Measurement (CCM) for the 2010 Census was to measure the components of census coverage for omissions and erroneous enumerations. In two different CCM interviews, a roster of people living at the sample address as of Census Day (April 1, 2010) and their living situations are collected. Because these interviews were conducted five and ten months after Census Day, the interviews also collect information on people who move in and out of the sample address. One possible error in reporting the moves could be recall bias as the interviews get further and further from Census Day. This study focused on reporting moves around March or April. The control panel interviews were done around Census Day. Then we compared the percent of moves reported across three panels spread over a ten month time period to the original control panel. To target movers, we selected half of the sample from our Master Address File with the assistance of information from the National Change of Address (NCOA) file. As a secondary analysis, we also reviewed how well we could target possible mover households using the NCOA file as a tool. ","This paper considers benchmarking issues in the context of small area estimation. We find optimal estimators within the class of benchmarked linear estimators under either external or internal benchmark constraints. This extends existing results for both external and internal benchmarking, and also provides some links between the two. In addition, necessary and sufficient conditions for self-benchmarking are found for an augmented model. Most results of this paper are found using ideas of orthogonal projection. ","In sample surveys, often there is insufficient sample size to obtain reliable direct estimates for certain domains. Precision can be increased by introducing small area models which borrow strength by connecting areas and incorporating auxiliary covariate information. For small area models, estimates at a lower geographical level typically will not aggregate to the estimate at the corresponding higher geographical level. Benchmarking is a statistical procedure which is used to adjust model-based estimates to satisfy constraint requirements. This paper briefly outlines two new approaches for constructing benchmarked estimates: minimum discrimination information (MDI) and a fully Bayesian model conditional on the constraint. Simulations assess the performance of several benchmarking procedures for Fay-Herriot models, offering new insight into benchmarking in practice: the proposed methods benchmark both first and second moments (where competitors often do not), procedures equivalent to the MDI estimator outperform competitors, and estimates at higher levels of aggregation should not necessarily be held fixed when benchmarking. ","Model based small area estimation techniques are popular in survey  sampling. Normal distribution of the sampling error and random small  area effects in a mixed linear model is widely assumed. However,  presence of outliers in the survey data could affect the fit of such  models and the resulting small area estimates may fail to borrow  information from other small areas. In order to account for possible  presence of outliers, a Bayesian hierarchical finite scale mixture of normal models is proposed to analyze unit level data. A class of noninformative priors has been proposed for the hyperparameters. Propriety of the resulting posterior distributions under different choices of hyperpriors is studied. Performance of the model  is illustrated through a simulation study. ","We evaluate revisions to a Bayesian beta regression model proposed in Wieczorek and Hawala (2011), for U.S. county poverty rates. For small areas, some of which have survey estimates of poverty rates of 0 or 1, a zero-one inflated rate model extends the beta distribution to allow for these extreme estimates. The addition of a model error term allows the model to produce shrinkage estimates. We can estimate the model parameters and shrinkage estimates for the small areas via Bayesian computation techniques. Using simulated draws from a \"pseudo population\" based on American Community Survey (ACS) data, we compare the results to ACS-like direct estimates and to the Census Bureau's current small-area model for county poverty estimation. ","Empirical Bayes and Bayes hierarchical models have been used extensively for small area estimation. However, the sampling weights that are required to reflect complex surveys are rarely considered in these models. In this paper, we develop a method to incorporate the sampling weights for binary data when estimating, for example, small area proportions or predicting small area counts. We consider empirical Bayes beta-binomial models, and normal hierarchical models. The latter may include spatial random effects, with computation carried out using the integrated nested Laplace approximation, which is fast. A simulation study is presented, to demonstrate the performance of the proposed approaches, and to compare results from models with and without the sampling weights. The results show that estimation of mean squared error can be greatly reduced, when compared with more standard approaches. Bias reduction occurs through the incorporation of sampling weights, with variance reduction being achieved through hierarchical smoothing. We also analyze data taken from the Washington 2006 Behavioral Risk Factor Surveillance System. ","Corrected scores is an approach that has been used for decades now to handle models which have measurement error in covariates. This approach is based on finding a score function that is unbiased in the presence of measurement error. We use this technique to deal with the Fay-Herriot model in small area estimation where measurement error is present and hence provide efficient estimates of parameters that have lesser variance compared to estimates yielded by other methods. ","In the context of the Fay-Herriot model, a mixed regression model routinely used to combine information from various sources in small area estimation, certain adjustments to a standard likelihood (e.g., profile, residual, etc.) have been recently proposed in order to produce strictly positive and consistent model variance estimators.  These adjustments protect the resulting empirical best linear unbiased prediction (EBLUP) estimator of a small area mean from possible over-shrinking to the regression estimator.  However, the existing adjusted likelihood methods can lead to high bias in the estimation of both model variance and the associated shrinkage factors and can produce a negative second-order unbiased mean square error (MSE) estimate of an EBLUP. %smaller than the corresponding naive MSE, which does not incorporate the uncertainty due to estimator of the model variance parameter, %and in some rare cases may even produce negative estimates.  In this paper, we propose a new adjustment factor that rectifies the above-mentioned problems associated with the existing adjusted likelihood methods. In particular, we show that our proposed adjusted residual maximum likelihood estimators of the model variance and the shrinkage factors enjoy the same higher-order asymptotic bias properties of the corresponding residual maximum likelihood estimators. We compare performances of the proposed method with the existing methods using Monte Carlo simulations.   ","The purpose of this presentation is to introduce researchers to Project Talent, thereby, providing a context for interpreting the other papers in this session. All session papers use Project Talent as their data source. Specifically, in this presentation information will be presented on Project Talent's research objectives, sample and data collection designs, data collected in the base year and follow-ups, and current activities and future plans for the study. Project Talent began in 1960 with a nationally representative sample of 440,000 9th-12th graders and followed them until approximately age 30 (1971-1974). An extensive set of measures were administered in the 1960 base year and three subsequent follow ups, including measures of general knowledge, interests, aptitudes, abilities, and personality. Cohort members are now 66 to 70 years old and plans are under way for collecting additional data to investigate aging and the relationship between earlier adolescent measures and later-life psychological, social, and financial outcomes.  ","All longitudinal studies face a common challenge of locating study participants for future waves. Long lags between data collections can make tracking participants particularly difficult and costly. This paper provides results from the 2011-12 Project Talent Follow-up Pilot Study, a pilot test developed to assess the feasibility of finding and reengaging a representative random subsample of Project Talent participants who had not been contacted in 37 to 51 years. We examine the impact of key factors (e.g., availability of Social Security numbers, participation in previous waves, and availability of recently updated information) on tracking success and explore the effectiveness of two locating strategies: (1) batch searches of administrative data; and (2) interactive internet, database, and phone tracking of participants not located using method 1. We evaluate the extent to which interactive tracking was able to locate subpopulations that were not found using batch tracking methods and identify subpopulations that may continue to be disproportionately under-located. Qualitative assessments of the strategies are also examined. ","Use of administrative records is an increasingly important part of social and behavioral research because it offers a relatively inexpensive and quick alternative to a full scale data collection. Previous studies have established that unique identifiers such as social security numbers (SSNs) are vital for linking survey data to administrative records. However, few have offered alternative approaches for linking survey data to such records when unique identifiers are not available. Without unique identifiers, administrative data linkages are prone to reductions in accuracy and increases in erroneous linkages. Erroneous linkages are particularly problematic to large-scale studies because they require more manual review, thereby increasing study costs and introducing more coder-specific errors. Using survey data from Project Talent and data from the Social Security Administration's Death Master File, this study uses a seeded sample of records with SSNs to evaluate and compare three methods for linking to administrative mortality records when SSN is not used. We will evaluate each method on the proportion of false negatives and false negatives produced, as well as overall efficiency. ","Key to research on mortality is determining mortality status via linking to administrative records.  Several methods for ascertaining mortality status exist, including the Social Security Administration's Death Master File (DMF), the National Center for Health Statistics' National Death Index (NDI), and consumer and commercial credit bureau databases. This study examines the effect of differential coverage of three mortality record sources for a random subsample of 4,159 Project Talent participants.  Project Talent is a longitudinal study that began in 1960 and measured the cognitive abilities, interests, personality, and demographics of approximately 440,000 9th-12th grade students.  In this study we summarize findings on coverage rates, mortality rates, and potential bias related to the different mortality record sources. Furthermore, we investigate whether and to what extent relying on different mortality record sources affects the results of survival analyses.  In addition to key demographics, we examine the relationship among early life personality and cognitive ability factors and mortality, which are rarely available for a large, nationally-representative sample.  ","In ranked-set sampling, a stratification by ranks is used to obtain a sample that tends to be more informative than a simple random sample of the same size. Previous work has shown that if the rankings are perfectly accurate, then one can obtain confidence bands for the CDF that are narrower than those obtained under simple random sampling. Here we develop confidence bands that work well even when the rankings are not perfect. These confidence bands are obtained by using a smoothed bootstrap procedure that takes advantage of special features of ranked-set sampling. We show that the coverage probabilities are close to nominal even for samples with just two or three observations. A new computational algorithm allows us to avoid the bootstrap simulation step when sample sizes are relatively small. ","MacEachern, Stasny, and Wolfe (2004, Biometrics60, 207-215) introduced a data collection method, called judgment post-stratification (JPS), based on ideas similar to those in ranked set sampling, and proposed methods for mean estimation from JPS samples. We propose an improvement to their methods, which exploits the fact that the distributions of the judgment post-strata are often  stochastically ordered, so as to form a mean estimator using isotonized sample means of the post-strata. This new estimator is strongly consistent with similar asymptotic properties to those in  MacEachern et al. (2004). It is shown to be more efficient for small sample sizes, which appears to be attractive in applications requiring cost efficiency. In this work, we further extend our method to JPS samples with multiple rankers. The new estimator for the JPS with multiple rankers solve the generalized isotonic regression with matrix partial order. The performance of the new estimator is  compared to other existing estimators through simulation.  ","In this article, we discuss an alternative ranked set sampling  scheme--paired ranked set sampling in which two units are quantified  in each set, and investigate its effect on sample mean. We show that  the correlation structure arise from the sampling scheme decreases  the efficiency of the sample mean. The stronger the correlation  structure is, the more the efficiency of the sample mean decreases.  Therefore the destination of the work is to find the optimal  allocation of the quantified units for paired ranked set samples. ","During the past decade, educational research has been galvanized by new legislation such as the No Child Left Behind Act. Increasing emphasis is being placed on accurately quantifying the success of treatment programs through student achievement scores, so precise estimation is vital for establishing the efficacy of new methodology. Ranked set sampling is a sampling design that uses the rankings of an inexpensive covariate in an attempt to select a more representative sample that provides better estimation of population parameters. The use of ranked set sampling for hierarchical linear models is developed for applications in educational data since a multitude of possible ranking information exists at both the student and school level. The performance of the maximum likelihood estimator of the treatment effect as well as a nonparametric estimator is explored, and results indicate that the precision of these estimators may be improved by ranked set sampling. Thus, a true difference in performance between the treatment group and control group may be more detectable when ranked set sampling is employed. ","Ranked Set and Judgment Post Stratified Sampling are known to produce samples that are more efficient per measurement than simple random sampling due to the use of auxiliary ranking information of unmeasured units. We believe common estimation methods for RSS and JPS do not use all potential information that unmeasured observations may contribute. We introduce new estimators that incorporate this potential information.  Our first proposed estimator, for the population mean, utilizes the expected mean of a set given the measured value within that set. We show this estimator has efficiency close to that of existing methods in the balanced case, with considerable improvement in the unbalanced case. Our second estimator estimates the underlying distribution of each rank and then of the whole population. We show that under mild conditions, the estimate is consistent and asymptotically unbiased. This method performs similarly to the empirical cdf in the balanced case and significantly outperforms it in the unbalanced case. ","The Census Bureau produces alternative poverty estimates using data from the Current Population Survey (CPS) Annual Social and Economic supplement (ASEC). As a part of these alternative estimates, the Census Bureau produces estimates of federal and state taxes, including estimates of several tax credits. One key step in this process is constructing tax units from the members of sampled households. This paper compares estimates of tax credits across different methods of forming tax units, and evaluates how these credit estimates compare to each other. These estimates will also be compared to what is reported in tax returns using IRS aggregate data. ","The Canberra Group is an expert panel that initially came together in 1996 to discuss and to enhance the comparability of national household income statistics. The result of this panel was the Canberra Group Handbook (The Canberra Group, 2001). This paper discusses the use of CPS ASEC variables to construct household income estimates based on the recommendations in an updated version, Canberra Group Handbook on Household Income Statistics Second Edition (The Canberra Group, 2011). This is the Census Bureau's attempt to apply Canberra Group methodology and concepts to broaden the money income definition to compare internationally. This paper will discuss the feasibility and limitations of using ASEC variables in a Canberra Group definition of household income. Summary measures will be presented for calendar year 2010. ","The Current Population Survey Annual Social and Economic Supplement (CPS ASEC) serves as the data source for official income and poverty statistics in the United States.  There is a concern that the rise in non-response to earnings questions could deteriorate data quality and distort estimates of income and poverty.  The CPS ASEC relies on a hot deck imputation procedure to address non-response.  This paper assesses the extent of the bias in poverty rates caused by earnings non-response and the hot deck procedure.  We use a dataset of matched CPS ASEC records to Social Security Detailed Earnings Records (DER) to study the impact of earnings non-response on estimates of poverty over the time period 1997-2008.  Initial results show substituting DER earnings data for earnings imputed in the CPS ASEC produces poverty rates that are higher than the official poverty rate but not as high as poverty rates produced from completely dropping imputed earners. ","In the past several decades, individuals 65 and over have experienced remarkable declines in poverty, from 35.2 percent in 1959 to 9.7 percent in 2008 (Meyer and Sullivan, 2010). These declines in official poverty statistics, however, are based largely on self-reported income data from the Current Population Survey Annual Social and Economic Supplement (CPS-ASEC). In this paper we evaluate the quality of the retirement income data in the CPS-ASEC by matching it to individual microdata from 1099-R forms filed with tax returns in the tax year 2008. Since income data in the CPS-ASEC is used in the calculation of several official economic statistics, we are able to test whether using 1099-R records instead would alter measurements ranging from poverty rates among the elderly to income inequality. ","The Office of the Assistant Secretary for Planning and Evaluation (ASPE), Department of Health and Human Services has supported a Census Bureau match of the 2006 Annual Social and Economic Supplement (ASEC), Current Population Survey to the Social Security Administration's 2005 Detailed Earnings Records (DER).  The purpose of this match is to examine how imputation affects estimates of earnings, wages and self employment income.   Overall, 84.9 % of all records for persons 15 and over with earnings on the ASEC were matched to a DER record.  In addition, tabulations are also available for selected demographic groups considered of policy importance.   The same information has been developed for those who did not match.  This presentation will compare those who matched with the 15.1% who did not match.  It will examine whether the matched data can be viewed as representative of the population, with or without a post-stratification adjustment.   Our results show that lower earnings persons failed to match more than higher earnings persons. While distributional differences exist depending on the demographic group examined, the patterns appear to be quite regular, interpretable and only of modest complexity. Selection bias exists in all survey settings, including here.  Nothing we found suggests that it is serious.  ","Small area estimation using models which borrow strength from relationships between variables across geographic areas has become increasingly popular. Typically these approaches combine direct estimates with model estimates. The American Community Survey (ACS) produces direct five year estimates at the census tract level for income and poverty. An improvement in the accuracy of these estimates as measured by the estimated sampling error is desired. This pilot study will compare the mean squared error of three potential model based estimation methods with direct five year estimates of income and poverty at the tract level. The goal is to make a preliminary assessment on the potential gain in accuracy from using these model based estimates. For maximum improvement these models require administrative data correlated with income or poverty. If one of these methods could produce significant i ","We describe the small area model which was developed and used to provide determinations of voting materials needed in languages other than English (Section 203 of the Voting Rights Act). In order to make these determinations estimates of characteristics for states and political subdivisions relating to citizenship, limited-English proficiency, and illiteracy are all needed. The method utilizes both 2010 Census counts available for detailed language minority groups in small areas along with the corresponding detailed characteristics collected in the American Community Survey (ACS). Point estimates under an empirical Bayes procedure and variances under a Bayes procedure are discussed. ","The Census Bureau is currently developing a new imputation-based  methodology to improve the estimates of the group quarters population  for small areas. As a part of that work, a new variance estimation  methodology was needed that would properly account for the imputation  variance component in addition to the sampling variance  component. Furthermore, any methodology would need to be incorporated  into the American Community Survey (ACS) replicate weight variance  methodology in order to minimize the impact on production tabulation  systems.  A benchmark was established using multiple imputation  techniques for a fixed sample. We then compared alternative methods  that attempted to incorporate the full measure of variance which  incorporates the imputation variance into the replicate  weights. Details on how the replicate weights are computed are  described. This work led to a recommendation of a variance estimation  methodology used in the 2011 ACS data products. ","There is a common perception that the 2010 American Community Survey (ACS) estimates, in general, should be consistent with the Decennial Census. While it may seem counter-intuitive that 2010 ACS estimates and the 2010 Census might disagree, there are several reasons why this may be the case. Exploring the differences in data collection methods, reference periods, and objectives allows us to understand the inherent differences between the ACS and Census methodologies. Of particular interest is the difference in the vacancy rates. The gross vacancy rate estimated by the 2010 ACS was 13.1 percent. This proportion is statistically higher than 2010 Decennial Census rate of 11.4 percent. For the first time, we are presented with the unique opportunity to match full ACS data collected to the decennial census. Our paper describes the different components of the occupancy status classification. Using the matched file, we produced a variety of cross tabulations of weighted proportions, comparing the ACS and Census occupancy status across several housing characteristics. We discuss the results of the match to better understand the outcomes influencing the difference in the vacancy rate. ","To detect potential errors in response data from the American Community Survey (ACS) in near real-time, we have developed an automated statistical process control (SPC) system. For each year from 2005 to 2010, the ACS collected data about roughly two million housing units, 4.5 million people in the household population, and 150,000 people in group quarters facilities. Beginning in June of 2011, the ACS housing unit sample was increased by 22 percent. We expect to see approximately the same increase in the number of interviews. Several SPC methodologies are being used to investigate responses, using traditional Shewhart charts as well as basic statistical tests for differences between proportions. We are also using these techniques to monitor any impact the realignment of our Regional Office (RO) structure may have on data quality. This paper describes the details of the methodology, and a summary of results to date including preliminary results from the assessment of the realignment of the RO structure. We also discuss the inherent challenges and obstacles faced when applying traditional process control methods to a large-scale, multi-mode, demographic survey. ","The International Workshop on Using Multi-level Data from Sample Frames, Auxiliary Databases, Paradata, and Related Sources to Detect and Adjust for Nonresponse Bias in Surveys was held in June of 2011. It was jointly sponsored by the National Science Foundation, the World Association for Public Opinion Research, and the International Association of Sampling Statisticians. This panel will discuss the recommendations of the workshop and latest developments in methods for  the detection of and adjustment for nonresponse bias in surveys.    ","The Integrating Data panel will stimulate discussion on how various researchers are integrating data sources in the context of transportation. Utilizing and combining data from different sources, such as combing survey data with administrative records, can provide users with a much richer, more useful dataset for uses in sampling, quality control, and analysis. Experiences in using multiple data sources for a myriad of purposes is presented, with the hope that exchanging knowledge and methods will lead to the increase and improvement of future data integration. The following will be discussed: (1) how critical transportation data from the economic census forms derivative data products; (2) the need of additional data sources to leverage and produce the National Census of Ferry Operators database; (3) the utilization of electronic on-board technology in commercial motor to study safety-related issues such as driver fatigue, hours of service, and carrier performance; and (4) how collective non-motorized transportation data measures travel behavior to better understand energy consumption, reducing vehicular congestion, urban development patterns, and promotion of healthier lifestyles. ","The growing availability of patient registries present a number of sampling issues and methodological challenges in survey research. For example, researchers must follow the guidelines specified in the 1996 HIPAA (Health Insurance Portability and Accountability Act) Privacy Rule because registries are administrative records that often contain personal identifying information and personal health information. Even with proper data abstraction procedures, the quality of patient data can create concerns. This is especially true when the administrative data are incomplete or in error, which may contradict the patient's questionnaire self-reports. On the other hand, the administrative data may be correct but the patient responded in error. This paper discusses the extent of agreement/disagreement of the registry data compared to patient questionnaire self-reports using experience from the National Cancer Institute Community Cancer Centers Program pilot study. Presented are results from comparisons of race/ethnicity, cancer type/location, and cancer treatment type.  ","The Bureau of Labor Statistics' Survey of Occupational Injuries and Illnesses (SOII) is an important source of information for workplace injuries. Recent work comparing the SOII to Workers' Compensation (WC) administrative data concludes that the SOII undercounts injury and illness cases. Because the SOII is a sample, case by case comparisons between the two sources must distinguish between WC cases which are missing from the SOII because of sampling and cases which are missing because of underreporting. Previous research made this distinction by identifying SOII sampled establishments within WC data. This approach requires accurate employer information in WC data and subjective analysis of an establishment match. As an alternative, after matching SOII and WC data at the case level, we estimate the number of linked cases for the population by applying survey weights to the linked cases in the SOII sample. This allows us to estimate as a residual the number of WC cases missing in SOII, without having to directly identify SOII establishments in WC data. We describe the relative merits of this approach, and provide an alternative measure of the SOII undercount using Kentucky data. ","The marked increase in Clostridium difficile infection (CDI) incidence and mortality across the United States based on hospital administrative data has made CDI a national public health challenge. To understand the population-level burden of CDI in the United States, reports of CDI infections from 7 states participating in CDC's population-based surveillance for CDI were studied. A mixed model was used to evaluate fixed effect risk factors across sites. Traditionally, a variable was kept at P = 0.05 significance level. We proposed, based on the traditional strategies, to further test and keep the variable if it reduced the variance components (random effects), regardless of its significant level. We calculated and compared the site specific direct standardization rates (SDRs) through the traditional and new modeling strategies using combined data as the standard population. Results showed that extra risk factors reduced the variations of SDRs between sites, indicating that variations between sites were partially explained by them. We concluded that using variance component reductions to further identify isk factors for heterogeneous data is appropriate. ","In this work we proposed estimators of the size of a hidden population, such as sexual workers and drug users. Specifically, we derive unconditional and conditional maximum likelihood estimators to be used along with the variant of link-tracing sampling proposed by F\u00e9lix-Medina and Thompson (Jour. Official Stat., 2004). In this variant, a sampling frame made up by sites where the members of the population can be found with high probabilities, such as bars and parks, is constructed. The population is not assumed to be completely covered by the frame. Then an initial simple random sample of sites is selected from the frame. The people in the sampled sites are identified and they are asked to name other members of the population. We say that there is a link between a site and a person if that person is named by at least one element in the site. Following an idea used by Pledger (Biometrics, 2000) in the context of capture-recapture, we derived maximum likelihood estimators under the assumption that the elements in the population can be grouped into a number of classes according to their susceptibility of being linked to a site in the initial sample. Elements in the same class have the same probability of being linked to a particular site, while elements in different classes have different link probabilities. This assumption allows us to model the heterogeneity of the link probabilities. The unconditional maximum likelihood estimator is obtained by using the ordinary maximum likelihood approach, whereas the conditional maximum likelihood estimator is obtained by using an approach proposed by Sanathanan (Annals of Math. Stat., 1972). The results of a simulation study indicate that the proposed estimators require relatively large sampling fractions to perform satisfactorily, otherwise they present problems of high variability and numerical instability. ","Model identifiability requires a model likelihood with a single global maximum. The ability to find unique parameter estimates for latent class models depends on such identifiability. Weak identifiability occurs when there are regions of the likelihood that are \"flat.\" In that case, a unique global maximum may exist, but so do many local maxima that provide nearly the same value of the likelihood. In this case, it can be very difficult to find the MLEs, which can lead to erroneous model estimates and conclusions. An important cause of weak identifiability is a violation in one of the model assumptions (e.g., local dependence). This research assesses the likelihood of having a weakly identifiable model based on the type and severity of assumption violation using a simulation approach. It also provides suggestions on how to detect weak identifiability and offers some approaches for avoidin ","This paper provides methods for estimating false match rates and false nonmatch rates for record linkage. The estimation of false match rates mimics and extends ideas from semi-supervised learning (Larsen and Rubin 2001, Winkler 2002) using an EMH algorithm (Winkler 1993, 1990) that extends the MCECM algorithm of Meng and Rubin (1993). The estimates of false nonmatch rates use capture-recapture ideas from Winkler (2004) in the situations where unique identifiers are available. It then extends the capture-recapture methods using the ideas of subspace projections and resultant variations due to Haberman (1974) when unique identifiers are not available. The estimation methods are verified empirically using high quality truth decks for which true matching status is known. ","This paper suggests that sampling theory may usefully be expanded from random sampling of population units to both random sampling of population units and random construction of these population units. It continues previous work under this title, Woodruff (2011, 2010, 2009). In survey sampling, a sample unit`s study variables are expanded to population totals by probability design based or model based expansions that implicitly treat a unit`s study variables as totals over entities called atoms contained in each unit. When a unit`s atoms are random samples from a population of atoms, a model is imposed on sample data that leads to Pre-sampling Model Based (PSMB) Inference. PSMB inference provides estimates that retain the best properties of both model based and design based inference and that eliminate the main shortcomings of each. The result can be orders-of-magnitude error reduction.  ","In March 2010, the U.S. Census Bureau conducted a large-scale field test of three alternative sets of questions for obtaining information on health insurance coverage. The Survey of Health Insurance and Program Participation (SHIPP) was a split-ballot design with sample from a Random Digit Dial frame and Medicare enrollment files. The SHIPP contained three panels comparing health insurance coverage questions from: the Current Population Survey Annual Social and Economic Supplement (CPS ASEC), the American Community Survey, and an experimental set of questions for the CPS ASEC designed to reduce measurement error. We collaborate with the Census Bureau to develop a strategy for combining the sample frames and expand on the analysis conducted by Pascale (2012). With the enactment of the Affordable Care Act understanding how respondents answer questions about their health insurance coverage is vital for monitoring the impact of the law. The SHIPP provides a unique opportunity to compare estimates from the same sample across surveys and provides valuable insight for modifications to the questions. Using the merged data across the panels we contrast responses to these coverage items ","The National Ambulatory Medical Care Survey (NAMCS) is an annual nationally representative sample survey of visits to about 3,000 office-based physicians, excluding anesthesiologist, radiologists, and pathologists, and 104 community health centers (CHCs). Prior to 2012, the NAMCS sample design utilized a multistage probability design involving samples of geographic primary sampling units (PSUs), physicians/CHCs within PSUs, providers within CHCs, and visits within provider practices. Estimates by state were often unreliable due to insufficient sample size and were not designed to be representative by state. The 2012 NAMCS has been redesigned to provide state-level estimates for the first time in 34 states with large populations. The remaining states are included in Census Division level estimates. The redesigned sample is stratified by state and selects physicians and CHCs instead of PSU ","Research suggests that the factors influencing the decision to participate in a survey may also influence the respondent's motivation and ability to respond to survey questions. If response propensities are positively correlated with respondent effort during the interview, the participation of reluctant respondents may reduce the quality of estimates. Thus, understanding the associations and common causes of nonresponse and measurement error is essential for reducing total survey error and designing high quality surveys.    ","In 2009, the National Health Interview Survey (NHIS) conducted a split-ballot experiment using the six disability questions often referred to as the ACS disability set. The questions were asked at the end of the family interview, and families were randomly assigned to one of two question formats: family-style (e.g., \"Is anyone deaf or does anyone have serious difficulty hearing?\" If \"yes,\" \"Who is it?\"); and person-style (e.g., \"Is &lt; person&gt; deaf or does &lt; person&gt; have serious difficulty hearing?\" The question is repeated for each eligible person in the family).    ","One challenge to using a multi-mode address-based sampling (ABS) design to target minority populations can be low eligibility. At issue is the relative cost efficiency of ABS as an alternative sampling design is inversely related to household eligibility. One potential way to improve operational efficiency would be to enrich the original address frame by using race/ethnicity-targeted lists. Such lists can be used to stratify a general population address list by indicating households likely to contain members of a targeted racial/ethnic group. This paper provides an initial investigation into the impact the use of targeted lists may have on household coverage and resulting survey data in a health survey. Using a binary logistic model, we find that the coverage of race/ethnicity targeted lists declines in dense, urban areas with large populations of renters and low priority group density. List coverage is less likely to be adequate for African American households compared to Asian or Hispanic households due to the use of surnames.   ","The use of complex sampling for the selection of study subjects is becoming more common in population-based case-control or cross-sectional studies. This is prompted by the usual advantages of conducting complex sampling, such as time and cost efficiency. More importantly, the use of proper complex sample designs can also obtain representative samples from the target population and thus avoid the biased selection of controls and/or cases. However, at least two complications, i.e. differential population weights and intracluster correlation, are induced by the complex sampling. Although most SNP-based association studies with complex sampling account for these complications, many recent haplotype-based genetic association studies with complex sampling tend to ignore them, which can lead to invalid inferences. Therefore there is a need to develop methods for taking into account the complications induced by complex sampling in haplotype-based association studies. In this paper, we propose methods that properly account for complex sample designs. Our methods are evaluated via simulation studies and illustrated using data from a case-control study, the U.S. Kidney Cancer Study. ","In light of the seemingly bleak future of analytically useful PUFs due to increasing potential for availability of individual level data in public domains that could be used for matching purposes, we propose an aggregate level PUF (AL-PUF) as an alternative product consisting of information at aggregate or micro-group (MG) level. In the context of CMS Medicare Claims data, for each MG of beneficiaries defined by demography, geography, and enrollment; AL-PUF would contain information about MG-size, and micro-means (MMs) over the MG for various clinical profiles. Definition of MGs is based on a sample (s1) such that they satisfy a threshold of minimum size to avoid disclosure of rare subgroups but are small enough as building blocks for various analysis domains of interest. To avoid disclosure of sensitive values of individuals with rare characteristics, we propose using two successive sub ","We are all aware that only a fraction of all error sources are taken into account when margins of survey error are presented.  Should we approach this problem by attempting to estimate mean squared error components, by improving survey processes so that we gradually approach more ideal bias-free processes or both? How should we handle the survey design problem when we have to deal with a multitude of nonsampling error sources, some of which defy expression? How should issues regarding survey quality that go beyond textbook sampling formulas be communicated to and discussed with users and clients? ","The purpose of this experiment was to determine if an increase in census mail response and speed of response could be realized (without decreasing data quality) by including a due date or deadline message on various mailing pieces and/or by sending the census mailing pieces closer to Census Day. Panel 1, the Control panel, was mailed according to the production 2010 Census schedule and did not display any due dates or deadline messages. Panels 2 through 5 examined the main effects of four deadline messaging treatments mailed according to the production 2010 Census schedule. Panel 6, the Compressed Schedule panel, was used to evaluate the main effect for the compressed schedule treatment, without an explicit deadline. Under the compressed mailing schedule, the advance letter, initial questionnaire package, and reminder postcard were moved to one week later to reduce the length of time between the mailing and Census Day. Finally, Panels 7 through 10 combined each deadline messaging treatment with the compressed schedule. More research needs to be conducted, but it is evident that deadline messages can improve response without negatively impacting data quality. ","The primary component of the 2010 Census Alternative Questionnaire Experiment was an ambitious series of 15 experimental panels devoted to race and Hispanic origin research. The main goals were to design and test questionnaire strategies that would increase reporting in the major race and ethnic categories and elicit reporting of detailed race and ethnic groups, lower item non-response, and increase accuracy and reliability of the results. The research questions are divided into three sets of panels: (1) examining modified race and Hispanic origin examples; (2) combining the separate race and Hispanic origin questions into one; and (3) testing the use of a spanner and the limiting of the term 'race.' The second major component of the study was a phone reinterview conducted a few months after the 2010 Census mailout. The reinterview was designed to probe more extensively into the racial a ","As part of the 2010 Census integrated communications campaign, the Census Bureau contracted with an advertising agency to deliver a paid advertising campaign to increase 2010 Census awareness and participation. This paper reports results from an experiment conducted to assess the impact of increased media spends in a small number of designated market areas (DMAs). In all, sixteen DMAs were matched into eight pairs with one area from each pair randomly assigned to receive an increase dosage of media spends during the awareness and motivation phases of the campaign. There are two primary research questions under the experiment: (1) How did changes in level of paid media investment relate to changes in Census awareness, knowledge, attitudes and advertising recall? (2) How did changes in level of paid investment relate to changes in Census mail return rates?  Results suggests that, for the m ","Traditionally, the Census Bureau has conducted an experiment during the decennial census to evaluate the cumulative effects of all content changes to the questionnaire from the previous census. The most recent chapter of this tradition was conducted as part of the 2010 Census Alternative Questionnaire Experiment. Specifically, research was conducted to compare the results from a control panel using the 2010 Census questionnaire to the results from a questionnaire that replicated the Census 2000 questionnaire wording, categories, order, and other essential design features. A number of changes were made to the 2010 Census questionnaire, compared to the form used in Census 2000. In addition to changes in the overall questionnaire format and appearance, almost every census data item underwent at least some change in terms of question wording, response categories, and/or instructions. By comparing the Census 2000-style questionnaire with the 2010 Census questionnaire in the same timeframe, we were able to eliminate the impact of real changes to the population to more clearly assess the combined effects of the questionnaire design changes.  This paper describes the results of the experiment.  ","One of the most expensive operations in the 2010 Census was Nonresponse Followup, for housing units that did not respond by mail. An enumerator collected the census information, using personal visits or phone calls, and sometimes took up to six contact attempts to obtain an interview. The objective of this experiment was to understand the effects of reducing the maximum number of contact attempts in order to save costs in Nonresponse Followup. Two experimental questionnaires, with a maximum of either four or five contact attempts, were systematically distributed in with the standard six-contact forms prior to the start of Nonresponse Followup. All other content on the forms was the same. Results showed that reducing the maximum number of contact attempts enumerators are expected to make appears to have no noticeable negative effects. There was no impact to the rate of successfully completing interviews, no increase in the use of proxy respondents (neighbors or other non-household members), and no increase to item nonresponse or overall form completeness. Further, significant high end cost savings are possible. Though these results are encouraging, an area-level study is recommended before full scale implementation is considered. ","The Medical Expenditure Panel Survey (MEPS) is an annual survey with an overlapping panel design where the sample for a panel is drawn from the responding households of the previous year's National Health Interview Survey (NHIS). Because of this relationship between the two surveys, data from the NHIS can be linked to the MEPS to expand the survey's analytic capacity. However, as the MEPS is conducted a year after conducting the NHIS, not all persons in a MEPS sample can be linked with the NHIS sample due to the joining of new persons in some households. Options for analyzing such a linked dataset are to exclude the cases with missing NHIS data and apply the original MEPS weight, exclude the cases with missing NHIS data and apply an adjusted MEPS weight, or impute missing values so that the full dataset can be analyzed using the original MEPS weight. This paper presents the results of an investigation on the importance of weighting adjustments for analysis and estimation when MEPS is linked with the NHIS core, sample adult or sample child files. ","The analytic capacity of surveys can be dramatically enhanced through the linkage to existing secondary data sources at higher levels of aggregation as well as through direct matches to additional health and socio-economic measures acquired for the same set of sample units from other sources of survey specific or administrative data. In this presentation, the capacity of an integrated survey design to enhance longitudinal analyses focused on mortality studies is discussed. Examples are drawn from the Medical Expenditure Panel Survey (MEPS), designed to produce estimates of health care utilization, expenditures, sources of payment, and insurance coverage of the US population. Analyses are conducted to examine differentials in pre-dispositional factors that distinguish a cohort of decedents from their surviving counterparts and to assess the relationship between antecedent health and healt ","The past twenty years has seen a notable increase in the technology for linking data across databases, in particular between survey data and administrative records. However, public concern about the use of linked data, about personal identifiers, etc., has resulted in \"incompletely linked\" databases. \"Incomplete linkage\" can be a result of record linkage error or of survey respondents withholding permission to link, and it is known that \"incomplete linkage\" varies across important subgroups. In theory, these \"incompletely linked\" databases can be treated as missing data. In practice, the choice of missing data technique is not obvious to the empirical researcher. This paper will explore multiple techniques for handling missing data, in particular reweighting and multiple imputation, in the context of the National Health Interview Survey data linked to Centers for Medicare and Medicaid Services data. The goal will be to examine some typical estimands from the administrative data and determine: 1) Do point estimates derived from the three imputation techniques differ substantively; and 2) What can be said about variance/uncertainty estimation in the context of these techniques? ","National Health Interview Survey (NHIS) survey weights account for complex survey design, nonresponse, and post-stratification. The National Center for Health Statistics (NCHS) can link Medicare data to NHIS respondents. A self-selected subset of NHIS participants are not \"eligible\" for linkage to Medicare since they refused to provide a social security number (SSN), a Health Insurance Claim (HIC) number, or other key personal identifying information. In research with NCHS, we described how to assess potential linkage bias using NHIS 2005 with propensity score estimation as a weight adjustment methodology. This impact of adjustment on distributions of weights, point estimates, and estimated variance was quantified. Weight adjustments to fine groupings of subjects can reduce bias from nonresponse, but tend to produce more variable weights than coarser adjustments. For most data users a single set of weights to reduce apparent nonresponse bias in survey variables would be sufficient, if weighting did not produce large increases in estimated variance.  Other data users might wish to consider the nuances of developing adjusted weights.   Recommended steps to be taken in forming adjusted weights are given. ","Record linkage is a very valuable and efficient tool for connecting information from different data sources. However missing data occur due to unlinked records and nonresponse. In this talk, we develop a multiple imputation model based on the propensity of linkage and compare our method with different imputation techniques. We apply our method to the National Health Interview survey (NHIS, year 2004-2005) linked with the CMS (Medicare) administrative data (year 1999-2007), data linked by the linkage program at the National Center for Health Statistics (NCHS). Simulation studies are conducted to evaluate our approach. ","Two years after the genocide that killed 800,000 Rwandans, primarily Tutsis, there were 80,00-90,000 imprisoned in a country of a few million and the prison population continued to grow by as many as 10,000 per month, the only release being death. In spite of international horror over the brutal loss of life, international notions of justice demanded due process and some semblance of a speedy trial for the accused. The post-genocide Rwandan government rightly claimed that the fragile judicial system, deprived of most of its personnel and much of its infrastructure, could not handle the prospective case load. Donor governments who had already constructed several large new prisons asserted that however, horrible the crimes of which they were accused it was not acceptable to put them in prison and throw away the key. Why not, proposed representatives of the US and other nations, with the ag ","Under Rule 23(a)(2) a party seeking class certification must show there is a question of law or fact common to the class. The plaintiffs in Wal-Mart showed that regression analyses of salary data for each region of the country indicated a significant female shortfall. In contrast, the defendant examined the data for the main departments at each store and found statistical significance in only 10% of them. It will be seen careful analysis of the defendant's summary of its regression results would have supported the plaintiffs' case. Other statistical aspects of the majority and minority opinions will be noted. ","This paper presents recent statistical analysis used in two expert reports for international human rights prosecutions: from Chad and Guatemala. Background on the cases is outlined and motivation for each of the analyses is presented. Both analyses draw on administrative data systems maintained by by law enforcement agencies in Chad and Guatemala. Each data source is described and the missing data challenges from Chad are noted, whereas the notable sampling challenges in Guatemala are presented. I discuss how these data were coded and managed to engage policy questions of command responsibility defined in international law. I present how analyses of document flow from superiors to subordinates (in the form of orders) and from subordinates to superiors (in the form of situation reports and other routine reporting) were used to examine whether political leaders and senior police officials had \"command responsibility\" over their units. We note lessons learned from both cases: focusing particularly on data visualization techniques used in Chad and sampling/inference methods used in Guatemala. We conclude by noting the current status of each case and suggest future research directions. ","We propose and study a new class of semiparametric functional regression models motivated by the complex nature of data encountered in modern scientific experiments. With a scalar response, multiple covariates are collected, a large number of which are time-independent and directly observed and a few may be functional with underlying processes. The goal is to jointly model the functional and non-functional predictors, identifying important scalar covariates while taking into account the functional covariate. In particular we exploit a unified linear structure to incorporate the functional predictor as in classical functional linear models that is of nonparametric feature. Simultaneously we include a potentially large number of scalar predictors as the parametric part that may be reduced to a sparse representation. Theoretical and empirical investigation reveals that the efficient estimation regarding important scalar predictors can be obtained and enjoys the oracle property, despite contamination of the noise-prone functional covariate.  ","In disease surveillance applications, the disease events are modeled by spatial-temporal point processes. We propose a new class of semiparametric generalized linear mixed Cox model for such data, where the event rate is related to some known risk factors and some unknown latent random effects. We model the latent spatial-temporal process as spatially correlated functional data, and propose composite likelihood methods based on spline approximation to estimate the mean and covariance of the latent process. By performing functional principal component analysis to the latent process, we gain deeper understanding of the correlation structure in the point process, and we propose an empirical Bayes method to predict the latent spatial random effects, which can help highlighting the high risk spatial regions for the disease. Under an increasing domain and increasing knots asymptotic framework, we provide the asymptotic distribution for the parametric components in the model and the asymptotic convergence rate for the functional principal component estimators. We illustrate the methodology through a simulation study and an application to the Connecticut Tumor Registry data. ","The two-phase sampling design is a cost-efficient way of collecting  expensive covariate information on a judiciously selected  sub-sample. It is natural to apply such a strategy for collecting  genetic data in a sub-sample enriched for exposure to environmental  factors for gene-environment interaction (G x E) analysis. In  this paper, we consider two-phase studies of G x E interaction  where phase I data is available on exposure, covariates and disease  status and stratified sampling is done to prioritize individuals for  genotyping at phase II. We consider a Bayesian analysis based on the  joint retrospective likelihood of phase I and phase II data that handles multiple genetic and environmental factors, data adaptive use of gene-environment independence. The methods are illustrated by an example from the Molecular Epidemiology of Colorectal cancer study. ","Partial linear models provide good compromises between linear models and  nonparametric models. How to determine which covariates have linear  effects and which have nonlinear effects is a fundamental and  theoretically challenging problem in multiple regression. Most existing  methods in practice are largely ad hoc and lack theoretical justifications. In this work, we tackle the structure selection problem  from a new perspective in model selection. A unified regularization  framework in reproducing kernel Hilbert space (RKHS) is developed to  automatically distinguish linearity and nonlinearity of the covariates,  and at the same time estimate their effects. We show that the new  estimator can discover the underlying true model structure correctly as  the sample size goes to infinity. Numerical examples are presented to  illustrate performance of the new procedure under various regression settings. ","Jon Rao has been a leading figure in survey sampling research for more than 50 years. Some of his contributions in sampling inference, selection procedures, estimation methods, resampling procedures, and small area estimation are reviewed. A brief biography is given. ","The rescaling bootstrap of Rao and Wu (1988) allows the use of flexible bootstrap methods for estimating variances and confidence intervals using data from complex surveys. In this talk we examine uses and extensions of the rescaling bootstrap, and discuss its influence on survey sampling practice. We then propose a bootstrap method for approximating the variance of estimates of population totals under balanced sampling designs, in which the Horvitz-Thompson estimates of the population totals of auxiliary variables equal the true population totals in every realization of the sample. We explore properties of the bootstrap methods and present algorithms for computing the bootstrap weights.   ","Sample surveys are widely used to obtain information about  totals, means and other parameters of finite populations.  In many applications, the same information are also desired  for subpopulations such as individuals in  specific geographic areas and socio-demographic groups.  Often, the surveys are conducted at national or similarly high levels.  The random nature of the probability sampling can result in  no sampling units from many sub-populations of interest.  Estimating parameters of these sub-populations with satisfactory precision  and evaluating their accuracy pose serious challenges to statisticians.  Lacking sufficient amount of direction information, statisticians  resort to suitable models to pool the information across  small areas. Most existing discussions have focused on  estimating small area means under some models corresponding  to imaginary scenarios. They are likely less effective if utilized  for estimating small area quantiles.  In this paper, we postulate that the small area population  distributions have some linear structure with error distributions  satisfying a density ratio model. That is, the small area error  distributions are all  tilted distributions from a common basis. Under this model, we  employ empirical likelihood to pool information in samples  across all small areas. The resulting approach not only allows us to estimate  small area means, but also small area quantiles.  We give a comprehensive discussion on this method and  provide some preliminary simulation results to illustrate its potential. ","Data from complex surveys are being used increasingly to build the same sort of explanatory and predictive models used in the rest of statistics. Unfortunately the assumptions underlying standard statistical methods are not even approximately valid for survey data.   The problem of parameter estimation has been largely solved through the use of weighted estimating equations, and software for most standard statistical procedures is now available in all major statistical packages. With one notable exception, the big gap in the output from these packages is an analogue of the likelihood ratio test and related quantities like AIC. The exception is Rao-Scott tests for loglinear models in contingency tables. It turns out to be straightforward to extend these tests to many other situations, e.g. GLMs and survival models. We show that the asymptotic null distribution is a linear combination of ch ","Instrumental variable (IV) techniques are typically conceptualized and implemented under a structural equation modelling framework(e.g., two stage least squares). This is not the only framework for IV. A study design approach to IV has been developed. This technique, called near/far matching, follows the logic of a randomized controlled trial to obtain causal estimates in an observational setting.    ","The instrumental variables (IV) method is an approach to estimating a causal relationship between a treatment and an outcome based on an observational study in the presence of unmeasured confounders. A valid IV is a variable that affects the treatment, has no direct effect on the outcome except through its effect on the treatment and is independent of the unmeasured confounders given measured confounders. There is often concern that an IV is not valid in the sense that it is correlated with unmeasured confounders. A sensitivity analysis is used to examine the impact of the violation. In many available sensitivity analysis methods, the sensitivity parameter that describes the invalidity of the proposed IV is on an absolute scale. It may be difficult for a subject matter expert to specify a plausible range of values for the sensitivity parameter on this absolute scale. We develop an approach that calibrates the value of the sensitivity parameter to the observed covariates that are thought to be related to the unmeasured confounders that are making the proposed IV invalid and to be more interpretable to subject matter experts. We will illustrate our method using a neonatology study. ","In many studies, researchers are not only interested in examining if an intervention works but also how it works through mediation analysis. Most standard and causal approaches on mediation analysis focus on continuous or binary outcomes. However, the outcome in dental studies is often a count or zero-inflated count. In this talk, we will discuss approaches for mediation analysis on general outcomes, especially count or zero-inflated count outcomes in dental studies.    ","Public reporting is a key part of health care improvement, but the transition from reporting to research is non-trivial.     ","Prognostic score-based methods have been proposed as a complementary approach to propensity score-based methods for achieving covariate balance in observational studies. Likewise, standard adjustments for survey nonresponse, such as response propensity matching, can be supplemented by incorporating information from respondent outcomes. In this talk I will discuss how methods for matching on response profiles can be extended and applied to adjustments for survey nonresponse. I will illustrate these ideas through simulation studies and applications to large scale surveys. ","This article examines alternative sampling designs for national surveys of students. These surveys often seek to oversample demographic subgroups of students, such as African Americans or Hispanics, in order to generate precise estimates for these subgroups. This paper explores multistage stratified sampling designs with primary sampling units (PSUs) defined geographically to provide cost-effective data collection. These designs typically select PSUs with probabilities proportional to size (PPS) using a measure of size (MOS) based on functions of student enrollment, with the MOS calibrated so that the sample yields the required number of students by demographic subgroup. The canonical design uses student enrollment. Using a sampling frame constructed for national surveys, we will generate simulated samples under a variety of sampling designs. Designs will be evaluated using several metrics. ","When conducting school-based student surveys at the national level, multi-stage sample designs are typically employed. These designs form county-based clusters as the first stage of sampling to support cost-effective data collection; it is cost-prohibitive to send staff to a sample of schools distributed randomly across the nation. To address this, schools are grouped into Primary Sampling Units consisting of geographically proximate groups of counties. We note that other design features tend to yield samples that are geographically clustered. These strategies, adopted to oversample sub-groups such as African American and Hispanic students, include stratification with disproportional allocation at the first stage, and the use of a weighted measure of size. This paper explores the possibility of developing sampling techniques that address both goals simultaneously. Can we use methods that oversample particular subgroups of students, and yield geographically proximate clusters of schools in support of efficient data collection without the explicit clustering of schools into design PSUs? The answer to this question is explored via GIS based analysis of simulated samples. ","Charting the Progress of Education Reform: An Evaluation of the Recovery Act's Role is sponsored by the Institute for Education Sciences in the U.S. Department of Education. The evaluation assesses how states, districts, and schools are working to implement education reforms. We required a nationally representative sample of school districts and schools to examine the role that Recovery Act programs may have played in such efforts. The school sample was nested within the district sample, and we required at least two sampled schools within each district for analysis purposes. In addition, we required stratification control for grade span and school performance. This required then a multiway (three-way) stratification structure. To carry this out, we utilized the new balanced sampling theory as developed by Deville and Till\u00e9 (2004). This paper describes our new methodology for executing this theoretical approach, given the large sample sizes, and also presents evidence that the methodology was successful in meeting the desired criteria. ","Rare and hard-to-reach populations pose significant challenges to the design and implementation of cost-efficient sample surveys. To find and enumerate these populations, multi-stage surveys are often used to avoid the construction of a sampling frame for the entire target population, and primary sampling units (PSUs) are selected with probability proportional to a size measure related to the population sizes in the PSUs. When multiple populations are of interest, composite size measures are used that are based on the population counts in the PSUs to achieve equal or nearly equal selection rates within the populations. Some composite size measures were described by Folsom, Potter and Williams (1987) and by Fahimi and Judkins (1991). The purpose of this paper is to identify the capabilities and limitations of using a composite size measure for a survey of students with disabilities in which some disabilities are prevalent and some are very rare. The paper will provide guidance on when the use of composite size measure achieves the desired objectives and when it cannot. ","The American Community Survey (ACS) does not publish single year estimates for areas with a population of less than 65,000. The majority of school districts in the United States contain fewer than 2,000 relevant school-age children. As a result, direct survey estimates for school districts are often highly variable and unreliable due to small sample sizes. This paper looks at two different approaches to modeling that address the problem of small samples in small areas. These small sample sizes can also cause a high frequency of zeros estimates. First, we look at a reweighting method proposed by Schirm and Zaslavsky (1997). Secondly, we look at a censored Fay-Herriot model, similar to that in Slud and Maiti (2011). This model takes up the specific case of no observations of poverty in the ACS sample, resulting in a zero estimate: a problem for over 25% of school districts. ","The 2009 Head Start Family and Child Experiences Survey (FACES) involved four stages of sampling: Head Start programs, centers, classrooms, and children. At the time of sampling, eligible children were those who were one or two years away from kindergarten and were new to Head Start in fall 2009. These children were followed through their first year of Head Start, and then followed for one or two more years, depending on their age, through kindergarten. Children who left Head Start after fall 2009 but did not go to kindergarten were considered ineligible for followup. There existed no published population counts for the study's baseline population, nor were there existing benchmarks for the Head Start retention and kindergarten transfer rates needed to define the study population at followup. This paper shows the steps we took to make use of an earlier cohort of FACES to ensure that the baseline and followup weights, which adjusted for sampling and response patterns, appropriately reflected their respective target populations. We also show how different assumptions about eligibility among those with undetermined status can substantively affect estimated totals and mean estimates. ","This paper explores how cognitive interviews can be used to gain insight on privacy and confidentiality concerns in factual surveys. In order to explore the benefits and limitations of this methodology in identifying such concerns, we present a case study that sought to understand living situations of respondents who listed in multiple places during the Census.  Based upon respondents' reaction to questions in this survey, we tried to determine whether certain questions were perceived as too private and/or respondents had concerns that the confidentiality of their data had been breached. This paper examines how privacy and confidentiality concerns were discussed and identified through the cognitive interview. We examine situations, in which these types of concerns were uncovered, what types of probing were useful in uncovering them and situations where they remained uncovered within the interview itself. Recommendations are discussed pertaining to how to best study privacy and confidentiality concerns in cognitive interviews and the limitations of the methodology for this purpose. ","The Current Employment Statistics (CES) program of the Bureau of Labor Statistics (BLS), surveys approximately 440,000 establishments in order to provide detailed industry data on employment, hours, and earnings of workers on nonfarm payrolls. The authors have been researching the use of CES survey data to produce industry estimates by firm size. This paper first details research to develop methodologies for producing CES estimates by firm size. This includes defining size classes, determining firm size, additivity across size classes, forecasting net birth/death residuals, seasonal adjustment, and the creation of a historical series of CES size class estimates. Next, the paper covers several issues addressed by the developed methodology. CES considered the adequacy of the sample for size class estimates along with movements of firms between size classes to understand how this might impa ","The BLS Current Employment Statistics (CES) program recently released experimental estimates of nonfarm payroll employment by firm size. These data offer enhanced opportunities for analyzing and understanding industry employment data. This paper highlights some of these opportunities. Among other things, it explores the size-class breakdown of employment at the NAICS supersector level; it examines patterns of job gain and loss by size class during periods of economic contraction and expansion; and it looks at varying patterns of seasonal hiring among size classes. ","The Bureau of Labor Statistics (BLS) Job Opening and Labor Turnover Survey (JOLTS) is an establishment survey that seeks to measure dynamic trends in the US labor market. The Survey publishes each month the number of job openings, hires and separations at the national, regional and major super-sector industry level of detail. The Survey also publishes establishment size class estimates as an experimental series. After consultation with some of the key data users, it was found JOLTS users prefer firm size (proxy for size of employer) estimates rather than establishment size estimates. The purpose of this paper is to provide details of the methodology used to develop JOLTS firm size estimates. The process begins with assignment of firm size to the frame and sample; development of birth/death models and independent population controls for employment at the industry by firm size level; and a ","The Job Openings and Labor Turnover Survey (JOLTS) program of the Bureau of Labor Statistics recently produced firm size estimates for research purposes. For each of the 3 firm size classes, data series were produced at the total private level for job openings, hires, total separations, quits, layoffs and discharges, and other separations from December 2000 through December 2011. For this paper, firm size classes are compared to each other, to national level JOLTS data, BED net employment growth, and to Current Employment Statistics by firm size. Differences in the movements of the time series before, during, and after the most recent recession are analyzed with respect to size of firm. The impact of the economic cycle on larger versus smaller firms is discussed as are other factors that potentially contribute to the time series behavior of the different size classes. ","The Bureau of Labor Statistics is conducting research on the feasibility of producing industry employment estimates by size of firm using the Current Employment Statistics (CES) survey, a monthly survey of business payrolls. The Office of Management and Budget has designated 12 size classes to serve as the basis for official estimates produced by federal statistical agencies, but a smaller number of categories must be used for CES estimates in order to reduce sampling error. This paper analyzes the problem of collapsing the 12 classes into a smaller number of categories for use in constructing CES estimates. We develop a methodology based on K-means cluster analysis modified to require that clusters retain the ordering of classes. We also require that each cluster has sufficient employment share and adequate sample in each industry. We use 4 characteristics of size classes in the cluster analysis: seasonality of employment, long-term trend in employment, cyclicality of employment, and share of employment related to business births and deaths. We also consider size categories used in related data and reclassifications of firms over time across size categories. ","The National Center for Health Statistics collects vital statistics for all states including mortality data from all causes of deaths. The data used in the analysis are weekly mortality counts that cover a span of 3 years from all causes of deaths from neighboring states. For prediction and regression purposes it is important to find relationships between these weekly time series. A quantity which measures linear relationships between time series is the measure of coherence. A certain nonlinear extension of linear coherence is called lagged coherence. Both measures were applied to selected neighboring states and they indicate strong associations between neighboring states particularly at low frequencies or for long cycles. This helped in identifying useful covariates for prediction and regression models for mortality time series. This paper illustrates some prediction and regression models. ","Current production models of poverty utilized by the Census Bureau for the Small Area Income and Poverty Estimates (SAIPE) program generally incorporate only a single-year of inputs. These models produce parameter estimates by assuming some degree of homogeneity across areas. With six years of consistent inputs to the SAIPE model, including the American Community Survey, we find that additional precision is obtained by modifying assumptions of homogeneity in parameter estimates across years, and relaxing some cross-sectional assumptions. The impact of these alternative assumptions is then illustrated in terms of the change in precision to the estimate within population clusters and within the context of the current production SAIPE models. ","This talk introduces improved algorithms for tracking boundaries in an image using change-point detection techniques. Boundary trackers move at the interface between two regions, weaving between them and making course corrections based on a decision function. By considering only local information, boundary tracking algorithms are able to follow boundaries very efficiently, making them quite suitable for work in large or high-dimensional images. As local algorithms, however, boundary tracking methods are extremely susceptible to noisy data or texture. We adapt the CUSUM algorithm to detect region changes on a local level, vastly improving tracking ability. Further improvements can be made by introducing a second change-point detection statistic to detect global off-boundary movement. We discuss the uncertainty involved in the tracking of noisy images and the extent to which false alarms and detection delays in change-point algorithms affect tracking. The adaptation of boundary tracking to the problem of image segmentation via hybrid schemes and applications to hyperspectral data and the tracking of fractal-like structures will also be discussed. ","Generalized estimating equations (GEE) and mixed/random effects regression models (MRM) are two approaches commonly used to evaluate longitudinal designs that involve repeated measurements. The merits of these two kinds of models are poorly understood by non-statisticians. In particular, this presentation will focus on appropriate uses of GEE and mixed models. Sisters Informing Sisters about Topics on AIDS (SISTA) is an HIV prevention intervention designed for African American women. Using SISTA as an example, this presentation will illustrate procedures for model selection and interpretation of results. GEE logistic regression models and random intercept models (mixed effects models) are applied to evaluate changes in HIV risk behaviors from baseline to two follow-up time points. Population-level and subject- level parameter estimates, and differences in parameter interpretation (e.g., evaluation of intervention efficacy at the population level vs the exploration of HIV risk change variability within and among subjects) from the two types of models are compared. Recommendations for model selection and parameter interpretation for the evaluation of HIV intervention preventi ","The Producer Price Index (PPI) program conducted a study to determine if nonresponse bias exists in PPI data. The study investigated nonresponse at unit initiation (when units are asked to participate in the PPI and provide initial item prices) and during the item repricing period (when units provide follow-up prices for the items they have agreed to reprice at initiation). The study consisted of three stages: In the first stage, regression models were used to analyze the relationship between unit initiation response and several explanatory variables as well as between item repricing response and these same set of explanatory variables. In the second stage, another regression model was used to analyze the relationship between item short term relatives and the explanatory variables which were determined to be significant in the first stage. The third stage tested the impact of adjusting s ","The Current Population Survey (CPS) adjusts the sampling weights for nonresponse to match population controls based on cells which combine similar primary sampling units (PSU) based on size and urbanicity. This would increase weights for responding units in PSUs with higher nonresponse. The adjustment method assumes that the nonresponse is random within the adjustment cells. The present study uses information from the Contact History Instrument (CHI) to adjust the weights based on the patterns of responses interviewers experience in contacting and attempting to interview households. This additional adjustment has the potential to reduce nonresponse bias. ","The Bureau of Labor Statistics Occupational Employment Statistics Survey produces estimates of occupational wages and employment for domains including all US MSAs and balance-of-State areas. Although the OES is designed as a cross-sectional survey, its size together with recent innovations in statistical/ computing methods make feasible estimators of the change in small area occupational employment over time on the scale of a large survey. The analyses use a zero-inflated binomial logistic-normal model to obtain, for each year separately, the best empirical prediction estimate of the proportion of total area /industry cell employment in each occupation. The Pseudo Empirical Likelihood alternative to raking is then used to constrain the marginal sums of the estimates to equal 1) relatively accurate estimates of occupational employment shares estimated at more aggregated levels, and 2) known population quantities at the area level. Two modifications of this estimator are also examined, including one that incorporates a size-adjustment weighting scheme, and another that uses a single average variance component in the composite estimators, obviating the need for raking. ","The National Crime Victimization Survey (NCVS) has provided annual estimates of the number of victimizations for several types of crime since 1972, with an almost exclusive focus on national rates. Most of the programs to prevent or reduce crime are implemented locally, however. To respond to a resulting interest in subnational statistics, the Bureau of Justice Statistics (BJS) has been recently supporting research on a variety of approaches to produce subnational estimates. In this paper, we report on the potential application of model-based small area estimation methods based on the NCVS and auxiliary data, particularly the FBI's Uniform Crime Reports, using empirical best linear unbiased estimation (EBLUP). We compare a time-series model introduced by Rao and Yu to a new variant, termed here the dynamic model. We will also indicate how the small area approach might be integrated with other approaches that BJS is currently considering, including possible expansion of the NCVS sample size to augment the survey's capacity to produce direct estimates for some or all states. ","Small area estimation (SAE) methods are used to produce state and substate estimates of substance use and mental disorders using data from a major U.S. behavioral health survey. State-level policymakers use these estimates to help justify funding for substance abuse prevention and treatment programs in their jurisdictions. Thus, it is important to determine how the SAEs compare to their design-based counterparts (where design-based estimates are less expensive and can be produced in less time) and if the SAE models can be improved to increase the precision of change over time.     ","NAMCS has been recently redesigned to allow for reliable direct estimates for larger states and  remaining smaller states grouped within Census divisions. When covariates correlated with the  outcome variable are available for every unit in the population it is possible to gain efficiency of  estimates in small areas by using model-based or model-assisted estimators. On the other hand  these methods are sensitive in varying degree to possible misspecification of model assumptions  with respect to the superpopulation model. In this simulation study we assess robustness of model-based estimators to possible misspecifications of the estimating models and compare their efficiency  to direct and model-assisted estimators. ","One prerequisite for successful small domain modeling is the ability to form a \"pool\" of like \"areas\", such that \"borrowing strength\" across the areas is a viable concept.  This may not always be possible for various reasons.   We consider estimation of employment from the Current Employment Statistics (CES) survey conducted by the U.S. Bureau of Labor Statistics. Detailed estimation cells are defined as intersections of industrial and geographic levels. Combining direct detailed level estimates across States is not always feasible due to possible substantial differences between States, intricacies of individual States' estimation structures, or simply due to logistics of the production procedures at State levels (for example, differences in the production timeframe.)   A simple area-level model is formulated for higher level estimates.  We explore the possibility of applying the parameters from the higher-level setting to States' detailed levels.   ","Replicate weight variance estimators are used to estimate the sampling variance  for survey estimates in many complex governmental surveys such as the American  Community Survey (ACS), Current Population Survey (CPS), and Survey of In-  come Program Participation (SIPP). Small area models for the sampling variance  require specication of the relative variance (square of the coefficient of variance,  CV2, or similarly, degrees of freedom for a Chi-square distribution) for the variance  estimator. In this paper, we will approximate the relative variance of the replicate  weight variance estimator for ratios using rst-order Taylor expansions. The result  of this work is to guide the model-based assumptions for small area variance models. ","The National Health Interview Survey (NHIS) is an annual health survey conducted by the National Center for Health Statistic. The survey design provides reliable annual estimates for health-related conditions for the nation and the four major geographical regions of the United States. However, direct estimates for some states or sub-state regions are unreliable due to insufficient sample sizes. We propose small area models that include both local area and short-term time random effects, which describe year-to-year variation over cluster samples, to estimate the prevalence of smoking for each of the fifty U.S. states and the District of Columbia. In particular, hierarchical Bayesian nonlinear mixed effect models, using the 2006 to 2010 NHIS data, is explored. Auxiliary variables will be obtained from the Area Resource File. Bayesian Markov Chain Monte Carlo (MCMC) approaches will be used for estimation. A major portion of this study is a discussion of various methods to estimate the time specific sampling covariances needed to implement the proposed models. Comparison of different models by model fits and model performance are discussed. ","Assuming standard regression model for one small area domain(and its complementary domain) with known auxiliary variable population totals BLUE of domain total of an estimation variable is derived as regression-synthetic estimator with known auxiliary variable population totals instead of domain means in the estimator based on type-B model. Assuming one auxiliary variable in the model BLUE of domain total of an estimation variable is derived. It reduces to synthetic estimator with ratio-adjustment (Ghangurde &amp; Singh(Survey Methodology(1977)) and then to ratio-synthetic estimator; the latter reduction involves a substitution of synthetic weights in terms of administrative data. It is assumed that the ratio of estimation variable total to auxiliary variable total in the domain is equal to the ratio in the population. The assumption is made explicitly in synthetic estimator but is implicit in ratio-synthetic estimator. ","We present a generalized estimating equations approach for estimating the concordance correlation coefficient and the kappa coefficient from sample survey data. In order to correctly account for the sampling design, the estimates and their accompanying standard error need to incorporate these features. Weighted measures of the concordance correlation coefficient and the kappa coefficient, along with the variance of these measures accounting for the sampling design, are presented. We use the Taylor series linearization method and the jackknife procedure for estimating the standard errors of the resulting parameter estimates. Body measurement and oral health data from the Third National Health and Nutrition Examination Survey are used to illustrate this methodology.   ","By the end of 2011, 32.3 percent of adults lived in cell phone only (CPO) households. Multi-mode address-based sampling (ABS) has been assumed to cover CPO households that are missed in traditional landline random-digit dialing (RDD) designs. The ABS frame covers nearly all addresses, and data collected via mail allows us to capture data from these individuals. No literature, however, has yet to publish statistics on how well multi-mode ABS designs capture the CPO households in a hard-to-reach population. We attempt to answer this question using data collected in Phase 4 of the Racial and Ethnic Approaches to Community Health across the U.S. (REACH U.S.) Risk Factor Survey. Our results suggest that ABS designs with a mail component cover CPO households and that the inclusion of these CPO households significantly changes some health estimates. ","Few studies conducted have examined the impact of the e-mail contact that informs the respondents about a web-based survey. One exception is Porter and Whitcomb's study that explored the impact of altering the e-mail contact to understand the effects of personalization, sponsorship, and scarcity on web-based survey response rates. Inspired by their work, our study was aimed to investigate the effect of e-mail contact. In our experiment, 620 college students were invited to participate in a web survey and randomly assigned into three groups. Participants in Group A received an e-mail contact with the statement: \"the survey takes about 5~10 minutes to complete\", while Group B and C received e-mails contact with different statements (10~15 minutes for Group B; 15~20 minutes for Group C). After we controlled for participants' characteristics and intention to participate in the Web surveys measured before the experiment, the logistic regression results indicated Group B and C decreased by 38.6% and 54.1% respectively to complete the survey than Group A. Our paper concluded that the email instructions, specifically survey completion time, can influence response rate of a web survey. ","The National Center for Health Statistics is integrating three hospital surveys into one survey named the National Hospital Care Survey. The new survey will measure the provision and utilization of medical care services provided by non-Federal, non-institutional hospitals with six or more inpatient beds and by freestanding ambulatory surgery centers in the United States. Similar to the prior surveys, data will be collected in national samples of hospital discharges, ambulatory surgeries, and visits to hospital emergency and outpatient departments. Relative to the prior surveys, the new survey covers an expanded hospital population and targets additional estimates. This paper discusses the sampling design for the new survey and selected differences between the old and new survey designs. ","The demand for direct estimates of health outcomes both over small geographic areas and across small population domains often exceeds survey cost constraints. Lately, indirect, model-based, methods for small area and small domain estimation have helped address this issue, especially since their usefulness and limitations are being better understood. Although these indirect methods can help to avoid the need for increases in sample selection costs, they can be labor intensive, requiring careful modeling and selection of covariates on an outcome by outcome basis. The development of a relatively quick and relatively model free procedure to determine which outcomes vary substantially across areas, as a first step toward full-scale small area estimation, is outlined and current progress is documented. ","This paper considers the best form of the collapsed stratum variance estimator. If you select 1 primary sampling unit per stratum and collapse 2 strata (say, A and B) together for each group, the usual estimator (Hansen et al., 1953) assigns a weight to stratum A's estimate equal to twice the measure associated with stratum B divided by the sum of the measures for the 2 strata. This estimator is known to overestimate the variance. We developed an alternative estimator that is approximately unbiased and assigns a weight to stratum A (B) of the square root of the ratio of stratum B's (A's) measure to its measure. We expected it would be a better estimator as an \"approximately unbiased\" estimate might be expected to produce better results than one known to be an overestimate. However, this paper shows the approximately unbiased estimator never results in a lower variance estimate than the overestimating variance estimator. A general cautionary conclusion from these results is that an \"approximately unbiased estimator\" can be worse and actually larger than an \"overestimate estimator\". ","Large national population-based surveys are often based on multi-stage cluster sampling. When using design-based estimation methods on such surveys, knowledge of the complete hierarchical sampling structure is required to correctly assess the sampling distributions of estimators.  However, for public-use data releases, the survey design information for analyses is substantially simplified.  For example, the NHIS is described as a stratified set of independently sampled first-stage clusters with the final adjusted survey weights to be used as fixed sampling weights.  While such simplified design structures can be justified under hypothetical sampling conditions, those sampling conditions rarely hold in practice.  To study the impact of simplified survey analytic structures, a finite pseudo population of 340,000 households has been created using nine years of NHIS.  This population captures many of the clustering features present in the U.S. population.  For this paper, sampling methods and estimation methods consistent with those of the NHIS are studied on this pseudo population.  Evaluations are presented under true and simplified assumptions. ","Since the introduction of dual frame estimators 50 years ago, many dual frame estimators have been developed to estimate the universe mean and total. All the estimators require knowledge of domain identity of items in the sample. In many cases, this information can be obtained from the frames. However, in other cases, this information is not available and can only be obtained from the respondents. In either case, it is possible that the information is erroneous. This measurement error usually results in biased estimation. The topic of our research is to examine how damaging this problem is to Hartley's estimator. We develop a bias correction method to eliminate the bias caused by misclassification. The performance and asymptotic properties of this method are also studied. ","It is said that a well-designed survey can best prevent nonresponse. However, no matter how well a survey is designed, in practice, nonresponse almost always occurs. The easiest way to deal with nonresponse is to ignore it, but frequently, ignoring nonresponse results in poor survey quality. Item nonresponse and unit nonresponse are two types of nonresponse. Imputation procedures are commonly used to compensate for the former while weighting methods are popular remedies for the later. This paper focuses on weighting methods that help reduce nonresponse bias. ","It is widely believed that the inferential value of sample survey data is jeopardized by selective non-response bias. Traditional methods for assessing non-response bias involve comparing early to late responders to assess differences in study outcomes and demographic characteristics. Since defining the cut-point for dichotomizing the sample is subjective and can substantially change the results of a non-response bias analysis, we investigate non-response bias using a continuous measure of response latency under Continuum of Resistance (COR) Model assumptions. The COR model asserts that late responders are similar to non-responders and there exists a smooth relationship between the outcome and the continuum of response resistance. Using data from a college drinking survey of &gt;19,000 college students over 6 years, we examine non-response bias in high-risk drinking outcomes and alcohol-related consequences. Study outcomes were regressed on time-to-response to reveal that response latency accounted for less than 0.07% of the variation in any of the outcomes (R-squared &lt; 0.0007). No p-values reached significance suggesting that non-response bias in the sample is minimal. ","Arbitron obtains hundreds of thousands of records annually from Survey Sampling International (SSI) for purposes of selecting household samples. The primary information that Arbitron gets from SSI is either a phone number or an address (sample point) and some geographic descriptive information such as county or subcounty. SSI has the ability to provide additional information about the sample points; including such things as name, age and race/ethnicity of the householder, existence of certain persons of certain age group or gender in the household. Achieving a sample of respondents that satisfies demographic and geographic proportionality is one of the main goals for Arbitron, since radio listening does vary by these characteristics. We analyzed the usefulness of the SSI auxiliary information to find Hispanic and young households in order to sample them at a rate that yields a proportional sample. We compared the demographic information from thousands of Arbitron respondents to the SSI frame information. We report the proportion and types of matches and non-matches by household characteristic. We discuss data quality metrics in terms of accuracy and coverage and the importance of each to achieve our survey's goals. ","One benefit of using landline telephone samples has been the ability to define geographic area using phone prefixes. Recently, numbers have become portable, and landline numbers have been changed to cellular phones. These changes have made prefixes less reliable in terms of geographic location. As surveys move to larger proportions of cellular samples, the need for other methods to obtain geographic information becomes more salient. Switch centers may be used as a proxy to ascertain likely locations of cellular users regardless of the prefix. Switch centers are associated with cellular numbers at the point at which phone numbers are activated, providing a \"home base\" location for each number within a cellular sample. This research uses 2011 telephone survey data of 38,000 respondents from eight regions located within four states to determine whether the use of switch centers increase the likelihood of reaching eligible adults within targeted areas. Results show that switch center information can significantly increase the likelihood that eligible respondents are contacted within targeted areas. Geographic data displays illustrate tower locations and success rates for eligibility. ","Non response is a common obstacle at Statistical surveys. Rather than omitting the non-responses and making inferences, many methods have been developed, in order to get the best out of the situations. Each one has its own advantages as well as disadvantages. Therefore it will be worthwhile to make a study to compare those, by using identical situations. It will produce a platform to make a comparison among different methods. Intensive study over this area may help to investigate in to clusters of experiment types such as whether particular method would be much beneficial for a certain types of experiments. Apart from the methods of handling non-response data, this study would associate large number of published Survey analysis by reputed Statistical establishments in order to identify the key areas of survey interests in the society.  Elaborating currently practiced imputation methods at the initial stage of the study would pave the way towards better understanding the outcome of results which would be simulations of complete data sets where the imputations are artificially enforced and the results are compared.   ","The Medical Expenditure Panel Survey (MEPS) is a nationally representative health survey that is used to produce estimates of health care expenditures and utilization. The MEPS uses the National Health Interview Survey (NHIS) as its sampling frame, allowing for limited paradata that may assist with developing discrete hazard models to assess the likelihood of a completed interview with sample households. Using data from the 2004 NHIS, along with 2005 MEPS contact history data, the purpose of this paper is to explore factors that may influence a successful interview in the MEPS. ","Arbitron continually collects data on radio listening from survey samples of people in close to 300 media markets across the United States. Users of the data have access to a web-based application that can be used to generate audience estimates for customizable sub-populations defined by the intersection of a number of dimensions: demographic, socio-economic, geographic, time of day. All told, there are billions of possible audience estimates.    ","Since 1939, the Current Employment Statistics survey has used a single page form for capturing data. Over time, problems with the design of the form have become apparent. In 2009, research on how to improve the form was completed. Prototypes based on recent research were created and fielded for testing during 2011 and 2012. The test consisted of systematic random samples which were selected and sent to regional data collection centers each month. Samples were stratified by geography, industry, single location vs. multiple locations, and size of firm. Results show improvements in interview times and reduced item non-response. ","Small sample sizes and no sampled units in subdomains are common problems in surveys. Recently we applied Small Area Estimation methodology to two major surveys in our Governments Division surveys: the Annual Survey of Public Employment and Payroll (ASPEP), and the Annual Finance Survey (AFS). In ASPEP we used the composite estimator to estimate the total gross payroll and employees for each state and function. In AFS we used the step-wise ratio to estimate the Revenues, Expenditures, Debt, and Assets for each state. The main idea is to borrow strength from larger areas as well as from previous years. The efficiency and the quality of the data in the sample were improved by applying Decision-based Estimation methods to a modified cutoff Probability Proportional to Size sample. The Decision-based Estimation improved model fit from which we benchmarked the final estimates. ","This paper introduces a coupling strategy of model-based signal extraction with a newly developed iterative approach to multidimensional direct concurrent filtering (M-DFA). The process of extracting desired signals in nonstationary seasonal time series is fortified using a three-step approach to obtain better control over user-defined properties. These properties can include timeliness of turning-points in trend and/or cycles, signal-noise ratio characteristics, and quality of seasonal adjustment. Futhermore, we show how the uSim software developed by the author simplifies this hybrid approach of user-defined signal extraction using a simple combination of graphical-user-interface controls for adjusting regARIMA model fitting, X-12-ARIMA-SEATS signal decompostions, and timeliness/accuracy controls in an iterative M-DFA algorithm. Finally, we give numerous numerical examples to demonstrate the potential and robustness of the proposed hybrid method. ","The X-12-ARIMA seasonal adjustment program has long had automated  procedures for selecting trading day, holiday and user-defined regressors using AICC. For Easter, selection involves multiple comparisons: a regARIMA model with no Easter regressor and three regARIMA models with different Easter regressors are compared with one another. Previous research showed that the automatic procedure selects a model with an Easter regressor too often. We present the results of a new study and propose modifications of the automatic regressor selection procedure to reduce type I errors. The modified procedure will be implemented in a future release of the Census Bureau's X-13ARIMA-SEATS program, the current release of which is intended to supersede X-12-ARIMA. The new study, based mainly on simulations, considered calendar regressors for stock series in addition to the flow series regressors considered previously. ","The U.S. Bureau of Labor Statistics (BLS) seasonally adjusts weekly Unemployment Insurance (UI) data for the Employment and Training Administration of the Department of Labor. Although concurrent seasonal adjustment is quite common for monthly and quarterly data, projected factors continue to be the norm for weekly data. Many studies, such as Pierce and McKenzie (1987), suggest that concurrent adjustment will lead to greater accuracy and smaller revisions. However, the assumption that weekly data are likely to be more subject to calendar effects, and also that the program would need to be run 52 or 53 times a year instead of 12 or 4, has led to the continued use of projected factors. Comparisons are made between concurrent estimates with ones projected out 13, 26, or 52 weeks utilizing week-to-week changes and revisions. ","We compare the AR spectrum test of X-12-ARIMA and the stable seasonal F-test of Lytras, Feldpausch and Bell (2007), who demonstrated that the latter is well-sized and the former substantially oversized when testing at the .05 level. In contrast, we present results from simulations of stationary seasonal ARMA models with a seasonal AR coefficient of THETA12=0.4 or less and show the F-test incorrectly identifies stable seasonality in the majority of these series, whereas the spectrum test rarely makes this error. We also explain why the spectrum test, limited by default to the last 96 months of data, much more readily identifies residual seasonality in poorly adjusted time series with moving seasonality than the F-test and show that, at the expense of a second run of the software on the last 108 months of the seasonally adjusted series considered, the F-test becomes competitive.  ","In 2011, the UK Government Statistical Service launched a task force to consider the impact of moving from X-12-ARIMA to X-13ARIMA-SEATS on outputs and hence users of UK statistics. The remit of the task force was to quality assure the results from seasonally adjusting and forecasting a range of time series - especially problematic types - using both software packages. The task force consisted of members from seven government departments, and the three devolved administrations of Wales, Scotland and Northern Ireland. Alongside the recommendation to change software, the task force recommended a two year training programme across the UK in the use of the new software for seasonal adjustment. This paper takes the reader through the process of evaluation, the wider assessment of requirements for training, to the mechanisms set-up to improve seasonal adjustment practice across the UK. ","Often analysts seek to combine information from two (or more) different sources, i.e., data fusion. I discuss a framework for multiple imputation inferences for data fusion contexts in which two sources do not have any overlapping records. The basic idea is to (i) formulate a joint model for the concatenated data, (ii) posit values of any unobservable parameters in the model, and (iii) generate multiple imputations under the posited model. However, in this context, the usual multiple imputation combining rules of Rubin (1987) lead to biased estimates of variance, even when the posited parameter values are correct. I present an alternative framework that enables valid estimation of variances for these data fusion settings. ","In 1999, dual-energy x-ray absorptiometry (DXA) scans were added to the National Health and Nutrition Examination Survey to provide important information on body composition. However, in 1999-2004, DXA data were missing at least partially for about 21 percent of the eligible participants; and the missingness was associated with important characteristics such as body mass index and age. Multiple imputation of the missing DXA data was performed. Features making the project interesting and challenging statistically included the relationship between DXA missingness and the values of other variables; the highly multivariate nature of the variables being imputed; the need to transform the DXA variables during the imputation process; the desire to use a large number of non-DXA predictors, many having small amounts of missing data, in the imputation models; the use of lower bounds in the imputation procedure; and relationships between the DXA variables and other variables, which helped both in creating and evaluating the imputations. This talk describes the imputation models, methods, and evaluations, and demonstrates properties of the imputations via examples of analyses of the data. ","Demands for survey data collected, especially funded by tax payers, is increasing for the analytical purposes. At the same time administrative data sources with personal identifying information is also becoming available. An intruder linking the such data sources may be able identify a respondent and glean information collected in the survey under the pledge of privacy and confidentiality. Multiple imputation technique was proposed to handle such situations whereby some or all variables on some or all subjects be synthesized based on the observed data to allow valid statistical analysis but preserve the privacy and confidentiality. This technique could greatly expand the information that could be provided to obtain national and sub national estimates, linked survey and administrative data files and combined data from multiple surveys. This talk will focus on some methodological issues and challenges in implementing this method and strategies for obtaining efficient estimates from synthetic data minimizing concerns about identification of respondents to the survey from the released multiply imputed synthetic data sets. Parametric and nonparametric strategies will be discussed. ","Questions on income in surveys are prone to two sources of errors. First, income is considered sensitive information and item nonresponse rates on income questions in general tend to be higher than nonresponse rates for other non-sensitive questions. Second, respondents tend to round their income. Depending on the respondent and on the value of income, the magnitude of rounding can range from rounding to the closest 5 dollar value to rounding to the closest 5000 dollar value. This kind of measurement error (often called \"heaping\") may lead to bias in non-linear statistics like the median income or the poverty rate. In this paper we propose a two stage imputation strategy that estimates the posterior probability for rounding given the observed values on the first stage and multiply re-imputes the income values given the rounding probabilities on the second stage. Missing values are also imputed at this stage. We provide a simulation study illustrating that the proposed multiple imputation model can help overcome the possible negative effects of heaping. We also present results based on data from the German panel survey \"Labor Market and Social Security\".    ","Like many surveys, the Health and Retirement Study (HRS) encourages respondents to use records to answer questions on financial data such as assets, mortgage payments, and the like. The question is how respondents react to that encouragement, and what effect it has on the quality of data they provide. In the 2009 HRS Internet survey, a random half was prompted to consult records at the start of the relevant section. The encouragement increased reported use of records from 38.7% to 46.7%. In the 2011 survey, the encouragement was moved to the advance letter for a random subset. This gives respondents sufficient time to gather the relevant documents, but may also encourage nonresponse. The prompt increased reported record use from 45.6% to 55.4%; however, the group that was prompted has a lower response rate to the survey (75.5%) than those not prompted (78.3%). In this paper we examine the effects of such encouragement on compliance (self-report of record-use) in the two surveys, and on the quality of the data reported (item missing data, rounding, etc.). ","Dependent verification is a commonly used approach to check the validity of survey operations such as coding, listing, and translation. In this context, dependent means that the verifier has access to the initial outcome of the operation, which he/she then reviews for errors. There is ample evidence that this method tends to discover only a fraction of the errors in the initial operation. The authors' experiments with dependent listing and coding verification have shown that listers add only 81% of the housing units that are missing from the initial frame and control coders miss at least 50% of the errors. Survey methodologists have dealt with this shortcoming in dependent verification by turning to independent verification, where the verifier has no access to the initial outcome. This approach reduces the problem of under detection of errors, but is more expensive and more complex to administer than dependent verification. In addition, independent verification is not itself error free. By shedding more light on the cognitive mechanisms at work in the verification task, we hope to be able to design more effective dependent verification procedures that neutralize its drawbacks. ","Given a random sample x1, x2, ., xn from a population, we examine circumstances that lead to some of the values being deemed outliers and the methodologies proposed to analyze the data set in the presence of these outliers. We review methods of determining outliers and propose general principles for how to proceed. Often a mixture approach is appropriate: most observations seem to follow a pattern or to satisfy a model, while the outliers remain outside the pattern or model and require further investigation. ","Probability and judgment samples are extensively used in financial audits. Probability samples need no discussion in this audience. Judgment sampling, even though much maligned, can have a useful role in a discovery context when conducting a compliance audit. Still, probability samples are required if attribute error rates are sought. Seemingly forgotten in recent years is that when problems are found through a discovery judgment sample, there is no direct way to accurately estimate the impact of a control failure. Without a probability sample follow up, conclusions, even if based on multiple judgment samples, can be misleading. The use of now standard meta-analysis tools for multiple judgment samples simply does not work. The nonstatistical intuition is that multiple judgment samples can, somehow, be combined and a stronger inference made. That is simply untrue and has caused much harm in some important cases. The just settled Cobell Indian Trust Case will be used as an example. ","Each year, survey organizations spend thousands of dollars recruiting, hiring and training interviewers who resign or are let go within the first few months of being hired. If an organization could identify individuals who are likely to attrit based on information that is learned during the hiring process, this could lead to significant savings in both time and money. Previous research shows that older phone interviewers are less likely to attrit than their younger counterparts (Schalk and van Rijckevorsel, 2007), while mixed results are found concerning education and attrition (Porter and Steers, 1973; Mobley, 1982). In addition to demographic characteristics, the University of Michigan's Survey Research Center also asks applicants to self-report their proficiency on different tasks and gives applicants an exam testing their computer skills. Analyses including proportional hazards models are conducted to investigate if these factors are associated with applicants who attrit within the first few months of being hired. ","A common characteristic in almost all sample surveys is the presence of nonresponse. Nonresponse can negatively affect the quality of the sample estimates, because of the possibility of selection bias. One potential factor that can influence nonresponse is the survey interviewer. In this study, we employ multilevel modelling techniques to investigate interviewer effects on nonresponse bias in complex surveys. We describe an application for the method using data from the 2001 UK Labour Force Survey (LFS). Information at individual level on both respondents and nonrespondents was acquired by linking each household member in the LFS dataset to the 2001 UK census individual records. The results show that there is evidence of nonresponse bias induced by interviewers. ","For in-person surveys, the negotiation between a respondent and an interviewer is a game  of chance where skill is important. A prioi, it is generally not known whether any given  case will be resolved as complete or incomplete, though there may be general knowledge  about what \"types\" of respondents may tend to be relatively difficult to interview. Aside  from the tools that may help to frame the interaction between the respondent and the  interviewer (e.g., advance letters, brochures, monetary incentives, etc.), the interviewer  must rely on his or her skill in listening to the respondent and providing information  tailored to the concerns of questions of the respondent. Through selection over time,  successful interviewers must inevitably have skills of this sort. The key question  addressed in this paper is whether interviewers have information that allows them to  assess the likely outcome of their work while they are in the midst of doing it. This  question is of interest for a number of reasons, but the issue that motivated this  investigation was the possibility that interviewers' expectations (and those of their  managers) might influence the patterns of work that lead to the ultimate distribution of  outcomes and thereby distort straightforward analysis of those patterns. ","Dual-mode surveys are use to increase response rates and/or reduce costs. Similarly, dual-frame surveys can yield substantial cost savings when sampling rare populations, or when different survey modes are used in each frame and one mode is much cheaper than the other. In longitudinal surveys however, respondents with a given set of characteristics might experience both higher attrition and greater propensity to answer via one of the two modes; thus increasing undercoverage of these respondents as time goes on. Similarly, in dual-frame surveys, one frame might be incomplete, bias, suffer from undercoverage, or have higher nonresponse than the other. Our proposed post-survey weight adjustment method utilises propensity scores to adjust the weights of respondents from the mode/frame suffering from undercoverage/bias by making their probability distribution closer to that of respondents in the other mode/frame. ","In the interest of accurately estimating a parameter of interest, generally a population total, calibration is a method that adjusts the sampling weights of each selected element such that the adjusted estimates of the totals of auxiliary, or benchmark, variables equal the known population totals. Calibration has been used to adjust for frame undercoverage, nonresponse, and sampling weights. To treat nonresponse, under the quasi-randomization model assumptions, the sample of respondents is treated as an additional phase of sampling, where the probabilities of response are estimated from a set of model variables. Under this model and varying response probability assumptions, we explore a special case of the calibration method to treat doubly cross-classified data that uses characteristics of the classification structure as the benchmark and model variables. The resulting calibration estim ","In this paper, we propose a new technique to calibrate the design weights in survey sampling by the method of maximum likelihood. We show that the design weights used in the Narain (1951) and the Horvitz and Thompson (1952) estimators are in fact maximum likelihood design weights. Later, we discuss two different situations: ( a ) when the variance of the calibrated weights is assumed to be known; and ( b ) when the variance of the calibrated weights is assumed to be unknown. Under situation ( a ), we obtain the linear regression estimator as a special case of it, and under situation ( b ) we obtain a new estimator, slightly different than the linear regression estimator. The calibrated estimators available since Deville and S\u00e4rndal (1992) belong to the former case ( a ) whereas case ( b ) is a new development in this area. A simulation study has been carried out to investigate the performance of the resultant estimators.  At the end, an application based on a real dataset from the biosciences is given. ","SUDAAN 11 is due to be released in 2012. SUDAAN\u00ae is a statistical software package for the analysis of complex survey and other cluster correlated data. This paper will highlight several new procedures, statistics, and features in SUDAAN 11. The new procedures include WTADJX, which computes survey weight adjustments (otherwise known as calibration weighting) and extends the capabilities currently available in WTADJUST; VARGEN, which allows users to compute statistics between variables; and IMPUTE, which is an extension of SUDAAN 10's HOTDECK procedure and includes three additional imputation methods beyond weighted sequential hot deck. In addition, new statistics include the Breslow-Day test for homogeneity of odds ratios, the kappa measure of agreement for cross-classified tables, confidence intervals for marginals in regression procedures, and the representativity indicator (R-indicato ","The Statistics of Income Division of the IRS started a panel sample of individual returns in 2007 for longitudinal analyses. This panel sample has also been used for cross-sectional estimations of multiple variables that are skewed and weakly correlated. Therefore, cross-sectional weights are needed to refer to the out-year population and multiple variables of interest. Calibration method is therefore applied to adjust weights such that sample estimates are close to population benchmarks. In this paper, we look at issues such as calibration cells, initial weights, prediction models and weight bounding. We share some experience in calibrating weights using R. We aim to produce final weights that are reasonable for multiple variables of interest.  ","The National Science Foundation's Survey of Doctorate Recipients is conducted every two or three years and collects detailed information on individuals receiving PhDs in science and engineering in the U.S. and some others with PhDs from abroad in these areas. Survey weights adjust for oversampling and nonresponse on a cross sectional basis. A significant portion of the sample (e.g., 60% on 3 or more surveys from 1993-2006) appears in multiple survey years and can be linked across time. No longitudinal weight exists that would enable estimation of statistical models or comparison of finite population characteristics using data from multiple survey waves together. This paper develops theory and methods for calibration estimation to construct such a longitudinal weight. Methods for producing positive weights and for variance estimation are described. Issues including the use of multivariate calibration targets are discussed. ","Probability samples are selected under the assumption that the sample frame closely resembles the target population. Occasionally, however, the sample frame has problems that are only discovered after the sample is selected. In this paper, we discuss a physician survey that was part of an evaluation of a Medicare pay-for-performance demonstration. Several problems with the sample frame were discovered after sample selection. In particular, the frame included duplicate physicians within and across practices. Moreover, for practices that participated in the demonstration, the frame identified all physicians in those practices as also participating in the demonstration, when this was not the case. This paper describes the process of calculating sampling weights that properly account for the original selected sample, the replacement sample, and the multiplicity adjustment for duplicates. We compare estimates obtained using the appropriately calculated weights (adjusted for nonresponse) with those obtained using a simple-minded weighting procedure. ","Dual-frame RDD sampling methodology, whereby both landline and cellular telephone numbers are mixed, was devised to improve coverage of the traditional landline RDD samples. Current practice of this methodology, however, is subject to inconsistencies and technical inefficiencies. Due to unavailability of current counts of cell-only households, researchers rely on outdated estimates to determine the sample mixture. This becomes a more pronounced problem for county-level RDD studies because even outdated counts of cell-only household are not available for counties. These create inconsistencies and inefficiencies for both sample selection and weighting applications. The authors will introduce an innovative technique for obtaining counts of cell-only households for all Census geographic domains down to the county level. This will enable researcher determine the optimal composition of the cellular and landline sample components for dual-frame RDD samples, as well as provide the needed population counts for proper construction of survey weights. Moreover, the employed methodology will provide fresh counts for each county on quarterly basis. ","The continued rise in cell phone penetration creates a real potential for undercoverage bias in many RDD sample surveys. To respond to such threats researchers have begun implementing dual frame RDD sampling strategies. In this paper we present a method for constructing first-stage sampling weights derived under an overlapping, dual frame design (e.g. cell and landline RDD numbers) based on probability 101 fundamentals. Because these two frames potentially overlap at the user level, selection probabilities must be adjusted for multiplicity of selection. Our method resembles weighting strategies consistent with a \"single frame\" approach and does not require estimation of a compositing factor traditionally used in the \"dual frame\" approach. ","Two approaches currently used to sample the U.S. residential population for random digit dialing (RDD) survey are the \"cell only\" and \"cell overlap\" designs. Each involves supplementing an RDD landline sample with an RDD cell sample. In either design, all households reachable by landline phone are retained in the landline sample. However, in a \"cell only\" design interviewers screen the cell sample for households with no landline service ( households with landline service are screened out), while in a \"cell overlap design\", all households accessible by cell phone are retained for the cell sample. This paper reports on weight construction for a large national telephone survey (the Health Tracking Household Survey) that used a cell overlap design. In a cell overlap design, households with both landline and cell service have a chance of being selected from either frame. To address this multiplicity issue we employed a composite weight for the dual-service group. The paper explains how the compositing factor was derived and investigates the impact of using different compositing factors on sampling error and potential bias. ","The National Center for Health Statistics reported that the percentage of wireless-only households had grown to 29.7% in 2011. This has substantial implications for any telephone survey designer who seeks a representative sample of the population and to create unbiased estimates. Cell phone samples are more costly to field and thus typically undersampled compared to landline phone samples, leading to larger weights in the cell phone sample and a resultant increase in variance and design effect. One option is to decrease variances through attenuation of cell sample weights, although bias impact must be considered.    ","Propensity score weighting adjustment is commonly used to handle nonresponse. When the response mechanism is non-ignorable in the sense that the response probability depends directly on the study variable, a follow-up sample is commonly used to obtain an unbiased estimator using the framework of two-phase sampling. In this two-phase sampling approach, the follow-up sample is assumed to respond completely. In practice, the follow-up sample is also subject to missingness.      We consider the problem of propensity score weighting adjustment when there are several follow-ups and the final follow-up sample is also subject to missingness. We propose a novel approach, so-called reverse two-phase sampling, that makes use of the monotone structure of missing pattern and enables efficient estimation. The proposed method is more flexible than the existing methods and can be directly applicable to co ","Item nonresponses are commonly encountered in complex surveys. However, it is also common that certain baseline auxiliary variables can be observed for all units in the sample. We propose a semiparametric fractional imputation method for handling item nonresponses. Our proposed strategy combines the strengths of conventional single imputation and multiple imputation methods, and is easy to implement with the number of variables involved is large, which is typically the case for large scale complex surveys. A general theoretical framework will be presented and results from a comprehensive simulation study will be reported.   ","The new WTADJX procedure in SUDAAN 11 does instrumental-variable calibration weighting using a flexible nonlinear weight-adjustment function.  We will review the theory behind this procedure and explore some of its uses.   ","Consider three different but related problems with auxiliary information: infinite population sampling or Monte Carlo with control variates, missing data under ignorability, and Poisson and rejective sampling with auxiliary variables. We demonstrate, in a unified manner, calibrated regression and likelihood estimators for these problems. This approach will effectively resolve long-standing issues in two existing approaches: how to achieve efficiency, while allowing for misspecification of a super-population model, in generalized regression and calibration estimation and how to find a simple approximation in optimal regression estimation. ","Multiple considerations enter into decisions about redistricting,  and conflicting criteria can lead to confusion about which criteria  to prioritize. Belin, Fischer, and Zigler (2011 Statistics, Politics,  and Policy) introduce the idea of a density-variation/compactness  (DVC) measure to serve as a one-number summary of a proposed  redistricting plan, favoring plans that can be expected to include  more districts where close elections would be expected. The talk  will review evidence from a variety of redistricting plans suggesting  that DVC measures could provide a check on attempts to introduce  partisan bias into the redistricting process. We will also discuss  the interplay between statistical criteria and legal imperatives  in the context of recent redistricting experience in California,  specifically considering whether language in California's redistricting laws calling for voters to have a \"real choice\" and for voters to be \"back in charge\" indicates a preference for plans with more competitive districts or whether it would be more proper to proceed without attaching any weight to considerations of electoral competitiveness. ","While the accuracy of pre-election polls in the general election has been steadily improving, there have been recent problems with pre-election polls in primaries and caucuses in the United States and in sub-national elections in a number of other countries. This paper will describe recent experiences with pre-election in a comparative context and discuss the range of sources for these problems, including sampling, interviewing, and likely voter model issues. ","Recent years have seen a wave of laws making it more difficult to vote in the United States. Their ostensible purpose is to prevent voting by persons not legally qualified and thus to improve voter confidence in electoral integrity. Are such laws in fact needed to address serious fraud problems? On the other hand, how many and what kinds of voters will be or have been disenfranchised by them? Is voter confidence positively (or negatively) affected by voter ID laws? We will survey what is known about such issues and offer suggestions for how statisticians can contribute. ","Election night decisions have been based, historically, upon several sources of information that become available on the day of the election. While this still is the case, recent changes both in society and in the election system have led to increases in the complexity of estimation. These changes and their effects on estimation will be described. However, changes will continue for the foreseeable future, and the likely ensuing effects also will be discussed. ","Although historically there has been low net undercount in the census, using a single measure to describe coverage may mask additional errors. The 2010 Census Coverage Measurement (CCM) program produced estimates of the components of census coverage in addition to estimates of net error. The components of census coverage for persons in the 2010 Census are correct enumerations, erroneous enumerations, whole-person imputations, and omissions. The data collected in 2010 CCM support the breakdown of correct and erroneous enumerations into various subgroups. The estimates include correct enumerations at the national level, for all states, and large counties and places. A key subgroup of erroneous enumerations is duplicates. This paper explores the results of 2010 CCM estimation, with particular emphasis on duplicates. We report estimates by the distance separating the linked records, includin ","This paper describes challenges presented by reducing the sample size of the National Hospital Discharge Survey (NHDS) by half for the years from 2008-2010. NHDS, a nationally representative sample survey, was conducted annually from 1965 to 2010 by the National Center for Health Statistics, Centers for Disease Control and Prevention, and is the only source of trend data on hospitalizations during this period. Until 2007, data were gathered from about 400 hospitals and 360,000 discharges. In 2008-2010, data were gathered from about 200 hospitals and 166,000 discharges. Data from the half sample years are particularly important since they encompass the period when health care reform legislation passed, which is the baseline period to which any subsequent changes will be compared. This paper describes our findings on the effects of the half sample on data reliability for certain categories, and the implications of these findings for researchers interested in studying certain illnesses and patient groups. ","Meta-Analysis is a method for combining summary measures from similar studies. This technique can be applied to randomized controlled experiments as well as to sample surveys; however, there are some issues when applying meta-analysis to randomized controlled experiments. Some of these issues include: applying large sample theory to small sample experiments; inappropriate claims of generalizability of results to large populations; and the existence of publication biases due to the submission of selected (significant) experimental results to scientific publications. In general, the above issues may not apply to well-designed sample surveys. For most surveys, published point estimates and confidence intervals are based on large sample theory and are generally reliable because of the inherent large sample sizes, high coverage of the target population, and high response rates, which ensure the representativeness of the sample. Hence, publication bias may not be an issue using meta-analysis with survey data. In this paper, we will demonstrate an application of meta-analysis to public health surveys. ","Measuring changes in teachers' attitude toward job condition over time is of great interest to educational researchers and policy makers. Results from different survey administrations must be based on a common scale in order to make valid comparisons. Scales can be linked to scales from previous surveys via linking procedure, such as concurrent calibration. In this study, teachers' responses to their job conditions collected via School and Staff Survey in 2003-04 and 2007-08 are linked to a common IRT scale by concurrent calibration procedure. Responses from the two survey administrations are scaled together in one single calibration run by specifying that the two samples are coming from two different populations. After scaling, teachers' attitude toward job condition scale can also be transformed to reporting metrics by setting the mean and standard deviation of the reporting metrics. ","Minority populations bear a heavier burden of cancer mortality than the non-minority group in the United States. Such disparity has been attempted to be understood with several factors, including delay in diagnosis and treatment. There are three dates of importance when examining diagnostic/treatment delays: date of the clinical/screening abnormality associated with the cancer diagnosis, date of definitive diagnosis, and date of first treatment. However, there is no agreed upon methodology for identifying these dates either from the medical records/medical claims data. This causes difficulties, errors, and inconsistencies between studies. A generalizable strategy to identify reliable dates is imperative before assessing any delays. This study has two aims:(1) Develop an algorithm to estimate reliable dates of diagnosis and treatment from Medicare claims data; 2) Measure disparity in the timeliness and its impact in clinic outcome. Our study population will come from the Surveillance, Epidemiology, and End Results (SEER) cancer registries. The study will utilize data from the SEER-Medicare linked datasets from 1992 to the most recent year for cancer survival status, 2009.  ","The Center for Disease Control and Prevention recently issued a report, widely cited in the popular press, on the increased incidence of multiple births in the United States over the last 30 years. Twin birth rates were extracted from annual birth data by a variety of mother's characteristics in order to examine this trend.     Our research extends this analysis by applying multivariate analysis to individual-level data obtained from public-use data sets on all births in the United States from 1985 to 2009. We combine the data into a single, multi-year data file (an .xdf file easily accessed by R) containing over 100-million birth records. To analyze the relationship between parental characteristics and multiple birth pregnancies, we first change the unit of observation from the baby to the pregnancy in order to remove replicated observations of parents of multiples. Then, estimating a logistic regression on all of the remaining observations, we show that the trends in increased multiple births are more strongly associated with the age of father than the age of mother, and that controlling for ages, the relative incidence of multiple births for black mothers has been declining. ","The parametric test for testing the equality of two frequency distributions has been developed. The tests of the equality of two contingency tables or two frequency matrices and two transition frequency matrices have also been obtained. Examples are cited for all cases.  ","The Current Population Survey (CPS) is one of the oldest, largest, and most well recognized surveys in the United States.  It produces monthly household information about employment, unemployment, and other characteristics of the civilian non-institutionalized population.  In this paper, we will evaluate the CPS sample design and estimation procedure, with specific attention to research problems in the areas of rotating panel design, sample size, systematic-sampling interval, AK composite estimate, and replication variance estimates.  We will provide some comments and suggestions for future research, and suggest some ways to improve current CPS methods.  ","One of key elements in the repeated sample design is the rotation pattern. High sampling overlap between periods reduces the sampling variance of estimates of period change and the degree of sample overlap between any two periods is determined by the rotation pattern design. Composite estimation is used in the rotation pattern design to reduce the variance of estimators. The aim of the paper is to determine an optimal rotation pattern by comparing the effect of different rotation patterns in terms of the variances, biases and mean square errors used as the performance criteria of monthly, month-to-month and year-to-year change of composite estimators. The analysis based on the CPS data on different rotation patterns indicates that the variances, biases and mean square errors of estimates for all levels under the 6-6-6 rotation pattern are less than those estimates under all other rotation patterns. These differences compared to those under the 4-8-4 are so small that they do not strongly support that the rotation pattern 6-6-6 is better than the current 4-8-4 rotation pattern. This leads that 4-8-4 rotation pattern is preferable and can be retained.  ","The U.S. Current Population Survey (CPS) is a rotating panel survey of U.S. households that measures the labor force statuses of the non-institutional civilian population. Each panel is a random sample of households that rotates into the survey for 4 consecutive months, rotates out of the sample for the next 8 months, and then rotates back into the survey for a final 4 months. CPS microdata undergo several weight adjustments so that estimated totals for various demographic characteristics match projected population totals. The final adjustment for demographic information is called the Second Stage adjustment, and it adjusts for age, race, gender, and ethnicity. The final stage of weighting--Composite estimation--employs the correlation in overlapping panels between adjacent months to reduce the variation of key labor force estimates. In the Second Stage adjustment each panel can produce an estimate of employment and unemployment, and each panel exhibits certain tendencies. This paper looks at how those tendencies have changed over time, their effects on Composite estimation, and a possible method to adjust for those effects. ","The Current Population Survey employs a two-stage rotating panel design, with each month's sample being made up of eight replicate second-stage samples. Panel correlations for state and national unemployment estimates are modeled, allowing us to make predictions for sample allocation which account for proposed changes in the sample design and changes in the underlying population. We use the panel correlations to estimate variance components. Calibration and composite estimators are considered. ","Assisted reproductive technology (ART) is increasingly common: according to CDC's National ART Surveillance System (NASS), 142,435 ART cycles were initiated in 2007. CDC's NASS collects limited data on pregnancy outcomes. State-based vital record systems contain more information on maternal characteristics and pregnancy outcomes, but limited information on conception methods.     ","Changes in women's menstrual bleeding patterns precede the onset of menopause. In this paper, the authors identify population subgroups based on menstrual characteristics of the menopausal transition experience. Using the TREMIN data set (1943-1979), the authors apply a Bayesian change-point model with 8 parameters for each woman that summarize change in menstrual patterns during the menopausal transition. The authors use estimates from this model to classify menstrual patterns into subgroups using a K-medoids algorithm. They identify 6 subgroups of women whose transition experience can be distinguished by age at onset, variability of the menstrual cycle, and duration of the early transition. The results suggest that for most women, mean and variance change points are well aligned with proposed bleeding markers of the menopausal transition, but for some women they are not clearly associated. Increasing understanding of population differences in the transition experience may lead to new insights into ovarian aging. Because of age inclusion criteria, most longitudinal studies of the menopausal transition probably include only a subset of the 6 subgroups, suggesting potential bias. ","Background   Childhood obesity prevalence has tripled in the last three decades in US. This study examines associations between childhood obesity and local community environments to inform policy and environmental changes to prevent childhood obesity.   Method  A valid body mass index(BMI) was obtained from 2007 NSCH 44,096 children aged 10-17 in all states and DC (2,828 counties and 17,541 ZIP Codes). Obesity was defined by BMI = the 95th percentile. Data were merged at the ZIP code-level with the ESRI Tapestry dataset. A multilevel model included individual-level sex, age, race/ethnicity, household poverty, ZIP code-level life mode (neighborhood lifestyle) and urbanization and county-level urbanity and income, and state-level random effects.   Results   Overall prevalence of childhood obesity was 16.4%. At the ZIP Code-level, it ranged from 25.5% to 8.7% along income octiles; from 27.1% to 8.8% for 12 life mode groups; from 23.6% to 10.8% among 11 urbanization levels.   Conclusions   ZIP code-level geographic disparities in childhood obesity prevalence are very substantive. Thus local community environment changes should be taken into account for childhood obesity prevention. ","Early childhood growth patterns are potential determinants of body composition and body weight at later ages. Weight-for-length (for age &lt; 2 years) and body mass index (BMI) (for age = 2 years) are commonly used measures of body compositions and weight. Standardized scores of these two measures are weight-for-length z- score and BMI z-score. For simplicity, we termed both as bmiz score. We planned to group 3365 children based on the patterns of temporal changes in the trajectories of bmiz score of their first five years of life. We collected (or interpolated when data for a time point was not available) bmiz score at every six-month interval of age (a total of 11 time points) for each child. We performed several suitable cluster analyses of (i) raw bmiz score of different time points (treating bmiz at each time point as a separate variable), (ii) PC scores out of principal component analysis of bmiz score, (iii) random coefficients (that specify the individual growth model) of mixed effects model, and (iv) coefficients of auto regressive model for each patient. In this research, we are attempting to evaluate the performance of these cluster analyses. ","In this research, we take into account the presence of excess zeros in fertility pattern estimation and propose a zero-inflated Poisson (ZIP) model with skewed-normal (SN) function added as its rate trajectory. The hierarchical structure stabilizes the estimation of zero-inflated probabilities at different childbearing ages.     ","One of the current challenges for the survey and analysis of smoking behaviors among young adults, the highest risk group for smoking, is the definition and measurement of 'social smoking.' That is, a non-daily pattern where smoking occurs primarily in social settings-a behavior more common among young adults. Such intermittent behavior, coupled with an uncertain definition of 'social smoking,' may impact the analysis of data on this behavior as well as efforts to increase smoking quit rates in this population. Using data from an internet-based survey of 3740 adults between 18 and 25 we examined the effects of differing definitions of social smoking. Results indicate strong but not complete agreement among answers to questions regarding this behavior. Of those who said they smoke 'only when others are present,' for example, 6 percent also said they smoke 'mainly alone.' Logistic regression models find a significant relationship of both self-described smoking behavior and identification as a social smoker with intention to quit smoking. This work may help with targeting cessation efforts and explain differences among survey findings in recent tobacco use trends. ","Mediation analysis is commonly conducted in the behavioral and social sciences with a goal of assessing whether the effect of an independent variable X on an outcome variable Y is through an intermediate variable M. Two regression models are fit, one being M on X (path a) and the other being Y on both X and M (path b). When X, Y and M all involve repeated measurements within clusters, paths a and b are typically multilevel (mixed-effects) models in which the effects of X on M and M on Y controlling for X are assumed cluster-specific and normally distributed. The normality assumption may result in poor performance when the random effects are not normally distributed (Bauer et al, 2006). We propose a Bayesian nonparametric approach to testing the average indirect effect using a Dirichlet process prior for the distribution of the random effects in both paths a and b. This approach makes use of recent results on the calculation of the moments of the random moments of the nonparametric random effect distribution with a Dirichlet process prior (Li et al., 2011). We evaluate the performance of the proposed approach using simulations, and illustrate the method with a smoking dataset. ","Short-term surveys provide precious information for economic fluctuations analysis. In short-term surveys like the Business Tendency Survey, most of the questions are qualitative and concern the evolution of different economic factors of the business activity. They provide economic information on the present situation and short-term perspectives. Usually, the respondents have to choose between three possible evolutions: increase (improvement, favourable, level higher than the normal), stability (normal) or decrease (unfavourable, level lower than the normal). The balance of opinion is defined as the difference between the proportion of respondents expressing a positive opinion and the proportion expressing a negative opinion. To analyze these types of surveys, the methods are well standardized and use both the multidimensional approach and time series (scoring, dynamic factor analysis, etc.)    ","Consumer confidence is a psychological concept, described as national economic conditions, is difficult to measure; it is an expression of the opinions and attitudes of consumers about the current and future strength of the economy. How to collect this information is important because it is an economic indicator for forecasting household consumption expenditure, consumer behavior in general and the country's economic situation. Moreover, consumer confidence indices which are formed by using the consumer survey data can provide a fairly accurate prediction of consumers' economic behavior (Carroll, Fuhrer, Wilcox (1994), Kwan, Cotsomitis (2004)). In this article, household micro-level data from the consumer survey, carried out with cooperation of the Central Bank of Turkey (CBRT) and Turkish Statistical Institute (TurkStat), is used. The performance of the survey is discussed from a design based perspective by using some statistical analysis and survey theory. The results of those steps taken to enhance efficiency are evaluated. Following this and some other research, there has been some updates on consumer survey design and began a pilot period in 2012. ","This paper analyzes condition estimates from the Medical Expenditure Panel Survey, a nationally representative survey studying health care use, access, expenditures, source of payment, insurance coverage, and quality of care. Each year a new panel begins and each panel has 5 rounds of data collection over 2\u00bd years that covers a two-year period. Alternative condition estimates based on MEPS data exist including responses to a set of priority condition-specific questions in the household CAPI instrument; and conditions associated with health events such as doctor visits (treated prevalence). This paper focuses on whether changes to the placement and streamlining the priority condition-specific questions of the MEPS CAPI instrument produce different condition estimates. Starting with Panel 12 (2007 Panel) in an attempt to improve the reporting of conditions associated with events, the priority condition-specific questions were moved to the first round of data collection and earlier in the CAPI instrument. In an attempt to reduce respondent burden these questions are streamlined in rounds 3 and 5: only asked of those who have not reported the condition previously.  ","Purpose: To evaluate a condition-specific approach to scoring a depression survey.  Methods: 1,340 organ transplant recipients completed the Center for Epidemiologic Studies Depression (CES-D) Scale, SF-36 Health Survey (SF-36), EuroQoL (EQ-5D) and Beck Anxiety Inventory (BAI). Data were analyzed using latent variable methods and multivariable linear regression.  Results: Three CES-D subscales were identified: somatic symptoms (SOM), isolation (ISO) and positive affect (PA). Multivariable models of CES-D subscale scores included the SF-36 physical and mental scales, EQ-5D valuations, and BAI subscales. Higher (worse) SOM scores were associated with lower (worse) physical and mental SF-36, lower (worse) EQ-5D, and higher (worse) BAI neurophysiologic, subjective and autonomic scores. Higher (worse) ISO scores were associated with higher physical and lower mental SF-36, lower EQ-5D, better BAI neurophysiologic and worse BAI subjective scores. Higher (better) PA scores were associated with better SF-36 physical and mental scores, and worse BAI neurophysiologic and autonomic scores.   Conclusion: A multidimensional scoring system for CES-D item data is applicable for transplant recipients. ","The Federal government relies primarily on three major national surveys in tracking adolescents' use of alcohol, tobacco, and illicit drugs. The National Survey on Drug Use and Health (NSDUH), Monitoring the Future Study (MTF), and the national Youth Risk Behavior Survey (YRBS) have shown similar trends in substance use over the past decade, but the surveys show significant differences in the levels of use of some substances. The school-based surveys (MTF and YRBS) generally report higher rates of use than the household survey (NSDUH). Prior methodological research has explored how various design features such as mode, setting, privacy, question wording, and coverage may affect substance use estimates from the surveys. This paper explores these factors based on new analyses of combined 2002-2008 data from the surveys. The findings will aid policymakers, program officials, and researchers in understanding and interpreting data from these surveys, and will also be useful to survey statisticians planning designs for other studies of adolescent behavior. ","In this talk we present a method for calculating the survey-weighted Kappa measure of agreement statistic and producing an appropriate variance and test of significance from complex data that may include clustering, weighting, stratification, and other features of sample surveys. The variance can be calculated using a variety of methods, including implicit Taylor series linearization and replication methods. Recent papers by Zajacova and Dowd (2011) and Winn, Clifford, Johnson and Kingman (1999) illustrate the growing popularity and need for survey-weighted Kappa in the public health setting. We use SUDAAN 11 to illustrate the survey Kappa in two examples discussed by these authors: 1) National Health and Nutrition Examination Survey (NHANES) III survey, estimating the Kappa measure of intra-dental examiner agreement between ratings of baseline vs. follow-up periodontal assessments of teeth; and 2) 2005-2008 continuous NHANES, estimating agreement between two measures of self-reported health status. ","Linking survey and administrative data is an attractive option for substantive researchers interested in studying complex phenomenon and for survey methodologists who utilize such data to study data quality issues and reduce data collection costs. However, there several barriers to linkage that can potentially reduce the quality of the linked data,    ","At its meeting in Dublin, Ireland on 25 august 2011, the General Assembly of the International Statistical Institute Official Statistics presented a review and expressed ISI concerns regarding the manipulation since 2007 of the Argentine CPI and the explicit harassment to the statistical Profession and individual statisticians in Argentina. Incidents of continued harassment to private statistical local research agencies and individual statisticians (several ex-INDEC technical staff) have continued to occur; so too the CPI manipulation, where from the defence of Human Rights to transparency one may speculate that if the GDP growth rate and the fall in poverty if INDEC is using the CPI as deflator the real GDP is overvalued, and people living under the poverty line is underestimated. The paper reports the work undertaken by a Commission of Universities to assess the technical work at INDEC ","Statistics and statisticians are vital to society, as indicators of progress, identifiers of responsibility and interrogators of fact. As such, statisticians have been at the forefront of efforts to apply scientific methodologies to human rights questions, including the development of information management systems to identify large-scale human rights violations. At the same as the role of statisticians in human rights becomes increasingly evident it is imperative to identify and address the role of human rights in statistical processes and outcomes. From the way data is collected, to its analysis, structure, and presentation, the process of statistics has human rights implications that require clear identification and analysis. This paper will contribute to this process by bridging the technical elements that comprise the process of statistics and the outcome of statistical analysis with applicable international human rights law and standards. ","The paper describes the range of responses by the ASA and others to attacks launched by the Argentine Government on statistics, statisticians, and allied professionals who are compiling and disseminating consumer price statistics using methods not approved of by that Government.    ","Partially Bayes inference refers to a class of methods where a prior is used but only for the nuisance parameter. Such methods are appealing to those who are unwilling to seriously commit themselves to the whole Bayesian philosophy, yet they see some of its potential benefits and hence are willing to experiment, especially for the part of the inference that they want to get rid of. Using a simple normal example, this talk demonstrates that being partially Bayesian does come with its hoped for benefits in the form of partial shrinkage, namely, there is a \"borrowing information\" as with full Bayes methods except the amount of shrinkage is less. The loss of the full benefit persists even if one carries out partial shrinkage sequentially for all parameters including primary parameters, namely, by using partial prior for each parameter in any particular order. However, when partial shrinkage is carried out simultaneously for all parameters involved, the full shrinkage benefit is restored. These findings not only reveal the evolutionary link between Bayes and Partially Bayes, but also remind us that like most relationships in life, benefits come with attachments, obvious or hidden. ","Should Bayesians worry about model selection? Maybe no, because after conditioning on the data, the Bayesian paradigm will make all the proper adjustments. Maybe yes, because selection which only reports effects which are \"large enough\" tends to yield upward biases. In this talk I will argue that the Bayesian paradigm will indeed make the proper adjustments, but only when a selection adjusted likelihood is used to acoount for selection uncertainty. In specific cases, this is seen to lead to additional selection shrinkage regardless of the prior. A general adjusted approach to Bayesian model selection is proposed. ","Improved Stein-type estimators are proposed for the k- dimensional mean vector in the case of spherically symmetric distributions with residual vectors under loss functions that are concave functions of squared error. It is shown that certain Stein-type shrinkage estimators have the strong robustness property that they beat the standard unbiased estimator simultaneously for all spherically symmetric distributions (with a finite second moment), uniformly for a wide class of concave losses. As an example, the James-Stein Estimator (with estimated scale parameter) dominates the usual estimator simultaneously for all Lp loss functions (truncated or untruncated) and all spherically symmetric distributions when the dimension k, is greater than or equal to 5 and the shrinkage constant is less than or equal to (k-4)/k times the maximum possible constant for usual squared error loss. ","In this talk we posit the shrinkage problem in a decision theoretic framework and introduce loss functions that govern different forms of shrinkage. We contrast the difference between models that shrink and loss functions that shrink. We study the optimal shrinkage weights for combining estimators under different loss functions. We illustrate the methods using the example of estimating gene-environment interaction under the assumption of gene-environment independence and derive the shrinkage estimators that have been proposed in the literature as optimal decision rules under various loss functions. ","Across a wide range of academic disciplines, government agencies, and business  sectors, panel surveys are an important source of attitudinal and behavioral data.  Panel data provides more complex relationship and more advanced  understanding of of mass behavior along time. Examples include the 2007-2008 AP/Yahoo  News Election Panel and 2006-2008 General Social Survey with multiple  follow-up waves. However, panel studies suffer from attrition, which  reduces the effective sample size and can lead to biased estimate if the  tendency to drop out is systematically related to the substantive  outcome of interest. Refreshment sample as external data source includes  new, randomly sampled respondents who are provided with the same  questionnaire at the same time as a second or subsequent wave of the  panel. We utilize refreshment samples to assess the effects of panel attrition and  to correct for biases via Bayesian modeling and multiple imputation. We focus on   large scale panel study with categorical responses. ","We discuss a degree-corrected version of a stochastic block model that aims to achieve a better resolution for community identification. We follow a fully Bayesian approach and conduct inference based on a principled centroid estimator of community labels. To this end, an efficient Gibbs sampler is developed. We demonstrate the proposed model and inference on a number of classical network datasets. Finally, we offer a few concluding remarks on the  model implementation and directions for future work. ","Multiple Imputation (MI) has established itself as a general-purpose solution for statistical analysis of incomplete data. With increasing acceptance of the method the demand for flexible and robust models has grown as well, and non-parametric elements have been implemented into MI algorithms. A Monte Carlo study is used to investigate the effects of e.g. replacing draws from the posterior distribution by (Approximate) Bayesian Bootstrapping, or using Predictive Mean Matching as a substitute for drawing from the conditional predictive distribution of the missing data. These apporaches fall into the category of 'implicit modeling', and results from the simulation study are compared to results based on 'explicit modeling' (i.e. fully-parametric Bayesian MI models).  ","We present Bayesian transition models for multivariate event histories in discrete time. Event histories consist of records of time points at which subjects make transitions between states in dichotomous state spaces. For example, subjects may pass into and out of the state of being employed. Our models allow simultaneous analysis of multiple state types (e.g., employment status and drug usage status) with simultaneous inclusion of nonparametric baseline hazards for multiple time variables (such as time in current state and age). In addition, we model informative censoring of the state processes. Methods are applied to retrospectively elicited lifetime histories of drug usage and related characteristics of 500 illegal drug users in Los Angeles. ","The distribution of alcohol outlets has been long linked to violent crime, particularly in urban areas. Because alcohol related crime and violence pose a serious threat to the general public's health, more work needs to be carried out to understand the spatial distribution of both alcohol outlets and violence. The purpose of this paper is to present the use of a Bayesian disease mapping framework to examine the relationship between alcohol outlets and violent crime in the city of Philadelphia. Using block groups, we are able to analyze the patterns of violence while adjusting for the spatial density of alcohol outlets as well as important neighborhood characteristics, including land use. Policy implications are also discussed. ","Customer research focuses on modeling attitudinal outcomes that ultimately managers can act upon in order to influence the bottom line. Not surprisingly, a key element in this process is to be able to formulate what factors contribute to form customers' overall evaluations. However, oftentimes survey instruments are packed with a large number of attributes that even though could arguably be used to describe the customer experience, also pose a challenge due to the interrelatedness between them. Model selection can be seen as an alternative to dealing with a large number of attributes, with particular interest in the presence of multi-collinearity. In this presentation, common linear and nonlinear modeling techniques are applied to customer survey data while utilizing different model selection strategies including information-based and Bayesian regularization approaches. The results are compared in terms of model performance and the potential impact on managerial recommendation. ","Interaction within small groups can often be represented as continuous-time relational data, where each observed event involves a sender and a recipient. Recent methods model the mechanisms guiding a sequence of such events by parametrizing the rate at which individuals interact in terms of the previous history of events and actor covariates (Butts 2008). We present a class of hierarchical Bayesian models for the situation where one observes many such sequences. In addition to facilitating inferences about event-level dynamics and their variation across sequences, this approach helps share information between sequences. An application to high school classroom dynamics is used to illustrate the benefits of this approach. Covariates about the students, teacher, and classroom settings are included in the model, as well as indicators for typical participation shifts common to human conversation (Gibson 2003). We present estimates from the model and describe their implications. Finally, we describe adequacy checking and model selection methods for this class of models. ","In stratified simple random sampling the values of sample sizes in the respective strata should be chosen to reduce sampling variances. For example, if the cost per unit is the same in all strata, Neyman allocation can be used for the purpose. His allocation for minimizing the variance will be the best for one variable, but it will not in general be best for other variables in a survey with many variables (items). Some compromise needs to be reached in the allocation. Huddleston et al. (1970) proposed an algorithm using nonlinear programming (NLP) for compromise allocation. Their classic approach can lead to infeasible NLP problems. Thus, we present a modification of their approach. However, the approach may not be satisfactory because the solution can be less precise than Neyman allocations or proportional allocations for some individual variables. We suggest an alternative approach using NLP based on a different principle. This approach for minimizing the sampling variances of the variables under study is simple to use and always provides solution. We illustrate the new approach by using real survey data. ","Stratification is used in sampling to create homogeneous groups. A number of methods have been proposed for stratification of populations using covariates of the variable of interest. These include Dalenius and Hodges' (1959) cumulative root frequency method, the Lavallee and Hidiroglou (1988) algorithm, and the Gunning and Horgan (2004) geometric stratification method. All of these methods assume you have one variable of interest and one correlated auxiliary variable known for the population. Many surveys have more than one important variables of interest as well as many auxiliary variables. The method we propose uses a superpopulation model to create a distance metric between elements in the population that depends on multiple auxiliary variables and considers multiple variables of interest. Using the proposed metric, a hierarchical clustering algorithm can be used to implement the opt ","Tommy Wright  U. S. Bureau of the Census    ","Social surveys often must estimate the sizes or the proportions of many small groups and small differences between group sizes. The discussion of the needed precision of the estimators and the corresponding sample size is difficult since many different objectives must be considered and often lay persons are involved. The size resolution and the difference resolution of a sample are two measures which are derived from approximations to the probability of not observing a group or a difference in the sample. The size resolution is an operationalization of the smallest group which can be estimated from a sample. The difference resolution describes a minimal difference between two group sizes which can be estimated from a sample. Since these resolution measures embody elements of statistical hypothesis tests without the need of a complete test specification they orient the users to a reasonable sample size while remaining simple enough to assist the discussion with various stakeholders. The European Social Survey serves as an example of the application of the resolution measures.    ","With complex survey data, one might try to implement conditional logistic regression by applying it to a pseudosample constructed by replicating the observations according to integer-scaled versions of the sampling weights. However, this method has been found to produce inconsistent results. To understand why, we investigate the performance of conditional logistic regression applied to replicated data from a matched pairs design with general covariates. We show both theoretically and empirically that as the number of replications approaches infinity, the resulting estimator converges identically to the one from an unconditional logistic regression with a fixed effect for each pair as applied to the original matched pairs sample. This latter estimator is well-known to be inconsistent, which at least partly   explains why conditional logistic regression applied to a pseudosample tends to fail. ","We consider interval estimation for small area proportions from stratified random sampling surveys. We focus on the case where the stratum sample sizes and the true proportions are small for all strata, and for simplicity we assume equal stratum sample sizes. The objective is to construct a confidence interval for each of the true stratum proportions, Pi. A commonly used small area empirical Bayes model for a single stratum's true proportion Pi assumes that both the distributions of the sampled stratum proportions and the prior distribution of the true stratum proportions are normal. The well-documented problems of the normal approximation to the binomial, particularly when the sample size is small and the probability of success is close to 0 or 1, raise questions about the adequacy of such a model when the Pi and the stratum sample sizes are small. We argue that a more reasonable model in this setting is to assume that the sampled stratum counts have binomial distributions and that the prior distribution of the true stratum proportions follows a beta distribution. We propose a new empirical Bayes confidence interval based on this model, and examine related simulation results.  ","In comparing groups using data from a cluster sample, the analyst needs to take into account the design effect in order to calculate statistical significance. There are several ways of handling statistically a situation where the same clusters are drawn but different units are selected for two surveys. The simplest seems to be to use a jackknife, creating one set of replicate weights. Then one can produce an estimate of the difference between means or proportions, and use the jackknife to obtain the variance of that estimate. Unlike separate jackknife estimates, this approach can use information about the common sample clusters.  This research used simulated data to compare several methods of testing for significant differences in proportions. The methods included jackknifes or Taylor Series that take into account the common clusters in the two samples. ","Although multiple imputation is widely accepted as the preferable approach to deal with item nonresponse in surveys, research on imputation involving multilevel databases is still limited. Nevertheless, it has been shown that a na\u00efve imputation ignoring cluster effects leads to biased results for hierarchical data. This is especially true if the aim of the analysis is to estimate intra cluster correlations or other quantities that combine variables from different levels of hierarchy. Imputation strategies that account for the hierarchical data structures are typically based on fixed or random effects models. While the fixed effects approach is easy to implement it can still lead to biased results if the analysis of interest involves a random effects model. On the other hand, random-effects models are more difficult to fit and might be subject to misspecification, especially if the normality assumption of the random effects is violated.  In this talk we evaluate different approaches for incorporating the hierarchical structure of the data in the imputation models.  ","Multiple imputation (MI) is a principled method in dealing with item-level missing data and has become increasingly popular in the public health and social science investigations where data production is often based on complex sample surveys. However, existing software packages and procedures typically do not incorporate complex sample design features in the imputation process. Failure to account for design features, particularly sampling weights, can introduce bias on final estimates and hence invalid inference. Recent work to accommodate complex sample designs (including clustering and stratification) in imputation includes the sample design in the formulation of the imputation model, which typically requires strong model assumptions and can involve expensive computation in practice. In this paper, we propose a new method to incorporate complex sample designs in MI. Specifically, we divide the imputation process into two steps: the complex feature of the survey design (unequal probability selection in particular) is fully accounted for at the first step, which is accomplished by applying nonparametric methods to generate a series of synthetic datasets; we then perform conventional parametric MI for missing data at the second step using readily available imputation software designed for an SRS sample. A new combining rule for the point and variance estimates is derived to make valid inferences based on the two-step procedure. We evaluate the performance of the new method in comparison with the fully model-based method through a simulation design. Results show that the new method is more robust to model misspecification and generally yields lower RMSE than the fully model-based method. ","A hierarchical generalized linear model is estimated where p continuous and K ordinal variables have ignorable missing data. The key idea is to: estimate the joint model for the (p+K) variables, given known covariates; multiply impute complete data given the estimated joint model by maximum likelihood for subsequent analysis of the hierarchical model. However, it is difficult to define and estimate the joint model for the (p+K) variables correlated at multiple levels. My approach is to: express the joint distribution as a correlated probit model where ordinal variables have underlying latent variables, jointly normal with continuous variables; estimate the distribution of p continuous variables; sort ordinal variables in the ascending severity of missingness, estimate the conditional probit model for each kth ordinal variable given multiply imputed random effects of p continuous and (k-1) less severely missing ordinal variables, and transform into the joint model for p continuous and k latent variables at lower level; repeat the same for ordinal variables at higher level. Estimated joint model and multiply imputed random effects generate multiple imputation of complete data. ","We undertook an investigation to assess the impact of missing data when computing percentages of plasma fatty acids (FA) by comparing a variety of imputation models. Plasma FA are short-term biomarkers accounting for several weeks of dietary intake. Twenty-four FA are measured as part of National Health and Nutrition Examination Survey (NHANES)- a complex, multistage, area probability sample of the US non-institutionalized civilian population. While measured in absolute concentrations, individual FA are most often reported as percentages of total FA. Due to the logistics and cost of obtaining complete profiles of the 24 FA for all NHANES respondents and other issues of multi-analyte quality control, a certain amount of missing data is expected. We assessed imputation models using Markov chain Monte Carlo under multivariate Normality with and without random effects. In general the estimates based on multiple imputation had better statistical properties than simpler alternatives such as complete case analysis. We also found accounting for the sampling design did not seem to be as important as the amount of missing data and which FA were missing. ","Sequential mixed-mode surveys combine different data collection modes sequentially to reduce nonresponse bias under certain cost constraints. However, as a result of nonignorable mode effects, nonrandom mixes of modes may yield unknown bias properties for population estimates such as means and totals. The assumption of ignorable mode effects governs the existing inference methods for sequential mixed-mode surveys. The objective of this paper is to describe and empirically evaluate the proposed multiple imputation estimation methods that account for both nonresponse and nonrandom mixtures of modes in a mixed-mode survey. This paper presents some empirical and simulation results for the bias of mean wage and salary income based on the public-use Current Population, 1973, Social Security Records Exact Match data. ","In longitudinal surveys, a seam effect occurs when the number of changes reported between two adjacent interviews is far greater than the number of changes reported within the same interview. The majority of studies pertaining to seam effects focus on its causes and design features aimed at reducing its effects. Adjusting for seam effects post-data collection is an area that has received less attention (Miller, Lepkowski, and Kalton, 1992). We empirically examine an imputation method that attempts to adjust for seam bias under a rotating panel design. Specifically, we use data from the Survey of Income Program and Participation (SIPP), a widely-cited data source for the study of seam effects, to demonstrate the method. We evaluate the method by comparing the effects of seam bias before and after imputation for estimates of change in Food Stamp program participation. ","In this paper, we consider fitting additive hazards models for case-cohort studies using a multiple imputation approach. In a case-cohort study, main exposure variables are measured only on some selected subjects but other covariates are often available for the whole cohort. This can be treated as a special case of a missing covariate problem. We propose to employ a popular incomplete data method, multiple imputation, for estimation of the regression parameters in additive hazards models. Finite sample properties of the proposed estimators are investigated via simulation studies and illustrated using a data example. ","There is now overwhelming epidemiologic evidence that particulate matter (PM), both fine particulate matter (PM2.5) and course particulate matter (PM10), is not related to total mortality in California.  I will examine all the long-term PM epidemiologic cohort studies in California, and discuss the ways the findings from these studies have be used and/or ignored.  I will discuss the limitations of these studies:  lack of access to key databases; the ecological fallacy; failure to consider other pollutants; failure to satisfy causality criteria; and failure to consider other competing health risks. Also, ethical issues underlying much of PM2.5 epidemiology will be discussed.  I will make a strong case that PM2.5 is not killing Californians and that there is not a scientific or public health basis for the many of the existing and proposed regulations designed to reduce PM levels in California. ","Estimates of public health benefits attributed to cleaner air are largely based on studies of spatial differences in long-term mortality rates. However, such studies tend to suffer from lack of specificity, such as uncertain exposures and neglected confounders and co-pollutants. Here we use meta-analyses to re-examine the results of Jerrett et al. (2011), comprising 992 estimates of long-term mortality-air pollution relationships among California members of a national cohort, with follow-up from 1982 to 2000. These risk estimates include strong and significant positive (harmful) spatial relationships for heart disease and strong and significant negative (beneficial) relationships for other causes including cancer, thus raising questions of causality and credibility. Excess risk estimates for all-cause deaths were essentially randomly distributed around zero. Relative model fits were not compared by Jerrett et al., thus precluding identification of the \"best\" models on this basis. However, only a selected few of these 992 estimates are emphasized in the Jerrett report. By considering the results as a whole, we find major differences among these relationships according to the regression model selected and methods of estimating exposures, none of which specifically considered latency periods. Strong correlations among the various pollutants considered make it difficult to define any \"true\" relationships; we found no significant differences among their risk estimates. Relation-ships with deaths from all causes should be the basis for air pollution control policies and, in a study of this regulatory importance, it is important to discuss both positive and negative findings and to consider the entire suite of results rather than a few that happen to conform to a priori regulatory objectives and were apparently selected for that reason. ","In environmental observational studies, often authors do not address the relative importance of variables under consideration, choosing instead to concentrate on specific claims of significance. Yet good policy decisions require knowledge of the magnitude of relevant effects. In this paper we examine data on the relationship between air quality and mortality in the United States. The analysis uses two methods for determining variable importance, regression analysis and recursive partitioning, showing how this puts predictor variables into a context that supports better environmental policy-making. In particular, using both regression and recursive partitioning, we are able to confirm a spatial interaction with the air quality variable PM2.5, a critical variable in this application domain. We also determine the relative importance of this variable in comparison to others used in air pollution research. We show that there is no association between PM2.5 and mortality west of Chicago and that where there is an association between decreased PM2.5 and increase longevity, it is much less important than other variables such as income and smoking. Our findings point to somewhat different policy recommendations from those developed by previous researchers. ","The U.S. EPA's Clean Air Scientific Advisory Committee's subcommittee on Particulate Matter (CASAC-PM) advises the EPA Administrator on setting National Ambient Air Quality Standards.  Although the Committee and staff are qualified and dedicated, the process could be improved in the interest of the public good.    ","Recently, reproducing kernel Hilbert spaces have been introduced to provide a common approach for studying several nonparametric estimators used for smoothing time series data (Dagum and Bianconcini, 2008 and 2011). Based on this methodology, Bianconcini and Quenneville (2010) focuses on the properties of the Henderson reproducing kernels when the filters are adapted at the end of the sample period, and with particular emphasis on the influence of the kernel order and bandwidth parameter. In this paper, we design a family of trend filters applied for real time estimation that are optimal in terms of reducing revisions when new observations are added to the series.    ","Statistical agencies routinely seasonally adjust large numbers of time series generated from periodic surveys that often use a rotating panel design in which a portion of the sample is retained each period. While these surveys produce highly reliable estimates for national aggregates, demographic and sub-national series are based on much smaller samples. As a result, survey error is a major source of variation in the observed series which is ignored when using conventional time series decomposition filters. Using frequency domain diagnostics, this paper examines the effects of sampling error on the X-11/X12-ARIMA and SEATS filters and the gains from combining these conventional filters with a survey error filter developed from a model based approach that explicitly accounts for survey error. Of special interest is how the outputs of the conventional filters compare to the optimal filter in real time. Data from the Current Population Series are used as examples. ","Most economic time series are computed according to the Gregorian calendar, a solar calendar brings rhythm to our lives and has a deep impact on the economy. The most well known and important calendar effect is seasonality.  But most economic indicators are also linked to a daily activity which is usually summed up and reported each month or each quarter. In this case, the number of working days, which varies from one month to another, can explain some short-term movements in the series. Apart from the day composition of the month, other calendar effects such as public holidays or religious events may also affect the series.   In this presentation, we focus on the trading-day effect caused by the day composition of the month. From the periodograms corresponding to the 7 days of the week, we derive the main theoretical trading-day frequencies. We exhibit the most common real trading-day frequencies from a large set of economic indicators. Finally, we check the accuracy of the visual spectral test implemented in X-12-ARIMA, using simulated series. The results permit us to propose some improvements to the test, but it is also shown the F-test will almost always perform better. ","In population-based household surveys, for example, the National Health and Nutrition Examination Survey (NHANES), blood-related individuals are often sampled from the same household. Therefore, genetic data collected from national household surveys are often correlated due to two levels of clustering (correlation) with one induced by the multistage geographical cluster sampling, and the other induced by biological inheritance among multiple participants within the same sampled household. In this paper, we develop efficient Hardy-Weinberg Equilibrium tests that consider the weighting effect induced by the differential selection probabilities in complex sample designs, as well as the clustering (correlation) effects described above. We examine and compare the magnitude of each level of clustering effects under different scenarios and identify the scenario under which the clustering effect induced by one level dominates the other. The proposed method is evaluated via Monte Carlo simulation studies and illustrated using the Hispanic Health and Nutrition Survey (HHANES) with simulated genotype data. ","Spanish-preferring seniors are an underserved group, immunized at 1/2 the rate of English-preferring non-Hispanic White counterparts. Also, response rates that are ~ 1/2 of overall response rates in national surveys of patient experience limit our ability to assess needs and improve care. We describe a prediction algorithm and randomized experiment addressing these issues. CAHPS surveys of Medicare beneficiaries are conducted by mail, with phone followup. Mail surveys are only in English, with a rarely-used option to request a Spanish language instrument, so responses in Spanish are few and mainly via phone followup. We used CMS administrative data, including address and surname, to predict the probability of Spanish-phone response in a previous year with 98% concordance. In the subsequent year, 1/2 of beneficiaries with the highest predicted probability of preferring Spanish were randomized to a bilingual mailing. This intervention substantially increased mail and total response rates in the target population, with a majority of mail response in Spanish in the intervention condition, so that both representation and data quality may have increased. ","In order to adjust individual-level covariate effects for confounding due to unmeasured neighborhood  characteristics, we extend conditional logistic regression estimators for use with ordinal or multinomial outcomes and complex survey data. For ordinal outcomes we use a proportional odds model, whereas for multinomial outcomes we use a baseline category logit model; in both models we include a neighborhood-specific intercept. Our estimators are consistent even when the within-neighborhood sample sizes are small and the sampling bias is strongly informative. The key to this consistency is our use of sampling design joint probabilities for each within-neighborhood pair. The estimators and asymptotic sampling distributions we present can be computed using standard logistic regression software for complex survey data, such as SAS PROC SURVEYLOGISTIC. We validate the methods using a simulation study, and we apply the methods to data from the 2008 Florida Behavioral Risk Factor Surveillance System survey, in order to investigate disparities in frequency of dental cleaning both unadjusted and adjusted for confounding by neighborhood. ","The analysis of longitudinal trajectories usually focuses on evaluation of explanatory factors that are either associated with rates of change or with overall mean levels of a continuous outcome variable. In this talk I will introduce valid design and analysis methods that permit outcome dependent sampling of longitudinal response data for scenarios where all outcome data currently exist, but a targeted sub-study is being planned in order to collect additional key exposure information on a limited number of subjects. I will propose stratified sampling based on specific summaries of individual longitudinal trajectories, and I will detail an ascertainment corrected maximum likelihood approach for estimation using the resulting biased sample of subjects. In addition, I will demonstrate that the efficiency of an outcome-based sampling design relative to use of a simple random sample depends highly on the choice of outcome summary statistic used to direct sampling and will show a natural link between the goals of the longitudinal regression model and corresponding desirable designs. ","Linear rank tests (logrank, Wilcoxon, Peto-Peto, etc) are commonly used to compare survival outcomes across two groups. Unfortunately, no simple extension of the linear rank tests have been proposed for complex sample survey data. With a random sample of independent subjects, any of the linear rank test statistics can be shown to equal a Cox partial likelihood score test, where the particular linear rank test is determined by a weight given to each risk set in the partial likelihood. For example, a weight of 1 gives the log-rank test, and a weight equal to the number at risk gives the Wilcoxon test. For complex survey data, we formulate our extension of the linear rank tests as an estimating equations score statistic for no group effect in the partial likelihood. The proposed method is applied to an NHANES study which explores if a dash diet (versus a normal diet) reduces the risk of death. ","The National Household Education Surveys Program (NHES), a system of periodic surveys sponsored by the National Center for Education Statistics, was fielded nine times using landline random digit dialing (RDD) between 1991 and 2007. Following the 2007 administration, with landline RDD coverage and response rates experiencing steep declines, NHES underwent a redesign. A two-phase address based sampling (ABS) approach, with mail as the primary mode, was pilot tested in 2009. Using findings from the 2009 Pilot Study, a large-scale methodological field test was designed. This Field Test, conducted in 2011, contained several methodological experiments aimed at affecting response propensities. With a nationally representative sample of nearly 40,000 addresses plus an additional supplemental sample of about 20,000 addresses that targeted Spanish speakers, this very large-scale Field Test was designed to support detection of small but substantively important effects. In this presentation, we discuss the NHES program's transition to ABS, describe the design of the Field Test, and review some key findings from the Field Test experiments and consider the implications for future ABS studies. ","Address-based sampling (ABS) with a two-phase data collection approach has emerged as a promising alternative to random digit dial (RDD) surveys for studying specific subpopulations in the United States. In 2011, the National Household Education Surveys Program Field Test used a two-phase ABS design with a mail screener to identify households with eligible children and a mail topical questionnaire administered to parents of sampled children to collect measures of interest. Experiments with prepaid cash incentives and special mail delivery methods were applied in both phases. For the screener, sampled addresses were randomly designated to receive either $2 or $5 in the initial mailing. During the topical phase, incentives (ranging from $0 to $20) and delivery methods (First Class Mail or Priority Mail) were assigned randomly but depended on how quickly the household had responded to the screener. The paper first evaluates the effects of incentives on response rates, and then identifies the optimal incentive level for attracting the hard-to-reach groups and improving sample composition. The impact of incentive on data collection cost is also examined. ","Mail surveys using address-based sampling are becoming a viable alternative as landline RDD surveys face coverage and response rate issues. An important aspect in any survey is obtaining responses from all segments of the population to avoid nonresponse bias. In 2011 the National Household Education Surveys Program (NHES) conducted a Field Test using a two-phase mail survey as a replacement for a long-standing RDD survey. The first phase was a mail screener to identify households with eligible children and the second phase was a topical interview with the parent of a sampled child. This Field Test contained several methodological experiments aimed at affecting response propensities including tests of methods to increase the response rates from Spanish-speakers. This presentation describes the experimental design used and the findings of the tests. In addition to the response rates, the characteristics of the respondents are presented because they are essential to evaluating potential nonresponse bias. ","Address based sampling (ABS) is increasingly being used in the current survey environment of declining response rates and coverage concerns associated with landline random digit dial surveys. The goals of this research are to determine the implications of using ABS frame variables for data collection, to evaluate data quality of demographic variables provided on ABS frames, and to examine the possibility of using ABS frame variables to guide survey design. This research will focus on ABS with mail as the mode of collection, although some of the findings will be pertinent to ABS surveys using other modes of data collection. The implications of using ABS on data collection will be evaluated by examining the association of frame data with eligibility rates, response rates, and characteristics of survey respondents. Quality of frame data will be evaluated by examining rates of availability of these items and by comparing these data to information reported by respondents. The ability to use ABS frame information for stratification and to guide operations will also be considered in light of the findings of these evaluations. ","This presentation discusses the evolution of an international component for the SDR. Conducted biennially since 1973 by the NSF, the SDR follows a sample of U.S.-earned doctorates in science, engineering, &amp; health (SEH) from degree award throughout their careers. Prior to 2003, SDR included only U.S. residents in its target population. In response to interest in migration of U.S.-trained doctorates, a methodological study was conducted in 2003 which demonstrated the feasibility of locating &amp; collecting data from U.S.-trained doctorates residing outside the U.S. This success led to the formal adoption of an international SDR component. The 2006 &amp; 2008 SDR selected separate samples for data collection from international residents, both those emigrating immediately upon degree receipt &amp; those emigrating later. National &amp; international residency is difficult to predict leading to the decision to combine the two designs in 2010 SDR to create an integrated sample design for all U.S.-trained SEH doctorates. The redesigned SDR now has the potential to provide 100% coverage of the population of SEH doctorates graduating from U.S. institutions in the 21st century regardless of residency. ","Traditionally, the Survey of Doctorate Recipients (SDR) is a longitudinal survey that collects information from U.S. residing individuals with a doctoral degree in a science, engineering, or health field (SEH) from a U.S. institution.  Beginning with the 2003 cycle, the SDR added a new component, the International Survey of Doctorate Recipients (ISDR) to represent U.S.-trained doctorates living outside the U.S.  Prior to the 2010 cycle, the traditional SDR, now named the National Survey of Doctorate Recipients (NSDR), and ISDR were implemented as two separate surveys.  In 2010, the survey sponsor, the National Science Foundation (NSF), developed and implemented a methodology to integrate the sampling frame, sample design, weighting adjustments, and variance estimation procedures for the NSDR and ISDR. The integrated SDR, including both the NSDR and ISDR, covers the entire population of U.S. trained SEH doctorates. This paper discusses the integration methodology and explores the impact of integration on the survey weights and reported estimates on the 2008 SDR data.  We compare the population estimates, the distribution of the weights, and the weighted estimates for a set of key variables under the integrated design to those from the traditional NSDR program.  ","NORC under contract with the National Science Foundation conducted a research study to develop new methods of nonresponse weight adjustments for the 2008 integrated National and International Survey of Doctorate Recipients (SDR). Primarily this research was designed to build a logistic regression or propensity adjustment procedure and to contrast it to the current weighting class methodology. Given we expected a propensity based procedure would offer greater flexibility in the use of available covariates, we hoped the transition would enable us to leverage new information in the form of prior survey response patterns for the panel cases and level of effort or paradata to reduce the likelihood for nonresponse bias. This talk will provide an overview of the prior weighting methodology, the new methods explored and related considerations. Comparisons will be conducted by examining the model diagnostics and the distribution of the weight adjustments between a baseline model using the weighting class cells to the new models tested which utilize a combination of main and interaction effects from the weighting class factors with the new paradata and historical panel data. ","The Survey of Doctorate Recipients (SDR) is a longitudinal survey of U.S.-trained doctorates in science, engineering, and health, which provides key information on their employment and demographic characteristics. With the recent addition of the international component, the SDR is able to include participants regardless of the location of their residency. This feature enables comparative studies of recent doctoral graduates between those who reside in the U.S. and those who seek employment abroad. It also provides data for studying their international mobility. A better understanding of the migration patterns has important policy implications and may improve the design and administration of such a worldwide program. We apply a Markov transitional model to analyze the longitudinal location data and assess the influence of post-graduation plan, country of origin, field of study, and past locations on the current location of residency. This model also links together the probability of transition between locations and the probability of missing observations due to dropout, locating problem, and nonresponse. We derive summary measures of the international migration patterns. ","The American Community Survey (ACS) is based on two samples: a housing unit sample and a sample of persons residing in group quarters. Group quarters include facilities such as nursing homes, college dorms, and correctional facilities, and they are currently underrepresented in the ACS, which affects the quality of the total population estimates produced in many small areas that have group quarters populations. The National Academies' Committee on National Statistics (CNSTAT) convened a panel on methods used for measuring the group quarters population in the ACS. The panel evaluated alternatives to the current sample design, weighting procedures, and other methodological features that can make the ACS group quarters data more useful for small-area data users. Of special concern was how these alternatives would affect the ability of the ACS to measure the characteristics of the total population for small governmental jurisdictions, census tracts, and block groups, using the five-year period estimates. This session will focus on the CNSTAT panel's recommendations and discuss the broader issues of including group quarters residents in surveys.  ","Accurate forecasting of zero coupon bond yields for a continuum of maturities is paramount to bond portfolio management and derivative security pricing. Yet a universal model for yield curve forecasting has been elusive, and prior attempts often resulted in a tradeoff between goodness-of-fit and consistency with economic theory. To address this, herein we propose a novel formulation which connects the dynamic factor model (DFM) framework with concepts from functional data analysis: a DFM with functional factor loading curves. This results in a model capable of forecasting functional time series. Further, in the yield curve context we show that the model retains economic interpretation. Model estimation is achieved through an expectation maximization algorithm, where the time series parameters and factor loading curves are simultaneously estimated. Efficient computing is implemented and a data-driven smoothing parameter is nicely incorporated. We show that our model performs well in forecasting actual yield data compared with existing approaches. ","In this talk, we will introduce two new families of multivariate association measures based on power divergence and alpha divergence, respectively, that recover both linear and nonlinear relationships between multiple sets of random vectors. Importantly, this novel approach not only characterizes independence, but also provides a smooth bridge between well-known distances that are inherently robust against outliers. Algorithmic approaches are developed for dimension reduction and selection of the optimal robust association index. Extensive simulation studies are performed to assess the robustness of these association measures under different types and proportions of contamination. We illustrate the usefulness of our methods in application by analyzing two socioeconomic data sets that are known to contain outliers or extreme observations. Some theoretical properties, including the consistency of the estimated coefficient vectors, are investigated and computationally efficient algorithms for our nonparametric methods are provided. ","Calibration estimators borrow strength from auxiliary information to improve the efficiency of survey estimates over other forms such as a simple Horvitz-Thompson estimator. Improvements to the mean square error (MSE) are directly linked to the association of the study outcome and the auxiliary data. Generalized regression estimators, or GREGs, are particularly flexible because they can be used to calibrate survey weights to a variety of auxiliary control totals, and to produce a variety of estimators depending on the specified form of the linear model. However, it is rare to find control totals that are generated from the target population and therefore known without error, and also strongly related to the outcomes of interest. Many researchers instead turn to control totals estimated from other surveys, assuming a negligible impact on MSE. We add to our previous research on estimated-control calibration by developing the bias and variance estimators for a GREG total, accounting for the uncertainty in the control totals. Theoretical and empirical comparisons are made to traditional GREG estimators to demonstrate the implications for ignoring this uncertainty. ","Dual frame surveys, in which independent samples are selected from  two frames to decrease survey costs or to improve coverage, can  present challenges for regression coefficient estimation because of  complex designs and unknown degree of overlap. In this research, we  developed four regression coefficient estimators in dual frame  surveys. Simulation results show that all the proposed methods work  well. ","The work of Kim, Heeringa, and Solenberger (2006) provided a theoretical basis of what is called model-optimized sampling methods for yielding sampling designs that give large variance reductions as well as the stability of the variance estimates. Their methods were based on a simple linear regression superpopulation model. Hong et al. (2009) suggested modified sampling methods based on the same model. However, in many real populations, using more complicated superpopulation models would be better with respect to the efficiency. For this, we suggest model-optimized sampling methods using general polynomial superpopulation models. We illustrate the benefits of our new approaches by comparing the efficiencies between the different models. ","In natural resource surveys, a substantial amount of auxiliary information, typically derived from remotely-sensed imagery and organized in the form of spatial layers in a geographic information system (GIS), is available. Some of this ancillary data may be extraneous and a sparse model would be appropriate. Model selection methods are therefore warranted. The `least absolute shrinkage and selection operator' (lasso) conducts model selection and parameter estimation simultaneously by penalizing the sum of the absolute values of the model coefficients. A survey-weighted lasso criterion, which accounts for the sampling design, is derived and a survey-weighted lasso estimator is presented. The root-n design consistency of the estimator and a central limit theorem result are proved. Several variants of the survey-weighted lasso estimator are constructed. In particular, a calibration estimator and a ridge regression approximation estimator are constructed to produce weights that can be applied to several study variables. Simulation studies show the lasso estimators are more efficient than the regression estimator when the true model is sparse. ","In cutoff sampling, inference -- for example, interval estimates with associated alpha-levels -- is problematic. Design-based samplers do not find an adequate random design on which to base variance estimates. Model-based samplers worry that gaps in information can lead to biases. We nonetheless describe some schemes for inference in cutoff sampling. ","When weighting is not an attractive option for treating unit-level nonresponse (UNR), as in a census, the use of administrative data and imputation can improve the quality of a data set.  The Survey of Earned Doctorates (SED) is a census of all new research doctorate recipients in the U.S. The SED boasts a 92.3% unit-level response rate. The item response rates are generally well above 90 percent. UNR is augmented with administrative data from the doctorate-granting institution, which brings many key variables to 100% item response.  The NSF commissioned an investigation to determine if imputation would be feasible in the SED. We used modeling to determine the strongest relationships among the variables, and serpentine sorting to organize a nearest neighbor hot-deck imputation procedure.   We tested the univariate distributions of the pre-and post-imputation data sets and the relationships among the pre- and post-imputation variables. The relationships among the post-imputation data set were not significantly different from the pre-imputation data set. Overall, imputation showed promise in treating UNR when administrative data are used to augment self-reported data. ","The National Survey on Drug Use and Health (NSDUH) provides national, state, and substate data on substance use and mental health in the civilian, non-institutionalized population age 12 and older. In the NSDUH, zero, one, or two people are selected from each selected household. \"Pairs\" account for about 60% of the annual sample, over 90% of which are members of the same family. The responses to some NSDUH questions have high positive correlations between pair members, especially the questions about family-level characteristics. The current imputation method, predictive mean neighborhood (PMN), does not exploit this correlation, even when the value for one pair member is missing and the value for the other pair member is not missing. This presentation discusses (1) a method for identifying variables for which the other pair member's response may be a better choice for imputation than the PMN-assigned value; and (2) for these variables, a method for identifying exact conditions under which the other pair member's response may be used. ","Traditional hot-deck imputation macros utilize SAS arrays and data steps. They keep only one nearest donor above the recipient in the sort order in the memory without limiting the number of times that a donor can be used. The new NORCSuite Impute macro using SAS IML has the following new functionalities: 1) a two-way search (above and below) algorithm that selects the global nearest donor without violating the user-specified donor use limit to control imputation bias; and 2) the number of donors kept in memory is raised automatically to avoid violating the donor use limit, and to minimize the underestimation of data variance inherent in hot-deck. The new macro also supports serpentine sorting using multiple sort variables with different variable types, imputing a set of variables together, and applying unequal donor use limits to different subsets of the data. In this paper, we will compare our new and previous hot deck imputation macros in terms of mean, variance (the amount of underestimation) and imputation bias. ","In 2007, Judkins, Krenzke, Piesse, Fan, and Haung reported on the performance of a new semi-parametric imputation algorithm designed to impute entire questionnaires with minimal human supervision while preserving important first- and second-order distributional properties. In a 2008 paper, we reported on procedures for post-imputation variance estimation to be used in conjunction with the semi-parametric imputation algorithm. In this paper, we discuss recent enhancements to handle very large longitudinal datasets for the Mental Health Treatment Study.  ","Inference in the presence of missing data is a widely encountered and difficult problem in statistics.  Imputation is often used to facilitate parameter estimation, which uses the complete sample  estimators to the imputed data set. We consider the problem of parameter estimation for linear  mixed models with non-ignorable missing values, which assumes the missingness depends on the  missing values only through the random effects, leading to shared parameter models (Follmann and  Wu,1995). We develop a parametric fractional imputation (PFI) method proposed by Kim (2011)  under this non-ignorable response model, which simplifies the computation associated with the EM  algorithm for maximum likelihood estimation with missing data. In the M-step, the restricted or  adjusted profiled maximum likelihood method is used to reduce the bias of maximum likelihood estimation of the variance components. Results from a simulation study are presented to compare the  proposed method with the existing methods, which demonstrates that imputation can significantly  reduce the non-response bias and the idea of adjusted profiled maximum likelihood works nicely in  PFI for the bias correction in estimating the variance components. ","We extend Jackknife empirical likelihood (JEL) method proposed by Jing etc. (2009) to make a likelihood-based inference with imputed data. Specifically, likelihood-ratio confidence intervals can be constructed using a chi-square limiting distribution if the empirical likelihood is applied to the jackknife pseudo values. The jackknife pseudo values are very close to the pseudo values for variance estimation discussed in Kim and Rao (2009). Using the empirical likelihood, we can incorporate additional auxiliary variables into estimation and improve its efficiency. The proposed method is applicable to some complex sampling designs such as stratified Poisson sampling under some regularity conditions. Extension to fractional imputation is also discussed. Simulation results are also presented.  ","SIMMER is a highly interdisciplinary program designed to assess current and future vulnerability to heat stress in urban environments through integration of physical and social science models. Focused on the city of Houston, the SIMMER project seeks to incorporate data from various sources including heat-related mortality and hospital admissions, a survey of Houston residents, the U.S. Census and other city and state agencies, remote sensing, high resolution climate models, and observed meteorological data. In this talk, we will discuss the SIMMER project and an initial analysis of this data, including assessments of vulnerability and a statistical model for linking mortality to vulnerability, socio-economics, weather, and climate. ","Epidemiologic studies of the health effects of air pollution involve multiple sources of spatial uncertainty including uncertainties in exposures and their surrogates, uncertainties in locations of individuals (home or work), and uncertainties in estimated health effects. Due to strong seasonal trends and sharp temporal variations in exposures and outcomes, air pollution epidemiology has given careful attention to temporal variations but less so to the impact of spatial uncertainties on inference for the associations of interest. In this presentation, we build on ongoing air pollution epidemiology projects in Atlanta, Georgia to provide an overview of spatial uncertainties in exposures, outcomes, and the associations between the two with particular emphasis on a comparison between geographically weighted regression and model-based spatially varying coefficient models. ","We investigate the association between short-term exposure to ambient ozone levels of ozone and the risk of an out of hospital cardiac arrest (OHCA) in Houston, Texas. The method of analysis includes a time-stratified case-crossover study coupled with conditional logistic regression and captures the variation of pollution patterns in Houston throughout the day and across the City using a hierarchical spatial-temporal estimate of pollution levels. The dense ambient air-pollution and meteorological monitoring network and the knowledge of pollution patterns for the region, supports this innovative development of the case-crossover methodology to a temporal scale of one hour and a continuous spatial scale. Health information was obtained from a database of 6,812 cases representing all qualified OHCA events in which an ambulance responded in the City of Houston for the five-year period of 2004 to 2008. The findings identify areas within the City and time of day when citizens are at greater risk of this acute health endpoint. ","Estimating the risks heat waves pose to human health is a critical part of assessing the future impact of climate change. In this article, we propose a flexible class of time series models to estimate the relative risk of mortality associated with heat waves and conduct Bayesian model averaging (BMA) to account for the multiplicity of potential models. We apply these methods to data from 105 U.S. cities for the period 1987-2005, we identify those cities having a high posterior probability of increased mortality risk during heat waves, and examine the heterogeneity of the posterior distributions of mortality risk across cities. Our results show that no single model best predicts risk across the majority of cities. Although model averaging leads to posterior distributions with increased variance as compared to statistical inference conditional on a model obtained through model selection, we find that the posterior mean of heat wave mortality risk is robust to accounting for model uncertainty over a broad class of models. ","Effectively profiling population dynamics in rural areas of the U.S. is challenging with the long period estimates available from the American Community Survey (ACS). Five-year period estimates from the ACS moderate dynamic population change because of the accumulation of the sample over 5 years. The Rural Statistical Area (RSA) initiative is an alternative source of data using the American Community Survey Public Microdata (ACS-PUMs) files. Through a partnership with the Economic Research Service (ERS) of USDA, the Census Bureau created an annual PUMs file with a population threshold of 65,000 people opposed to the current 100,000 limit. The ERS created an algorithm using counties as the building block for the file and Urban Influence codes which categories counties by size and distance to metropolitan centers. The resultant PUMAs effectively separate the rural territory from influences of large cities and offer 1-year period estimates. This paper discusses the usefulness of this file by demonstrating its ability to effectively profile dynamic population change in rural areas of North Dakota impacted by energy development activity. ","The American Community Survey (ACS) is now providing 5-year estimates for areas as small as census block groups, but questions remain about the data. This presentation describes an initial examination of 5-year ACS data for selected tables. For all block groups nationwide, the analysis explores the distribution of the ACS sample, reported margins of error, and some conspicuous outliers. Observations are illustrated with data from individual block groups as well as summaries across all block groups. And while not definitive, comparisons with estimates from the 2000 census long form and 2010 cesus counts shed light on the reasonableness of the small area ACS data for different types of areas. Among the findings of interest are the number of cells with values of zero, the percent of households interviewed in small towns versus large metropolitan areas, and the performance of ACS estimates in rapid growth areas. ","Research has shown that children growing up in neighborhoods with high poverty rates are at higher risk of health problems, teen pregnancy, dropping out of school, and other social and economic problems compared to children living in more affluent communities. In this paper, we use new five-year data from the Census Bureau's American Community Survey to evaluate estimates of children living in high-poverty neighborhoods. We disaggregate results by race/ethnicity and by state to determine whether children in certain population subgroups or geographic areas have fared better than others. Results of this analysis will help data users assess the reliability and usefulness of the American Community Survey as a replacement for the decennial census long form. ","Demographic Analysis (DA) has been used for more than fifty years to evaluation the accuracy of U.S. Decennial Census counts. Demographic Analysis, which relies heavily on data from birth and death certificates, is a particularly good method for evaluating the count of children (persons under age 18) in the Decennial Census. This paper uses DA to evaluate the count of children in the 2010 Decennial Census. DA is used to look at the total count of children as well as several demographic dimensions such as age, sex, as well as race(black and Non-black) and Hispanic Origin. Analysis reveals an estimated net undercount of children compared to an estimated net overcount of adults (age 18+) in the 2010 Decennial Census. Based on the middle series of the Demographic Analysis series, there was an estimated net overcount of 0.7% for adults but an estimated net undercount of 1.7% for children. Children under age 5 had the highest net undercount rates, while persons age 14 to 17 experienced an estimated net overcount. The estimated net undercount of male and female children was almost identical, but gender differences started to e ","The main goal of Statistical Disclosure Control (SDC) methodology is to provide society with access to confidential data such that individual information is sufficiently protected against disclosure and at the same time data utility is preserved for valid statistical inference. The Post Randomization Method (PRAM) advocates release of perturbed data, where values of categorical variables are perturbed via some known probability mechanism. Estimation with perturbed data without accounting for PRAM leads to biased estimates, hence raising issues with data utility. To address these issues, we propose a number of EM-type algorithms to obtain unbiased estimates of generalized linear models (GLMs) fitted to perturbed data. A few measures of disclosure risk will also be evaluated and discussed, as well as applications of the proposed methodology to data from the 1993 Current Population Survey. ","To limit the risk of disclosing survey respondents' identities or sensitive attributes, statistical agencies may release multiply imputed synthetic data sets in place of the observed data to external users. Given the statistical agency and external user are separate bodies, their sources of input will potentially be different. This is known in the multiple imputation literature as uncongeniality. In this paper, we present a formal definition of uncongeniality for multiple imputation for synthetic data. We use this framework to address common examples of uncongeniality, specifically ignorance of the original survey design in analysis of synthetic data, and when the imputation model conditions upon a different set of records to those analyzed in the analysis procedure. We conclude the formal definition assists the imputer to identify the source of a lack of data utility preservation between observed and synthetic data analytic results. Motivated by our definition of congeniality, we derive and illustrate an alternative approach to synthetic data inference to recover the observed data sampling distribution given the synthetic data. ","Sensitive information in microdata can be effectively protected by replacing full or a portion of original data with values synthesized via multiple imputation (MI). Point estimators are defined as the averages of individual estimates from each of the $m$ released data sets. In this talk, we present a general large-sample framework providing the $sqrt(n)$-consistency and asymptotic normality of the point estimators. In particular, asymptotic distributions will be provided for cases where synthesis models are based on full or partial original data. We will also show that the regular empirical variance estimator $W+(1+1/m)B$ as used in the missing data setting overestimates the asymptotic variance of the point estimates. Our results are consistent with some previous work done in a Bayesian context. ","We propose new data masking procedures for building secure database. The new class of procedures have a number of features that make them particularly attractive for general-purpose data masking tasks. They satisfy the low disclosure risk requirement for ideal data masking. As compared with the existing data masking methods, the new procedures substantially increase the utility of secure databases. In particular, unlike the existing approaches, the new procedures are capable of preserving important nonmonotonic relationships among attributes. Maintaining such relationships can be key to the success of database research and to the determination of the optimal level of policy, managerial and economic interventions.   The proposed methods do not rely on data being constrained to be of a particular type or have a particular distributional shape. In fact, the methods provide unified, flexible and robust algorithms to mask general types of confidential variables with arbitrary distributional shapes. Our evaluations in simulation and real datasets demonstrate that the new methods perform better than the existing ones and represent a significant advancement in data masking techniques. ","In observational studies, propensity score (PS) methods have been used to reduce the bias of the treatment effect estimator. The equal frequency (EF) subclassification method, which has been widely applied, equally divides the sample space into subsets using the PS percentiles and assigns equal weights (EW) to subclasses. Some researchers have used an equal variance (EV) method, which divides the samples by equalizing the estimated variances of the treatment effect estimator among subclasses and assigns inverse variance (IV) weights. We conduct simulations to indicate that under quadratic term misspecification, the EF-IV estimator provides the lowest bias and root mean square error as compared to the ordinary least square estimator and other propensity score estimators. Our theoretical results demonstrates that if higher variation occurs with larger bias for within subclass treatment effect estimates then the EF-IV estimator has a smaller overall bias than the EF-EW estimator. We show that if the variance in the EV approach is larger than the harmonic mean of the within subclass variances of the EF approach, then the EF-IV estimator has a lower variance than the EV-IV estimator. ","This paper addresses the topic of nonresponse bias in business surveys. Some common techniques for investigating nonresponse bias post-data collection have been established from a design-based perspective. Many of these techniques, however, cannot account for small nonresponse adjustment cells which are common for business surveys. For the Monthly Wholesale Trade Survey nonresponse bias analysis, we tried alternative, model-based approaches. To evaluate the potential for nonresponse bias, we examined propensity and prediction response models using frame data compiled from auxiliary data sources, including the Economic Census and the American Community Survey. Additionally, we modeled response propensity and prediction given the current adjustment cells variables.  To minimize variance and bias, the variables used to define nonresponse adjustment cells should be highly predictive of key survey estimates and the likelihood of responding to the survey. Evidence otherwise is evidence of nonresponse bias. Our findings are discussed.  ","It is common in practice to address potential nonresponse bias by carrying out post-survey weighting adjustments. Response propensity weighting (Rosenbaum and Rubin 1983) uses auxiliary variables to construct a propensity model that assigns each responding case a weight to compensate for differential probabilities of becoming a respondent. Calibration techniques also use the auxiliary variables to create post-survey weighting adjustments by finding a new set of weights that minimizes the distance from the original weights, but that reproduces population totals on the auxiliary variables exactly. Taking advantage of records available from the sampling frame for our study, we examined the effectiveness of these two common and explicitly model-based post-survey weighting strategies. We found that neither weighting method resulted in significant bias reductions. ","Adjustment factors for non-response and coverage are applied to the initial weights to produce the final analytic weights. The adjustment for under- or over-coverage is applied using post-stratification. As Cochran suggested, the effectiveness of the post-stratification adjustment does completely depend on available true population parameters, or the control totals. In practice, however, the population parameters are frequently estimated directly or indirectly through modeling. This paper will discuss soundness of using estimated (as opposed to true) population parameters in post-stratification adjustment. Numerical examples will be presented. ","Several design-based, model-based, and model-assisted methods have been developed to adjust survey weights for nonresponse or coverage errors, to reduce variances through the use of auxiliary data or by restricting the range of the weights themselves.  Some methods directly change the weights, like calibration weighting and design-based ad hoc weight trimming methods.  Other methods implicitly adjust the weights, like robust superpopulation modeling approaches.  The generalized design-based method models the weights as a function of the survey response variables and using the smoothed weights predicted from the model to estimate finite population totals.  This paper provides empirical examples of how several adjustment methods change a given sample's weights and the resulting impact on estimates. ","The National Health Interview Survey (NHIS) is a continuous survey that has collected health data using personal interviews since 1957. The unit response rate historically has been high, above 90% for the first forty years of the survey. In the past decade, the unit response rate has declined to approximately 80%. We discuss changes in the methodology for unit nonresponse adjustment over the history of the survey, with a focus on potential future changes that employ auxiliary data. ","Weighting for nonresponse is often done using a single model in which refusals and noncontacts are grouped together. Nonresponse to a survey can occur at multiple stages of contact. When an existing survey serves as a sampling frame there is a rich set of covariates that can be used in adjustments based on a sequence of propensity models. The Mental Health Surveillance Study (MHSS) uses clinical interviews administered to a subsample of respondents to the National Survey on Drug Use and Health (NSDUH) within 4 weeks of completing the NSDUH interview. Two distinct sets of MHSS nonrespondents are observed - those who immediately refuse to participate at the end of the NSDUH interview (initial refusals), and those who initially agree to participate, but cannot be contacted or refuse when contacted for the clinical interview. A comparison of nonresponse adjusted estimates using a single model and a two-step approach, where initial refusals are modeled separately from noncontacts and later refusals, reveals potential advantages of the two-step method. The two-step approach has the potential to yield less biased estimates without appreciably increasing standard errors. ","Post-stratification is used in survey statistics as a method to improve variance estimates. In traditional post-stratification methods, the variable on which the data is being stratified on must be known at the population level. In many cases this is not possible, but it is possible to use a model to predict values using covariates, and then stratify on these predicted values. This method is called Endogenous Post-Stratification Estimation (EPSE), originally proposed in Breidt and Opsomer (2008). In this presentation, we investigate methods to automatically select the number of post-strata for EPSE. We do this in the context of models fitted by Random Forests (Breiman 2001) with the stratum boundaries set at quantiles of the predicted distribution. ","The Mental Health Surveillance Study (MHSS) uses data from clinical interviews administered to a sub-sample of adult respondents from the National Survey on Drug Use and Health (NSDUH) for estimating the prevalence of serious mental illness (SMI). First, probabilities of having SMI are estimated for each adult NSDUH respondent based a logistic regression fit to the MHSS sample. Then, a Receiver Operating Characteristic (ROC) analysis classifies NSDUH respondents as either having or not having SMI based on the estimated probabilities. A \"cut-point\" estimator of prevalence results from using those classifications. Due to the discrete property of ROC classification, the asymptotic variance of cut-point estimator cannot be linearized, and a replication method of variance estimation is needed. We discuss the theoretical reasons why Fay's balanced repeated replication (BRR) is superior to rival replication methods in this context. We then evaluate the results of a simulation study using the MHSS respondent sample to see whether Fay's BRR actually produces nearly unbiased variance estimates for the estimated SMI prevalence among all adults and among the Hispanic subpopulation. ","Due to relatively high levels of sampling variability, direct design-based variance estimators are often smoothed before publication. For example, some Federal Statistics programs publish the medians of a sequence of monthly direct variance estimates, or functions of these medians. The properties of these smoothed estimators depend on several underlying conditions, including sample size, effective degrees of freedom for the direct estimators; correlation of the direct estimators across months; and temporal patterns in the true variances. We compare and contrast these properties with the corresponding properties of generalized variance function (GVF) estimators.  ","The Multidisciplinary Treatment Planning (MTP) Survey will be conducted using an instrument jointly developed by the National Cancer Institute (NCI) and the Commission on Cancer (CoC) to survey CoC-accredited hospitals in the United States on how multidisciplinary treatment planning is offered to cancer patients for one of six cancer types (Brain/CNS, Lung, Head &amp; Neck, Gynecologic, Gastrointestinal, and Breast cancer). This survey used a census of facilities and only one cancer type is randomly selected for each facility by an unequal probability sampling method using case volume as the basis to determine the selection probability. The sampling method poses a unique challenge for variance estimation because a single cancer type is selected independently from each facility, which can be regarded as the sampling stratum because each facility is included with certainty. It requires combining facilities into variance strata and special handling of nonresponse. This paper presents a theoretical framework for variance estimation and the results of a simulation study of the proposed variance estimator based on the jackknife technique. ","Bootstrap algorithms are simple and appealing solutions for variance estimation under a complex sampling design, however, they must account for the non-iid nature of data.  Literature about bootstrapping finite population samples appears to have developed according to two major approaches. A more practical \"ad-hoc\" approach refers to the so-called scaling problem  and is based on  a data-rescaling  so that, in the linear case, the resulting bootstrap estimate for the variance perfectly matches the analytic variance estimate. A more fundamental \"plug-in\" approach is based on the mimicking bootstrap principle and on the bootstrap population created on the basis of (original) sample data.  Recent proposals suggest a direct bootstrap  matching the linear case variance but avoiding any data scaling  under mixed re-sampling designs. In this paper,  a new perspective to the bootstrap population plug-in approach is provided that avoids the physical reconstruction of the bootstrap population. Basic sampling designs, both  with and without replacement as well as unequal probability designs are considered. Focusing on probability-proportional-to-size sampling, a simulation study is conducted that compares all the approaches considered.  ","Creation of replicate survey weights affects estimation methods and can lead to very different variance estimates compared to the arguably more common computation of a single survey weight variable and use of Taylor Series Linearization. The choice seems to be based on familiarity with one over the other or made at the institution level, rather than based on evidence or criteria that could lead to the selection of the optimum method given a particular study. Replicate weights can overestimate the true sampling variances while the Taylor linearized weights can underestimate the variance reduction from using know population control totals. This study aims to tease out conditions that make either weighting and estimation approach more optimal, particularly with respect to the variables used in weighting and their relationship to the  survey variables. ","Standard confidence intervals based upon a normal distribution can perform poorly when the sampling distribution is not normal. EL confidence intervals may be better in this situation, as EL confidence intervals are determined by the distribution of the data (Rao &amp; Wu 2009). The range of the parameter space is also preserved. This may not be the case for standard confidence intervals based upon a normal distribution, as standard confidence intervals can have negative lower bounds for a positive point estimator. Chen and Sitter (1999) proposed a pseudo EL approach which can be used to construct confidence intervals for the H\u00e1jek (1971) ratio estimator (Wu &amp; Rao, 2006). The pseudo EL approach is not entirely appealing (Rao &amp; Wu 2009) as it is not a genuine EL approach, it is not applicable to the Horvitz-Thompson (1952) estimator, and is limited to small sampling fraction. We propose a nov ","The estimation of confidence intervals for quantile estimation involves three steps: (1) point estimate of cumulative distribution function (CDF), (2) Confidence interval for the estimated CDF, which is a binomial proportion, (3) a method for converting CDF intervals to quantile intervals. The point estimate for CDF in a simple random sample is well known i/(n+1). We provide an equivalent estimate using weights for survey data. There are several options for confidence intervals of binomial proportions and two approaches for converting intervals of CDF to those for quantiles: The estimating equations Woodruff [1952], or Complete estimation of confidence interval for the entire CDF Francisco and Fuller [1991]. These result in several combinations of methods that are very similar and not easy to discriminate. To sharpen the differentiation, we consider all confidence levels and compute corr ","Rapid increases in the US cell-phone-only population have motivated use of dual frame, landline and cell phone random-digit-dial, sample designs to minimize potential coverage error. Given cost considerations, the size of the cell sample is not typically proportional to the size of the cell phone population, yielding variability in survey weights that increases standard errors for estimates. As a result, cell sample sizes may be small for states and local areas, which can also serve to increase the size of standard errors.  We provide a framework for weighting a dual frame sample design from the National Immunization Survey, which monitors the vaccination rates of children 19-35 months and adolescents 13-17 years for the nation, states, and selected local areas. This framework addresses overlap between the landline and cell phone samples through a composite estimation approach, and the variability in survey weights through weight attenuation for the cell phone only domain, while controlling impact on bias and mean-squared error. Bias, variance, and mean-squared error of the weighting approach are examined and contrasted with those of other common dual frame weighting approaches. ","State and local health surveys of the adult population in households include the Behavioral Risk Factor Surveillance System, the New York City Community Health Survey, and the Los Angeles County Health Survey. These surveys currently use partial or full overlap dual frame random-digit-dialing sample designs with single-mode telephone data collection. The questionnaires contain numerous skip patterns and may include add-on modules, physical measurement studies and special follow-up studies. We discuss RDD sampling methods to enhance coverage of adults with telephone service, the inclusion of cell-mostly adults, the construction of a probability sample of adults with telephone service including the within-household sampling of an adult from landline telephone households, and the inclusion of young adults in college-based group quarters. Given the objectives of each survey the strengths and weaknesses of the telephone sample designs and data collection methods are discussed. We conclude with a discussion of the implications of the rapid adoption of personal communication/Web access mobile devices and the decline in the use of first-class mail in the U.S. ","After nearly two decades as a landline RDD survey, the National Household Education Surveys Program (NHES) was redesigned in 2009 to an address-based sample (ABS) with mail as the primary mode. To determine household eligibility and do within-household sampling requires two phases of data collection: A Screener followed by a Topical survey in eligible households. To transition, we conducted a pilot study in 2009 and a large-scale methodological field test in 2011. These tests included embedded experiments designed to examine the effects of alternative approaches on response and coverage rates. The transition to this ABS approach has been successful for NHES, with the NHES:2011 Field Test attaining higher response and coverage rates than those attained in the most recent NHES landline RDD administration.  In this paper, we describe the considerations that led to the decision for NHES to transition to ABS, and the data collection approaches taken, and compare these to alternative approaches taken by others. We present overall findings from the Field Test and use results of the embedded experiments to discuss the implications for increasing response in a two-phase survey setting. ","Differential nonresponse has been noted in address-based sample designs between addresses where a telephone number can be \"matched\" and \"unmatched\" addresses. The latter are correlated with cell phone only, younger adult, black, and Hispanic homes. We focus on approaches used to maximize participation among unmatched homes, using a national study in which commercial and Census Block Group (CBG) data were used to develop indicators of potentially hard-to-reach households. For one test group, addresses in areas with high black penetration, a Hispanic surname, or an 18-34 age indicator were provided a $5 incentive to complete a 10 minute PAPI questionnaire; all others were provided $2. A second group used the same criteria, but only for addresses in CBGs with average incomes below $50,000. In addition, percentage of renters, Spanish speakers, and areas with incomes below $12,000 were included as criteria. Again, those with an indicator received $5, while all others received $2. We report on participation rates and compare which approach produced completed interviews more reflective of Census estimates for hard-to-reach populations.  ","Until 2009, Address-Based Sampling (ABS) frames were restricted to the Computerized Delivery Sequence (CDS) file, which the United States Postal Service (USPS) makes available through licensing agreements with qualified vendors. Research based on the CDS has found the coverage of ABS frames for in-person surveys to be sufficient in urban areas but problematic in rural areas. In 2009, the USPS made available the No-Stat file, a supplement to the CDS file that contains approximately 8 million predominately rural addresses not found on the CDS. As such, the No-Stat file has the potential to significantly improve rural coverage of ABS frames.    ","Probability sample selection procedures gift methodologists with quite a bit of control before data collection; the \"optimal\" design for a given frame ensures that the selected sample is representative. This situation can change after data collection. Not all sample units respond and those that do will not always provide data on every questioned characteristic, which can lead to biased estimates of totals. The degree of bias is determined by several factors, including the representativeness of the respondent set, the magnitude of the aggregated missing data values, and the effects of improper adjustment procedures. Business surveys are characterized by highly skewed populations, so a large proportion of the estimated totals originate from a small set of cases. When surveys collect totals and associated additive details, the effect of non-response bias can be particularly evident with the ","Current practices in survey research prioritize high response rates as the key indicator of survey quality. Survey organizations often expend large amounts of time and resources to attain responses from more difficult to interview respondents, particularly those that require refusal conversion, tracking or are otherwise hard to reach. Longitudinal surveys face the additional challenge of maintaining a high response rate over multiple rounds in order to minimize the effects of attrition. In this paper, we use contact record paradata from the 2009 cohort of the Medical Expenditure Panel Survey in order to construct three scenarios that simulate the effects of restricting recruitment effort spent on locating, contacting and obtaining cooperation from respondents respectively. For each of these scenarios, we examine differences in the pattern of attrition over the course of the survey and the changes that occur in estimates of key indicators as a result. For this survey, we find that restricting contacting effort results in the largest increase in overall attrition. Reducing effort put toward locating and contacting are found to produce sizeable changes in the resulting estimates. Reducing refusal conversion effort produces a comparable level of additional nonresponse to locating effort, however the impact on estimates is minimal. ","Units which have not responded to a survey are generally subject follow up. The survey statistician must determine which units are given priority for contact. In this presentation, we use the conditional bias of unit as a tool to construct a scoring system, which will maximise the efficiency of estimators adjusted for nonresponse. The conditional bias of a unit can be seen as its contribution to the nonresponse error. The results of a simulation study will be presented.  ","This paper provides an overview of methods of adjusting statistical analyses for record linkage error.  It also provides new examples of the severe errors that can occur in statistical analyses when there is record linkage error. ","Record linkage computation under Fellegi-Sunter theory depends on the values of conditional pattern probabilities.  Using good parameter values tailored to the given data set can significantly reduce the number of record pairs requiring clerical review.  Under the na\u00efve Bayes (conditional independence) model, these parameters can be computed using the EM algorithm using pattern counts as input data, without using training data.  Alternatively we consider using a Bayesian model and MCMC  to compute a distribution of the parameters also without using training data.  We compare the results to the EM algorithm output.   ","Record linkage is a valuable tool for combining information from different data sources.  The National Center for Health Statistics has developed a record linkage program to link the center's population-based surveys with administrative data, including Medicare.  However, not all survey participants provide key information for record linkage.  In addition, for Medicare linkages, data are available for the Fee-for-Service program, but less consistently available for the managed care programs, such as Medicare Advantage. In this talk we discuss multiple imputation of missing data in linked National Health Interview Survey (NHIS)-Medicare files.  We study mammography status based on Medicare claims for women 65 years and older.  In our study, mammography and Medicare Advantage status are missing for NHIS respondents not linked to Medicare; and mammography status is missing for some linked respondents who have Medicare Advantage coverage.  To address both issues, we first impute mammography status for unlinked respondents; then we impute Medicare Advantage status based on the imputed mammography status.  We conduct simulations and apply our method to the linked NHIS-Medicare files. ","The Census Bureau's Person Identification Validation System (PVS) assigns unique person identifiers to federal, commercial, and survey data to facilitate linkages across files. PVS uses probabilistic matching to assign a unique Census Bureau identifier for each person. This paper presents a method to measure the false match rate in PVS following the approach of Belin and Rubin (1995).     ","For each pair of records from two files, the Fellegi-Sunter model for record linkage provides a matching weight or score that is an aggregate of similarity scores for the fields being compared.  The matching weight is the likelihood conditional on (typically unobserved) match status. In this paper, I focus on accurate estimates of these conditional probabilities.  The probabilities themselves can vary greatly by data source and are heavily dependent on typographical error in the quasi-identifying fields such as name, address and date-of-birth.  In the Decennial Census, computing these probabilities automatically without training data in more than 400 contiguous regions yields increased accuracy of matching and reduces clerical review by as much as 2/3.  In this talk I will discuss the probability models that have been applied in this setting. In particular, I will discuss EM fitting with little or no training data and possibly alternative models that may not be as 'optimal' but can still work well. ","A screening procedure for evaluating and ranking the small-area variability of binary outcomes from the National Health Interview Survey is presented. It is created to facilitate the process of efficiently allocating resources for the development and estimation of small-area statistical models. The procedure is developed within a Bayesian framework and is examined under different sampling and prior distribution assumptions. Posterior means and standard deviations for the small-area variability of several health outcomes are numerically computed under different model assumptions, and posterior probabilities compare the outcome variability rankings. The performance of the screening procedure is evaluated with computer simulations.   ","Direct estimates for small areas or subpopulations may not be reliable because of small sample sizes for such objects. Procedures based on implicit or explicit models have been used to construct better estimates for given small areas, by exploiting auxiliary information. In this paper we consider binary responses, and investigate different cases associated with different amounts of available information. We use generalized linear mixed models and present bias and mean squared error results for different prediction methods. ","In Gilary, Maples, and Slud (2012), we compared three possible models for small  proportions in small survey domains: Fay-Herriot (Fay and Herriot 1979), GLMM  (Jiang and Lahiri 2006), and Beta-Binomial (Prentice 1986). The comparisons used  bootstrap-based condence intervals which were justied by asymptotic theory or  established only for large samples. Here we build upon that work by conducting  simulations of small area data in moderate-sample settings, for two purposes: to  evaluate the performance of dierent analysis methods when using each of the three  simulation models; and to assess the validity of predictors and Wald-type condence  interval coverage properties for each method.    ","The National Crime Victimization Survey (NCVS) has provided annual estimates of criminal victimizations for four decades, almost exclusively focusing on national results. Previous papers have described an effort to develop small area approaches to produce estimates for states, and for large counties and cities. In this paper, we report the results from this effort and compare the estimates to the ongoing Uniform Crime Report (UCR) of the FBI. We describe methodological refinements to the methods for time-series modeling proposed by Rao and Yu. We also summarize substantive implications of the new results for understanding the geographic distribution of crime in the United States. ","When competing small area models are proposed for a particular set of estimates, they should be evaluated and compared not only by large-sample theory but also by their small-sample properties on real or realistic data. As an alternative to evaluations on data simulated directly from simple parametric models, we evaluate models by a simulation study tailored more closely to the source data that will be used in production. We use the 2007-2011 American Community Survey (ACS) 5-year unit-level sample data as a universe, which is likely to account for relationships among variables and other complexities that may not be reflected in a purely model-based artificial population. We then sample the population repeatedly with a design that mimics the ACS sampling design. In that sense this is a \"design-based\" simulation, although the simulation's response mode and unit nonresponse behavior are model-based, and item nonresponse is not yet implemented. This simulation framework allows for comparison of different statistical inference approaches, with no method being inherently favored over others. Possible future improvements and potential drawbacks of this approach are also discussed. ","The U.S. Census Bureau's SAIPE (Small Area Income and Poverty Estimates)  program estimates poverty for various age groups for states, counties, and  school districts of the U.S.  We focus here on poverty estimates of school-aged  (5-17) children for counties. The corresponding SAIPE production model  applies to log transformed direct survey estimates for each county of the  number of 5-17 year-olds in poverty, with logged covariates obtained from  tabulations of administrative record sources (e.g., tax return data) and  a previous census (2000) long form estimate. We explore an alternative model  assuming a binomial distribution for rescaled survey estimates of the number  of school-aged children in poverty, with an effective sample size defined so  the variances of the binomial proportions equal the corresponding sampling  variance estimates. The model assumes a normal distribution for logits of  the underlying county poverty rates, with a mean function using logit  covariates derived from the covariates of the production SAIPE model, and  with additive random effects. We apply a bivariate version of this model to  direct estimates from the American Community Survey (ACS) for 2011 and  the previous 5-year (2006-2010) ACS estimates.   ","Assuming known auxiliary variable domain means model-based estimator of domain mean of estimation variable is derived as regression-synthetic estimator. In the case of one auxiliary variable weighted least squares estimator of domain mean of estimation variable is ratio-synthetic estimator,which is design-consistent and BLUE; it is extended to post-strata. Assuming known x-total and y-values of units in complementary domain to be zero, model-based estimator of domain y-total is design-consistent and BLUE. It is extended to post-strata and reduced to synthetic estimator with ratio-adjustment, by making well-known assumption of synthetic estimation within each post-stratum, and then to ratio-synthetic estimator by a simple substitution. Under the model the assumption in ratio-synthetic estimation is the same as in synthetic estimation; it is referred to as borrowing strength for the domain from the population. The model is extended as a mixed model to derive BLUP estimator as extension of  ratio-synthetic estimator assuming sampling of clusters of households and unequal error variances. The model with equal variances for units is obtained as a particular case of the mixed model.  ","Rapid-response surveillance of influenza vaccination rates involves collection of weekly survey data, typically based on relatively small sample sizes yielding relatively high variability. Such variability also adversely affects the stability of estimates across time, the result being estimated trends that may show occasional declines, even though the true population trends are by definition non-decreasing. Composite estimation, combining data across time periods, offers the opportunity for more stable estimates of coverage levels and trends as well as estimated trends less subject to period-to-period decreases. Use of survival analysis techniques is another alternative that ensures non-decreasing estimated trends. This presentation will profile variability associated with direct estimates of levels and trends associated with the influenza module of the National Immunization Survey, propose a composite estimation and a survival analysis approach for combining data across time, assess the variability associated with composite and survival estimates of weekly influenza vaccination rates, and discuss potential error associated with use of data collected in different survey periods. ","Weighting adjustment for panel nonresponse needs to incorporate information about nonrespondents collected in the earlier waves of the panel. We propose a cross-classified method for panel surveys with complex sampling designs by first grouping respondents and nonrespondents with similar estimated response propensities to form response propensity strata and then cross-classifying the propensity strata with design variables. Simulation shows that when design variables are not related to nonresponse, the cross-classified method yields survey estimates that have bias and root mean squared error similar to the estimates weighted by reciprocals of the response propensities and the response propensity stratification method. When design variables are related to nonresponse, the cross-classified method yields estimates with smaller bias than the other two methods if design variables are not included as covariates in the response propensity regression, but is comparable in the bias if the response propensity model is correctly specified. We apply these methods to the Galveston Bay Recovery Study, a panel study of trajectories of wellness in a community following a disaster. ","The nonresponse bias of average weekly earnings in the Bureau of Labor Statistics' Current Employment Statistics (CES) survey is assessed. The impetus for this study is the low response rate for hours and earnings data in the CES survey, a longitudinal survey of business establishments, that provides monthly estimates of employment and average weekly earnings, among other statistics. Although we cannot produce a theoretical bias and do not have \"true\" figures of average weekly earnings, we can assess the direction and relative magnitude of bias by comparing CES employment and earnings to employment and wages of the Quarterly Census of Employment and Wages (QCEW) program. The QCEW program collects employment and wages from employers covered under the States' Unemployment Insurance (UI) tax systems on a quarterly basis. Records of the QCEW are used as the frame for the CES survey and QCEW employment data are independent population controls for CES employment figures on an annual basis. ","The National Survey on Drug Use and Health (NSDUH), an annual survey of the U.S. civilian, noninstitutionalized population aged 12 or older, is a major source of substance use and mental illness data.  The 2010 estimates were produced using weights poststratified to 2010 population control totals (intercensal estimates)  derived from the 2000 decennial census; however, the 2011 estimates were produced using weights poststratified to 2011 population control totals derived from the 2010 decennial census. This study was done to determine whether the change in the source of the control totals had an effect on the level of change observed between the 2010 and the 2011 estimates. To examine this \"census effect,\" 2010 estimates were also produced using weights poststratified to 2010 population control totals derived from the 2010 decennial census, resulting in two sets of weights for use on the 2010 data. NSDUH estimates were compared using both sets of 2010 estimates along with the 2011 estimates. Substance use estimates were more affected by the census effect than were mental illness estimates, and they were more notable for estimated totals compared with rates.  ","The Medical Expenditure Panel Survey Household Component is an annual two year panel survey of Households sponsored by the Agency for Healthcare Research and Quality and conducted by Westat.  The survey collects data on household characteristics, insurance coverage, healthcare use and expenditures.  The survey is conducted in overlapping panels with responding units reporting for five rounds of collection covering a two year period.  The current work investigates options for subsampling for further collection in a way that produces overall unbiased estimates but optimizes estimates for high expenditure cases.  Simulated subsampling was done for four methods of sampling: simple random sampling; probability proportional to size using propensity of high expenditures as size measure; oversampling of high expenditure cases; and stratified sampling with Neyman allocation.  Results of these simulations indicate that if the subsampling is performed at the person level then either stratified sampling or probability proportional to size allocation are viable options.  However, if the subsampling is at the Dwelling Unit level then stratified sampling with Neyman allocation is clearly optimal. ","The National Social Life, Health, and Aging Project (NSHAP) is a longitudinal study of the health of older adults, concentrating on the role of social relationships in the aging process. In 2005 and 2006, NORC and a group of investigators at the University of Chicago interviewed a nationally representative sample of adults aged 57 to 85 for Wave I. In 2010 and 2011, these respondents as well as their spouses or cohabitating romantic partners were interviewed for Wave II.     ","This paper describes the weighting procedures developed for the Medical Monitoring Project (MMP), a nationally representative surveillance system for HIV-infected persons receiving medical care in the U.S. Unlike traditional surveys, MMP collects data on sampled persons from three different data sources: face-to-face interviews, medical record abstractions (MRA), and an extract of data reported to the National HIV Surveillance System. In the 2009 data collection cycle all three datasets were weighted independently for differential probabilities of selection and non-response and adjusted for multiple opportunities to enter the sampling frame(s). In 2010 we improved our weighting process by using a unified classification process for eligibility that uses information from all three datasets and parallel weight adjustments for the three sets of weights. We also used a single estimated population total to harmonize weight sums across the three sources. In addition to describing weighting methods, we describe design variables (strata and clusters) defined for variance estimation applicable to all datasets as well as to an analytic dataset of matched interview and MRA records. ","There has been a series of occasional papers about robust covariate control in the analysis of clinical trials in Statistics in Medicine and other journals. The robust semiparametric and nonparametric methods for statistical inference of estimated effects are fairly easy to apply with 21st century computers, but many prefer to continue using t-tests and confidence intervals based on ordinary least squares (OLS) for outcomes that clearly do not follow normal distributions. Presumably, issues of tradition and communication make it very hard to deflect this inertia. In addition, recent papers have demonstrated that the tests are asymptotically equivalent, and the more complex but less parametric procedures make little difference in practice. However, in the literature, there is not sufficient examination of whether the tests and confidence intervals based on OLS are robust to substantial excess kurtosis, particularly in small sample sizes. This paper indicates through simulation where the boundaries lie for two types of strongly nonnormal outcomes: binary outcomes and compound binary/gamma outcomes. We found that traditional OLS methods work very well down to very small sample sizes for these outcomes.  ","Factorial experiments identify interaction effects, which are crucial for understanding optimal treatment combinations.  However, the multiplicity of effective treatment conditions introduces problems of multiple inference and possible sparsity.  Shrinkage estimation provides a potential solution to both of these problems. We consider the properties of shrinkage estimators for factorial experiments under the randomization-based mode of inference (Neyman 1923).  We characterize bias and precision trade-offs and make recommendations for applied work. ","It is common practice in statistical data analysis to perform data-driven variable selection and derive statistical inference from the resulting model. Such inference enjoys none of the guarantees that classical statistical theory provides for tests and confidence intervals when the model has been chosen a priori. We propose to produce valid \"post-selection inference\" by reducing the problem to one of simultaneous inference and hence suitably widening conventional confidence and retention intervals. Simultaneity is required for all linear functions that arise as coefficient estimates in all submodels. By purchasing \"simultaneity insurance\" for all possible submodels, the resulting post-selection inference is rendered universally valid under all possible model selection procedures. This inference is therefore generally conservative for particular selection procedures, but it is always less conservative than full Scheffe protection. Importantly it does not depend on the truth of the selected submodel, and hence it produces valid inference even in wrong models. We describe the structure of the simultaneous inference problem and give some asymptotic results.  ","It is tempting to use familiar statistical machinery like generalized linear models, generalized estimating equations, and spatial autoregressive models to estimate causal effects using social network data.  However, as some researchers have noted, in many network settings these models are inconsistent or uninterpretable.  I describe three distinct sources of dependence in social network data, explain why network dependence is generally incompatible with the assumptions of the standard models listed above, and give conditions under which the assumptions of generalized linear models and generalized estimating equations will be met even in the presence of network dependence.  I explore the limitations of these methods even when the necessary assumptions are met and describe alternatives that take account of network dependence directly. ","The Fay-Herriot model for small area estimation has been applied to improve the efficiency of teacher value-added estimation. However, constraints such as large amount of missing data in teacher-level characteristics (e.g., qualification, experience, professional developments, prior performance) make it impossible to apply a single best Fay-Herriot model.  Researchers are forced to use a set of working models according to the patterns of observed information.  The working models are often times misspecified since they are entirely determined by availability of data rather than their goodness of fit to the data. This talk presents a model-averaging approach, which uses a jackknife-based weighting strategy adapted from Hansen and Racine (2012, J. of Econometrics, 167, 38-46) to combine multiple working models to form an efficient model-averaging estimator.  This model-averaging approach overcomes the difficulty of comparing working models with different likelihoods, and improves the estimation efficiency for small areas (i.e., teachers with small classrooms).  We applied this approach to estimate the value-added of roughly 600 teachers form a large school district.  ","The Quarterly Financial Report (QFR) program investigates statistical methods for identifying substantial macro-level revisions of the income statement and balance sheet data. Currently, macro-level relative revisions are identified as suspect if the absolute values are above a defined threshold, which is determined by subject matter expertise. In this paper, process control methodologies are explored to detect substantial revisions in the data, specifically p-charts and stair-step charts. The inputs necessary for these control charts are not readily available for revision estimates. As a result, a focus is placed upon estimating the control chart parameters. Once these parameters are developed, various evaluation diagnostics are employed to assess their validity. Finally, the performances of the new and existing revision identification methodologies are compared. ","Statistics Canada is undergoing a redesign of its business surveys. One key component of the new framework is the adaptive selective editing methodology. Using historical and partially collected data, estimates and quality indicators are produced while collection is still underway. Item scores are calculated in order to gauge a unit's impact with regard to the quality indicator. These scores are then aggregated within each collection unit, creating a global unit score. Based on these, decisions regarding selective editing will be made, including producing priority lists for follow-up.   This talk will describe the adaptive selective editing methodology with quality indicators as focal points as well as the strategy proposed to integrate sampling, active collection and selective editing. Empirical results and potential savings in the new integrated business program will be discussed.     ","In 2002, the U.S. Census Bureau began using a modified Hidiroglou-Berthelot (HB) edit for outlier identification to find outlying tabulations in the Geographic Area Series (GAS) reports of the Economic Census. This outlier-detection procedure compares  ratios of tabulations, either of the same item over two time periods (historic ratios) or of two different but related items from the current time period (current cell ratio). The methodology implemented in production was developed by a group of subject matter experts and methodologists from five of the eight trade areas covered by the Economic Census. Seeking to expand the use of this methodology for the 2012 Economic Census, we conducted a feasibility study for the manufacturing, mining and construction sectors to see if they could also use this approach or a further modified version. The data collected by these sectors differ from the service sectors in several meaningful ways, such as the number of the collected items and the correlation between historic ratio pairs. This paper presents the results of our empirical investigation along with our conclusions. ","The National Agricultural Statistics Service (NASS) is introducing an automated system for significance editing (SignEdit System) to the production environment for establishment surveys.  The system is integrated with Blaise 4.73 to provide for interactive edit of judgment samples based on the sort order of a unit score assigned to each farm report.  To expedite selection of the judgment sample, methodology for a selective edit threshold is being considered.  The threshold may be vector-valued to support subdomain differences.  The methodology was adapted from a trusted procedure in process control for biotech vaccines into a new procedure for gauging subdomain-level threshold values.  The equivalent of two years process history was used.  Operational weight-adjustment procedures were considered.  We were able to capture intermediate adjustment factors from summary calculations and characterize their relative impact to the unit score.  Methodology for setting and maintaining threshold values is under development for a smaller survey with plans to scale up to production for a larger survey.  Operationalization of any threshold is contingent upon their reliability and performance. ","Record linkage is the process of identifying which records in two or more databases correspond to the same real-world entity. Three major challenges of this process are (1) achieving high linkage quality, (2) scalability to linking very large databases, and (3) protecting the privacy and confidentiality of personal identifying data that are used in the linkage process.    ","Increasingly, administrative data is being used for statistical purposes, for example registry based census taking. In practice, this usually requires linking separate files containing information on the same unit, without revealing the identity of the unit. If the linkage has to be  done without a unique identification number, it is necessary to compare keys which are derived from unit identifiers and which are assumed to be similar. When dealing with large files like census data or population registries, comparing each possible pair of keys of two files is impossible.  Therefore, special algorithms (blocking methods) have to be used to reduce the number of comparisons needed. If the identifiers have to be encrypted due to privacy concerns, the number of available algorithms for blocking is very limited. This paper describes the adoption of a recently introduced algorithm for this problem and its performance for large files.   ","To assess performance of Privacy Preserving Probabilistic Record Linkage (P3RL) with Bloom filter encryption we compared different linkage strategies using real and simulated data.  Methods: Real data (N=4486) from a Cancer Registry were linked to simulated data (N=200'000) based on the telephone registry. Names, first names, date of birth, sex, nationality, marital status, zip code and tumor category were used. The cancer cases were partially included into table B (97.8%). Table B was run through a data generation tool to create missing variables and through an error simulation tool to randomly simulate data errors. Several linkages with and without encryption and pre-processing were performed. We adapted GRLS, the linkage software from Statistics Canada, to handle Bloom filter hash codes and calculate dice-coefficients.  Results: Record linkages with encrypted and unencrypted names gave similar results (in terms of sensitivity and positive predicted value): 99.3% of the cases in table A were found in B with encrypted and pre-processed names.   Discussion: P3RL seems a valid alternative to the use of unencrypted names but needs carefully and locally adapted pre-processing of names. ","In this paper, we adjust the Kuk (1990) model for both protection and efficiency by making use of proportions of two non-sensitive characteristics which are unrelated to the main sensitive characteristic of interest.  The situations  where the proportions of the two non-sensitive characteristics in the population of interest are known and unknown are investigated.  To avoid any confusion, we first briefly explain the Kuk's model. Then we  discuss an adjustment in this model that makes use of two non-sensitive characteristics. We compare the adjusted model and Kuk's model through a simulation study from both the protection and efficiency points of views.  ","In this paper, a notion of \"quasi empirical\" Bayes estimation is proposed for estimating the proportion of sensitive attribute in a population by making use of both a prior distribution of prevalence of the sensitive attribute in addition to the known prior distribution of an unrelated characteristic. The proposed quasi empirical Bayes estimates are compared with those of the unrelated question model due to Greenberg et al. (1969) by means of a simulation study. A quasi Cramer-Rao lower bound of variance is also suggested and compared to the variance of the Greenberg el al. (1969) estimator.   Simulated situations are reported where the proposed lower bound of variance remains below the variance of the Greenberg et al (1969) estimator.   ","One method of protecting confidentiality of tabular data is to apply random perturbation on select variables in the underlying microdata. Perturbation variability needs to be appropriately accounted for in variance estimation for estimates derived from a data file altered through random perturbation. In previous work, we had studied methods for estimating variances using a single perturbed data set, and developed a variance estimator that incorporates a variance component associated with data perturbation. In this paper, we further explore three alternative approaches that can be considered in comparison to the initial estimator, with a goal of increasing the stability of the variance estimation, especially when estimates are extreme. The first alternative modifies the initial estimator through use of multiple perturbed data sets. The second alternative is a limited bootstrap approach that can be done by conducting the perturbation of the bootstrap samples multiple times, producing the replicate estimates, and subsequently computing the variance among the replicate estimates. The third alternative adjusts the initial estimator through the idea of small area estimation. Computational aspects of estimators are discussed. A simulation study was conducted to evaluate and compare the performance of the initial and alternative variance estimators using select variables in two test sites from the American Community Survey 2005-2009 sample data. The results are summarized in terms of the coverage rates and margin of errors of the estimators. ","When statistical agencies release micro data to the public, a major concern is the control of disclosure risk while ensuring utility in the released data.  We consider likelihood based finite sample inference based on synthetic data for three probability models: exponential with an unknown mean, and normal with an unknown mean and either known or unknown variance, leading to some new results and conclusions! A comparison of this analysis with the one under noise multiplication also reveals some interesting features.   ","National statistical institutes have for a long time been aware of the importance of preserving confidentiality of the information provided by respondents. Different rules to decide whether there are any disclosure risks for estimates of, for example, totals are already well developed and they are also widely used. However, there has not been much focus on possible disclosure risks when publishing estimates of functions of totals, such as changes over time. Estimates of changes are often of interest in business surveys and disclosure rules also have to be used for these estimates, to check for possible disclosure risks.  This paper will discuss possible rules for estimating disclosure risks for estimates of changes over time. The performance of some rules are compared on the survey Business cycle statistics for industry, where one parameter of interest is change in industrial production volume. ","The U.S. Bureau of Labor Statistics' Quarterly Census of Employment and Wages program is a virtual census (97%) of employees on nonfarm payrolls, published in fine geographic and industry detail.  To preserve respondent confidentiality, a large proportion (60%) of the tabulated cells is suppressed, which significantly reduces the utility of the data.  In this work, we compare a number of alternative disclosure limitation methods that replace sensitive data with altered or synthetic values.  Among the methods included in the comparison are \"data smearing,\" which replaces each establishment's data with an average from a sample of data from similar establishments in the area; random noise perturbation; averaging by industry; as well as a number of other methods for perturbing or replacing sensitive data.  The methods are applied to establishments of all sizes, but, the comparison focuses on the utility and protection of data from small establishments (fewer than 10 employees) that comprise about 80 percent of the population. ","Why do people hold cash in their wallets? And is the answer the same in different countries? Survey evidence indicates that cash has different uses in different countries, ranging from daily purchases to hedges against bank crises and currency collapses. We study the role of consumer risk perceptions in bounding household cash balances in the United States, Egypt and Mexico. From the standpoint of cash as a payment instrument, there are two principal risks consumers bear with regard to choice of cash balances. When cash balances are too low, consumers face the risk of having insufficient cash for payments where counterparties only accept cash, or for those where cash use confers additional benefits (pricing, privacy, or other social benefits). Conversely, high cash balances bring additional risk of loss. Rather than study the incidence of losses and cash-only transactions directly, we study consumer perceptions of risk from either too high or too low cash balances. We compare the relationships between risk perception and cash balances in several countries, controlling for key financial and demographic traits. ","What is the role of budget and expenditure control in consumers' choice between payment instruments? We analyse this question using a rich set of 2012 data collected by means of an extensive payments diary survey among more than 7,000 Dutch consumers. We add to the existing literature on self-control and payment behaviour in that we distinguish between three types of budget-related needs: (1) the need to have an insight into total expenses, (2) the desire to keep track of the type of expenses, and (3) the need for budget control. Our preliminary results suggest that the desire for budget and expenditure control affects consumers' payment choice. Thanks to the rich dataset we are able to assess differences across individuals. Here we find that the need for control varies with a person's income, financial situation and risk aversion. Second, we present evidence of a significant role of self-control, mental ability and other personal characteristics in consumers' views on what payment instrument provides the best tool for tracking expenses and budgets. Yet, we show that all consumers tend to prefer this particular instrument over another when deciding how to pay.  ","We conducted four experiments and applied the results to design improvements for the Federal Reserve Bank of Boston's Diary of Consumer Payment Choice (DCPC). We asked if diarist payment reporting is affected when using mixed modes to report daily activities. Additionally, we measured diary experience, fatigue, and survey conditioning effects. Finally, we asked if the amount of \"lead time\" between mailing the diary packet and the assigned diary start date changes diarist behavior and reporting accuracy.   The results of these experiments allowed the Boston Fed to make improvements to the DCPC which reduced both costs and administrative overhead. ","In this paper we utilize Bayesian methods to solve a unique missing data problem in the Canadian Financial Monitor (CFM) survey. The CFM is a comprehensive annual survey of the financial attributes of Canadian households, which includes a module on the use of emerging payment methods for retail transactions, e.g. credit cards with a 'contactless' feature. From 2009 to 2010 there was a change in the survey design which made it impossible to distinguish between missing and negative responses to questions regarding the extensive margin of use for these payment methods. This resulted in item non-response rates of 40%-80% for these and related questions in 2010 and 2011. By specifying a probability distribution for the complete data, we first implement a data augmentation routine to impute missing values in the 2009 data. This approach allows us to formulate a prior distribution which we use to solve the more extensive missing data problems in the 2010 and 2011 surveys. We compare our method to other well know imputation procedures by intentionally deleting observed data at random, imputing based on the candidate routines, and comparing the results via the mean squared error statistic. ","A survey of banks can shed light on consumer and business usage of deposit money.  As the payment and financial systems evolve, consumers and businesses are using an increasing variety of methods, such as cash, cheques, cards, and so on to make their payments.  In making their choices, consumers and businesses face different constraints and opportunities, have different preferences, and therefore exhibit different behaviors.  Estimates of national aggregate payment totals from the \"supply side\" are reasonably precise and exhibit stability over time, but reflect an unknown mix of consumer and business payments.  Using a unique survey of a representative sample of U.S. banks that hold deposit money, we explore what we can learn about consumer payments.  To do so, we estimate a model of a bank's payments \"output\" as a function of the \"inputs\" of consumer accounts, business accounts, stock of transaction deposits, stock of money market deposits, and other environmental factors.  Within this framework, we derive separate aggregate consumer and business payment estimates for check, debit card, and ATM withdrawal transactions.   ","This research concerns the adaptation to the Current Population Survey (CPS) of single-stage weight adjustment techniques developed in a 2010 Census research report by Slud and Thibaudeau. Those techniques involved weight optimization with respect to a loss function in the spirit of Deville and Sarndal (1992, JASA), subject to population-control constraints, with additive penalty terms for discrepancies between weight-adjusted survey totals and corresponding known or base-weighted estimated totals for certain survey attributes, and with an additional nonlinear penalty term designed to force weights not to be too different from design weights scaled to the population total. The novel elements of the current research include: defining several appropriate quadratic penalty terms corresponding to the current multistage CPS nonresponse adjustment; developing a methodology to define penalty multipliers by tracking properties of the current CPS weights across weighting stages; enforcing weight compression by a penalty term in place of the current CPS approach based on cell collapsing; and implementing the method on CPS data for detailed comparison with the weights as currently adjusted in CPS. ","A composite estimator is currently used to estimate the monthly total of the employment, unemployment, and other characteristics of the non-institutionalized   civilian population in the The Current Population Survey (CPS), a household sample survey sponsored by the US Bureau of Labor Statistics and conducted  by the US Bureau of the Census. Statistical of the current composite estimator, however, is unknown except that it has a non-negligible bias unless some conditions are satisfied. We study the bias and variance of the composite estimator and develop an adaptive method to find the best values of the tuning parameters in composite estimation. Other issues, such as the use of administrative information and variance estimation, will also be studied. ","We develop a general theory for parametric bootstrap method in the context of a general mixed model and illustrate its usefulness in analyzing a variety of longitudinal complex survey data.  As a special case, we consider the problem of estimating small area characteristics using time series and cross-sectional model that combines data from previous time points of a longitudinal survey and relevant auxiliary variables.  We use our general methodology to demonstrate the flexibility of our parametric bootstrap method to produce highly accurate mean squared error estimates of complex estimators with calibration and winsorization adjustments. ","The work of this paper is prompted by the particular case of the Current Employment Statistics (CES) Survey conducted monthly by the U.S. Bureau of Labor Statistics. Besides estimates at the national level, the survey yields estimates of employment for numerous domains defined by intersection of industry and geography, providing important information about the current status of the local economy. Variances of the employment estimates are estimated from the sample. However, the sample based estimated variances can be unstable, especially in smaller domains.  More stable variance estimates can be obtained using a model-based generalized variance function (GVF). The modeling is based on past years of the survey and, assuming a satisfactory model fit, the result can be applied to predict variances for the current period. However, some features of the design or population characteristics may change from one year to another, making it necessary to adjust the model parameters. We here give a method for evaluating the suitability to current data of a GVF model based on past years' data and suggest ways to calibrate the GVF to the current data. ","Statistics Canada produces high income statistics that provide information on demography, taxation, and income for high income populations. These statistics involve estimation of percentiles and quantities in percentile groups using a sample that covers around 1/5 of the Canadian population. For variance estimation, re-sampling methods may take unacceptable time due to the extremely large sample size; linearization cannot be applied directly because of the non-smoothness of the estimators. In this paper we propose a weighted estimating equations approach to derive linear variance estimators.  Data from Prince Edward Island (P.E.I) is used to illustrate the results obtained. ","Suppose we drew a large stratified simple random sample, then based on information provided from that sample re-stratified the sample and drew a smaller subsample from which a more detailed set of variables was collected.   This is an example of a two-phase sampling design.  Two-phase sampling is becoming an increasingly popular sample-selection procedure.  We will use the WTADJX procedure in SUDAAN 11 to estimate the variance of a reweighting expansion estimator based on data collected under the two-phase design described above.  We will compare our approach to variance estimation with alternative methods sometimes used when software like SUDAAN 11 is not available.  ","Systematic sampling is often used in surveys due to its implementation simplicity and efficiency. Variance estimation for systematic samples remains problematic, however, since no direct design-based estimators are available. Fay (1995) introduced the Successive Difference Replication (SDR) variance estimator, in the context of variance estimation for the Current Population Survey (CPS) conducted by the US Census Bureau. In this presentation, we report on the results of an evaluation of the statistical properties of SDR variance estimator. We compare this estimator with several alternatives often considered for systematic sampling, including those based on two-per-stratum approximations and simple random sampling. ","A set of unweighted normal equations assume that the response variable of each equation is equally reliable and should be treated equally. When there is a reason to expect higher reliability in response variable in some equations, we use weighted least squares (WLS) to give more weights to those equations. For an analysis of a survey data, sampling weights, as a relative importance variable, should be used for unbiased and efficient estimates. We will briefly go over the least squares theory and related issues, and propose a specific form of \"weight\" variable when we apply the sampling weights to the normal equations. The National Health and Nutrition Examination Survey (NHANES), a periodic survey conducted by the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC) will be analyzed to demonstrate the proposed approach.  ","The usage of traditional design-based methods for complex-survey data often leads to estimation difficulties or unreliable analyses when sample sizes are not sufficiently \"large\" at some level of multi-stage sampling. In these situations model-based estimation methods are often suggested as alternatives to compensate for data deficiencies. For this study, we focus on both design- and model-based statistical inference based on a sample of ~1000 households taken from a reduced-scale pseudo U.S. population that captures many features of geographical and household clustering within the true U.S. population. This pseudo population was developed from nine years of the National Health Interview Survey (NHIS) data. A simulation study is performed on this pseudo-universe where we imposed a complex design including multilevel sampling from strata, primary clusters, secondary clusters, households, and persons along with post-stratification weighting adjustments. Sampling properties of design-based regression estimators (Binder 1983) and multi-level model-based regression estimators are compared. ","Creel surveys are used in recreational fishing to estimate angling effort, catch and harvest. One such type of survey is the aerial-access creel survey, which relies on two components: 1. A ground component in which fishing parties returning from their trips are interviewed at some access-points of the fishery; 2. An aerial component in which the number of fishing parties is counted. The aerial survey is commonly done at a peak of activity rather than at a random time of the day. A common practice is to sample less aerial survey days than ground survey days. This is thought by practitioners to reduce the cost of the survey, but there is a lack of sound statistical methodology for this case. In this paper, we propose various estimation methods to handle this situation and evaluate their asymptotic properties from a design-based perspective. Also, we propose formulas for the optimal allocation of the effort between the ground and the aerial portion of the survey. A simulation study investigates the performance of the estimators. Finally, we apply our methods to data from the annual Kootenay Lake survey (Canada) and propose an optimal sampling strategy for future years. ","Large-scale sample designs generally use auxiliary information obtained through frames or other data sources.  In some cases, limitations on this information may reduce the efficiency of the resulting sample design.  For example, in designs that base selection probabilities on measures of unit size, the size measures may be subject to measurement errors, or may be unavailable for some population units.  For such cases, one may view standard \"probability proportional to size\" designs as approximations to nominally optimal designs that could be produced if one knew the mean function,  E(  Y | Z ), and variance function, V( Y | Z ), of a survey variable  Y, conditional on auxiliary data Z, which may include the imperfect size measures.   Properties of the resulting designs will depend on (1) the precision with which  E(  Y | Z ) and V( Y | Z ) are known; (2) the extent to which these conditional moments display similar patterns across different survey variables Y; and (3) information on (1) and (2) provided through preliminary empirical studies.  Following a brief review of some theory and literature, this paper explores issues (1), (2) and (3) through a detailed simulation study. ","Targeting cellular respondents in specific areas is a difficult process.  Unlike landline RDD samples, cellular numbers are not tied to geographic areas and may reach individuals outside of designated sample areas.  Marketing Systems Group has developed a methodology for constructing cellular sampling frames using rate centers.  A rate center delineates the local call boundaries set by service providers for billing purposes.  As such, subsets of 1000-series blocks of cellular numbers assigned to specific wireless service providers can be mapped to geographic areas of interest for targeting purposes.  The goal of this research is to determine the degree to which rate centers can accurately identify eligible respondents in BRFSS geographically defined areas.  For this purpose, the frame locations for cellular RDD samples from a number of counties are compared to those verified by respondents as part of the BRFSS survey administration.  Results suggest that geographic targeting of cellular respondents in more populated areas is more precise, and that the size of the area of interest can affect the reliability of targeting as well. ","The 1960s, with the election of John Fitzgerald Kennedy, signaled a new era of social activism in America. Many efforts planned during the Great Depression but put off by WWII were waiting in the wings and some got a new start.      Those that emerged in the early 60s were sponsored by the dominant culture, e.g., efforts to address civil rights plus income, wealth and wellness inequalities. The intellectual leaders of these efforts were heroes (to me) like Mollie Orshansky, Joe Pechman, Dorothy Projector, Dorothy Rice, and Joe Steinberg, among many others.      The work of some of these people will be my brief today. Incidentally, all were at one time employed by the Social Security Administration. Thus, much of what gets said here does not concern only the Federal Reserve Board but also deals with the larger movement that the SCF was a part of and that involves government agencies as disparate as the Census Bureau and the Internal Revenue Service plus such nongovernment organizations as the Institute for Survey Research at the University of Michigan and the National Opinion Research Center at the University of Chicago. ","The triennial Survey of Consumer Finances (SCF) is distinguished as an outstanding source of information on income and balance sheets, demographics, and economic expectations of U.S. households.  It includes representative coverage across the income and wealth spectra, enabling users to study assets that are widely distributed in the economy as well as those that are much more concentrated.   This is made possible by a dual frame sample design that includes both a traditional area probability sample and a list sample that is used to oversample high-wealth households.  Since its institution, the list sample has been developed using administrative data accessed through a partnership between the Federal Reserve Board of Governors (FRB) and the Statistics of Income Division (SOI) of the Internal Revenue Service.  This partnership recognizes the essential contribution of the SCF in helping policymakers understand the impact of current and proposed tax policies on American households.      ","Viewed in a larger context, sometimes the worst problems become the greatest opportunities for learning.  The Survey of Consumer Finances (SCF) could be seen as a methodological nightmare.  Among other things, the survey must describe the highly skewed distribution of wealth, the subject of the survey is generally considered sensitive, the questions are necessarily at least somewhat technical, financial literacy varies across households, understanding of and attention to language more generally may also vary, the interview is long, and interviewers may vary in the extent to which they understand and follow the intended protocols; the joint consequence is that both unit and item nonresponse and other sources of nonsampling error have been important problems.  There is an obvious immediate need to cope with such \"emergency room\" problems in order to have data for analysis, but for the long term it is more important to frame resolution in terms of steps toward prevention and other \"public health\" measures.  Such reframing can lead to continuous improvement. ","Since the Survey of Consumer Finances delves into every financial detail of a household's finances, maximizing data quality is a constant challenge.  NORC designed and implemented a refined data quality process and review cycle.  All interviewer commentary including call record entries, interviewer comments recorded during questionnaire administration and interviewer debriefing notes recorded post-interview are reviewed for potential quality issues related to respondent identification and questionnaire administration. Along with FRB staff, the NORC team evaluates the questionnaire data and identifies potential errors and anomalies that require follow-up. These evaluations are incorporated into a timely and systematic feedback loop delivered to interviewers. This feedback triggers remedial actions designed to address quality deficiencies (e.g., broadcast memos regarding proper protocol, self-directed electronic training, and supervisor-led training) and provides an on-going assessment of interviewer performance. We will describe the processes used to identify data quality issues, our data quality improvement protocols, and data quality measures over time. ","The Survey of Consumer Finances (SCF) has been an indispensable tool for policy analysis for the Federal Reserve Board since the first modern SCF was fielded in 1983.  The breadth and scope of the data in the SCF, combined with the unique sample design that provides unparalleled coverage of the top of the wealth distribution, allows policy makers to better understand the evolution of household balance sheets and household financial behavior.  Our paper will provide an overview and examples of the key areas where the SCF provides important information to policy makers.  In addition to the well-known estimates of household wealth and measures of wealth inequality, the SCF is used to study a variety of household finance issues, such as portfolio composition, financial vulnerability, pension coverage, education loans, savings behavior and to analyze changes to household balance sheets.  The SCF is also used to study many other issues, including house price indices, financial institutions, tax policies, deposit insurance, and bank market structure. ","The decrease in coverage for traditional random digit dialing (RDD) samples is well documented (e.g., Blumberg et al. 2011). The decline in landline connections, particularly for young people, makes coverage especially problematic (Keeter et al. 2007). Although mobile phones can be added to landline sample frames to increase coverage, this dual frame approach introduces challenges, as they are more prone to nonsampling errors in comparison to RDD and are often counted against the respondent's minutes (Brick et al.  2011). Non-probability Web-based supplements have been suggested as a means to reducing problems with RDD coverage and picking up cell-only households without respondent-side costs.  However, this brings new questions.  First, do we find cell-only households among non-probability Web samples?  Second, how do Web-based results differ from national level random sample results?  Third, how demographically different are the samples from mode varying probability samples? In this paper, we present an analysis of a series of Google Consumer Survey questions including home cell-phone usage and compare the results to those from three national-level random sample surveys.  ","When conducting telephone surveys in the modern landscape ascertaining the telephone status (e.g., wireless only, wireless mostly, or landline mostly) of survey respondents is important for computing weights.  However, the dominant items used to assess telephone status have not been fully tested for reliability and validity (AAPOR, 2010). Indeed, previous  cognitive testing (Welch, et al, 2012) demonstrates that respondents have widely varying ideas about the meaning of the qualifiers 'some', 'most', and 'somewhat' in telephone status items that are currently in the field.  Further, this initial work suggests that respondents prefer quantitative measures.  Our current research extends the qualitative work that was done by implementing an experiment using new response options for the wireless mostly item (Blumberg and Luke, 2012) in a telephone setting in a series of national, general population surveys (N~2,000).  We will explore the effect of implementing the reliability improvements suggested by cognitive testing on the distribution of telephone statuses by age, race/ethnicity,sex, and socioeconomic status.  Implications for future dual-frame RDD surveys will be discussed. ","The first goal of a statistical sampling design is to reproduce the target population in a \"small scale\", with a lot fewer sampling units but still holding the population\u00b4s main characteristics. A random sampling approach followed to build a series of nationwide cell-phone surveys in Mexico is described. The resulting characteristics of the samples obtained are presented and contrasted to the actual target population showing the sample's performance in reproducing the right demographic characteristics as well as the population's geographic distribution. Some important practical advantages of cell phone interviews versus other commonly used methodologies are pointed out. It is concluded that, if taking into account some practical considerations, surveys based on mobile phones can be successfully applied in Mexico to attain nationwide research goals, including those strongly depending on the associated geographic distribution of the sample. ","To meet the strategic goals and objectives for the 2020 Census, the United States Census Bureau must make fundamental changes to the design, implementation, and management of the decennial census.  The Census Bureau designed the 2013 National Census Contact Test (NCCT) to collect data from household members in order to validate landline and cell phone numbers as well as email addresses associated with the household's address that we had obtained from multiple commercial sources.  In January 2013, we conducted telephone interviews with respondents using a web-based Computer Assisted Telephone Interviewing (webCATI) instrument.      In this paper, we present the 2013 NCCT study design, several methods such as validating phone numbers prior to telephone interviews, and some preliminary results that begin to assess the quality of our contact frame.  The ultimate goal of the Census Bureau for this study is to utilize our contact frame using commercial sources to initiate contact with respondents via landline or cell phone and/or email in an overall effort to reduce future census costs while maintaining or improving quality.   ","Dual-frame sampling designs gained popularity in recent years mainly due to the need to rely on landline and mobile frames for telephone surveys. In this context some respondents can be reached via landline phone only, some via mobile phone only and some via both. Knowing selection probabilities, one can calculate design weights for such a design. Yet to control for nonresponse, it is common to assume that response propensities are equal for the same person whether s/he was reached via mobile or landline phones. Recent research shows that respondents reached via mobile phone differ from those reached via landline phone within the 'overlap' group suggesting that response propensities of the same person depend on the frame they are selected through. We propose a method which releases the assumption: we calculate the response propensity for the frame that each respondent was selected through, and separately estimate the response propensity if this respondent had been selected through the other frame. We illustrate our findings using a technically similar situation where frames refer to different time points. ","The National Immunization Survey (NIS) is a nationwide random digit dial (RDD) survey monitoring the vaccination coverage among children of ages 19-35 months. Prior to 2011, NIS used a traditional landline telephone sample design. Beginning in 2011, official vaccination coverage was calculated using a dual-frame landline and cell-phone sample design. Recent estimates from the National Center for Health Statistics indicate that the proportion of children living in landline-only households was 4.5 percent, creating the possibility for a viable single-frame cell-phone sample design for the NIS. To assess optimal NIS sample designs in terms of data quality and cost, this paper compares alternative single-frame and dual-frame sample designs via a simulation study. Specifically, we compare vaccination coverage rates, key data collection indicators, and design effects for the following sample designs: (a) current dual-frame sample design, (b) landline telephone sample supplemented by a sample of the cell-phone-only population, (c) cell-phone sample supplemented by a sample of the landline-only population, and (d) single-frame, cell-phone sample design. ","To counter the burden associated with survey participation and increase participation rates, many surveys offer incentives to participants, but relatively little is known about the efficacy of incentives by geography. We use data from the 2011-2012 National Survey of Children's Health (NSCH), a large population-based survey of parents or guardians of children under 18 years old conducted by the National Center for Health Statistics. The NSCH included over 66,000 landline and 31,000 cell telephone interviews. We examine whether incentives are more effective in certain geographical areas of the U.S. using geographical information tied to landline numbers and area codes associated with cell telephone numbers. The incentive treatment models differed between landline and cell, but both were refusal-based and involved cash payments - some prepaid, some promised. We compare refusal and incentive offer rates, participation rates post incentive offer, and completion rates within sample types to ascertain the effectiveness of incentives by varying geographical factors (e.g., area codes, states, regions), and geographical area types (e.g., urban vs. rural). ","Deville and S\u00e4rndal (JASA 1982) coined the term \"calibration weighting\" to develop and formalize an approach to sample-weight adjustment within a probability-sampling context. The initial goal of calibration weighting was to increase the efficiency of estimated means and totals for outcome variables roughly correlated to one or more of a set of calibration variables with known population totals. Since then, calibration weighting has also been used to reduce or remove selection biases due to unit nonresponse and/or frame errors. In fact, the technique can sometimes be used when nonrespondents are not missing at random, that is to say, when the probability of a unit responding is a function of outcome variables with known values only for the respondents. We will discuss some of the well-known and lesser-known properties of calibration weighting, some recent new developments (and software), and some of the limitation of a technique that ultimately requires respondent samples to be \"sufficiently large.\" ","The National Compensation Survey (NCS) conducted by the Bureau of Labor Statistics (BLS) is an establishment survey which produces as one of its outputs an estimate of the employer costs for employee compensation (ECEC) for establishments in the United States. The survey has been redesigned by reducing sampling stages from three to two, and a new set of sample allocation goals have been derived. This current study is to validate the new allocation goals in comparison with the current sample allocation goals via simulation. The study uses a simulated population dataset, and calculates the population level relative standard errors of the ECEC estimates under the two sets of sample allocation goals using the variance formula for the two-stage PPS design, where the PPS method is used to select establishments at the first stage and to select occupations within the selected establishments at the second stage. This paper discusses these steps and the comparative merits of the two sets of sample allocation goals, demonstrating that the current allocation needs to be revised. ","Traffic crashes are a leading cause of death in the United States, but seat belt use has been shown to save lives and prevent injuries. The National Highway Traffic Safety Administration (NHTSA) requires every state and territory to conduct a survey to monitor seat belt use. NHTSA issued the new Uniform Criteria for State Observational Surveys of Seat Belt Use on April 1, 2011 which the states are required to follow for their study design. These criteria require the states to update survey methods and select observation sites that are representative of the state's road segments in a cost-effective manner. Westat reviewed state study design plans for compliance with the operational, GIS, and statistical requirements of the criteria. We present the tools and process used for the reviews, as well as some observations and future recommendations on the statistical requirements. ","The National Health Interview Survey (NHIS) is a continuous survey that has collected health data using personal interviews since 1957. The NHIS sample design is complex, with multiple stages of sampling, beginning with primary sampling units (PSU) and then additional stages of sampling within each PSU to obtain a sample of addresses. The current NHIS sample design period (2006-2015) has a state-level stratification, but only a limited number of states have adequate sample to support reliable estimation. Recently, additional funding became available for sample expansion in select states. The NHIS has a time-static PSU selection procedure that keeps the first-stage sample areas fixed over the length of a survey sample design period. To increase sampling efficiency, we decided to put resources into expanding the number of sampled PSUs rather than only expanding the within-PSU samples. The preferred method was to extend the probability proportionate to size (PPS) procedure that occurred at the beginning of the current design so that the expansion probabilities also remained PPS. In this paper we discuss issues that arise in such an expansion. ","The National Household Food Acquisition and Purchase Survey (FoodAPS) employed two strategies to efficiently oversample households receiving U.S. Supplemental Nutrition Assistance Program (SNAP) benefits and other low income households: (1) composite measures of size (MOS) in a multi-stage sampling design; (2)use of a hybrid sampling frame at the penultimate level of sampling. FoodAPS was fielded in 2012 and collected data from nearly 5,000 households. The sample used a three-stage design, where primary and secondary selection units (PSUs and SSUs) were selected using composite MOS that reflected the projected prevalence of and sampling rate for each of four target groups: SNAP households and three income-defined strata of households not receiving SNAP. Within SSUs the study employed a hybrid frame approach: addresses from SNAP administrative records were merged with addresses from a commercial Address Based Sampling (ABS) frame. The paper will review all phases of sampling: how the composite size measures were constructed; how the two frames were used within SSUs. In addition the paper will attempt to evaluate the impacts of the approach on analysis weights and sampling errors. ","This paper summarizes the main considerations and results of the studies conducted in the Instituto Nacional de Estad\u00edstica y Censos of Argentina, to evaluate the impact that the change in the methodology of selection of dwellings within each area (Secondary Sampling Unit) could have in the variances of certain estimators due to the passage from the method of selection of dwellings by systematic sampling to the method of selection of compact segments, in the household surveys. The results of a simulation exercise with microdata files from the 2010 Population Census are presented, comparing the different results obtained in relation to estimators for several population totals and their corresponding variances. It has been proven that going from systematic sampling to cluster sampling, during the last stage of selection, does not conduct to a significant increase in sampling errors, having therefore cluster sampling evident benefits in the organization of fieldwork.  ","The Company Organization Survey is conducted annually between economic census years to collect data on multi-unit companies operating in the United States. The non-probability sample consists of certainty multi-unit companies, single-unit companies suspected of being multi-unit companies, and a targeted sample of multi-unit companies. Since organizational change is likely for larger companies, multi-unit companies with 250 or more employees are selected for the sample with certainty, and single-unit companies that meet certain criteria are selected. The targeted sample aims to maximize the number of multi-unit companies with less than 250 employees with organizational changes. Organizational change occurs when at least one of a company's units is opened, closed, moved, or sold. This research focuses on revising the sample for multi-unit companies. A logistic regression model was used to predict the likelihood of an organizational change. Results suggest that an improved sample could be obtained when multi-unit companies with more than 500 employees are selected with certainty, and companies with less than 500 employees are targeted by their likelihood of organizational change. ","Indirect sampling extends the application of sampling theory for settings when the sampling frame and target population are not identical, but rather connected through specified links. This approach has found widespread application, particularly through the generalized weight share method (GWSM) and can be applied generally to sample from networks.    ","The US Energy Information Administration (EIA) uses electric power plant operations and maintenance (O&amp;M) costs in the National Energy Modeling System (NEMS) for purposes of dispatch and retirement decisions.  EIA collects power plant characteristics on the Form EIA-860. Power plant cost data are collected by two other governmental agencies:  the US Department of Agriculture (USDA) and the Federal Energy Regulatory Commission (FERC).  This session presents the challenges that EIA faced when cross-referencing power plant characteristic data to the power plant cost data, in order to generate meaningful O&amp;M costs for input to the NEMS model.  EIA used a multi-tiered approach to matching plant-owner-level cost data to unit-level generator data.  EIA's method was to (1) determine which power plants in EIA databases were represented in FERC and USDA data, (2) determine which units at the plants were represented by the cost data, and (3) apply cost data only to the generator capacity and generation that respondents owned.  In the absence of common identifiers, EIA relied on clues in the cost data to determine which plants and associated units matched the EIA data. ","EIA's monthly natural gas survey collects volume and revenue data that support the estimates of state level volumes and prices published in EIA's Natural Gas Monthly. This paper and presentation focus on research over a nearly ten year period to improve imputations for volumes that are either not reported or of questionable quality. EIA considered (1) incorporating local heating degree data to improve predictions of distributor level volume, (2) developing multiple measures of success, (3) incorporating variables not included in earlier research, (4) identifying improvements likely to sustain themselves over the long term, and (5) testing novel ways of predicting the sales and transportation components of volume. While no procedure appeared superior after validation testing on 2011 data, a useful by-product of the research was linking the current sample selection code into the imputation code for the monthly natural gas survey.   ","Electricity transmission and distribution systems are increasingly moving towards the use of a \"Smart Grid\", a densely instrumented network for electricity delivery that facilitates remote control, automation, and feedback to consumers.  This evolution introduces new challenges in modeling and forecasting electricity usage data.  We describe a model of the daily load curve for residential electricity consumption that includes the effects of dynamic price incentives on the demand response. The model represents the observed values of the daily load curve by a set of periodic smoothing-spline basis functions, with the coefficients of the basis functions evolving according to a linear Gaussian state-space model that incorporates mean shifts, day-of-week and holiday adjustments, temperature effects, as well as the dynamic price incentive effects.  This modeling and forecasting methodology provides the ability to model intraday load substitution effects that are induced by the specified dynamic pricing schedules, the ability to use fine-grained (5 to 15 minute interval) observational data and enables fast updating of model forecasts as new usage and weather data arrive. ","Power plant operations and maintenance (O&amp;M) costs reflect the competition ability of electricity providers to economically generate electricity and the O&amp;M costs impact the dispatch decisions to meet the electricity demand. Multiple boosted regression models are constructed to examine the marginal effects of power plant characteristics and operational status on their O&amp;M costs and to estimate the non-samples. We examine the O&amp;M cost data from the regulated electricity providers collected by FERC and from the rural electricity cooperatives collected by USDA. We find that different fuel types and technologies have apparent different O&amp;M cost behaviors and separate models are necessary for different subgroups. One challenge that we encounter is that the data used for model construction is at an aggregated level and the outcome to be estimated is at an individual generating unit level. In order to ensure that the models are applicable to unit-level estimations, we validate the models with the same aggregate data used for model construction but at a unit level. The results show that although the model fitted values tend to underestimate the costs, the unit estimates are less biased. ","Prior research has shown that the same groups that are sought in screening interviews are often seemingly underrepresented in the screening data.  This underrepresentation may be a product of deliberate misreporting by the screener respondents (or motivated underreporting) or may reflect interviewer motivation to deal with potentially difficult respondents by encouraging them to screen out.  This paper reviews the results of an earlier study in which screening interviews were administered by telephone and of a new study in which screening interviews were done face-to-face.   In both studies, different versions of the screening questionnaire were administered.  The different versions of the screener varied in whether they used a direct screening question (\"Is anyone who lives here between 35 and 55 years old?\") or collected several characteristics for each household member (including their ages).   The more detailed screening questions reduced coverage errors but also increased the nonresponse rate in the telephone study.   We examine data from the new face-to-face experiment to see whether this same tradeoff is apparent when the screeners are done in person.  ","Common wisdom is that the length of the interview has a negative effect on data quality. One manifestation of this is that respondents may exert less cognitive toward the end of an interview. Despite the logical nature of this assertion, there is very little empirical evidence of this phenomena. This study investigates this hypothesis by comparing answers to global questions that were randomly placed either at the beginning or at the end of a telephone survey. The survey was an adapted version of the Consumer Expenditure Quarterly interview survey (CEQ), which collects quarterly expenditures for a consumer unit (CU). The interview has a series of interleafed global questions on expenditures for the CU. This survey averaged approximately 50 minutes. The research was sponsored by the Bureau of Labor Statistics and carried during the 2012 Practicum at the University of Maryland's Joint Program for Survey Methodology. A total of 677 CU's were interviewed. The presentation will report on analyses comparing affirmative answers to global questions by section placement and whether any differences vary by characteristics of the respondent and by interviewer. ","To avoid asking respondents questions that do not apply to them, surveys very often use filter questions to determine routing into follow up items. Filter questions can be asked in an interleafed format, in which follow up questions are asked immediately after each relevant filter, or a grouped format, in which follow-up questions are asked only after multiple filters have been administered. Most previous investigations of such questions have found that the grouped format collects more affirmative answers to the filter questions than the interleafed format. However, there are several other factors which can also affect the responses to the filter questions, such as the repetitiveness of the follow up items, the number of follow up items, and how the questions are displayed in web surveys. Using results from two telephone surveys and two web surveys, as well as linked administrative data, we provide evidence-based guidance on how these questions should be asked in the two modes. ","If interviewers are paid per interview completed, they have strong incentives to shorten the interview for instance by answering filter questions so as to skip lengthy follow-up questions. If many interviewers show such \"rational\" behavior data quality will be damaged seriously. We provide evidence from a large german panel, that such rational interviewer behavior is indeed widespread. We find that ego-centered networks generated in waves 2 and 4 of the Family Panel are surprisingly small. We also find large interviewer effects which cannot be explained by interviewer or respondent characteristics. We try to identify those interviewers who are particularly responsible for the large interviewer effects by using the jack-knife. We identify three groups of interviewers: Those who elicit very small networks (\"fraudulent\" interviewers), those who generate particularly large networks (\"diligent\" inter-viewers), and the rest. Further we show that only the diligent interviewers provide networks of a reasonable size. In addition, substantial results obtained with network data from normal/fraudulent interviewers differ from those obtained with data from the diligent interviewers. ","There is evidence that survey interviewers may be tempted to edit answers to filter questions in a way that minimizes the number of follow-up questions. This is relevant when data on ego-centered networks are collected in a survey. The number of persons reported on a network generator question can have a huge impact on subsequent interview duration if multiple follow-up questions on each alter are asked. We analyze interviewer effects on a network size filter question in the third wave of the German PASS panel. PASS is a mixed-mode panel survey using CATI as well as CAPI interviews. Applying multilevel models we find no interviewer effects in CATI mode where interviewers were paid by the hour and frequently supervised. In CAPI, however, where interviewers were paid by case and no close supervision is possible, incentives to cheat are considerable. Here we find a strong interviewer effect on network size. As the area-specific network size is known from telephone mode (where allocation to interviewers is random), interviewer effects and area effects can be separated.   ","Social science researchers increasingly make use of data that is confidential because it contains linkages to the identities of people, corporations, etc. The value of this data lies in the ability to join the identifiable entities with external data such as genome data, geospatial information, and the like. However, the confidentiality of this data is a barrier to its utility and curation, making it difficult to fulfill US federal data management mandates and interfering with basic scholarly practices such as validation and reuse of existing results. We describe the complexity of the relationships among data that span a public and private divide. We then describe our work on the CED2AR prototype, a first step in providing researchers with a tool that spans this divide and makes it possible for them to search, access, and cite that data. ","The generating process of administrative data is in most cases quite different to the generation of surveys data. Admin data are not generated for researchers needs; it is usually created in the workflow of admin processes. Most of the data are quite huge and there is little knowledge about the data quality. In addition there is a lack of knowledge on the researchers side, how to handle and analyze admin data. Due to a high disclosure risk access to admin data is restricted and in most of the cases only possible through research data centers (RDCs).  According to that RDCs have to deal with this situation by finding (new) solutions for generating an environment, which supports researchers. Thereby the environment must support the whole data life cycle from data generating to the archiving of data, programs and results. To enable replication studies this environment has to include several dimensions like identifiers for the data, archiving solutions and tools for replication including data documentation, program and knowledge sharing and so on.   We will show new developments in a German RDC, which try to fulfill researchers' and replication standard needs.   ","More and more studies are collecting detailed individual information and respondent histories.   These developments have increased the risks that subjects can be identified through deductive disclosure.   ICPSR, which is the world's largest repository of social science data, has been active in developing procedures for evaluating and safely sharing data that pose disclosure risks.   Although most data at ICPSR are available without restrictions, a growing number of studies are only available under a data use agreement.  We will describe key aspects of ICPSR processes including disclosure review, data use agreements, security plans and frontiers for providing access to these types of data as well as the legal and institutional frameworks governing access to restricted-use data.  The presentation will emphasize areas that are most problematic (e.g. data protection plans) and new technologies, such as virtual  analysis systems.   ","Access to potentially identifiable data are typically restricted: fortunately, secure access mechanisms which protect the identity of data subjects are now well established. However, standards for producing properly curated data and metadata are far from satisfactory applied. Long-term consideration of this issue is essential, not only for replicability of results, but also for users of data with long-term research interests. Data custodians with responsibilities for identifiable data are more willing to concentrate on the potential effects of the disclosure of such data without considering the long term consequences of cost implications, lost research and even lost data. This paper will argue 1) that data custodians should particularly consider long-term curation of data collections which are potentially identifiable as a key responsibility, 2) that data access providers can generally follow well-established principles with restricted-access data but with additional requirements for information security and 3) that secure access methods improve the levels of replicability which are possible within data which is not identifiable. ","In public health surveillance, surveys designed to estimate the prevalence of an outcome are often complex, including design features such as stratification, clustering, and/or disproportionate weighting. As a result, variance and confidence interval estimation can be challenging. Standard methods for constructing confidence intervals for a proportion include variance approximation methods used with a Wald-type interval and replication-based methods. Alternative methods include adaptations to the Wilson confidence interval. We evaluate these methods in a variety of settings, including low prevalence and small sample size settings. We make recommendations for which methods to use when estimating confidence intervals for proportions in different types of complex surveys. We use our findings to inform the design and analysis of an HIV drug resistance surveillance system for low- and middle-income countries. "," In sample surveys variance generally decomposes into the sum of two  terms: variance among primary units and variance within primary units.  If the secondary units are composite, variance within a primary unit  can be decomposed into variance among secondary units and variance within  secondary units. This can be extended as necessary. This decomposition  is the central idea in stratified sampling, cluster sampling, regression  and ratio estimation, and ANOVA. It can also be applied to cases where   the primary units overlap.    ","Randomized construction of population units is combined with randomized sampling of these population units to provide a more complete description of the stochastic structure of sample data. This additional structure imposes a model on population units` data, a model that is a consequence of this additional structure and thus with the same credibility as probabilities of selection in Design Based Inference. This expanded theory is called, Pre-Sampling Model Based (PSMB) Inference. It eliminates problems with both design effect and model fit. The result can be estimators with much smaller sampling error than Design Based Estimators. PSMB estimators and design based estimators are evaluated with respect to sampling error from repeated sampling of population units under stratified cluster designs. Causes of increased error in Design Based Inference are noted. ","As the population and methods used to collect survey data change,  the traditional single frame surveys may miss parts of the  population. In order to obtain better  coverage of the population of interest and cost less, a number of  surveys employ dual frame design, in which independent samples are  taken from two overlapping sampling frames. In this research, we  propose partially linear models for regression structure in dual  frame surveys. We intend to model the domain (the nonoverlapping  components of the union of two frames) effect (or called treatment  effects) parametrically and to model the effect of the covariate  nonparametrically. We incorporate the dual frame survey  weights into the partially linear model by constructing a weighting  parameter from the overlap domain through cross validation method.  An extended difference-based variance estimator is used to  approximate the noise level of the model. A combined inference frame  work is used to derive the estimators' properties. Simulation  studies are conducted to investigate the finite sample properties of  the proposed estimators.    ","Bayesian methods have been gaining popularity as an alternative to the traditional design-based methods for estimation from complex surveys. In this paper, we apply Calibrated-Bayesian methods to estimate vaccination rates from the National Immunization Survey (NIS).  NIS is a large telephone survey, which has been continuously conducted to monitor childhood vaccination coverage among U.S. children aged 19-35 months since 1994 (http://www.cdc.gov/nchs/nis.htm).  Official design-based vaccination coverage rates at the national, state, and selected urban area levels estimated using data from the NIS are available at the Website http://www.cdc.gov/vaccines/stats-surv/nis/default.htm#nis. Data from the recent NIS public-use files are used to compute and compare the Bayesian estimates with the design-based estimates. We also compare sub-domain estimates based on the two methods by selected demographic characteristics. ","Small area estimation methods are used to produce State and substate estimates of substance use and mental disorders using data from the National Survey on Drug Use and Health. Design-based estimates could be used as an alternative because they are less expensive than small area estimates and take less time to produce. Thus, it is important to determine how the small area estimates compare with their design-based counterparts in terms of accuracy and precision. A previous study demonstrated that small area estimates were generally more precise than design-based estimates while exhibiting only small levels of bias. In this paper, those results are extended by conducting an additional simulation study to evaluate the performance of synthetic estimates. These estimates are commonly produced for small areas where no sample data can be obtained, and this study aims to provide some guidance about the quality of such estimates. ","We use simulation to study and compare the performance, in terms of coverage rate and length, of four  methods of constructing confidence intervals for population size based on a two-stage Capture-Recapture (CR) experiment. Two methods are based on the asymptotic normality of point estimators and two are obtained from inverting chisquare and likelihiood ratio tests. In the scope of the settings we studied, we found that the method based on inverting a chisquare test is best and that none of the methods performs well if sampling fractions are small. As a practical matter, our conclusion is that that CR designs are most useful for relatively small populations, such as endangered species, where there may be a rough prior estimate of poulation size to guide sample size selections or in a populations where large samples are easy to obtain. ","I would like to discuss new techniques in randomized response sampling which are useful in collecting data on sensitive characteristics. Personal experience of collecting and analyzing data using randomized response techniques will be discussed. In addition, I would like to invite Big Data experts to discuss the possibility of use of Big Data in the randomized response setup. All social scientists are most welcome to join the roundtable. ","The design effect is the ratio of the variance of an estimated parameter from a survey using a complex sampling design to the variance of the same estimate under simple random sampling with the same total sample size.  This is a useful tool estimate sample sizes for future surveys that use complex sampling desings. It also provides a measure of the precison of the design.  To compute the design effect we need to estimate both the numerator and the denominator of the ratio.    ","For studies where the objective is to estimate the prevalence rate of members of a sampled population who fall in a rare subgroup, this paper examines the relative statistical precision of prevalence estimates from a multiple-frame samples compared to single-frame household samples with the same data collection budget.  We first examine relative cost-efficiency for simple un-clustered samples and then consider the effect of cluster sampling.  Findings are illustrated for the case where the subgroup consists of victims of rape and sexual assault (RSA) in a civilian non-institutionalized population of persons 12 years and older.  Two sample designs are considered: (i) dual-frame sampling from a conventional household frame plus a frame constructed from police reports of RSA, versus (ii) single frame sampling from the household frame.  We conclude for this illustration that a dual-frame design will be more cost-effective to the extent that RSA prevalence among police reports exceeds the RSA prevalence in the population as a whole.  However, gains in the dual-frame design are diminished in direct relationship to the size of intra-class correlation when cluster sampling is considered. ","The Survey of Economically Successful Americans (SESA) is designed to understand the influence of exceptionally wealthy individuals on the American political process.  Our pilot study had the goal of targeting the top 1/10 of 1% of households, estimated at $20-40 million in net-worth.  One challenge was the absence of a sampling frame that efficiently captured such high-wealth individuals, and limitations in publicly-available sources such as the American Community Survey.  We created a composite frame from market-research sources, including lists of business executives and \"wealthy\" individuals.  Most sources carried limitations in data resolution i.e. top-coding, as well as inconsistent accuracy.  Our current research uses external data sources to enhance our results with the goal of improving both the coverage and hit-rate of our methodology.  Examples of newly available data sources include estimates of total liquid assets, home value, and stock-sales that were not available during our pilot phase design.  We present models that outline the most efficient approach for conducting nationally-representative household surveys of very wealthy populations.   ","Since 2003, the Centers for Disease Control and Prevention has conducted the National HIV Behavioral Surveillance System (NHBS) in 20 jurisdictions, defined by metropolitan statistical area (MSA) with the largest burden of HIV disease, in three populations at high risk for HIV acquisition: heterosexuals at increased risk (HET), men who have sex with men (MSM), and injecting drug users (IDU).  Designed to monitor prevalence and trends in risk behaviors, HIV testing, and use of prevention services in these populations, surveillance is carried out in each group every 3 years in rotating annual cycles as a behavioral survey.  Each jurisdiction conducts about 450-500 interviews during each annual cycle.  Identifying and enumerating individuals in these populations is difficult so these populations are hard to reach for disease surveillance.  No practical method exists for constructing sampling frames for standard random sampling methods.  Therefore, NHBS employs novel approaches to sampling these populations. Respondent-driven sampling is employed to survey HET and IDU.  Venue-based sampling is employed to survey MSM.  We present overviews of the sampling designs and report on results.   ","The CDC conducts the National HIV Behavioral Surveillance System (NHBS) to monitor prevalence and trends in risk behaviors, HIV testing  and use of prevention services among people at high risk for HIV including men who have sex with men (MSM), injecting drug users and low socioeconomic status heterosexuals.  MSM are considered a difficult-to-reach population for surveillance and there is no practical way of constructing a sampling frame to permit standard random sampling methods. NHBS uses venue-based sampling (VBS) to recruit MSM into the survey.   This paper describes the development of survey weights for the third MSM NHBS cycle (MSM3). The weights were computed separately for each jurisdiction. The weighting process takes into account a two-stage cluster design of venues and day-time periods attended by MSM. Sampling and weighting were stratified by month and site. While a general methodology was developed and applied in all sites, our approach includes specific procedures to account for variations in field operations at each site, such as differences in the data collection period and venue sampling frequency.      ","General population health surveys are inadequate for monitoring characteristics of HIV-infected persons because the low prevalence of the disease (&lt;   0.5%) leads to small sample sizes. The U.S. National HIV Surveillance System (NHSS) includes all persons with a diagnosis of HIV, but provides limited data on their demographics and the medical care they receive. The Medical Monitoring Project (MMP), a nationally and locally representative probability sample of HIV-infected adults in care, was designed and implemented to monitor clinical and behavioral outcomes. To produce valid, nationally representative estimates of the characteristics of the population in care as well as its size, we implemented a cross-sectional, three-stage sampling design that sampled jurisdictions, then facilities, and then patients receiving medical care using PPS methods. MMP data collection includes basic information from NHSS for all sampled patients, interview data on behaviors and experiences, and abstracted medical record data for subjects who consent or where state law allows. MMP methods could be adapted to monitor populations of interest or evaluate outcomes and care for other rare conditions. ","Respondent-driven sampling (RDS) is a new sampling method specifically proposed for rare populations. Unlike traditional sampling methods which require extensive screening efforts, RDS exploits the population networks from selected seed respondents through incentivized coupons and samples from them without screening in waves. While use of RDS has increased tremendously with the pressing needs for studying rare populations, its formal base for inferences is relatively weak and sits on strong assumptions.   This paper tests sensitivity of the memoryless nature of the Markov process assumed for RDS, which states that as recruitment waves continue, the sample characteristics become independent of the seeds. We use a publicly available RDS data targeting those at risk for HIV/AIDS, such as drug users and MSM through face-to-face interviews. Estimates of key variables from the full sample will be used as benchmarks and compared to those from subsamples, which will based on a subset of the seeds selected at random and nonrandom. The subsample distributions will be examined for the bias and the coverage of the nominal 95% coverage intervals as sensitivity test criteria.    ","Statistical agencies often collect sensitive data for release to the public   at aggregated levels in the form of tables.  In this article, we propose a new disclosure limitation method to replace the full set of micro-data with synthetic data  for use in producing released data in tabular form.   This synthetic data is obtained by breaking each unit's data into small pieces and spreading   them among the nearest neighbors by repeatedly sampling from the set of fragmented original micro-data.  The synthetic data is produced in a way to give approximately unbiased estimates for aggregate cells as the number   of units in the cell increases.  The method is applied to the U.S. Bureau of Labor Statistics' Quarterly Census of  Employment and Wages data, which is released to the public quarterly, in tabular form,   aggregated across varying scales of time, area, and economic sector.  ","When releasing microdata to the public, data disseminators typically are required to protect the confidentiality of survey respondents' identities and attribute values. To satisfy these requirements, removing direct identifiers such as names and addresses generally is not efficient to eliminate disclosure risks, so that data must be altered before release to limit the risks of unintended disclosures. Statistical agencies can release the units originally surveyed with some values, such as sensitive values at high risk of disclosure or values of key identifiers, replaced with multiple imputations. So far such research has been done mainly focuses on individual respondents, i.e. simulating attributes of each individual respondent for protecting their privacy. In this study we developed nonparametric Bayesian models for generating household data, i.e. simulating people nested in households. Dependence structures among variables at both the individual level and the household level need to be preserved in such models. ","This paper focuses on applying a random perturbation approach that protects microdata for the purpose of releasing data to the public. The classical challenge is to balance the need to reduce disclosure risk and retain data utility. An approach has been developed that provides the data producer flexibility to achieve the balance. Hot deck cells are formed from sampling weights, model predictions and/or covariates, the locality of the target records, and categorized bins of the target variable. Expanding or contracting the bin sizes allows the data producer the flexibility to control the distance between original and perturbed values. An evaluation was conducted to study the impact of the bin categories, sampling weights, model predictions and locality effects using the American Community Survey 2005-2009 sample data. ","We present an algorithm for releasing graphical degree sequences of simple undirected graphs under the framework of differential privacy. The algorithm is designed to provide utility for statistical inference in random graph models whose sufficient statistics are functions of degree sequences. Specifically, we focus on the tasks of existence of maximum likelihood estimates, parameter estimation and goodness-of-fit testing for the beta model of random graphs. We show the usefulness of our algorithm by evaluating it empirically on simulated and real-life datasets. As the released degree sequence is graphical, our algorithm can also be used to release synthetic graphs under the beta model. ","In 2012, the Census Bureau began collaborating with the National Center for Health Statistics to collect interviewer observations of sample neighborhoods and sample units in the National Health Interview Survey (NHIS).  These observations seek to measure aspects of the sample unit environment that may predict both the likelihood of response to survey invitations and the health status of residents. This paper traces the development of an interviewer observations instrument. We describe the work with NCHS to create the module of observations to be collected at the initial NHIS interview contact attempt. This module was created with the dual purpose of providing NHIS-specific observations and more generic measures that might be applied across Census surveys.  The paper details the development and selection of the observations, the final automated module, interviewer training, and interviewer debriefings and feedback prior to and during a NHIS field test conducted in 2013.  ","Recent work by West (2013, JRSS-A) presented results from a simulation study designed to examine the implications of measurement error in a simple binary interviewer observation for nonresponse adjustments, where weighting classes are determined solely by the binary observation. This work suggested that the error rates currently observed in the National Survey of Family Growth (NSFG) could lead to adjusted estimates with reduced quality relative to complete case estimates, if weighting class adjustments for nonresponse are based solely on the binary observations. This study presents results from a more comprehensive set of simulation studies designed to examine the effects of differential measurement error in interviewer observations (i.e., different error rates for respondents and non-respondents) on weighting class adjustments for nonresponse that include additional auxiliary variables. Practical scenarios where differential measurement error will have the largest impact on the quality of the nonresponse adjustments will be discussed, presenting survey statisticians with guidance on problematic error rates. Suggestions for future research in this area will also be presented.  ","Face-to-face surveys provide interviewers the opportunity to gather information beyond the scope of the survey. These observational paradata may prove useful for streamlining the data collection process and enhancing post collection weighting adjustments, both improving the quality of survey estimates. However, interviewer observations can be error-prone especially when the observation requires a judgment based on limited information. This research uses survey paradata to evaluate new interviewer observations made during the collection of the 2013 National Health Interview Survey (NHIS). Multilevel models discern the variation in compliance with the new task attributable to interviewers, finding that the characteristics of the case, not the interviewer, reduced variation in compliance. We then examine variation in the observations by whether contact was made when recording the observations, finding inferential but not factual observations vary based on making contact during the observation. When interacted with contact, 11 of the 15 observations are predictive of either the number of contact attempts made on the case or the odds of refusal-two measures of level of effort. ","The U.S. Census Bureau recently developed a set of interviewer observations to support adaptive survey design across a host of Census-administrated surveys. Examples include the presence of smokers at the sample unit, the age composition of the sample unit, and the physical condition of the sample unit.  Using paradata collected with the 2013 National Health Interview Survey (NHIS), we estimate a set of response propensity models to isolate the predictive power of the new observational measures. We discuss our results in light of the potential future application of the interviewer observations in the NHIS. ","Means or other central tendency measures are by far the most common focus of statistical analyses. However, as Carroll (2003) noted, ``systematic dependence of variability on known factors'' may be ``fundamental to the proper solution of scientific problems'' in certain settings.  We will discuss methods we have recently developed to assess the degree to which individual variability in a predictor variable predicts a health outcome of interest in a longitudinal setting. We focus on combining information from mean profiles and residual variance to predict categorical outcomes in a joint hierarchical modeling framework. We consider applications to predict dementia onset using word recall measures obtained over time from the Health and Retirement Survey (Elliott et. al. 2012), and hot flash severity in menopausal women from follicle stimulating hormone (FSH) measures during menopausal transition (Jiang et al. 2012).  ","Objective measurement of physical activity using wearable accelerometers is now used in large-scale epidemiological studies and clinical trials of lifestyle and exercise interventions. These devices measure the frequency, intensity, and duration of physical activity at the momentary level, often using measurement epochs of 1 minute or less yielding a large number of observations per subject. In this talk, we describe a Bayesian mixed-effects location scale model for the analyses of physical activity as measured by accelerometer using data from a randomized lifestyle intervention trial of 204 men and women. We model both the mean and variance over time as a function of subject-specific random effects and time varying covariates. Our model allows us to measure how covariates can influence both the mean and the variance of physical activity over time. It also allows us to measure whether changes in variability over the course of the study are predictive of treatment relapse. ","The investigation of within-person variability has increased in recent years and has proven be a fruitful field in the investigation of cognitive change. At the same time, intensive measurement designs have become more and more important because they allow the estimation of within-person effects. Here, within-person variability is estimated with a mixed effects location scale model. That model allows one to include explanatory variables at the within- and between-person level and it estimates random effects in the location and scale part simultaneously. We used this model in a Bayesian framework to demonstrate its application to three level data. 304 participants have been measured on a reaction time test at four bursts, each one year apart. Each burst contained four sessions of weekly (1 to 2 weeks apart) measurements.  We found considerable individual differences among participants in all parameters. The location and scale parameters were not independent whereby the random scale parameters correlated with the random intercept of the location part of the model.  ","Questionnaires are commonly used in many studies, and the items that comprise the questionnaire are often scored on an ordinal scale, for example on a Likert scale. For such questionnaires, item response theory (IRT) models provide a useful approach for obtaining summary scores for subjects (i.e., the model's random subject effect) and characteristics of the items (e.g., item difficulty and discrimination). In this presentation, we describe an extended IRT model that allows the items to additionally exhibit different within-subject variance, and also extend the ordinal IRT model by adding a subject-level random effect to the within-subject variance specification. This permits subjects to be characterized in terms of their mean level, or location, and also their variability, or (square of the) scale. Additionally, we allow the random effects to be correlated. We illustrate application of this location scale IRT model using data from the Nicotine Dependence Syndrome Scale (NDSS) assessed in an adolescent smoking study.   ","To evaluate the effect of longitudinally measured risk factors on the subsequent development of disease, often summary measures of these factors are calculated to capture features of the risk profile.  These methods consider correlations among repeated measurements on subjects as a nuisance.  Using an example of hormone profile changes in the menopausal transition, we demonstrate that residual variability in subject measures, in addition to risk factor profiles, may also be important in predicting future health outcomes of interest.     ","Recently, a system was developed at the US Census Bureau to upload each day's Computer Assisted Personal Interviewing (CAPI) data from interviewer laptops and analyze the results using regression models. The intent was to track the possible effect of changes in management at the bureau's Regional Offices on survey responses to key variables. We discuss this application of daily modeling in light of lessons learned from a completed project. Furthermore, we discuss a new system being developed to track components of survey error across sample design changes in the 2010 Redesign of demographic surveys.  ","The NHIS is one of six household surveys that are currently part of the Demographic Data Monitoring System (DDMS), which captures survey responses from interviewer laptops on a daily basis and will frequently load survey cost data as well. Within the DDMS is a component to model survey outcomes and display tabular and graphical output.  This paper presents details of the models used to study the effect of the Census Bureau's Regional Office realignment on key estimates of the NHIS, as well as issues encountered in developing the system and interpreting the results.  ","In 2012, the U.S. Census Bureau reduced the number of regional offices from twelve to six.  There were concerns that the changes in the field might adversely affect estimates from our demographic surveys, including the National Crime Victimization Survey (NCVS) and the Consumer Expenditure Quarterly Interview Survey (CEQ).  To study this, we developed statistical models to measure the effect of the new management structure on certain key response variables and monitored them on a daily basis.  The four key variables chosen for the NCVS were total person crime, total violent crime, total property crime, and total overall crime.  The CEQ survey had seven key variables quarterly rent, monthly rent equivalent, property value, monthly gasoline expenditures, quarterly health and medical expenditures, weekly grocery expenses, and quarterly contributions.  Using logistic regression and linear regression models, we measured the effect of cases coming in under the new management.  In this paper, we will discuss the details of the models, an interpretation of the results, and discuss further research ideas.  ","In 2012, the U.S. Census Bureau shifted field operations from 12 to six regional offices (ROs).  To monitor this shift in management we built models for key variables for many different surveys to determine if there were significant changes in our estimates.   Models were run on unedited Computer Assisted Personal Interview (CAPI) data on a daily basis.  This paper discusses the models used for the American Community Survey (ACS).  First, we will give a brief overview of the scope of this project and discuss the key variables, predictor variables, and how models were built using 2011 ACS unedited data.  Second, we discuss the automated system used to generate graphs for each key variable.  These graphs track the daily change on the coefficient for the management structure indicator variable from the models.  Finally, we present the results of some of our models and discuss possible ways to improve/use these models in the future. ","During the 2012 calendar year, the U.S. Census Bureau underwent a realignment of its regional offices for operational efficiency; reducing their number from twelve to six. In order to monitor the quality of demographic survey data during this transitional period, an automated system was produced to display graphical and tabular data based on statistical models. The system, named the Demographic Data Monitoring System, was implemented within five months using rapid application development to ensure its availability by January, 2012. A variety of software options were evaluated, keeping in mind the aggressive timeline and evolving requirements. Senior management, including the U.S. Census Bureau director and deputy director, used the system to ensure the realignment did not unexpectedly impact demographic survey data. ","A proposed revision of wage imputation procedures in the OES Survey is based on relatively accurate estimates of area and establishment wage levels obtained by using the OES and QCEW datasets together in the multivariate EBLUP estimator of Lohr and Prasad (2003). Current OES procedures define donor cells by the same time period /MSA / Industry / Establishment-size /Occupation as the non-respondent cell, and the mean wage distribution of this donor cell is imputed to the non-respondent. In the case of insufficient response, the cells are collapsed over industry and size groupings. Where necessary, the cells are reformed by State / Industry / Establishment Size, and the process repeats as for the MSA base-level strata. The base-level strata default to USA in a similar manner. The proposed method replaces the OES base-level strata with three successively more aggregated sets of establishment groupings based on multivariate EBLUP estimates of establishment wage levels. Simulation is used to compare the performance of the new approach against the OES method and a variety of alternatives. The results suggest that imputations based on the new approach outperform the current method .    ","Despite the fact that incorporating complex sample design features are important to obtain correct Multiple Imputation (MI) inference, this is not typically done in practice. On one hand, existing model-based MI techniques typicaly require strong model assumptions and expensive computation; On the other, there is no sensible way to accommodate survey weight into imputation model. We developed an innovative MI method that is attentive to design features and robust enough to sufficiently capture the correlational structures among survey variables. Under the new method, the complex feature of the survey design (including weights, clustering and stratification) is fully accounted for at the first step through a nonparametric synthetic data generation procedure; conventional parametric MI for missing data is performed at the second step using readily available imputation software designed for an SRS sample. Our simulation study demonstrates that our method has advantages over existing MI methods, particularly in the presence of model misspecification and/or informative sampling. Extensive applications are conducted on survey data from BRFSS and NHIS. ","When estimating quantiles of an unknown univariate distribution it is common to use the sample quantile as an unbiased point estimator for the true quantile and estimate its variance using some kind of resampling method, such as the bootstrap or the jack-knife. However, as we illustrate in this talk, using this strategy for a dataset for which missing observations have been multiply imputed will lead to conservative variance estimates based on Rubin's combining rules. The reason is that the sample quantile is not a self-efficient estimator as defined by Meng (1994).  We propose a straightforward maximum likelihood estimator for the quantile using a box-cox transformation that allows valid inferences after multiple imputation if the assumptions of the box-cox transformation are met. We illustrate through simulation and real data applications that the estimator is more efficient than the estimator based on the sample quantile and unbiased given that the sample data can be approximated by a normal distribution after the box-cox transformation.   ","The Survey of Income and Program Participation (SIPP) collects detailed information about income and program participation.  These key questions can suffer from higher rates of nonresponse, so a sequential hot deck procedure is used to impute all items with missing data.  Single imputation techniques like these do not incorporate the uncertainty of the imputation and can lead to underestimates of the true variance.  However, if rates of item missing data are comparatively low, the underestimates may be slight with little impact on significance.  Multiple imputation methods were used to impute missing data in the 2008 Panel of the SIPP to incorporate this uncertainty into the imputations.   The imputations are made using several different multiple imputation methods and compared based on estimates of variance, as well as, fractions of missing information for key SIPP statistics.  Additionally, we compare to similar estimates based on no imputation and based on various single imputation methods, like the current hot deck method.   ","The American Housing Survey (AHS) is the largest nationally representative survey of housing in the United States. It is conducted every two years by the U.S. Census Bureau for the Department of Housing and Urban Development (HUD) and has followed the same sample of housing units since 1985. The AHS imputes some items for non-response using hot-deck, cold deck, and regression-based methods. The AHS is currently undergoing a survey redesign and will introduce a new sample in 2015. As part of the redesign, the Census Bureau is evaluating current imputation methodologies and designing new approaches to impute items in 2015 and beyond. Utility costs are a financial housing characteristic that is imputed in the AHS.  Utility cost data are currently adjusted and imputed using a regression-based method that utilizes utility bill and housing characteristics from the AHS and housing characteristics, consumption, and cost data from the 2001 Residential Energy Consumption Survey (RECS) adjusted for inflation.  This research re-estimates utility models with data from the 2009 RECS and explores the implications of estimating utility costs without AHS utility billing data. ","We focus on strategies of making inferences in the presence of missing data after conducting a Multiple Imputation (MI) procedure. MI's combining rules for inferences (Rubin, 1987) are tested against a new methodology developed using the full posterior density of an estimand of interest. This density is approximated using a particular mixture distribution. Simulated experiments show this approach improves inferences in some aspects, making them more stable over repeated analysis and creating narrower bounds for certain common statistics of interest. Extensions to existing literature have been executed that provide further stability to inferences and also a strong potential to identify ways to make the analysis procedure more flexible. The new approach suggests a variation to the existing mixture approach, utilising Rubin's variance estimates for the statistics of interest to improve its overall performance. The competing methods have been first compared using simulated data sets and then a real data set concerning analysis of the effect of breastfeeding duration on children's cognitive ability. ","Influential units occur in business surveys because the distributions of economic variables tend to be highly skewed. In the presence of influential units, classical estimators (e.g., Horvitz-Thompson estimator) exhibit larges variances. Winsorization (Type I and Type II) is often used in business surveys in order to obtain more stable estimates. The winsorized estimators involve a tuning constant. We propose a simple method for determining the tuning constant, which consists of minimizing the maximum estimated conditional bias. We show that the resulting estimator is consistent. In the case of stratified simple random sampling, winsorization is typically applied independently within each stratum. However, the resulting estimator of the overall total may be considerably biased. To overcome this difficulty, we propose a method, where winsorized estimators within each stratum are modified so that their sum is calibrated on the winsorized estimator obtained independently at the overall level. Results of a simulation study will be presented.   ","Recent research on the use of M-estimation methodology for detecting and treating verified influential values in economic surveys found that initial parameter settings affected its effectiveness.   The study relied on simulated data designed to reflect the population properties for two industries in the Monthly Retail Trade Survey (MRTS), but the approach to determining settings for the initial parameters used an empirical analysis that does not generalize well.   The need to expand the application of the M-estimation methodology to all the industries in the MRTS and the Monthly Wholesale Trade Survey stimulated the development of a more general methodology that uses historical data to determine the initial parameter settings.  This paper discusses the effectiveness of several methods for setting initial parameters under several scenarios for the occurrence of influential values. ","The California Employer Health Benefits Survey collects data from employers on up to five plan types offered to workers.  Data are frequently analyzed at the employer level (unit of observation).  Responses, however, can vary by plan type.  Aggregating continuous variables (e.g., annual premium cost) across plan types is straight forward, but aggregating categorical variables can be problematic.  Also, enrollment into each offered plan type is not uniform adding to the problem.  In this paper, five methods are contrasted for aggregating categorical plan level variables (e.g., self-insurance indicator) to the employer level.  Methods include: (1) a simple average across plans; (2) a weighted average across plans by plan type; (3) a random proxy of a representative plan; (4) the proxy representative plan having the largest enrollment; and (5) stacking the data making the plan the unit of analysis.  T-tests found no significantly different estimates between each method pairing.  While each method provides statistically comparable estimates, method (5) may be best as it uses more information and minimizes the standard error. ","The U.S. Energy Information Administration (EIA) periodically produces population estimates of residential energy consumption and expenditure for space heating, air conditioning, water heating, and other energy end uses using data from the Residential Energy Consumption Survey (RECS).  EIA uses fuel end-use models to decompose the sample households' total annual fuel consumption into a sum of components corresponding to the various end uses, appropriately weighting them up to produce population estimates of total and average energy end uses at various tabulations of interest.  Within the fuel models the end-use components are represented as non-linear functions of relevant household variables available from RECS, or, in the case of the weather-dependent end uses of space heating and air conditioning, heating and cooling degree-days, respectively.  Historically, degree-day data used in the end-use models are calculated on a constant base temperature of 65 degrees Fahrenheit.  This paper analyzes the effects of replacing this constant value with a linear regression-modeled local base temperature on the ultimate population estimates of the entire vector of end-use consumption. ","Prior to 2011, the Canadian census of population was conducted with a mandatory long-form sent to 20% of the households and a mandatory short-form sent to the rest of the households. The 2011 Census was conducted with a mandatory short-form sent to the entire population. A voluntary survey called the National Household Survey (NHS) was created to collect the information that used to be collected with the long-form. To minimize the impact of non-response, a sample of 30% was selected for the NHS and after several weeks of collection, the follow-up efforts were concentrated on a random subsample of the remaining non-respondents. The design-weighted response rate was 77%, while for the last census long-form the response rate was 94%. This paper describes the estimation methodology used in the 2011 NHS. Design weights were first calculated. A non-response weight adjustment was done using census and linked administrative data and by converting a nearest-neighbour and whole-household imputation approach to a reweighting approach. Weight calibration to many census totals was performed. Variance was estimated with a multi-phase variance formula and Taylor linearization. ","In vaccination coverage studies, it is often of interest to present cumulative vaccination coverage levels using such methods as Kaplan-Meier estimates.  Comparison of the estimated survival curves across subgroups may be confounded by imbalanced distribution of covariates. In general, adjusted survival functions in the context of complex sample survey design have not been described. We propose two methods to adjust the survival curves for complex sample survey data. (1)The inverse probabilities of being in a certain group are defined as the new weights and applied to obtain the inverse probability weight adjusted Kaplan-Meier survival function. (2) Survival functions are evaluated for each of the unique combination of covariate levels in a complex sample survey based on the estimated baseline cumulative hazard rate obtained from Cox proportional hazards model developed on the entire database. A weighted average of these individual survival curves is calculated with weight equal to weighted sample size of the individual curve at each of the unique covariate combination to obtain the Cox corrected group adjusted survival curve. The two proposed methods were applied to 2011 NIS data. ","Through a combination of paper surveys and radio panels, Arbitron records and publishes radio ratings every month or quarter for about 300 media markets in the United States. One current area of applied research is to investigate the feasibility of modeling radio listening based on a series of demographic variables for a particular marketplace.  The nature of radio listening data violates several assumptions inherent in the linear regression model. Due to varying amounts of heavy listeners, radio data exhibit a certain amount of skewness, which infringes the normality assumption. The constant variance assumption is also problematic. Furthermore, the listening data is nonnegative and can be zero-inflated for certain radio formats (i.e., soft rock, country, etc.).   We will proceed to build a mixture distribution regression model. We will consider the available mixture distributions, link functions, mixture constants and also decide how many components to use in  the model.  Finally, we will compare the resulting predictions to those generated by the linear regression model as well as the log-transformation model.    ","While addition of cellular numbers has offered an effective remedy for improving coverage of the traditional landline RDD samples, current practice of the resulting dual-frame methodology is subject to technical and operational inconsistencies. On the technical side, most survey researchers rely on ad-hoc assumptions to determine the mixture of landline and cellular numbers for their samples. This inconsistency, which is mostly due to the unavailability of current counts of cell-phone-only households, has implications for both sample selection and subsequent methods used to weight the resulting survey data. On the operational front, there are surveys that continue to rely on outdated data collection protocols, such as screening out respondents reached via cellular numbers who are also reachable by landline numbers, which sap the available budget without producing any notable gains. We will introduce practical guidelines for dual-frame RDD surveys that ameliorate the existing inconsistencies for sample selection and weighting applications, as well as eliminating the need for the costly practice of screening out responding dual-users. ","The answer to this rhetorical question can be derived from the abstract of my Y2K vignette \"Missing Data: Dial M for ???\" (JASA, 2000):    ","It is common in research to have many variables measured on a modest number of cases and to have a variety of data types (e.g., continuous, binary, ordinal, nominal).  Longitudinal data and other clustered data structures are also common.  This talk will present methods that have emerged in an effort to develop flexible model-based imputation methods for high-dimensional data sets.  Key ideas include handling missing continuously-scaled items using a factor-analysis strategy to reduce the number of covariance parameters in a multivariate normal model, using growth-curve models and factor-analysis ideas together for longitudinal continuously-scaled variables, using a parameter-extended Metropolis-Hastings algorithm to sample the correlation matrix in a multivariate probit model in a way that can be extended to several ordinal variables, and applying the parameter-extended Metropolis-Hastings idea to a multinomial probit model in a manner that can be extended to several nominal categorical variables.  Examples are offered to illustrate the methods, and simulation studies are used to explore statistical properties of the procedures and compare them with alternative approaches.   ","I discuss regression discontinuity designs from a potential outcome perspective. Looking at both sharp and fuzzy regression discontinuity designs we examine the critical assumptions for identifying causal effects and relate them to approaches based on unconfoundedness assumptions. I illustrate the methods using a data from a remedial summer program for high school students where only  those students who scored below a threshold on a test were required to participate. Some students with score higher than the threshold participated in the program despite not being required to do so, and some students who score below the threshold nevertheless did not participate.   ","Confounded post-treatment variables are often present in intervention studies. Principal Stratification (PS) is a framework to deal with such intermediate variables. Due to the latent nature of the principal strata, strong structural assumptions are often invoked to sharpen inference. Distributional assumptions may also be invoked, usually leading to weakly identified models. Information on multiple outcomes is routinely collected in practice, but rarely used to improve inference. Covariates are also collected, but often used to either make the assumptions more plausible, or improve the prediction of missing potential outcomes. We show, using various inferential paradigms, including Bayesian and frequentist perspectives, the potential inferential gains from jointly modeling two (or more) outcomes, or jointly modeling outcomes and covariates. These results can also be used to assess the plausibility of structural assumptions, such as exclusion restrictions. The role of the auxiliary information is shown in two examples to evaluate the effects of a real social job training program on participants' employment and of another real job training program on trainees' depression. ","The American Community Survey (ACS) produces direct five-year estimates at the census tract level for income and poverty. Small area estimation using models that borrow strength from relationships between variables across geographic areas may improve the accuracy of these estimates. Typically, these approaches combine direct estimates with model estimates that make use of administrative data. A 2012 pilot study used simulated administrative data to demonstrate the potential gain in accuracy from using three model based estimation methods. The Longitudinal Employee-Household Dynamics (LEHD) Origin-Destination Employment Statistics files constitute a publically available source of administrative data potentially correlated with income or poverty. This paper uses the LEHD files and published ACS data to produce tract-level estimates and estimated mean squared errors using each of these three model-based estimation methods. Results are compared with the sampling variance of the published ACS estimates, which are assumed unbiased.  ","The Census Bureau measures demographic, social, and economic characteristics of the United States population through the American Community Survey (ACS). Coverage is the measure of completeness of the estimate of persons living within housing units (HUs) and group quarters (GQ). Undercoverage exists when HUs, GQs, or people do not have a chance of being selected in the sample. Overcoverage exists when HUs, GQs, or people have more than one chance of selection in the sample, or are included in the sample when they should not have been. The Census Bureau produces ACS coverage rates for the nation and states every year by comparing ACS estimates to the Census Bureau's Population Estimates Program estimates. The 2010 Census offered a unique opportunity to measure the coverage of the recently produced 2010 ACS 1-year and 2006-2010 ACS 5 year estimates, as it provided an up to date listing of HUs and population and highly detailed demographic groups for comparison. Of particular interest were the coverage rates of American Indian/Alaska Native (AIAN) persons and of population living in AIAN areas, which are the focus of this paper.   ","The American Community Survey releases Public Use Microdata Sample (PUMS) files annually for users to calculate their own estimates. PUMS contains individual housing unit and person records for a limited set of geographic areas. Two methods exist for users to calculate Variances for estimates: a generalized variance method (design factor) and a replicate weight based method.    ","Although ordinary logistic regression is a widely-used tool, such models are often inappropriate given complicated data structures.  We discuss methods to assess the quality of logistic regression models and explore alternatives to traditional regression models. To illustrate our findings, we investigate if characteristics of an address in the American Community Survey (ACS) can predict if a mailing is undeliverable as addressed (UAA) by the United States Postal Service. In 2012, local post offices reported that over 10% of mailed questionnaires in the ACS were UAA. By identifying the address and geographic characteristics for those mailings which are returned as UAA, we hope to identify certain types of addresses that are especially problematic and to provide suggestions for their improvement. To obtain this information, we will compare a variety of logistic regression approaches including mixed effects, generalized estimating equations, and spatial models.  We will also investigate the use of classification trees for variable selection.  We will discuss how to select an appropriate model and if our results can inform approaches to decrease the ACS UAA rate. ","The American Community Survey (ACS) selects national housing unit address samples on a yearly basis.  Each sample is selected systematically, using geography and estimated occupied housing unit counts within specific geographies as sort variables.  Every housing unit address on the ACS frame is eligible for sample once every five years, with approximately one-fifth of the addresses being eligible in a given year.  Weighted  response rates for the yearly samples average above ninety-seven percent, so the ACS has respondent information on the vast majority of the sampled units.  But, there is still a two-plus percent nonresponse rate, and it is uncertain as to whether these cases are systematically different from the respondents, for one or more estimation categories of interest, e.g., race.  Sample representivity statistics attempt to quantify the representativeness of the responding units to the nonresponding units and, by extension, to the entire frame for these categories ","Estimates from probability samples may not match known totals of populations due to sampling variation, undercoverage, or nonresponse. In this situation, special weighting adjustments such as raking or poststratification are implemented so the sums of weights match known totals. This process improves the face-validity of the survey because the total estimates reproduce the \"true\" or accepted total population generally produced by official statistical agencies. The totals, which are used as benchmarks in weighting, are sometimes generated combining different sources and in some situations they do not match the eligible population of the survey. In this paper we take a heuristic approach and propose methods for computing control totals that are consistent and represent the population of interest while maintaining the face validity of the estimates. ","In this paper, a new two-step technique for the calibration of design weights is proposed. In the first step, the calibration weights are set proportional to the design weights in a given sample. In the second step, the constants of proportionality are determined based on different objectives of the investigator such as bias reduction or minimum mean squared error. Many estimators available in the literature can be shown to be special cases of the proposed two-step calibrated estimator. A simulation study, based on a real data set, is included at the end. A few technical issues are raised with respect to the use of the proposed calibration technique, both limitations and benefits are discussed. ","In calibration methods for sampling weight adjustment, there is no built-in mechanism for ensuring variance reduction although typically it does lead to variance reduction. We introduce new stratum-specific scale parameters in the calibration or generalized raking model to capture possibly varying nonresponse bias, coverage bias, and design characteristics by strata or super-strata. Approximate unbiasedness of calibration estimators is still maintained in the presence of these extra parameters which are estimated outside the calibration equations by minimizing the generalized variance of key study variables or alternatively the unequal weighting effect for simplicity. Besides, instead of trimming potential extreme weights produced in calibration, we propose modeling to smooth extreme weights to avoid introducing bias. Using a simulation study, various calibration methods are compared in terms of bias and variance. ","Non-response in surveys is usually addressed using imputation or weighting adjustments, given that no follow-up sample has been selected. These procedures are reasonable if the respondents and non-respondents are similar. If they are not, these procedures may lead to serious biases. A follow-up sample of the non-respondents will eliminate (attenuate) any such bias. However, since not all units selected in the non-response sample will respond, this leads to a three-phase sampling design. In this paper we provide weighting procedures that properly account for this, as well as the associated population and estimated variances that reflect the weighting. Also, given that a sample of the non-respondents will be followed up, we provide the allocation between the initial sample and follow-up sample size given cost constraints. ","In the absence of non-response, pseudo-population bootstrap procedures have been extensively studied in the literature; see Gross (1980), Booth et al. (1994), Chauvet (2007) and Wang and Thompson (2012), among others. This type of bootstrap procedures consists of creating a pseudo-population from the original sample and selecting bootstrap samples from this pseudo-population using the same sampling design that was utilized to select the original sample. We extend these procedures to the case of item non-response, where linear regression imputation is used to compensate for the missing values. Our procedures can be used even if the sampling fractions are large. In the presence of imputed data, two inferential frameworks are used in order to study the properties of point and variance estimators: the non-response model approach and the imputation model approach. We present two pseudo-population bootstrap schemes: the first leads to consistent bootstrap variance estimators with respect to the non-response model approach, whereas the second scheme leads to consistent estimators with respect to the imputation model approach. Results from a limited simulation study will be presented. ","Nonreponse may affect the quality of the estimates produced by statistical agencies when the respondents and the nonrespondents do not share the same characteristics with respect to the study variables. In this work, we focus on item nonresponse. We study the problem of preserving the relationship between items requiring imputation. Shao and Wang (2002) proposed a joint random regression imputation procedure and showed that it leads to asymptotically unbiased estimators of coefficients of correlation. We propose a calibrated imputation procedure, which consists of two steps: in the first step, missing values are imputed using the Shao and Wang procedure. In the second step, the imputed values derived in the first step are modified so that the imputed estimators of the first and second moments as well as the imputed estimator of the cross-product are calibrated on Minimum In Variance QUadratic Estimators (MIVQUE). More specifically, we seek a new set of imputed values in step 2, as close as possible to the original set of imputed values, so that appropriate constraints are satisfied. Results from a simulation study suggest that the resulting imputed estimators are efficient. ","The National Agricultural Statistics Service's Dairy Products Program estimates the production of butter, cheese, frozen products, dry milk and whey products as well as stocks for major states. Data are collected every month from over 1,000 facilities in the U.S. that produce dairy products. Like most surveys, non-response is an issue and is currently handled using manual imputation, primarily using the previous year's value to impute for the current year. We use computerized imputation methods that mimic the manual process to assess the efficacy of the current process as well as explore alternative computerized machine imputation methods such as a ratio imputation that could account for seasonality as well as overall growth or contraction of production while meeting operational needs. By repeatedly \"poking holes\" in observed Dairy Products data via several missingness mechanisms, we compare the performance of these methods. Performance is judged by seeing which method most accurately estimated the true dairy production and stocks totals calculated from the complete observed data. ","Missing data arises in almost all research, part or all of the data are missing for a subject. There are a number of alternative ways of dealing with missing data and we all have to decide how to deal with it from time to time. In this study, we evaluated complete case, single value and multiple imputation methods with prediction measurements of logistic regression model. We found that for missing completely at random (MCAR) mechanism, multiple imputation is better than the other two with small sample size and the single value imputation is doing almost as well as the multiple imputation. The three methods started showing less difference when the sample size was increased. ","Consumers access and communicate information today through diverse platforms enabled by a wide selection of devices - be it tablets, smart-phones, laptops or even e-readers. Soon more consumer will be responding to customer feedback surveys on tablets and smart phones.In this study we will conduct a simple \"Net Promoter System\" survey. NPS surveys ask customers to evaluate their experience via a simple \"Likelihood to Recommend\" question and one or two follow-up questions. These surveys' brevity and focus on direct contact with customers make them particularly good candidates for smart phone and tablet mode surveys.The \"Likelihood to Recommend\" question is asked on a 0 to 10 scale. Past experience with NPS surveys proves that customers from different cultures  use 0 to 10 scale differently, leading to different NPS results.Our goal is to understand whether and how answering these questions on different gadgets will affect NPS scores. The findings will particularly be useful to build better understanding of how online survey mode can affect scale use and help companies take advantage of mobile platforms for customer feedback. ","Survey researchers commonly face issues with missing data during analysis. Ad-hoc missing data methods, such as complete-case analysis, are easy to implement but they have well-known disadvantages of potentially yielding biased results and having reduced power due to deleting observations with missing values. The impact of missing data on survey estimates depends on the pattern of missing data, percent of missing data, and parameters to be estimated. Since most surveys experience some missing data, survey data analysis should account for missing data. Weighting adjustments may compensate for non-coverage and total nonresponse, but imputation methods that assign values for missing responses are more commonly used to compensate for item nonresponse. The IMPUTE Procedure in SUDAAN v11 performs the following imputation methods: weighted sequential hot deck imputation, cell mean imputation, linear regression for continuous variables, and logistic regression for binary variables. Data from public use files for the 2004 National Health Interview Survey are used to provide examples of each imputation method and the advantages and disadvantages of each.  ","One of the more time-consuming steps in the SCF production cycle is editing the survey data. Given the complexity of the SCF, editors must review the entire case and only make edits after careful and deliberate consideration to avoid introducing further errors. Performing such an exhaustive investigation for over six thousands cases requires a considerable number of work hours, but fortunately there are many methods of increasing the efficiency of editing, as well as gather information from the process to aid in continuously evaluating  data quality.    ","The Survey on Quebec Accommodation Establishment Occupancy is used to collect monthly data about accommodation establishments. Following the survey redesign that took place in 2011, an automated edit and imputation system (SIVEMEH) was created to handle item nonresponse for total rental income. One of the requirements of this improved system was to consider, when available, income from a previous month, current income (by category) and establishment type as auxiliary variables. These requirements have led to a system using composite imputation. The main imputation methods are nearest neighbour imputation and historical imputation. In some cases, the number of rooms occupied and the number of rooms available are also imputed to preserve the relationships between these variables and rental income. Large establishments received special attention during the creation of the system. This paper discusses the steps leading to the development of this improved imputation system and the first results obtained since its implementation in 2012. Future work is also discussed. ","Objective: The objectives of the study were to evaluate and modify the performance of existing eGFR equations in South-Asians. METHODS:GFR(mGFR) was measured by using urinary clearance of inulin in 581 adults in a cross-sectional study conducted in Karachi, Pakistan. CKD-EPI and MDRD Study equations were assessed and correction factor for CKD-EPI equation was derived and modified using linear regression. The main outcome measures were bias, precision, RMSE and accuracy P30 reported as cross validated estimates along with bootstrapped 95%CI based on 1000 replications. RESULTS: The CKD-EPI equation performed better than the MDRD equation in terms of lower RMSE p&lt; 0.001, greater accuracy at P30 p&lt; 0.001 and improved precision p&lt; 0.001. However, both equations overestimated GFR. The application of a correction factor(0.7xeGFR CKD-EPI1.059) to CKD-EPI for eliminating bias improved accuracy p&lt; 0.001. The performance of new equations developed in South-Asians was comparable to the modified South-Asian CKD-EPI equation. CONCLUSIONS:CKD-EPI is more accurate and precise than the MDRD Study equation in estimating GFR in South-Asian population in Karachi and should be preferred for eGFR reporting. ","The purpose of our study was to examine the consistency of reports of current and prior smoking and specifically to explore whether there are differences in the consistency of proxy-reported and self-reported smoking behaviors. Data came from the 2002-2003 Tobacco Use Supplement to the Current Population Survey, where the current smoking behaviors and smoking history of participants were reported by self- and proxy-respondents on two occasions, one year apart. Consistency was assessed with respect to ever-smoking and the age of fairly regular smoking initiation. Model-based and additional estimates were used to test the partially-ordered hypotheses of interest. Here we present our results with respect to consistency of the regular smoking initiation age.  ","The Qu\u00e9bec Survey on the Experience of Health Care is a telephone survey conducted between October 2010 and December 2011 among 48,100 respondents aged 15 and over living in a non-institutional dwelling in the province of Quebec. This paper focuses on the survey weighting strategy, which incorporates the use of paradata in the nonresponse adjustment. The idea was to take into account the difficulty in contacting each person from the sample, because this information is linked to the probability of response and to some of our main survey indicators, such as health condition. To achieve this goal, the number of phone calls was included in the logistic regression models used to create the weighting classes. Another distinctive feature of the nonresponse adjustment strategy was the idea of treating the people who were unable to respond to the questionnaire due to illness or disability differently from other nonrespondents. This type of nonresponse occurred when no proxy respondent could answer for the selected person. For that reason, it was decided to use only the proxy respondents to account for these nonrespondents.   ","Sample size determination is a crucial part of the planning process of a survey and it can be accomplished in different ways, some of them require information not available or that may be obtained with a substantial cost. Sample size calculation can be done by using the design effect estimator proposed by Kish. This estimator is also used as an efficiency measure for a probability sampling plan and to build confidence intervals. Even though the design effect estimator is widely used in practice, little is known about its statistical properties and there are no variance estimators available. In this paper we propose a method to estimate the variance. With this estimator it is possible to assess the precision of the estimators during the planning stage of a survey. An example using stratified sampling is given. ","Forecasting in food crop has become one of the crucial agricultural factors nowadays. With the help of the modern technology and statistical tools scientists try to evaluate and forecast the crop production, minimizing the errors as much as they can. Time series techniques served the purpose of forecasting for many years by fitting in classical models such as ARIMA, considering the past behavior of the data that already exist. General Regression Neural Network (GRNN) is one of the promising and upcoming areas of research in Statistics at present. It is a special tool to predict and compare system performance in practice. In this paper we discuss and compare the forecasts of rice production using the method ARIMA with the introducing tool GRNN. ","Missing data has struggled researches for many decades. Statisticians have put enormous effort to develop statistical technics to overcome this issue.  A perfect method has not been developed yet.  Technological advancements in previous decades have revolutionized the way statisticians handle the missing data. Adapting neural networks, one of such technological advancement to handle missing data has provided some better results than the classical statistical techniques. Many studies have carried out to investigate the performance of neural networks when handling missing data.  This study aims to compare the outcome of K-Mean and K-Medoid General Neural Network methods in handling missing data. ","In 2011, the first completely register-based census was conducted in Sweden. Several registers, such as the Total Population Register and the Real Property Register, have been matched to enable the results. Being the first of its kind in Sweden, the Census means that some methodological questions have been studied in more depth than previously. This paper summarizes the experience from the methodological work, the choices that have been made and the trade-offs. We particularly focus on the measures taken to evaluate and report the quality of the final statistical register, including an evaluation study. We also briefly describe measures taken to ensure the confidentiality of the published statistics ","We discuss an application of Hsu &amp; Berger's (1999) stepwise method for constructing simultaneous confidence intervals for the differences of two binomial proportions, where comparisons to a common control are of interest. While the original method was presented in the context of determining minimum effective dose in drug trials in terms of normally distributed data, the procedure is well suited for any model where the parameters monotonically increase or decrease over some specified groups. Applications of the method to survey data with binary outcomes are discussed with a focus on examining the relative prevalence of forward shifting error. First, logically ordered objectives are defined in the context of comparing forward shifting across initiation age of regular smoking. Next, the 2002-2003 Tobacco Use Supplement to the Current Population Survey data are used to construct the simultaneous confidence intervals adjusted for the multi-stage sampling design and other covariates. Simultaneous confidence intervals will also be constructed using the Bonferroni method for comparison. The key results as well as advantages and disadvantages of the approach are addressed. ","When evaluating a linear modeling approach and a hot deck approach for imputing an ordinal variable with one predominate category, we noticed rather large differences in the distribution of the imputed values. For the linear model approach, we used IVEware. IVEware does not have a cumulative logistic regression model for modeling an ordinal outcome variable. Therefore, we had two choices: treat the ordinal outcome variable as a nominal categorical variable or as a continuous variable. We chose to treat the ordinal outcome variable as a continuous variable and rounded the results to an integer. We will conduct a Monte Carlo simulation to determine if there is a more appropriate imputation approach under these conditions. In addition to the two imputation approaches above, we will treat the ordinal variable as a nominal variable with IVEware and using a proportional odds model for an ordered categorical variable with MICE. We will use three evaluation criteria: bias, coverage, and confidence interval length. ","The Bureau of Labor Statistics Quarterly Census of Employment and Wages Program developed a web-based data collection program to address the data collection needs of its Annual Refiling Survey (ARS).  The purpose of the ARS is to review and update the classification codes and geographic location codes assigned to the approximately 9 million worksites on this database.  Around 1/3 of these worksites are reviewed each year.      ","It has been shown that response rates are inadequate for measuring response representativeness and nonresponse bias (Groves 2006; Groves and Peytcheva 2008). Further, Schouten et al. state that \"subgroup response rates come closest [to supporting the data collection monitoring, targeting and prioritizing] but do not account for subgroup size, are univariate and are not available at the variable level\" (2011, p.1). Recent work has led to the development of \"R-indicators\", which are \"designed to measure the degree to which the respondents to a survey resemble the complete sample\" (Schouten et al. 2011, p.232). We examine R-indicators and partial R-indicators for weekly data collection returns in order to assess the representativeness of the respondents for the 2008 and 2010 National Survey of Recent College Graduates (NSRCG), sponsored by NSF. We use sample frame data to estimate response propensities and to examine unconditional partial R-indicators. We show the overall R-indicator provides a better indicator of representativeness than the response rate and that several frame variables are disproportionately contributing to an overall lack of representativeness in the response. ","Survey length is thought to have an impact on response rates. We examine the impact on response rates of adding a one-page, 14-question supplement to a mixed-mode survey. To respond quickly to the 2009 H1N1 flu pandemic, a survey supplement on vaccine uptake was added to the Pregnancy Risk Assessment Monitoring System (PRAMS), a state population-based survey. 30 of the 38 states conducting PRAMS implemented the supplement. By mail, the supplement was stapled to the back of the mail survey booklet. By telephone, the extra questions were asked at the end of the interview. We compared mail response rates and overall response rates after the supplement was implemented with response rates immediately before the supplement, with rates at the same time the previous year, and with rates experienced by states not implementing the supplement. Response rates increased in 14 states, decreased in 13 states, and did not change in 3 states after the supplement was added. The median mail and overall response rates among the 30 states were identical before and after the supplement was added, 53% and 66%, respectively. In conclusion, the supplement did not systematically decrease response rates. ","We study maximum likelihood estimation of the population mean for a survey experiencing unit nonresponse, i.e., when a sampled unit does not respond to the entire survey. We consider situations where post-stratification information is externally available for the population. Without external information, unit nonresponse, may lead to missing-data mechanisms that are missing not at random   (MNAR), which generally require a model for the missing-data mechanism. We develop a new model-based approach to weaken the missing at random (MAR) assumption by inclusion of external information for situations where the data are MNAR in the classical sense defined by Rubin (1976), but post-stratification information is externally available. This framework is then extended to also incorporate covariate information that is fully observed for the sampled units. We compare and contrast the proposed model-based method to existing design-based methods empirically for incomplete categorical data. ","The secondary data analyst is fundamentally hindered from implementing bootstrap variance estimation for complex survey data because information for adjusting bootstrap replicate weights for post-stratification and non-response are usually not publicly available.  Taking this is as a given, we ignored post-stratification and non-response weight adjustments in order to implement replicate adjustments described in Rust &amp; Rao 1996, then proceeded with bootstrap estimation (1000 replicates) of a simple weighted statistic (the sum) for complex survey data (NHANES) on the tobacco smoke biomarker NNAL in urine for comparison with the estimate from Taylor series linearization for the full survey sample.  The bootstrap and Taylor series estimates were found to be very close (&lt; \u00b10.52 percent).  We therefore proceeded to use the bootstrap to estimate variances for the optimal cutpoint and c-index from ROC curve analysis of NHANES urinary NNAL data to discriminate smokers from non-smokers.  The optimal cutpoint was 19.92 NNAL ng/g Cr [bootstrap CI 19.77:20.08; CV=12.39%] and c-index (equivalent to the area under the ROC curve) was 0.98978 [bootstrap CI 0.98970:0.98985; CV=0.11%]. ","In October 2008, the federal government issued its first-ever Physical Activity Guidelines for Americans to provide science-based guidance on the types and amounts of physical activity that provide substantial health benefits for Americans. Guidelines for Children and Adolescents ask for 60 minutes or more of aerobic, muscle strengthening, or physical activity daily. While the number of children in the U.S. who meet the Physical Activity Guidelines is unknown, the percentage of children who are physically active in the U.S. is declining. To better assess this situation the National Health and Nutrition Examination Survey (NHANES) National Youth Fitness Survey (NYFS) was conducted in 2012, simultaneously with the regular NHANES. The study was planned and conducted in concert with the 2012 NHANES but was an independent sample of 1500 children 3-15 who are not participants in the full NHANES, and exams took place in a separate mobile examination center customized specifically for the NNYFS. The unique planning and design of the NNYFS will be presented. An overview of the utility of the physical fitness and activity data collected in the NNYFS and simultaneous NHANES will be covered. ","In this study, we use quantile regression to answer the question: Is there a difference in the importance of questions for the students who score the course or the instructor as low, medium, or high?  Two advantages of using quantile regression are that in a series of observations, there may be a small portion of \"outliers\" which are represented more accurately by the quantile regression method and that the distribution of the sample can be accurately fitted.  We have collected data on over 5,000 student evaluations across multiple disciplines (Arts and Humanities, Business, Engineering, and Natural Science and Mathematics) and across undergraduate and graduate sections of courses.  We collected data in the Fall quarter for the years 2005, 2008 and 2011. We report our findings and note that using Quantile Regression can provide insights into which components of the instructor or course weigh against a student rating a professor higher or lower.  And we note differences that we found across disciplines and course levels. ","Race and ethnicity have often been seen as determinants of people's thoughts and behavior, yet measurement has varied over time.  While the 2010 Census provided 5 responses to measure Hispanic origin, the NHIS provided a simple yes or no response.  Further, measurement of race uses a slightly different item stem in the 2010 Census ('What is your race?'), the NHIS seems more accepting of multiple responses ('What race or races do you consider yourself to be?').  We conducted an experiment with over 17,000 respondents comparing variations in these items on race and ethnic identification and found little difference in overall proportions of option endorsement.  In a second experiment, we changed the nature of response entry with 3000 respondents and found significant differences between a multiple response format (as used in the Census and NHIS) and a yes-no grid format - the yes-no grid format had the highest multi-racial identification along with highest Hispanic identification.  We detail some of the substantial implications of varying response format on race-ethnicity measurement.  ","While most of the research on individual predictors of antisocial behavior focuses on the street and violent crimes, surprisingly less attention is paid to white-collar crime. In our experimental survey study we focus on tax evasion and insurance fraud. To estimate the prevalence of these crimes and to study the influence of individual characteristics on committing these crimes, we use a special interview technique for sensitive surveys (the crosswise model; CM), and compare it to direct questioning. The CM makes the interview conditions in sensitive surveys more anonymous. Therefore, the CM is expected to induce a higher sense of privacy on the question-and-answer process and generate more valid prevalence estimates of the white-collar crimes ('more-is-better' assumption). Our empirical results show that the CM reduces social desirability bias and elicits more valid self-reports regarding the sensitive behavior compared to standard direct questioning. ","Multiple imputation is a commonly used method to deal with incomplete data sets and is used by researchers on many different analytical levels. Imputation substitutes missing data with some values instead of discarding the entire case from the analysis. While dealing with large data sets with more than a few incomplete categorical variables, it is not possible to apply log-linear modeling due to limitations of sparseness. It is because we are not able to set up and process the full multi-way cross-tabulation required for the log-linear analysis. The latent class model is a plausible multiple imputation tool to solve this problem (Vermunt 2008). Another possible solution of a limited number of categorical variables associated with the log-linear method is to use hot-deck imputation (Rubin 1987). In this study, several multiple imputation methods for large categorical datasets will be tested. An advanced restricted latent class model-based multiple imputation method is proposed to be a better, more representative approach than the unrestricted latent class model since it specifies equality and inequality constraints on sums of conditional response probabilities. ","Clustered data commonly arise in social and biomedical sciences. As one example, multiple-source reports are often collected in child and adolescent psychiatric epidemiologic studies. Researchers use various informants (e.g. parent and child) to provide a holistic view of a subject's symptomatology.  These studies often have missing data due to multiple stages of consent and willingness to participate. Fitzmaurice and colleagues described estimation of multiple source models using a generalized estimating equation (GEE) framework, assuming MCAR missingness. Multiple imputation is an attractive method to fit incomplete data models under the less restrictive Missing at Random (MAR) assumption. We demonstrate how to utilize multiple imputation in conjunction with a standard GEE in a study of eating disorder symptoms with parallel reports from parents and adolescents in the ALSPAC study. While point estimates were fairly similar to the GEE under MCAR, the MAR model had smaller standard errors, while requiring less stringent assumptions regarding missingness. This approach is available within general purpose statistical software, and is recommended as a principled analytic approach. ","Generalized estimating equations allow for modeling of correlated data provided that any missing data are missing completely at random (MCAR). However, there are many instances in which the MCAR assumption is violated, such as the presence of informative cluster sizes.  It has been shown that cluster weighted generalized estimating equations allow for valid parameter estimation in the presence of informative cluster sizes and can be used in a longitudinal setting when cluster sizes remain fixed over time.  Here we consider the setting in which cluster sizes may change over time.  Through Monte Carlo simulation, we compare the performance of several weighting schemes, including time-dependent weights. ","Imputation of survey data is widely employed, and practitioners may draw from a large body of methods for doing so. The methods chosen must satisfy the requirements of the data users while leveraging auxiliary data and accounting for the complexities of the collection instrument. Individual, household, and family income are measures commonly collected in surveys of households, and while these data often carry relatively high importance when compared to other variables, they are often also subject to high levels of nonresponse, and estimates of income can be quite sensitive to misspecification of the imputation model. This paper addresses the imputation of family income using percentile-constrained inverse-CDF, regression, and hot-deck techniques for the 2012 Ohio Medicaid Assessment Survey (OMAS). OMAS is a telephone survey of 22,929 households with a primary objective of estimating the number of Medicaid-eligible and uninsured persons in the State of Ohio. As Medicaid eligibility is determined by income, its imputation can impact policy decisions.  The techniques are presented in the context of the larger, post-collection data activities, including imputation of related variables. ","Test-retest reliability of survey data is commonly assessed using descriptive measures, such as intra-class correlation coefficient, kappa and Pearson correlation. In addition, logistic regression models can be used to estimate the odds ratios of agreement between responses reported repeatedly. In some cases regression models can be used to assess the magnitude of the difference between responses. While the model-based reliability analysis allows for controlling for multiple factors and identifying subpopulations and survey administration characteristics that contribute to the highest and lowest degree of agreement, as well as estimating the variance properly, they have certain limitations when used to describe the degree of data agreement. The goal of our talk is to discuss these approaches and illustrate their applications via examples using the Tobacco Use Supplement to the Current Population Survey data. ","Here we explore planning for the allocation of resources for use in obtaining official statistics through model-based estimation.  Concentration is on the model-based variance for the classical ratio estimator (CRE).  This has application to quasi-cutoff sampling (simply cutoff sampling when there is only one attribute), balanced sampling, econometrics applications, and perhaps others.  Multiple regression for a given attribute can occasionally be important, but is only considered briefly here.  Nonsampling error always has an impact.  Allocation of resources to given strata should be considered as well.  Here, however, we explore the projected variance for a given attribute in a given stratum, for resource planning at that base level.  Typically one may consider the volume coverage for an attribute of interest, or related size data, say regressor data, to be important, but standard errors for estimated totals are needed to judge the adequacy of a sample.  Thus the focus here is on a 'formula' for estimating sampling requirements for a model-based CRE, analogous to estimating the number of observations needed for simple random sampling. Balanced and cutoff sampling are considered. "," Survey weighting adjustment takes the commonly used unequal sampling design, coverage discrepancy and nonresponse propensity into account and makes the sample be more representative of the target population. The weights are constructed on the variables that affect the probability of inclusion of samples to facilitate the finite population inference. Model-based inference predicts values of survey variables in the non-sampled units by including the survey design as model predictors. Since data analysts and survey organizers have different working duties, the analysts are often unclear about weights construction procedure. In this paper, we assume that we do not have any information on the predictors used in weighting. It is the weights themselves that implicitly contain all the information used in weighting. We propose a Bayesian procedure to account for the weights' uncertainty and include them as predictors for a nonparametric regression model under Gaussian process prior to make inference for the underlying finite population. We use simulation studies to evaluate the performance of the Bayesian procedure and apply it to the Fragile Families study.   ","The U.S. Energy Information Administration (EIA) conducts a monthly survey of natural gas suppliers, which collects volume and revenue data. These data are used for estimating State level natural gas total sales volume and average price published in EIA's Natural Gas Monthly. This paper documents the methodology used to estimate total volumes and average prices. We briefly compare the form of a design-based classical ratio estimator (D-CRE) to that of a model-based classical ratio estimator (M-CRE). While these two estimators are algebraically equivalent, the D-CRE may lead to a discrepancy in practice. Since the survey is based on a quasi-cutoff sampling design, a modeled-based estimator, like the M-CRE, is appropriate. Furthermore, unlike with the D-CRE, we can obtain associated standard errors for M-CRE estimates from a cutoff-sample. Finally, we discuss correlations of components in the price estimator, whose variance is based on the Taylor series estimator of a ratio, and the covariance computation involved in calculating standard errors of price estimates. This approach, of estimating price and its standard error, can be beneficial to other EIA surveys. ","Most complex surveys can be analysed efficiently by software that loads all the data into memory but there are a small number of large surveys where loading all the data into memory is inefficient or infeasible on typical laptop or desktop computers.  For example, the American Community Survey (ACS) includes 3,000,000 people per year and 80 sets of replicate weights, and the Nationwide Emergency Department Sample  (NEDS) includes more than 25,000,000 hospital visit records per year.     I will describe a computational architecture where computations involving arithmetic on full-size vectors are performed by the MonetDB database engine, controlled by dynamically-generated SQL code.  For most computations, only summary statistics need to be transferred to R for computations not available in SQL, in a few cases the whole data set is needed but can be transferred in small chunks. The system supports means, totals, medians, linear and loglinear models, smoothers and scatterplots, and is fast enough to allow interactive analysis of ACS datasets on a modern laptop. ","Hot deck (HD) imputation is a common method for handling item nonresponse in surveys, but most implementations assume data are missing at random. We combine the Approximate Bayesian Bootstrap (ABB) distance-based donor selection method of Siddique and Belin (2008) with the Proxy Pattern-Mixture (PPM) model (Andridge and Little 2011). The PPM model defines distances between donors and donees under different missingness assumptions, creating a proxy HD with distance-based donor selection to perform imputation, with an intuitive sensitivity analysis (SA). Missingness in the outcome is assumed to be a linear function of the outcome and the proxy variable, estimated from a regression analysis of respondent data. The SA allows for simple comparisons between ignorable and varying levels of nonignorable missingness. The PPM HD provides a more concise SA than using the more than 6 various `shaped' ABBs of Siddique and Belin. Compared to the parametric PPM model, the PPM HD is potentially less sensitive to model misspecification. We compare the bias and coverage of estimates from the PPM HD with the ABB HD through simulations and apply the method to data from the Ohio Family Health Survey. ","Researchers have long acknowledged that disability is a dynamic characteristic (Adler 1992, Verbrugge, Reoma and Gruber-Baldini 1994, Wolf and Gill 2008). Nonetheless, the concept is often treated as static over short periods in longitudinal studies. The disability status of a respondent is asked during one interview and assumed to remain constant over several interviews or for the life of the panel. I explore this assumption using reoccurring data on disability status from the Survey of Income and Program Participation (SIPP). In the 2008 panel, the six-question set of disability questions from the ACS were added to a reoccurring topical module. I employ structural models from Heise (1969) and Wiley and Wiley (1970) to separate reporting error from real change under two assumptions about the measures' reliability. Both methods assume that disability status follows a first-order Markov process. With these methods, I find that the disability measures in the SIPP had relatively moderate to low reliability with coefficients between 0.414 and 0.638. Conversely, an individual's true disability status is strongly correlated with the person's status one year later (r=0.937). Thus, the supposition that disability remains relatively consistent over short periods has some validity. ","Response rates are an important indicator of survey quality and the potential for nonresponse bias. Until the American Association for Public Opinion Research (AAPOR) developed a standard definition for response rates in 1998, the survey research community used different formulas or rules to calculate them.  By having a set of industry standards, response rates became easier to interpret and to compare across surveys.  While this was a major improvement, the response rates (essentially one formula with six variations) were overly simplistic in terms of how they dealt with eligibility rates for those with undetermined status.  The AAPOR standards give some guidance on computing the eligibility rate and applying the response rate formulas to more complex samples.  This paper provides additional guidance and examples for estimating the eligibility rate, implementing the response rate formulas in complex samples, and applying multiple eligibility rates when eligibility is nested. This paper also provides alternative but algebraically equivalent response rate formulas for one-, two-, and three-stage samples, some of which may be easier to interpret or implement than the AAPOR versions. ","Two years ago, the 2011 Egyptian revolution took to the nation's streets in the form of tens of thousands of protestors. The ongoing Egyptian revolution, as a part of the greater Arab Spring, plays an integral role to the future of the region. As we continue to watch in great interest, the field of survey research faces the challenge of applying sound methodology in the face of political and social unrest. Given the volatility and constantly changing landscape of Egypt, face-to-face sampling at a national level is currently not a desirable mode of surveying due to the security of the field team, accessibility of the target population, and field length. While in the United States, prominent researchers and government studies gather data that help guide sample design and explore the options of dual-mode telephone sampling, this level of data and research is not readily available in Egypt. D3 Systems, Inc. has designed a large-scale national face-to-face multistage probability survey to better understand the telephone owning population in Egypt and help guide future dual-frame CATI sampling plans.   ","The National Center for Education Statistics (NCES) has begun developing online, self-paced, computer-based training (CBT) modules to facilitate and encourage interested researchers, students, policy analystsrs, and practitioners to make hands-on use of its detailed public use and restricted access complex survey data sets.  Although NCES also disseminates information from its many data collection programs through publications, online tabulations, and basic online data analysis tools, more advanced research questions and finer grained analyses using our detailed data sets are best addressed by using special survey software that accommodates complex designs through appropriate weighting and variance estimation routines.  An update on the development process will be offered including presentation of the basic format, structure, functionality, and content of the CBT modules, along with a draft release schedule for modules pertaining to specific surveys.   ","In smoking cessation research, heaping of self-reported cigarette numbers is a common problem because participants tend to report some rounded-off integers such as the multiples of 5, 10, or 20 instead of the exact counts. These excessive values show additional peaks in histograms and distort the conventional probabilistic distributions, e.g. Poisson or negative binomial, for count data analysis. Empirical evidence also suggests that in addition to heaping, particular patterns of smoking exactly half or one pack per day exist among smokers.  In this work, we propose a finite mixture model for the actual latent count of cigarettes per day (CPD) to account for inflation of 10- and 20-CPD due to the personal smoking patterns, and a proportional odds model to connect the latent to the observed CPD for heaping behavior.  Covariates e.g. age, gender, racial/ethnic groups are considered. We demonstrate this approach by analyzing data from the Center for American Indian Community Health (CAICH) to investigate American Indians' smoking behavior. ","  Likert scales are commonly used in observational studies, such as surveys involving the frequency of school violence, an important public health issue associated with adverse health outcomes. We demonstrate how the proportional odds model and the trend odds model can be applied to data measured in Likert scales, allowing for random cluster effects. We use survey data from 28,882 students in 81 middle schools from a large urban school district to examine simple and adjusted effects of binary, continuous and multinomial predictor variables that have been found to be associated with violence exposure. We show, for example, that the trend odds model indicates that children with disability have a higher risk of severe violence, and that the odds is more pronounced (e.g., a cumulative odds ratio that increases by 66%) at the higher Likert levels. Different latent distributions of the underlying Likert scale distributions are also presented and discussed.     ","The Medicaid records that states submit to the Centers for Medicare &amp; Medicaid Services (CMS) through the Medicaid Statistical Information System (MSIS) do not contain names and addresses, but they do contain Social Security numbers (SSNs). Any attempt to link Medicaid records to other databases must rely almost exclusively on these SSNs. The effectiveness of these linkages and the validity of any research based on these linked data are directly dependent on the quality of the SSNs recorded in the MSIS files. This paper documents how often SSNs were present in the MSIS records that states submitted for the final quarter of federal fiscal year 2009 and uses Social Security Administration data enhanced by the Census Bureau to assess the validity of reported SSNs by age group and state of residence. Implications for linkage of Medicaid records to other databases are discussed. ","The National Center for Health Statistics has linked many of its population health surveys to administrative records from the Centers for Medicaid and Medicaid Services (CMS).  One collection of linked CMS administrative files is the 1999-2008 Medicaid Analytic eXtract, or MAX, files.  Analyses using the linked files are complicated by several factors, including:  1) temporal relationships between the NCHS survey year and the MAX data years; 2) Medicaid program eligibility and enrollment, which can be non-continuous for a specific beneficiary and can differ across states and time; 3) linkage-eligibility requirements, including NCHS Ethics Review Board guidance on linkage of administrative records for child survey participants.  Statistical approaches to address these issues can range from study design and analytic file construction to missing data methodologies and modeling.  This presentation will discuss the strengths and limitations of the NCHS-MAX linked files for health-related inferences in the context of these issues, illustrated with examples from the linked 1994-2005 National Health Interview Survey-MAX files.   ","Racial and ethnic disparities in health and health care are complex and continue to challenge researchers and policy makers. With the intention of improving the measurement and monitoring of disparities, certain provisions of the Patient Protection and Affordable Care Act (ACA) of 2010 will require states in the coming years to collect, report and analyze data on demographic characteristics of applicants and participants in Medicaid and other federally supported programs. By linking Medicaid records to 2010 Census, American Community Survey, and Census 2000, this new large-scale study examines and documents the extent to which pre-ACA Medicaid administrative records match self-reported race and Hispanic origin in Census data. Linked records allow comparisons between individuals with matching and non-matching race and Hispanic origin data across several demographic, socioeconomic and neighborhood characteristics, such as age, gender, language proficiency, education and Census tract characteristics. Identification of the groups most likely to have non-matching and missing race and Hispanic origin data in Medicaid relative to Census data can inform strategies to improve the quality of demographic data collected from the Medicaid population.    ","The ability to observe within-subject change over time is the primary objective of most panel surveys. When characteristics of the data collection process systematically affect reporting differently at different times, it becomes difficult to differentiate true change from measurement error. The Medical Expenditure Panel Survey (MEPS) employs an overlapping panel design in which new cohorts enter the survey every January and are interviewed five times covering a cumulative two-year reference period. Underreporting is a perennial concern for household surveys and this concern may be exacerbated in panel surveys because of issues such as panel conditioning (Kalton et al 1989). In particular, a review of the literature pertaining to the accuracy of household-reported healthcare utilization data suggests that medical events tend to be underreported (Bhandari and Wagner 2006; Zuvekas and Olin 2009). Separate MEPS panels consistently exhibit a pattern of disproportionately high medical event reporting in the first round relative to all subsequent rounds and an additional decline at the final round of data collection. The fact that this pattern persists across separate panels suggests that these differences may reflect measurement error. Steps to repair this error will depend on its cause. One hypothesis is that respondents reduce their reporting in Round 2 in order to reduce burden. Alternatively, the error may be cognitive in origin, with longer reference periods in Round 2 resulting in a greater level of forgetting on the part of the respondent. In this paper we compare the plausibility of these hypotheses for explaining changes in response patterns using both paradata and survey responses. We find no support for the hypothesis that burden leads to lower reporting, however, we do find a negative association between the length of the reference period and the level of reporting. ","Level-of-effort data are paradata generated by the process of collecting data in surveys. Level-of-effort data are developed from call record data and include proxy measures for contactibility and cooperation. For example, the number of calls required to complete an interview is a proxy for contactability. Many surveys use these data as predictors in nonresponse adjustment models. However, recent research has found that these data may include measurement errors which may reduce their effectiveness for adjustment purposes (Biemer, Chen, and Wang, 2013).     ","The Computer Audio Recorded Interviewing (CARI) system is a laptop software application that seamlessly records the verbal exchange of pre-specified questions between the interviewer and the respondent without disrupting the normal interview process. After the interview, any recordings obtained are digitally stored and can be reviewed later for quality assurance purposes. CARI was incorporated into the 2012 Survey of Income and Program Participation Event History Calendar (SIPP-EHC) instrument. The purpose of this research was to determine the impact on response rates and item level responses due to the addition of CARI to the survey instrument. Distributions and item nonresponse rates were compared within the 2012 CARI SIPP-EHC test and with the 2011 SIPP-EHC test. Cooperation rates were calculated to determine the percent of respondents who agreed to be recorded. Statistical models were constructed to determine if certain characteristics of respondents or geographic regions predict CARI consent propensity. Finally, we examined interviewer effects.  ","The Current Population Survey (CPS) is a monthly household survey of 72,000 households conducted by the U.S. Census Bureau for the U.S. Bureau of Labor Statistics to measure employment, unemployment, and other characteristics of the civilian non-institutionalized population in the United States.  The CPS began in 1940 and provides data for key economic indicators.  In this paper, we will study CPS rotation panel bias, investigate whether the estimates of unemployment for different Month-in-Sample (MIS) panels are statistically significant, and explore the assumptions underlying the AK composite estimate.  We are particularly interested in the comparison of MIS 1 versus MIS 5 because they are based on personal interviews and are key elements in AK composite estimate.  We apply a nonparametric statistical method with dependent permutations across time to real CPS data from 2006-2010.   ","Measurement error modeling approaches have been used extensively in nutrition studies to estimate distributions of usual dietary intakes by accounting for and adjusting for day-to-day variability and measurement errors in observed intakes. Similar procedures have recently been developed for studies of physical activity and energy expenditure, but applications usually focus on study data obtained from adult populations. In this paper, we use measurement error modeling procedures to estimate the distributions of usual physical activity and the sources of variation in physical activity data collected via accelerometers from a sample of 4th- and 5th-grade U. S. students. The students were part of the Randomized Experiment of Playworks study. We found that most of the variability in the physical activity data was due to intra-individual (day-to-day) variations in measured activity. Conversely, in studies of adult populations, the majority of variability in physical activity was inter-individual variability; intra-individual sources of variations in activity were fairly minimal. Adjustments for measurement errors and other sources of intra-individual variations should be made when estimating usual physical activity outcomes, especially in populations of children.  ","Educational survey assessments have been used for decades to monitor what students know and can do. Such type of assessments is aimed to provide group-level scores for various populations, with little or no consequence to individual students for their test performance. Therefore, the students' test-taking behaviors, particularly the level of test-taking effort, have become a long-standing question. This paper presents a data-mining procedure to examine students' test-taking behaviors using response time collected from a computer-based survey test. An exploratory approach is considered to classify students' responses to test questions as reflecting either solution behavior or rapid-guessing behavior. Results of behavior classification are then summarized by three different statistical measures to show the degree of engagement in solution behavior. Considerations about how to identify response time thresholds and several validity checks to ensure reasonable classification of behaviors are also discussed. The procedure detailed in this paper is applicable to any computer-based assessments or surveys where timing data are available. ","Statistics Canada's G-Confid system uses cell suppression to protect the values of sensitive (confidential) cells in tables of magnitude. It uses a heuristic approach to generate a suppression pattern that minimizes the resulting loss of data. While G-Confid achieves its primary purpose users would like it to handle additional situations such as the treatment of weighted survey data, of negative values and of waivers. Waivers are used when, in an attempt to disseminate more data, a statistical agency obtains from certain large respondents the permission to release information that may disclose their value. Some users would also like to influence the generated suppression patterns, for example to decrease the likelihood of suppressing cells that are of greater interest, or to orient the suppression towards cells of poor quality. After giving an overview of G-Confid, the paper will describe approaches that can be used to address these and other needs. Although the approaches operate within the confines of the G-Confid system they may be implemented within other cell suppression programs. ","Some variables in magnitude tables may have negative values: net income, net production, capital expenditures, and so on. Negative values pose a computational challenge for the application of disclosure-limitation techniques to tabular data. A table cell may be identified differently (as sensitive or not) if some of cell contributions are negative rather than positive. A negative value reduces rather than increases the cell magnitude, which increases the percentage contributions of the largest observations. There are three common strategies for dealing with variables with negative values: (1) taking absolute values of the negative contributions and applying sensitivity rules to these positive values, (2) increasing disclosure limitation by increasing or decreasing the parameters of the sensitivity rules, and (3) changing the variable to a positive variable and then applying the sensitivity rules as usual. This paper will discuss the application of sensitivity rules in statistical disclosure limitation to magnitude tabular data with negative values. ","To maintain confidentiality national statistical agencies traditionally do not include small counts in publicly released tabular data products.  They typically delete these small counts, or combine them with counts in adjacent table cells to preserve the totals at higher levels of aggregation.  In some cases these suppression procedures result in too much loss of information.  To increase data utility and make more data publicly available, we propose to generate synthetic values for the small counts from a Bayesian hierarchical model.  We do not disturb the counts in the data tables that were considered safe.  We also discuss how the same model allows for computation of several disclosure risk measures.  ","National Center of Science and Engineering Statistics employs the method of cell suppression in tabulation (or cs-tabulation) for disclosure-treatment of sensitive cells in the Survey of Earned Doctorates (SED). The symbol 'D' is used to replace cell values subject to suppression including complementary suppressed cells. However, its impact on estimates for underrepresented minority of race/ethnicity groups and women may be quite severe. To alleviate this concern, it is proposed to enhance cs-tabulation by estimating 'D' cells via log-linear modeling. The resulting complete table is expected to have high utility for users at large because model-based best prediction is used for suppressed cells from the available unsuppressed information. In particular, the estimates can be used to check the underlying trend over different subgroups cross-sectionally and for a given subgroup longitudinally. The proposed method uses only information released under cs-tabulation and therefore does not increase the disclosure risk. Also it preserves all the unsuppressed cells and marginal counts. Applications to NSF-SED data are discussed.  ","Suppressions in a table complicate, thwart or completely prevent statistical analysis of the table.  Techniques similar to those of the intruder and based on prior knowledge, public information, and inconsistencies among suppressions can be applied by the analyst to deconstruct suppressions, yielding exact values or tighter bounds on suppressions.  Alternative tables-tables satisfying the new, stricter relationships between suppressed entries-can be created and used to approximate analysis on the true, but presumably still safe and unknown, table.  We present proposals for table deconstruction and statistical analysis of suppressed tables of count or magnitude data. ","The bootstrap is a convenient tool for estimating design variances of finite population parameters or model parameters. It is typically implemented by producing design bootstrap weights that are made available to survey analysts. When analysts are interested in making inferences about model parameters, two sources of variability are normally taken into account: the model that generates data of the finite population and the sampling design. When the overall sampling fraction is negligible, the model variability can be ignored and standard bootstrap techniques that account for the sampling design variability can be used. However, there are many practical cases where the model variability cannot be ignored. We show how to modify design bootstrap weights in a simple way to account for the model variability. The analyst may also be interested in testing hypotheses about model parameters. This can be achieved by replicating a simple weighted model-based test statistic using the bootstrap weights. Our approach is related to the Rao-Scott test (Rao-Scott, 1981). We illustrate through a simulation study that both methods perform better than the standard Wald or Bonferroni test.  ","In this paper, we apply the recently developed parametric bootstrap method in constructing confidence intervals for the well-known Fay-Herriot model in estimating survey-weighted small area proportions. Through design-based simulation studies from a real finite population and extensive purely model-based simulation studies, we examine the coverage properties of the parametric bootstrap confidence intervals for the Fay-Herriot model. We also compare them with those obtained from other competing methods for the same model. ","For randomly imputed survey data, two types of bootstrap variance estimation have been proposed: the adjustment approach where imputed values are adjusted to reflect the expectation of the imputed values in a bootstrap sample; and the reimputation approach where missing values are reimputed in a bootstrap sample. Both of them are known to provide consistent variance estimation if hot deck imputation yields approximately unbiased point estimation. In our simulation study, we apply the two approaches to variance estimation of sample quantiles under doubly protected hot deck imputation. Doubly protected hot deck imputation provides approximately unbiased point estimation if the response model or the imputation model is correct. Because to preserve the distribution of a variable under study is one of objectives in exercising hot deck imputation, variance estimation of sample quantiles is of practical importance. The simulation shows that the reimputation approach provides consistent variance estimation if either the response model or the imputation model is correct while the adjustment approach perfomrs poorly when the imputation model is incorrect. ","Address Based Sampling (ABS) has the advantage of allowing the use of census files to oversample minorities. This study uses a file listing zip code areas and their population for three ethnic groups to identify the optimal strategy for oversampling minorities to maximize the minimum effective sample size for each minority group. Several different sampling strategies were simulated, assuming the ability to randomly sample from zip code areas or combinations of zip code areas. One of the strategies involved sampling zip code areas with replacement using various measures of size that took into account the number of residents in each of the three groups (African-American, Hispanic and other) and then randomly sampling a resident for each time a zip code was sampled. A second set of approaches created strata based on the prevailing minority and the proportion of that minority in the zip code area, and then sampled strata with high minority residence at a higher rate. A third set of approaches used a self-weighting sample and combined or cumulated it with a second sample that used one of the strategies mentioned above. The strategies were evaluated not just in terms of the proportion or number of minorities in each sample, but of the effective sample size due to weighting for each minority. ","Previous research on the United States Postal Service's Delivery Sequence File (DSF) has found it to have high national population coverage rates (95-97%) (see Fahini 2009).  This coverage varies, however, by survey mode.  Some addresses are not complete enough to mail to or match to a telephone number.    ","In survey sampling initial sample size allocations are generally computed using information such as strata size, response rates and desired target sample size. In practice within strata variability is often unknown so proportional sample allocation is generally preferred. Proportional allocation allows every unit across the different strata to have the same probability of selection and the selected sample units often belongs to the target population. In studies whose target population are Hispanic households, for instance, proportional allocation may yield a sample of households that are not all Hispanic households. The probability of selecting a Hispanic household would depend on the actual number of Hispanic households in the strata and differs by strata. We discuss an allocation approach that uses vendor-provided data about the likelihood of a household unit belonging to the target population. Using linear programming techniques we illustrate how to obtain optimal sample allocation for different strata response and accuracy rates. We also present a sensitivity analysis of accuracy rates obtained from the vendor-provided data. ","In recent decades, the U.S. Census Bureau has measured the net coverage of the census of population and housing through a comprehensive post-enumeration survey. Following field work, data collection, matching, and processing, the Census Bureau released estimates of net under- or overcount for the U.S. as a whole, for large geographic areas (e.g., states, large counties and cities), and for major demographic groups, e.g., by race, Hispanic origin, gender, and age group. In the 2010 Census, for the first time on a production basis, the Census Bureau produced estimates of the components of census coverage: correct and erroneous enumerations, whole-person census imputations, and omissions. In this paper, we briefly define these components, and present estimates of coverage for the household population of the United States, and for several census operations, including date of mail return, month of nonresponse follow-up, and type of census response (self vs. proxy). Analysis of these results will help plan improvements to procedures for the 2020 Census. ","When there is no available list sample frame, area probability sampling enables researchers to select samples and obtain reliable estimates for household populations. But a considerable amount of time and cost is associated with on-site enumeration, precluding it from being a viable approach in building sampling frames for many household surveys. Area sampling using information technology (IT) may be an alternative for saving time and effort. In this paper, we present a new methodology for constructing household sampling frames based on the existing Internet information services in a metropolitan survey, which was conducted to investigate health problems among residents from communities surrounding industrial complexes in Korea. This method uses highly advanced IT including mailing address information and map service. It is also useful in the cases where the list of enumeration districts is not open to the public, like Korea. Since there is no limitation in the size of areas as well as the number of dwellings in the survey population, this approach would be applicable to national surveys. Some survey results are provided. ","Many participating countries in the first round of the Programme for the International Assessment of Adult Competencies (PIAAC) utilized their national population registries as sampling frames. A list of residents is an ideal frame for PIAAC if it provides adequate coverage and includes reliable information to sample and locate individuals for the interview. One challenge faced by some countries with registry samples is the inability to locate some of the selected persons at the registered address, due to erroneous registry data, resulting in high nonresponse rates. For countries with a high proportion of inaccessible persons, we developed an alternative approach that employs address-based sampling in conjunction with person-based sampling from a registry. This paper describes the multi-frame design and discusses the advantages of such a design as compared to single-frame designs that employ person-level sampling from registries. ","The U.S. Census Bureau has initiated substantial research efforts to determine operational uses of administrative records for the 2020 decennial census.  This paper examines administrative records coverage of the U.S. from the unique perspective of comparing it to 2010 Census Coverage Measurement estimates which were produced to assess and measure the quality of the decennial census.  For this evaluation, an administrative records universe was researched and designed so as to be directly comparable to the 2010 Census counts and the 2010 Census Coverage Measurement estimates.  By comparing the coverage of administrative records to these two universes at various levels of geography, by demographic group, and by census operational variables, this paper assesses the quality and utility of administrative records.  These findings will help inform 2020 Census research projects evaluating how administrative records can be used in frame development, imputation, and improving quality control. ","This paper examines sub-state spatial and temporal variation in survey misreporting about participation in the Supplemental Nutrition Assistance Program (SNAP). I link several years of the American Community Survey to SNAP administrative records from New York (2008-2010) and Texas (2006-2009) and calculate county false-negative (FN) rates for each year. I find that, within a given state and year, there is substantial heterogeneity in FN rates across counties. In addition, I find evidence that FN rates persist over time within counties. This persistence in FN rates is strongest among more populous counties, suggesting that when noise from sampling variation is not an issue, some counties have consistently high FN rates while others have consistently low FN rates. This finding is important for understanding how misreporting might bias estimates of sub-state SNAP participation  rates, changes in those participation rates, and effects of program participation. ","The Census Bureau is conducting research on the use of administrative records (AR) as tools to enhance survey data products. A possible use of AR is improvement to edit and allocation procedures for ongoing data collections, such as the American Community Survey (ACS). Previous research (Benetsky, et al. 2012) explored this potential with the ACS question on residence 1 year ago. The analysis linked the ACS and the National Change of Address (NCOA) database by Personal Identification Key (PIK). Of the 35 million NCOA records, 45 percent received duplicate PIKs. These records were dropped to enable a straightforward first look into the results of the ACS and NCOA data linkage. This paper now analyzes the records with duplicate PIKs to determine a set of patterns and characterize migrants with duplicate PIKs. Cursory review showed several possible reasons for a duplicate, including, multiple moves within the data year, consolidating or splitting mailing addresses, and simply being PIKed incorrectly. This research will provide insight into the additional quality and production issues involved in using NCOA to improve ACS edit procedures. ","The 2010 National Survey of College Graduates (NSCG) used 2009 American Community Survey (ACS) respondents as its sampling frame.  This sample design afforded NSCG the opportunity to use a rich set of data available from the ACS to conduct weighting research.  In particular, research on response propensity based locating and nonresponse adjustments and weight trimming benefited greatly from the ACS data.  We researched response propensity models using ACS data collection paradata, as well as ACS survey data, to aid in predicting NSCG response patterns.  We used education, occupation, and demographic information from the ACS to evaluate the effect of various weight trimming techniques on the mean square error for a wide range of variables.   ACS' rich set of information enhanced NSCG's ability to improve our weighting methodologies and therefore produce higher quality survey estimates. ","The purpose of this research was to examine the consistency of responses for similar questions asked in the American Community Survey (ACS) and the National Survey of College Graduates (NSCG) to better leverage the use of the ACS in the sample design and estimation of the NSCG. This research evaluated the ACS responses used in the NSCG sample design to determine if the current NSCG design was efficient for key statistics, and to identify new ACS responses that may strengthen the design. Additionally, this evaluation identified variables that could be used in model assisted and model-based estimation for the NSCG. The research consisted of two phases: response comparison and examination of response differences. We looked at individual responses to find patterns that lead to the inconsistency of occupation and education. This included constructing statistical models involving questionnaire responses and paradata to find covariates related to inconsistency. ","The National Survey of College Graduates (NSCG) has been conducted by the Census Bureau for the National Science Foundation (NSF) since the 1960s.  It is the nation's only source of detailed statistics on the science and engineering (S&amp;E) labor force.  The NSCG uses a rotating panel design and selects its sample on a biennial basis from the American Community Survey to allow both cross-sectional and longitudinal analysis of education, employment, and demographic characteristics of the S&amp;E labor force.  Under this design, the NSCG data is collected and released on a biennial or triennial schedule.  The 2010 survey cycle marked the initial use of the ACS as a sampling frame for the NSCG.  The 2010 NSCG responses allow NSF the ability to produce estimates of the S&amp;E labor force as of the 2010 calendar year.  The next NSCG survey cycle is scheduled for 2013 and will allow NSF the ability to produce estimates of the S&amp;E labor force as of the 2013 calendar year.  Using the results from the 2010 NSCG survey cycle, we examined the feasibility of producing 2011 and 2012 S&amp;E labor force estimates using 2011 and 2012 ACS data.   ","One goal of adaptive design is to allocate data collection resources efficiently, rather than exhausting money and time simply to increase response rate. Data monitoring is vital to this effort as it provides updated views of response information and data quality throughout the collection period. More up-to-date information can lead to interventions including: reducing contact attempts for low-impact cases unlikely to respond, switching contact mode to maximize response probability, or even stopping data collection. In the 2013 NSCG, data monitoring will help show the evolving state of data collection and inform interventions in a mode switching experiment.     ","With the recent and rapid growth in GIS technology, this has become a powerful tool in the survey researcher's kit. GIS has been used for listing households, but maybe there is potential for additional uses. For example, GIS could be used to geographically stratify the population or, in some case, even geo-locate population elements, to assign interviewers, to monitor field progress, and to establish effective weighting groups. Furthermore, GIS could be used to more effectively present the data for easy user understanding and to even shed light on relationships between variables. It would interesting to discuss what uses are being made of GIS in current survey research practice and what participants see as the future role of GIS in survey research. ","The Census Bureau is evaluating the use of administrative records to reduce costs and improve data quality for the 2020 Census.  Person and housing unit records from the census are matched to administrative records from federal, state, and commercial sources using record linkage techniques to evaluate how the auxiliary information can improve planning and operations.  The matching relies on complete, accurate identifiers including name, address, date of birth, and sex.  In the 2010 Census, ten million person records lacked name data.  Without names, can these records be used in linkage applications?  This paper addresses this question and explores the quality of variables other than name that could be used to link these census records to administrative records. ","In 2009, the National Center for Health Statistics (NCHS) released the 2006 Linked Mortality Files (LMF), its population health surveys linked to the 2006 National Death Index (NDI).  Auxiliary data from the Social Security Administration's Death Master File, data from previous linkages to administrative records from the Centers for Medicare and Medicaid Services, and other direct data collection efforts were used to improve the 2006 LMFs.  Extensive research on weighting, scoring, and other aspects of the probabilistic linkage methodology was also performed for the 2006 LMFs.  Since then, limitations on surveys' capture of SSNs (9 to 4 digits) may challenge the quality of future NCHS-NDI linkages.  However, updated algorithms (e.g., to account for Asian and Hispanic naming conventions) and tools can be applied to the process to improve linkage quality.  This presentation will discuss our research efforts to improve the linkage methodology, with attention to the impact of the 4 digit SSNs and new algorithms for names, for the upcoming release in 2013 of the 2010 LMF. ","The task of deduplicating a datafile can be solved by estimating the elements of a linkage matrix, where each entry is associated with a pair of records, and it is equal to one if both records refer to the same underlying entity, and zero otherwise.  The current approach for supervised probabilistic deduplication consists on training classifiers on pairs of records that are hand-matched, and then predicting the matching status of the remaining pairs.  Unsupervised probabilistic deduplication typically uses a mixture-model implementation of the Fellegi-Sunter methodology for record linkage to link the datafile with itself.  These previous approaches ignore the dependencies among the entries of the linkage matrix, since they output independent linkage decisions for all pairs of records.  We propose a method that outputs estimates of the linkage matrix such that transitive decisions are guaranteed.  The proposed method uses a simple stochastic blockmodel for multigraphs.  We also show how this method can be extended to handle simultaneously deduplication and record linkage. ","In this paper we review some recent advances on Bayesian methodology for performing record linkage and for making inference using the resulting matched units. In particular, we frame the record linkage issue into a formal statistical model comprising both the matching variables and the other variables  included in the inferential stage. This way, we account for matching process uncertainty in linked data inference but we also cause a feed-back propagation of the information between the interesting inferential model and the record linkage stage. To obtain posterior summaries we will use  standard Bayesian computational techniques.  Although the resulting methodology for linked data inference is quite general, we will focus on population size estimation and on the multiple regression context.     ","In this talk, we consider statistical models with nonignorable missing data. Without any further assumption, unknown parameters may not be identifiable when the missing mechanism is nonignorable. We develop a pseudo likelihood method under nonignorable missingness without specifying the form of the missing mechanism. For the identifiability issue, We derive explicit conditions for generalized linear models. We establish asymptotic theory for this pseudo likelihood based estimator and provide an efficient algorithm to handle the computational challenge. We illustrate the method through the analysis of HIV-CD4 data and some simulation studies. This is joint work with Dr. Jun Shao, University of Wisconsin, Madison. ","Most methods that deal with the estimation of response probabilities assume either explicitly or implicitly that the missing data are 'missing at random' (MAR). However, in many practical situations this assumption is not valid, since the probability to respond often depends on the outcome value.  The case where the missing data are not MAR (NMAR) can be treated by postulating a parametric model for the distribution of the outcomes under full response and a model for the response probabilities. The two models define a parametric model for the joint distribution of the outcome and the response indicator, and  the parameters of this model can be estimated in principle by maximization of the likelihood corresponding to this distribution. Modeling the distribution of the outcomes under full response can be problematic since no data are available from this distribution.  Sverchkov (2008, 2010) proposed a new approach that permits estimating the parameters of the model holding for the response probabilities without modelling the distribution of the outcomes under full response. In the present paper we show how this approach can be used for testing whether the response is MAR or NMAR   ","The propensity-score-adjustment method is a popular technique for handling unit nonresponse in sample surveys. If the response probability depends on the study variable that is subject to missingness, estimating the response probability requires additional distributional assumptions about the study variable. Instead of making fully parametric assumptions about the population distribution and the response mechanism, we propose a novel approach that is based on the distributional assumptions of the observed part of the sample. Since the model for the observed part of the sample can be verified  from the data, the proposed method is less sensitive to failure of the assumed model. Variance estimation is discussed, and results from two limited simulation studies are presented to compare the performance of the proposed method with the existing methods. The proposed method is applied to the analysis of election Exit Poll data in the 2008 general election in Korea. ","Missing response data problem is ubiquitous in survey sampling, medical, social science and epidemiology studies. It is well known that non-ignorable missing is the most difficult missing data problem to deal with. In this paper we study a semiparametric non-ignorable missing data problem, where the missing probability is assumed to be known up to some parameters, but the regression model for the responsevariable conditioning on the covariate is not specified. By employing the empirical likelihood method we can construct constrained empirical likelihood. Moreover the semiparametric likelihood ratio statistic can be used to test whether the missing of some of the responses is non-ignorable or completely at random. A real AIDS trial data set is used for illustration. Analysis result indeed shows that the missing data of CD4 count around 2 years are non-ignorable. The naive sample mean based on observed data only is biased.   ","The Foreign Trade Division at the US Census Bureau is responsible for the production of monthly import and export statistics. Data processing begins with extensive micro-editing using an automated editing and imputation system; records that do not pass the edits are automatically imputed. However, imputation may not be successful for a small portion of the failed records. Records for which imputation is not successful are sent to subject matter experts for manual review. Commodity analysts use their expertise to manually adjust failing records and must review a large number of records under tight time constraints by the monthly statistics publication deadline. Due to time and resource constraints, we have an ongoing effort to improve the current procedures while preserving (or improving) data quality. We investigate selective editing strategies for these data. Selective editing requires a score function to assign a priority ranking to erroneous records. We propose a score function that includes a measure of how suspicious a record is and a a measure of the anticipated error in the suspicious record. We present methodology and results of an evaluation study of the application of selective editing to foreign trade export data records. ","A common practice of a national statistical agency (also most scientific investigations) is to assess its estimate for a particular real-valued parameter by comparing it to comparable estimates from other sources. The nature of the assessment is to explain the differences. For example, the national count from Census 2010 has been compared to three other estimates; one (actually five) from demographic analysis, one from the Census Bureau's Population Estimates Program, and one from a nationwide sample survey. Tsao and Wright (1983) introduced a simple tool (maximum ratio) and used it to prove a statement regarding the closeness of a set containing a particular estimate and competing estimates to the unknown true value of the real-valued parameter. In this paper, we (1) provide a visual proof of the statement, (2) present a test for using the simple tool, (3) give applications for a variety of real data, and (4) extend the simple tool and statement to vector-valued parameters.   ","The 2010 Census Service Based Enumeration operation provided people experiencing homelessness an opportunity to be included in the census by conducting the enumeration over a three-day period at shelters, soup kitchens, regularly scheduled mobile food vans and targeted non-sheltered outdoor locations. Since these locations enumerate a transient population, potential exists for a person to be counted at more than one location. Likewise, persons with \"no address\" who were enumerated at these locations may have also filled out a Be Counted Form. People who were enumerated at soup kitchens and/or mobile food vans could have also been counted at their permanent residence.In an attempt to count people once and only once at the correct location in the census, the Census Bureau conducted an unduplication process using a rule-based and probabilistic matching methodology based on response data to identify duplicate persons. Based on certain predetermined criteria, these duplicates were removed from the final census population count. This paper will discuss the various data processing techniques,challenges and possible research forunduplicating people experiencing homelessness in the census. ","The Common Core of Data (CCD) nonfiscal surveys consist of data submitted annually to the National Center for Education Statistics (NCES) by state education agencies (SEAs) in the 50 states, territories, and other agencies. CCD survey staff edits the data to produce a clean data file, which NCES uses to construct general-purpose publications and specialized reports. The principal users of CCD nonfiscal data are the federal government, the education research community, state and local government officials, including school boards and LEA administrators; and the general public. With the 2010-11 survey year, public concerns were raised about the potential for extreme erroneous results to not be flagged in the editing process. As a result, a new methodology of editing based on comparing multiple years of data for an edited data element vs. the current year/prior year editing process previously used. This paper discusses the results of this new edit methodology on the data quality for the 2011-12 CCD survey year, as well as the potential for this method to be applied in other ways moving forward. ","The U.S. Census system for editing and imputing demographic characteristics of persons in households has worked admirably since the 1960 Census. The current study describes a statistically principled random imputation of the relationship, age and sex (ras) characteristics (items) of 2010 Census household (hh) persons.  This system imputes the ras items for all persons in a hh simultaneously. The imputation is independent of any set of edit rules.  After identifying the full set of edit rules, the vectors of the ras values of each person in a hh that passed all edits are identified. Probability distributions of valid hh vectors for all completely classified US hhs up to 8 persons is produced. Next, the EM algorithm distributes the partially classified household vectors to the completely classified distribution to produce maximum likelihood estimates. To impute the missing items in a hh, a random draw is made from the distribution of vectors that match the partially classified hh's reported variables. A truth deck of persons, matched between the census and the CCM survey and with missing census items but reported CCM survey items, is used to calculate measures of agreement. ","Ratio edits occur when the ratio of two highly-correlated items are compared to upper and lower limits and then either imputed or flagged for further review if they fall outside these limits.  The effectiveness of the ratio edit is dependent on the limits calculated.  We propose a more rigorous approach to setting the limits by calculating statistical tolerance intervals as an alternative to the rules-of-thumb typically implemented.  Both nonparametric and parametric calculations will be discussed.   ","In this paper we analyse the impact of interviewers and regional characteristics on unit non-response and non-contact in a voluntary face-to-face survey of households. We contribute to the existing literature by studying area and interviewer effects as well as interactions of both within a unified framework. The data for our analysis comes from the new German survey on Private Households and their Finances (PHF), which oversamples \"wealthy\" areas in Germany. Making use of the special sampling design of the PHF, we analyse differences between wealthy and other regions as contact and cooperation behaviour across these groups may differ. Results from multilevel logistic regressions show that dissimilarities between areas with respect to wealth explain only a small part of the variance of households' response behaviour. Interviewer effects on the contrary explain large portions of the observed differences in contact and response outcomes. ","Exploiting a quasi-experiment we use non-parametric re-weighting and regression approaches to estimate the causal effect of the interview method on item-nonresponse and the unconditional observed income distribution. The minor change of interviewing households via the telephone instead of personally leads to major changes in item non-response, which increases by roughly 20% - 30%. The observed distribution of income is compressed translating to a decrease in the Gini coefficient by about 10%. Commonly used rankings of countries by Gini coefficients of the income distribution might largely be an artifact of different survey techniques as the interview mode than true differences in income inequality. ","There has been a growing interest in the study of hard-to-reach populations like those comprised of injection drug-users and the homeless. New link-tracing sampling designs and inferential methods for measuring attributes of such networked populations have therefore received an abundance of attention recently.  In this talk I will present a novel strategy for estimating the size of networked populations when samples are selected via a link-tracing design. Preliminary estimates of the population size are obtained with the use of common mark-recapture estimation methods and are based on the conventionally selected members of the samples. A Rao-Blackwellization strategy is then implemented by incorporating the adaptively selected units into the analysis and will in turn achieve an improved estimator. Results from an empirical study of a networked population at risk for HIV/AIDS will be presented to demonstrate the efficiency of the new strategy. ","In surveys that ask respondents to recall behavior over a specific period of time, there is a well-known  tradeoff in increasing the length of the recall period. On the one hand, collecting data over longer  periods of time provides the researcher a view of a wider variety and larger sample of behavior patterns.  On the other hand, there is worry that respondents tend to forget events or begin relying on cognitive  techniques other than pure memory recall to provide estimates, resulting in biased responses. One can  thus ask what the optimal length of the recall period should be for a particular study. In this work, we  study the effect of the recall period on the accuracy of reporting past usage for four different payment  instruments. We develop a model mapping the actual number of uses to the reported numbers, and  present a methodology for estimating this model using data from a three day diary study of consumer  payments. We then consider the implications of this model for mean-square errors based on reported  values for different recall periods. ","We have fielded an experimental module in the American Life Panel (ALP) where we ask individuals to report the number of their purchases and the amount spent by debit cards, cash, credit cards, and personal checks. The experimental design features several stages of randomization. First, three different groups of sample participants are randomly assigned to an entry month (July, August, or September, 2011) and are interviewed 5 times, at intervals of 3 months. Second, for each method of payment a sequence of questions elicits spending behavior during a day, week, month, and year. At the time of the first interview, this sequence is randomly assigned to refer to specific time spans (e.g. last week) or to typical time spans (e.g. typical week). In all subsequent interviews, a specific sequence becomes a typical sequence and vice versa. In this paper, we analyze the data from this experiment. We show that the type (specific or typical) and length of recall periods greatly influence household reporting behavior. Differences between specific and typical periods decrease with the length of the reference time frame. ","Address based sampling (ABS) provides quantitative researchers with a vast array of ancillary information that can be appended to the sampling frame at the block-group level for virtually every sampling unit. This study is based on a national probability sample of over 600,000 addresses randomly selected from an ABS frame for a media survey, where over 40 demographic and behavioral variables  from the 2000 Census and other sources have been appended. With these data,we derive two survey response propensity models using random forests and logistic regression with principal components. The internal, temporal and external validity for both models in terms of predicting whether or not a selected household responded to the survey is evaluated. The internal validity was assessed using an iterative series of hold-out samples; temporal validity was assessed by applying the models to a second national random sample selected during the same year as the test sample using the same sampling design. Finally, external validity was assessed by applying both of the models to a small national probability sample selected over a year later from a broader sampling frame with the same target population. ","When a survey is offered in more than one mode of administration, the potential for bias attributable to the mode may threaten the cross-mode exchangeability of responses or comparability of results. We demonstrate the utility of Item Response Theory (IRT) in quantifying the presence of mode effects, providing insight into the nature of the effects, and adjusting cross-mode results without post-hoc adjustment.  Such IRT applications are of interest when the survey instrument informs an underlying latent trait. We present a Bayesian hierarchical IRT model that can accommodate multiple modes of survey administration and provide cluster-level parameter estimates of the latent trait when observed groupings of respondents are of interest. We illustrate the model with data from a randomized mode experiment in which responding subjects within each of 45 evaluated institutions were randomly assigned to one of four response modes: mail, phone, IVR, and a mixed mode of mail with phone follow-up. Results indicate presence of form-wide mode of administration effects that differ across response categories and the underutilization of an interior response category in certain modes. ","In the context of mixed-mode surveys, researchers would like to be able to predict the relative propensity of sample members to respond in each mode. Such predictions could then form the basis of mode allocation mechanisms that would increase the effectiveness of mixed mode surveys. To make response propensity predictions it is necessary to have relevant auxiliary data for all sample members. With longitudinal and multi-stage surveys, such data can be collected from the sample members themselves at a previous stage. In particular, it may be possible to directly ask which mode(s) the respondent prefers. We use experimental longitudinal survey data to examine the extent to which the predictive power of models built on socio-demographic and behavioural characteristics can be enhanced by including responses to direct mode preference questions. The predictive power of alternative versions of mode preference questions is compared. ","Mixed-mode surveys combine different data collection modes to reduce non-observational survey errors under certain cost constraints. In this survey design, usually there is no control over who responds by which mode. As a result, data are obtained by a nonrandom mixture of survey administration modes. Without adjusting for this nonrandom mixture of modes, the standard method of estimation that combines responses from different modes has a bias that depends on both mode effects and the mix of respondents that choose each mode. Unless mode effects are zero, data should be adjusted for both nonresponse and nonrandom mixture of modes. We present alternative methods that account for both nonresponse and the nonrandom mixture of modes. Although in principle the separate mode effects are not estimable in a mixed-mode survey design, the alternative estimators do allow estimation of the difference in average mode effects. In addition, the bias properties of alternative methods can be better understood when compared to the standard estimation method. The alternative methods use models to impute each respondent's values for each counterfactual response mode-e.g. a telephone response value of in-person respondents. Combining the observed values with the imputations results in a \"completed\" data set for each mode. Alternative estimators are then used to combine these mode-specific \"completed\" data sets in an attempt to reduce bias associated with confounded and nonrandom influences of mode choice and mode effects. This paper presents some results for empirical comparisons of mean personal income and percent health insurance coverage based on the alternative methods and standard method. The public-use 2012 Current Population Survey (CPS) March data are used for empirical evaluations. ","We analyze the results of a national survey collected in two modes: SAQ on the web, followed by personal CATI of web non-respondents. We apply regression and implied utility-multiple imputation mode effect adjustments. Since some items may exhibit mode effects, such as social desirability bias, a randomized split-sample design has been built into the study to allow for a rigorous comparison of the item response distributions in the two modes. A logistic model for Yes/No responses or an ordinal logistic model for Likert scales was fit to the data with explanatory variables that included demographic variables and the mode indicator. The regression mode effect adjustments zeroed out the mode variable and formed predictions using the estimated regressions coefficients. Another mode adjustment is based on econometric  framework of implied utilities in logistic regression modeling, in which the alternative is chosen with the greatest utility. The latter was treated as a version of multiple imputation. We found the community involvement items and experiencing a major financial crisis recently to exhibit the strongest mode effects. ","In the fall of 2011 the Bureau of Labor Statistics fielded a survey to assess the number and nature of green technology jobs in the U.S.  This mixed-mode (mail, web, phone, fax) survey collected information on occupation and wages of workers who spent more than half of their time using green technologies or practices (GTP).  Approximately 35,000 establishments were selected from the population stratified by geography, industry, and ownership.  About 30 percent of sampled establishments either did not respond to the survey or did not provide sufficient information.  We seek to improve the current practice to account for nonresponse where estimates are adjusted to employment (from frame) within cells based on sample design factors.  This work uses logistic regression to identify the \"best\" response propensity model, which modestly predicts response propensity.  It is difficult to determine whether response is MAR, with additional adjustment unnecessary, or NMAR, where the auxiliary information fails to account for bias in the outcome estimate.  We use methodology based on Sverchkov (2008 &amp; 2011) in order to determine that nature of the nonresponse. ","Mixed models are commonly used for the analysis of small area estimation. In particular, small area estimation has been extensively studied under linear mixed models. Recently, small area estimation under the linear mixed model with penalized spline model, for fixed part of the model, was proposed. However, in practice there are many situations that we have counts or proportions in small area estimation as a response variable; for example a dataset on the number of incidences in small areas. In this talk, small area estimation under generalized linear mixed models using penalized spline mixed models is proposed. A likelihood-based approach is used to predict small area parameters and also to provide prediction intervals. The performance of the proposed approach is evaluated through simulation studies and also by real datasets. ","The paper develops empirical Bayesian benchmarked small area estimators under multiplicative models. Corresponding uncertainty measures are found. Second order correct estimators as well as botstrap estimators of these MSE's are derived. The methods are illustrated with an actual data set.  ","A model by Fay and Herriot has been extensively used in developing model based estimates of small area means. Explanatory variables in this model are assumed to be free from measurement error. Often, however, the explanatory variables are obtained from surveys, and hence are subject to measurement error. When explanatory variables in Fay-Herriot model are measured with error, the standard treatment of Fay-Herriot model is not appropriate. Ybarra and Lohr (2008) considered a Fay-Herriot model in the small area context where explanatory variables are subject to functional measurement error. In this talk, we consider a multivariate Fay-Herriot model with structural measurement error to covariates. We pursue hierarchical Bayesian approach to this model based on suitable noninformative priors. Propriety of the resulting posteriors has been investigated. Illustration to estimation of four-person family median for the fifty U.S. States based on the Current Population Survey and other administrative data has been considered.  ","Dual frame survey methods are popular for blending estimates from two independent surveys that measure the same quantities. The relative weights assigned to observations from the surveys may be used for direct small domain estimates. In this paper, we explore methods for direct small domain estimation from two surveys when one survey may be biased relative to the other. These methods are explored in the context of small area estimation with the US National Crime Victimization Survey. ","Adaptive design is a means to reduce survey costs by taking focus away from maximizing response rate exclusively.  An optimal adaptive procedure would minimize factors such as cost and respondent burden, while maintaining or improving the quality of the estimates. This work theoretically and empirically assesses the quality of key estimates from the National Health Interview Survey (NHIS) under various adaptive sampling data collection rules. We examine simple data collection decision rules that involve real-time imputation for non-interviews and rely on estimates of errors accounting for non-response in survey indicators. We present the decision-theoretical framework (DeGroot, 1970) connecting together the decision rules, the imputation methods, and the error estimation. We run simulations to explore the complications of the situations motivating the rules, as well as more elaborate rules that use multiple imputation to deal with the added complexities of the missing data mechanism (Little and Rubin, 2002). ","In this paper we apply multivariate linear mixed-effects models for missing data (Schafer and Yucel 2002) to the Monthly Wholesale Trade Survey. The MWTS is a longitudinal survey of U.S. wholesale businesses that provides relative month-to-month change estimates of sales and total inventories. As such, it is important that nonresponse adjustments preserve relationships among variables. The current method for handling missing data is to impute via a ratio adjustment to prior month data. We test the feasibility of this model, and we examine the patterns of missingness among the data. We develop a mixed-effects imputation model for sales and inventories, create multiple imputations of missing values using a Markov chain Monte Carlo procedure, and compare the results to those of the current method. We discuss how this methodology could be explored for use in a real-time, adaptive design framework. ","Data collection operations for longitudinal surveys are imperfect and often lead to situations where information is missing for some survey units at many points in time. This means there are either gaps in the sequence of longitudinal data expected from the survey, or attrition, meaning all the information for specific units is missing permanently after one point in time. In both cases, we can exploit the information from the units that are intermittently missing to produce whole-unit imputations that are partially adjusted for potential non-response biases. Because of their higher propensity for nonresponse overall, the intermittent responders share additional features with the units subject to permanent attrition, relative to compliant respondents. These similarities can be exploited through parametric modeling or non-parametric techniques, such as the hot-deck, to produce whole-unit imputations with mitigated biases. We examine some of these techniques (Benedetto and Stinson 2009) and proposed some new ones (Markov modeling O'Donnell and Thibaudeau 2012).    ","For applied work with generalized variance function models for sample survey data, one generally seeks to develop a model that produces variance estimators that are approximately unbiased and relatively stable.   Through simulation, we evaluate the bias and variance of model coefficients, and the bias and variance of the GVF estimator. In addition, we compare and contrast confidence interval coverage rates and widths of the GVF estimator to design-based estimators. We study these properties with varying degrees of freedom for the GVF estimators and a refined bias adjustment factor for nonlinear transformations in the lognormal model. ","Project TALENT is a large, nationally representative longitudinal study developed by American Institutes for Research and conducted from 1960 to 1974. The goal was to assess the interests, abilities, and demographics of 9th-12th graders and their trajectories into adulthood. More than 1,200 junior and senior high schools participated. Replicate weights were not constructed at the time, preventing the estimation of standard errors. In this paper, the retrospective construction of 118 sets of student-level replicate weights is described. The process entailed adjustment of the original base year (1960) student weights and school weights to better estimate the educational and life experiences that are most important to individuals' life trajectories. CHAID analysis was performed to generate variance strata and variance primary sampling units. Finally, the student-level replicate weights were constructed using a jackknife procedure. The use of replicate weights is illustrated by estimating standard errors for quantiles of composite cognitive scores constructed from student questionnaires. ","Many statistical agencies collect data that suffer from item nonresponse and erroneous values. Such data also may be required to satisfy a system of linear constraints; some values that do not satisfy the pre-defined constraints are blanked and imputed by statistical agencies. Further, the data may have complex distributional features, including nonlinear relationships and highly non-normal distributions. We present a fully Bayesian, joint model for modeling or imputing data with missing/blanked values under linear constraints that (i) automatically incorporates the constraints in inferences and imputations, and (ii) uses a flexible Dirichlet process mixture of multivariate distributions to reflect complex distributional features. Our basic strategy is to augment the observed data with draws from a hypothetical population in which the constraints are not present, thereby taking advantage of computationally expedient methods for fitting mixture models. Missing/blanked items are sampled from their posterior distribution using the Hit-and-Run sampler, which guarantees that all imputations satisfy the constraints. We illustrate the approach using manufacturing surveys in Colombia. ","Random digit dialing (RDD) phone surveys are facing considerable challenges. While substantial research alleviates many of these problems, the costs associated with RDD surveys remain high and response rates remain low.  The use of Internet panels affords a more cost-effective alternative survey and sampling strategy. While initially criticized for considerable coverage bias and the lack of a probablity-based sampling mechanism, substantial research in this area has begun to alleviate these problems as well.  However, the role of Internet panels in population health surveillance is not yet clear. We present the design and results of a pilot study investigating the use of Internet panel surveys for behavioral health surveillance. The pilot study was conducted in four states and Metropolitan Statistical Areas within these states. We will present comparative analyses that assess the advantages and disadvantages of different Internet sampling methodologies across a range of parameters including cost, geography, timeliness, usability, and ease of use for technology transfer to states and local communities.    ","The Consumer Expenditure (CE) Survey implements a statistical disclosure limitation (SDL) process in its microdata release to conceal sensitive and identifiable information in order to protect the household's confidentiality. This process is also known as \"topcoding,\" e.g. the high (low) end income household's annual income will be replaced by the average of high-income (low-income) household's annual income in the microdata for public users. Topcoding will have a numerical impact on microdata's utility and data quality. Several data utility measures have been applied to assess synthetic microdata, such as confidence interval overlap, propensity score comparisons, cluster analysis and difference in empirical CDFs (cumulative density functions).  In this study, we evaluate the numerical impact of topcoding on CE microdata utility focusing on the analysis of expenditure items that are: highly correlated with income and highly topcoded, highly correlated but not highly topcoded, not highly correlated but highly topcoded. ","The nature of Medicare Claims data makes usual methods of creating synthetic or nonsynthetic PUFs infeasible due to data complexity, numerous identifying variables (IVs), and difficulty in computing disclosure risk under an assumed intruder IV knowledge as it may increase over time. In view of this, we consider a two-prong strategy: creating a working PUF with high confidentiality at the cost of analytic utility, coupled with DUA-based microdata access for testing applicability of  procedures developed for the working PUF and for final analysis. The working PUF or data entrepreneurs' synthetic PUF (DE-SynPUF) has high pseudo analytic utility because it retains the real data structure, and is useful to data entrepreneurs for application development and researchers for training. DE-SynPUF was created by treating claims individually, with no explicit preservation of intra-claim relationships. Moreover, all claims were subject to ad-hoc treatment to reduce risk as measured by IV driven k-anonymization. An application of DE-SynPUF to 2008-10 claims data is presented. ","As part of a US Federal Government-wide effort to increase data transparency and access, an effort was undertaken to see if a \"disclosure safe\" data file could be produced containing beneficiary summary and claims data from the Current Medicare Files. To be \"disclosure safe\" virtually all the health records had to be certified as not traceable back to an individual. Under the Health Insurance Portability and Accountability Act (HIPAA), such certified data could be made public.  To accomplish this goal, separate De-identification and Re-identification teams were assembled. The work of the De-identification team is described in a companion paper in this session.  Here we focus on methods employed by the Re-identification team to test whether the data could be traced back to a single entity. The De-identification team had no restrictions on its work, but there was a clear goal to produce something as useful as possible given disclosure constraints. Due to the nature of the data, synthetic methods were of limited use. Other methods, such as data swapping, data coarsening, and suppression were used in combination to produce a file that could be certified as \"disclosure safe.\" ","Creating a unit level PUF that is analytically useful and disclosure-safe is difficult due to the proliferation of publicly available indirect identifiers from various known and unknown sources that might exist at present or in future. In creating an aggregate level (AL-) PUF for Medicare Claims data, the data structure is transformed from a beneficiary level file with rows representing beneficiaries and columns analytic variables to a file with rows representing small clusters of beneficiaries called micro groups (MGs) and columns representing MG size and various domain means at the MG level (termed micro means--MMs) where domains are subpopulations of beneficiaries defined by variables corresponding to analytic goals. This allows information to be presented without sharing unit level data. Uncertainty in the MG size and associated MMs is introduced by random subsampling followed by weight calibration. The MGMM structure of AL-PUF is somewhat similar to the method of micro-aggregation for unit level PUFs where values of continuous variables deemed to be identifying are blurred by averaging over small clusters of observations based on similarity indices. However, the main difference is that in AL-PUF, MMs are provided for various domains defined by one or more variables and so joint relationships between variables are not distorted unlike the case of micro-aggregation. Moreover, as a result of subsampling, AL-PUF does not require the framework of identifying and sensitive variables used in traditional methods for creating unit level PUFs. For analysis domains, descriptive and analytic parameters of interest can be estimated using the weighted sums of products of suitable domain MMs and MG size over all MGs. Estimation of variance and covariance of point estimates can be obtained using essentially standard sampling methods because sampling errors introduced in MMs and MG size are due to multi-phase sampling. AL-PUF achieves high confidentiality by using MG sizes sufficiently small to reflect adequate uncertainty due to sampling errors but large enough to avoid problems with unit level PUFs. It has high analytic utility because analytic domains could be defined for any subset of variables of interest and the corresponding estimates remain approximately unbiased because uncertainty is only due to random subsampling. Examples from a 15% random sample of the 2010 Medicare claims data on chronic conditions are presented for comparisons between AL-PUF and an existing unit level PUF (termed chronic conditions PUF) based on k-anonymization and micro-aggregation.  ","Remote analysis servers (RAS) with output treatment of user queries, as an alternative to traditional input-treated PUFs, is an active research area. Success of RAS depends on protection from differencing attacks via repeated queries. We describe a new application of a recently proposed method of query-based (Q) PUF to Medicare claims data which is safe from differencing attacks. In Q-PUF, the user cannot arbitrarily define analysis domains but is required to choose from a checklist of pre-screened variables. The method is termed Q-PUF, despite being an output treatment, because the data producer controls the type and scope of allowed analytic variables just as with PUFs. There are three components of Q-PUF: first, construct a checklist of analysis domains that are deemed adequately large to provide estimating functions of corresponding parameters and safe for analysis; second, provide an interface for users to communicate with the microdata via queries; third, additional restrictions specific to analysis output. If needed the domain checklist may be updated. For Medicare claims data, we provide empirical examples of common interest including descriptive and model-based inference. ","The IALSA network was formed to facilitate cumulative research on within-person change. Non-experimental longitudinal studies of aging, health and cognition differ markedly in attrition, practice, selection, birth cohort, country, culture, language, and availability of covariates, posing a challenge to comparison of results, measurement harmonization and implementation of meta-analysis. Various consortia have approached these challenges, but lack of measure comparability in currently available data often aggravates these attempts and risks optimistic homogenization of disparate measurements. Coordinated analysis focused on construct level replication has likewise been criticized for potentially leaving the researcher with sets of results that are not easily reconciled. Such analysis, however, reduces variation due to analytic method or inclusion of different covariates seen in the general literature, and is compatible with meta-analysis when measures are comparable. While working on coordinated analyses with construct level reproducibility, we actively pursue opportunities for co-calibration, harmonization, and meta-analysis. Promises, challenges, and synergies will be discussed. ","Harmonization is increasingly viewed by the research community as a promising avenue to support advancements in the health and social sciences. An expanding number of countries are fostering the development of research initiatives allowing the creation of harmonized or compatible datasets. However, the use of various designs and scientific methods across studies are often essential to address local technical and cultural challenges, and meet the specific scientific objectives of local researchers. Because of the complexity and heterogeneity of the information collected by studies, there are major challenges to be faced when aiming to retrospectively harmonize data (harmonization of pre-existing data) and thereby enable valid comparison and/or integration of information across studies. Collaborative research programs making use of harmonized data thus always require the use of rigorous methodological approaches, but their achievement can be facilitated by the streamline access to specialized software. The approach and resources proposed by the Maelstrom Research group of investigators will be discussed as one of the potential avenue to support data harmonization and integration. ","Many constructs studied by social, behavioral, and medical researchers cannot be directly observed. In such cases, scales are created that reflect the construct of interest. Observed behaviors, often responses to a set of items, are taken as manifestations of an unobserved common cause. Understanding the measurement structure of a construct is especially important when trying to understand individual or group changes. However, the manifestation of a construct can change over time thereby changing its measurement structure. For example, a set of items may function differently (e.g., become more or less related to the construct) over age. Historically workarounds for modeling the nested structure of data has been common place in the item response theory (IRT) framework. This presentation will address recent advancements in multilevel IRT that allow for such nesting to be appropriately modeled when examining changing measurement structures. Two hypothetical examples (changing item sets and changing dimensionality) are presented using traditional and multilevel IRT methods. The results as they apply to over-time/cross-study analyses are compared and interpretations discussed. ","Meta-analysis is a common method for synthesizing existing evidence on a given topic. Traditional meta-analytic methods combine summary information provided in study reports. This form of aggregate data meta-analysis is commonly used to synthesize treatment effect estimates and bivariate correlations in the social and behavioral sciences. These methods are only useful when a collection of effect sizes estimate a common parameter. When, for example, treatment effect estimates are unconditionally estimated in some study and conditionally estimated in others, aggregate data meta-analysis is not easily able to combine that body of evidence. This paper will discuss some practical limitations to aggregate data meta-analysis in the context of combining conditional effect size estimates from ordinary least squares regressions. Then it will discuss some of the benefits of using individual participant data meta-analytic methods and how individual participant data may be synthesized with aggregate data. The paper will conclude with a discussion of how to build capacity for this area of research through replication and data sharing initiatives. ","The purpose of this paper is to identify paradata and multi-dimensional factors affecting nonresponse in the U.S. decennial census by using social integration (or social isolation) as a theoretical navigator. A person's response propensity has been empirically found to reflect the extent to which one is socially integrated (Abraham, Maitland and Bianchi, 2005) or isolated (Groves and Couper, 1998; Chun, 2009). We develop response propensity models specific to the Census nonresponse followup universe by tailoring to: 1) a followup mode of telephone, in-person, and administrative records, and 2) nonresponse outcomes such as occupied nonresponse household, vacancy and deletes, as well as proxy respondent.  We apply the concept of \"pandata,\" the data linked among multiple quality data sources which include paradata, census and administrative records, to craft robust response propensity models. We will use response data files from the 2010 Census, and then merge paradata associated with nonresponse followup (e.g. contact history) as well as household and neighborhood characteristic paradata borrowed from the 2010 Planning Database (e.g., household structure, poverty measures).    ","The NSLTCP includes a mixed mode (mail, web, telephone) survey of approximately 17,000 residential care facilities and adult day services centers in the U.S. The survey is planned to be conducted biennially, and its analytic goal is to collect information from these providers which is unavailable from other sources. We hypothesized that, of the modes available to complete the survey, respondents would prefer self-completion modes over telephone. In this analysis we used paradata from call records to gain empirical evidence to test our hypothesis. As other projects have done, we will use the results of this analysis to optimize our data collection strategy for future years of the survey (Kreuter et.al.,2010). We first examined mode completion choices and assessed whether they varied by provider size or type (adult day versus residential care). We then focused on the effectiveness of protocols that telephone interviewers followed that were intended to bring two groups of respondents to completion: 1)respondents who started by web but broke off; 2) cases that did not start but when contacted by telephone interviewers stated their intention to self-complete by web or paper (\"Do-it Yourself-ers\"). ","Beginning in January 2013, the American Community Survey (ACS) added an Internet response option as a fourth mode of response to their existing mixed-mode survey. Motivated by the need to provide cheaper, faster, and better statistical information, the Census Bureau has outlined a vision that includes the future use of adaptive survey design methods. Potential benefits include a reduction in nonresponse follow up lag time, a convenient or preferred mode of response, and cost savings by maximizing response in the cheaper modes.  As a first step in addressing this vision, we explore the use of adaptive survey design methods in the ACS by linking administrative data to the April 2011 ACS Internet Test sample to develop a model-based assignment of mode switch strategies, focusing only on switching Internet nonrespondents to mail. We describe the combined use of survival analysis and optimization methods to assign cases to a given mode switch strategy. We simulate the adaptive survey design process and review the cost-benefit trade-offs of using an adaptive versus a non-adaptive approach. In addition, we attempt to validate our approach using the November 2011 Internet Test sample. ","This study examines reluctance to respond to the Current Population Survey (CPS).  The CPS is administered to a household for four consecutive months, followed by a break of eight months, and then administered for another four consecutive months.  These eight interviews (panels) form the basis of this analysis.  The respondents concerns and contact issues during the first panel were summarized using a factor analysis (Dixon, 2010).  This study examines and compares the factor structure of the later panels to the first.  This study also examines paradata, including respondent contact history recorded by interviewers to help better understand the survey experience and willingness to respond.  This study found a similar pattern of concerns as previous studies for the first panel; however, the following panels exhibited different patterns than the first.    ","In the European Social Survey (ESS), a biennial survey on attitudes and values that is conducted in more than 30 European countries, paradata are collected to monitor interviewer performance and the response process, calculate final disposition codes, and assess nonresponse bias. Central specifications aim at highly standardized fieldwork procedures.    ","Because survey modes usually involve different sampling designs, estimates of mode effects are often confounded with differences in sample composition. We conducted an experiment where 1,000 subjects were randomized between two interview modes: a self-administered Web survey and the identical survey conducted by an interviewer. The randomization allows unbiased estimation of mode effects. Using a latent class model, we show that interviewers tend to elicit less extreme responses on attitudinal scales and lower correlations among items than in the self-administered mode. These differences cannot be attributed to sample composition and explain some anomalous results found in earlier studies. ","This nationwide survey evaluating trust and confidence in financial services, healthcare services, and important American institutions provides a unique opportunity to perform analyses of mixed mode survey results. This study evaluates the impacts of weighting and nonresponse adjustments for an Address Based sample frame survey. We explore the impacts of weighting and compare these results across survey modes on key survey measures. An important performance indicator for government is trust and confidence of the American people. Financial and healthcare services are both fundamental to the health and well-being of most Americans and families. The U.S. population has experienced tumultuous circumstances with the high rates of foreclosures, bank closures and fraud, high financial service fees, high healthcare costs and a shifting healthcare system. There is much interest in the performance of the healthcare and financial services sectors as related to consumer satisfaction and ensuing trust and confidence.     ","Total Survey Error (TSE) is the sum of sampling error and the nonsampling errors that arise at every step of a sample survey.  Since 2006, TSE modeling has provided a framework for evaluation of potential biases in the National Immunization Survey (NIS), a project conducted on an ongoing basis by the Centers for Disease Control and Prevention to measure the vaccination status of young children and adolescents.  For the 2011 TSE analysis, we utilize revised inputs from the National Health Interview Survey (NHIS) that account for differing provider reporting methods between the NIS and the NHIS.  Also for 2011, the NIS was fielded as a dual-frame RDD landline and cellular telephone sample, marking the first year TSE is assessed under the new dual-frame design.  For TSE modeling, we specify a distribution function for each component of error, derive estimates of the component distributions from the best sources, and then apply a Monte Carlo simulation-based approach to combine the components of error into a total error distribution for each estimate examined.  We compare estimates of bias derived from the TSE analysis for 2011 to bias obtained from TSE analyses conducted in 2009-2010. ","With 10 to 30% response rates and evidence of differences between respondents and nonrespondents, many organizational surveys are plagued by nonresponse bias, yet the problem remains largely unaddressed (Cycyota &amp; Harrison, 2006; Paxson, 2002; Smith, 1997). Frameworks for organizational survey participation such as Willimack et al. (2002) have been developed with a focus on large businesses and government survey requests, while factors driving participation among smaller nonprofit organizations responding to non-government survey requests may be different. With data from an Indiana University/American Society of Association Executives Foundation web survey of a stratified sample of 9,519 US member-serving nonprofit organizations, we match data from IRS 990 tax forms to decompose nonresponse and its potential effects on estimates using variables correlated with outcomes of interest at each of the following stages: (1) unable to obtain an email address to send survey request, (2) undeliverable email messages, (3) refusals, and (4) partial completions. In addition, we examine the effect of a follow-up with underrepresented subgroups to determine its effect on nonresponse bias. ","Nonresponse in face-to-face household surveys has been a matter of concern for several decades in many countries. The prevention or avoidance, and the special estimation techniques are the most common methods that are used to solve the problem. This study focuses on the former. In Korea, many housing units have \"access impediments\" that prevent strangers from contacting them. For example, about 50% of all households live in high-rise apartment buildings with locked central entrances or security guards. Moreover, the proportion of not-at-homes during the day or evening is very high and nearly a fourth of households have just one resident. It is essential, therefore, to make a special effort to bring a good response rate. In this paper, we present how the sample households were contacted based on a new administrative cooperation to reduce nonresponse in a metropolitan household survey. Also, to assess the quality of data collected through such survey process, we explore the coverage changes across various subpopulations and the differences of responses according to the number of call-backs, as well as average number of calls per response required complete the survey.  ","Objective: Develop a simulation which determined if nonresponse bias for the 2008/09 Baccalaureate and Beyond Longitudinal Study can be reduced more effectively by targeting nonrespondents (NR) who are the most different from respondents (R).    ","For the 2010 Census, the Nonresponse Followup (NRFU) operation verified the status of units that did not respond by mail in areas that received a mailback census questionnaire. The NRFU workload totaled 47 million units at a cost of $1.6 billion. The Census Bureau then classified these units into four categories: occupied, vacant, deleted, and unresolved. Among occupied, vacant, and deleted NRFU units, respectively, 24%, 98%, and 90% of interviews were completed via proxy respondents . In general, survey results often demonstrate reduced data quality when allowing proxy respondents for household interviews. By understanding the unit- and area-level characteristics that lead to proxy respondents, we can better understand where lower data quality might be present in census results. This research implements a modeling approach to predict proxy status. The approach relies on housing unit-level covariates defined by census operations prior to beginning NRFU fieldwork. In addition, the model uses tract-level characteristics known prior to the census as independent variables to predict proxy status for the NRFU universe. ","Response rates for telephone surveys have declined over the last several decades.  This decline is often attributed to the availability of call screening technologies and respondents' reluctance to answer calls from unknown numbers.  Some research suggests calling respondents from local area codes and using identifiers that are recognizable and trustworthy improves response rates.   At the National Agricultural Statistics Service, an experiment was conducted to determine if the information presented on caller ID influenced response rates to our surveys.  From our national calling center we manipulated the caller ID by varying area codes and identifiers that appeared on the caller ID to see if response rates increased.  A small improvement was seen in the number of contacts made, that is they answered the phone, but it did not make them more likely to participated in the survey.  In this presentation we will discuss the findings from this study.   ","This paper describes efforts to address nonignorable nonresponse in the Program for the International Assessment of Adult Competencies (PIAAC), an international survey of adult literacy. A source of nonignorable nonresponse in PIAAC is from persons who cannot complete the survey because of a language barrier, reading/writing barrier, or learning/mental disability. Such persons are part of the target population. However, they cannot be represented by respondents because their reason for not completing the survey is directly related to the survey outcome (proficiency in the assessment language). We report on data collection, weighting, and estimation procedures implemented in PIAAC to limit this source of bias and allow for comparability between countries. ","This paper provides an overview of three popular calibration weighting methods for complex surveys: (i) the regression weighting method; (ii) the exponential tilting method; and (iii) the pseudo empirical likelihood method. Computational algorithms for each of the methods are discussed, and finite sample configurations of the three types of weights are examined through simulation studies. The pseudo empirical likelihood approach to calibration is shown to have several advantages, including stable weights, efficient and reliable computational procedures, and the method can easily be used for generalized raking, a special calibration problem where auxiliary population information is in the form of known marginal totals for a contingency table.    ","In longitudinal surveys, sampled individuals appear in multiple survey years and can be linked across time. In the NSF's Survey of Doctorate Recipients (SDR), 60% of respondents appear on 3 or more surveys from 1993-2006. In some of these surveys, such as the SDR, survey weights are created on a cross-sectional basis. Longitudinal weights enabling estimation using data from multiple survey waves together do not exist. This paper applies calibration estimation for construction of such a longitudinal weight with the help of auxiliary variables in each survey year. Population totals of auxiliary variables are an influential factor in the calibration procedure but are unavailable. Instead, cross-sectional estimated totals are used. Due to this replacement, variance could be seriously underestimated when using the traditional calibration variance estimator. A new variance estimator is derived that can mitigate the negative bias. Performance of the estimator is studied using simulation and application to SDR survey data. Choices of multivariate calibration targets are compared. Other methods such as jackknife and bootstrap for variance estimation also are discussed. ","The linear calibration approach is commonly used to adjust sample weights for multipurpose estimations. Although the linear calibration estimator has many good properties, it has two major limitations. One is the efficiency issue and the other is the convergence issue. Many modified methods have been suggested in the literature, but they do not address both issues at the same time. The modified methods intended for efficiency gain only apply to a single study variable, while modified methods intended for stable estimations for multiple study variables do not offer efficiency gains. In this paper, we first review some modified calibration methods and offer some thoughts about calibration applications to balance efficiency gain and multipurpose use. In particular, we propose a composite calibration weighting method and some modifications of the regular linear calibration method. The proposed methods aim at improving the efficiency for some 'priority' study variables, while still keeping the multipurpose property. Results of a limited simulation study are presented as well. ","We propose a model-based extension of weighting design-effect measures.  We develop a summary-level diagnostic for different variables of interest, in single-stage sampling and under calibration weight adjustments.  Our proposed design effect measure captures the joint effects of a non-epsem sampling design, unequal weights produced using calibration adjustments, and the strength of the association between an analysis variable and the auxiliaries used in calibration. We compare our proposed measure to existing design effect measures using a case study and simulation involving establishment-type data.    ","A familiar complaint that students have on finishing a class in applied sampling or in sampling theory is: \"I still don't really understand how to design a sample.\" Students learn a lot of isolated tools or techniques but do not have the ability to put them all together to design a sample from start to finish. One of the main goals of this short course and the associated textbook is to give students, new survey statisticians, and other survey practitioners a taste of what it is involved in designing and weighting single- and multi-stage samples in the real world. This includes devising a sampling plan from sometimes incomplete information; deciding on a sample size given a specified budget; allocating the sample to the strata and stages of the design given a set of constraints; and constructing efficient analysis weights. Our goal will be accomplished during the course through discussions of actual case studies and hands-on exercises involving the use of computer software.","In the Quarterly Census of Employment and Wages program of the U.S. Bureau of Labor Statistics, quarterly establishment records are linked longitudinally by a software-based logistic regression system that assigns linkage probabilities to potential establishment matches. Each field in the administrative record is given a weight according to its non-missing rate, and records are linked if their estimated linkage probability exceeds a minimum threshold. In this paper, we consider a simplified matching alternative that relies on the principal data fields related to the administrative definitions of establishment linkage. A linkage system based on critical matching criteria, such as location and administrative business information, is constructed. The results of the alternative record linkage approach are compared to those generated from the logistic regression software.  ","All longitudinal studies face a common challenge of both locating study participants and having participants respond to future waves. One challenge is ensuring that adjustments can be made to the weights to correct for any non-response and tracking loss. This paper provides results from the 2011-12 Project Talent Follow-up Pilot Study (PTPS12), a pilot test developed to assess the feasibility of finding and reengaging a representative random subsample of Project Talent participants who had not been contacted in 37 to 51 years. To adjust the weights for non-response and tracking loss a CHAID (Chi-Squared Automatic Interaction Detector) Analysis was used to develop the weighting cells. This paper will focus on weighting two different methods for weighting the PTPS12. The first treats those not tracked and non-respondents as the same group and consider the impact on bias analysis. The second method first adjusts for tracking loss and then adjusts for non-response separately and again considers the impact of the bias analysis. A comparison of the two methods will be used to determine which of the two has a lower variance and a smaller bias.  ","Most survey designs contain revealing information about the geographic locality of the sampled items. When combined with other sources, such information could violate the confidentiality of the survey data. We propose the modification of existing methods for masking design information by constructing combined strata variance estimators. In our approach variance estimation error is minimized conditionally on the realized PSU selection, which is often different from the optimal way of selecting  PSU with probability proportional to their size (PPS). Advantages of the proposed method for combining strata over other methods are demonstrated by comparing relative deviations between standard errors of the estimates of totals and means calculated using grouped and ungrouped strata. Coverage of the finite population target variables by their estimated confidence intervals is analyzed in a simulation experiment for different ratios between variability of the target variables and PSU sizes. Optimal properties of the proposed method for grouping strata are validated in application to real survey data.  ","In response to an increased interest in making Government data open and machine readable, the National Center for Education Statistics led the Department of Education's (ED) effort to identify data sources from ED programs, as well as statistical data. This effort started in 2010. A Department-wide committee identified individual data sources.  Then, working with ED's OMB submissions and other documentation, metadata were identified and collected to describe each data source.  Extant sources were used to identify the data elements in each data source. This information was organized into an electronic hierarchical searchable data base-The ED Data Inventory. The initial rollout in the fall of 2013 included information on the most recent data collection in each identified K-12 data series. This Inventory addresses the mandate identified in the March 2013 Executive Order  to provide an inventory of all datasets owned, managed, collected, and/or created by each agency. This is an ongoing project. The presentation and demonstration will provide more details on the structure and content of the ED Data Inventory as of August 2014, how the Inventory works, and next steps. ","The National Center for Education Statistics (NCES) has completed six sets of computer-based training (CBT) modules covering nine of its detailed complex survey data sets. These are the products of the first year's work on a continuing project to create CBT training modules to facilitate and encourage interested researchers, students, policy analysts and practitioners to make hands-on use of NCES' public- and restricted-use detailed data sets. The presentation and demonstration will provide details on the development process and the surveys included as well as the structure, operation, content, and how to access the Distance Learning Data Set Training modules now available. Next steps and the next group of survey data sets having modules developed will also be included.     ","The intracluster correlation coefficient is often used to model the design effect associated with cluster sampling.  Typically, the average cluster size is inserted into the design effect formula even though the cluster size will vary in practice.  Cluster size variation is particularly a problem when the design effect for domain estimates is desired and the domain of interest is not evenly distributed over all sample clusters.  Should an alternate way of computing the intracluster correlation be developed or is some other simple solution available to resolve this problem? This paper presents an alternate, but simple, model for accounting for both clustering and the variation in cluster sizes.    ","With ongoing protests and civil unrest, a population with rapidly changing opinions, and a future in flux, reliable and timely survey data is difficult to collect in Egypt.  In spite of political and social challenges to conducting surveys in Egypt, D3 Systems, Inc. has conducted two large-scale national probability surveys with a focus on better understanding the telephone owning population.  The first survey is a national dual-mode RDD probability survey that was fielded in two waves in the summer of 2013. The second survey is a national face-to-face multi-stage probability survey, designed to measure the telephone owning population in Egypt. The latter study was introduced at the 2013 JSM without final data which was delayed due to the unrest.   We explore the methodologies, resulting samples and discuss how the results of the F2F survey impact our CATI sample designs. ","In health care research, longitudinal analysis of health care episodes is preferred to cross-sectional analysis because it avoids arbitrary observation periods and provides more information on temporal changes in utilization behavior. Analysis of longitudinal claims must account for left and right censoring to construct valid episodes of care. Analyzing individuals with probable mental illness from the Medical Expenditure Panel Survey (MEPS), we apply two-part negative binomial models to account for a significant proportion of zero service use. We use the Gibbs sampler to simulate the posterior mean and standard deviations from the joint distribution of observations. We compare Bayesian predictions to three alternatives: a method na\u00efve to censoring; a na\u00efve method that drops censored cases; and a maximum likelihood estimator of the censored two-part negative binomial model. Bayesian and MLE estimators reported similar mean numbers of mental health care visits per treatment episode. Typically used na\u00efve methods substantially under-estimated the number of visits per episode. We conclude that Bayesian approaches to censoring can greatly improve health care episodes analyses. ","Objective: Test whether implementation of a Mahalanobis distance measure as part of a responsive design reduces nonresponse bias in the 2008/2012 Baccalaureate and Beyond Longitudinal Study.    ","When analyzing data sampled with unequal inclusion probabilities, correlations between the probability of selection and the sampled data can induce bias. Weights equal to the inverse of the probability of selection are commonly used to correct this possible bias. When weights are uncorrelated with the sampled data, or more specifically the descriptive or model estimators of interest, highly disproportional sample design resulting in large weights can  introduce unnecessary variability, leading to an overall larger root mean square error (RMSE) comparing to the unweighted or Winsorized methods.    ","We have previously shown that prenatal maternal total n-3 long chain polyunsaturated fatty acid (LC-PUFA) concentrations and n-6:n-3 ratios along with methylmercury (MeHg) exposure have interrelated effects on psychomotor development index scores in children in the Seychelles (a high fish eating population).  The aim of this study was to examine the dietary patterns and determinants of maternal and child LC-PUFA and MeHg status.  Pregnant women and children at age five completed four day food diaries that were analyzed using WISP v.3.0 to generate food intake amounts.    LC-PUFA and MeHg status were measured in the mothers and children.  In order to connect the nutritional and exposure status from these biomarkers to eating behavior based on fish intake, clustering was performed on the food intake variables.  This created six and four subgroups of dietary patterns in the mothers and children respectively.  Shellfish consumption was positively associated with MeHg status in mothers but not children, while white fish consumption was positively associated with maternal n-6:n-3 ratio and negatively associated with DHA and total n-6 ratio in children. ","Currently, most surveys ask for occupation with open-ended questions. The verbatim responses are coded afterwards into a classification with hundreds of categories and thousands of jobs, which is an error-prone, time-consuming and costly task. Our goal is to facilitate survey coding using machine learning methods. The idea is to use answers that were coded before to predict correct codes for new answers. Our method allows for further covariates in addition to the verbatim answer. We also account for the limited size of available training data. Performance is further improved when information from a dictionary is included. The proposed algorithms can be used for automated coding without human interaction but we expect it to be most helpful for computer-assisted coding. ","The ever growing size, variety and velocity of data in healthcare industry are getting beyond the ability of typical database tools for storing and analysis. Healthcare Industry has to deal with and utilize data to better its mission of patient care and prevention. Data preprocessing techniques; data cleaning, data integration, data transformations and data reduction will significantly improve knowledge mining from data. How well healthcare industries are managing the data explosion and capitalizing on the opportunities it presents, and to what extent they are extracting value from the data at their disposal are important to improve health care efficiency. For example; this situation is more profound in radiology discipline as radiologists have to deal with, analyze and make rapid decisions based on vast and tremendous amount of data that comes in various forms. The industry is moving toward the aggregation and analysis of information - a term commonly referred to as advanced analytics and big data. This paper discusses the growth of big data and utilization of advanced analytics in medical field.  It explores how radiology can be benefited from utilizing advanced analytics and sol ","Response rates to follow-up surveys have an impact on the estimation of outcome events. In a cross-sectional study, initial surveys on breast cancer risk factors and mammogram use were obtained in person from 21,852 women residents of Arkansas recruited through breast cancer awareness events from 2007-2013. A random sample of 2200 women stratified by age in years, race and rural/urban residence were selected for a mailed follow-up survey conducted in December 2013.  Response rates were assessed one month after mailing and are expected to increase with time.  Among women in the sample, 20% were African-American, 47% were college graduates and 71% resided in metropolitan areas.  The median age was 35-49 years of age.  The overall response rate to date was 19.9%.  The response rate varied by age, race, and educational level, but not by urban/rural status.  Response rates increased with age and educational level. African-Americans had a lower response rate than those of other racial groups. The variation in survey response rates by demographic characteristics suggests that weights may need to be developed to generalize sample results to the population under study.    ","26% of adults aged 40 report difficulty understanding speech in background noise increasing to 48% by age 70.  We investigate the characteristics of this decline in hearing function and its association with cognition.    ","In 2011, both the Behavioral Risk Factor Surveillance System (BRFSS) and the National Survey on Drug Use and Health (NSDUH) measured lifetime history of diagnosed depression among adults 18 or older. The NSDUH, an annual survey of the U.S. civilian, noninstitutionalized population aged 12 or older, is a major source of substance use and mental illness data. The BRFSS is an annual state-based telephone survey of the civilian, noninstitutionalized adult population aged 18 or older and collects data on health risk behaviors. This paper compares estimates from these two sources and explores how survey methodology influenced the measures. Differences in data collection features including questionnaire and sample design were examined as potential sources of differences in estimates. Based on the review of the survey methods, the lifetime depression estimates were deemed comparable and estimates were computed.   ","Advances in social network analysis and parameter estimation for mechanistic process models are changing the way that scientists study infectious disease. Although classic epidemiological models assumed that populations mix homogeneously, recent work shows that epidemic size is partially determined by heterogeneity in host mixing. Here we studied how the homogeneity assumption impacts parameter estimation for a bighorn sheep population with recurrent juvenile disease. First, we constructed contact networks for 46 birth cohorts, and used variance decomposition to quantify how host contact structures, year-specific, and population-specific factors contribute to epidemic severity. Greater than 92.5% of the variation was attributed to contact structures. We then generated approximate posterior distributions for two parameters related to epidemic spread, 1) number of outbreak starting points, and 2) disease-induced mortality rate. A comparison of posteriors from homogeneous and network-based contact structures suggested that failing to account for contact heterogeneity led to biased parameter estimates, and substantially changed perceptions about the system's epidemiological process. ","This paper presents a new sampling strategy called Outside Caliper Matching (OCM), and the comparison of OCM to two established sampling strategies, Simple Random Sampling (SRS) and Counter Matching (CM). OCM is similar to CM but is appropriate for sampling using continuous exposure data. Like CM, OCM increases the diversity of exposure values and retains more information from the cohort. To achieve this goal, OCM puts restrictions on the caliper (minimum difference in exposure rank orders between the case and the control) instead of using exposure levels as CM. This avoids categorizing the continuous exposure to get exposure levels needed to implement CM. Simulation results indicate that using a wider caliper yields better relative efficiency. And comparison between OCM, CM and SRS shows that OCM has the highest relative efficiency when the rate ratio is within a practical range of 0.05 to 20. Therefore, we conclude that  OCM with a caliper of half the maximum rank order would be the best of the three sampling strategies when exposure variable is continuous. ","Postcensal population estimates (PPEs) are commonly used with case counts to estimate the prevalence of health outcomes. However, PPEs are subject to error and have an unknown level of uncertainty. We used Monte Carlo (MC) simulation to evaluate how a specified level of uncertainty could affect Autism Spectrum Disorder (ASD) prevalence estimates in 2008.  We assumed that the uncertainty in the true population size followed a triangular distribution with a minimum value of the intercensal estimate, a maximum value of the PPE, and a mode of the liner growth population estimate in 2008.  Based on 1000 simulated population sizes drawn from this distribution, we calculated 1000 ASD prevalence estimates and their 95% confidence intervals (CIs). The resulting distribution of possible values for the prevalence was summarized using the mean of the 1000 prevalence values. Uncertainty in the CIs was evaluated by defining an overall lower limit as the minimum of the lower CI values across simulations and the upper limit as the maximum of the CI upper limits. The results indicate that uncertainty in PPEs should be taken into account in estimating point and interval estimates of ASD prevalence.  ","Establishment microdata is subjected to strict confidential protection. Releasing it while providing meaningful statistical inference is a challenge. This is especially true for micro discrete/count data with fixed marginal totals. We develop a new statistical disclosure limitation (SDL) technique based on data synthesis. We propose a mixture of Poisson distributions for multivariate count data and a synthesis method subject to fixed marginal totals. We apply the procedure to a data from the Colombian Annual Manufacturing Survey. The results show that the synthetic data can provide high utility. We also present a method  for disclosure risk assessment. ","This paper provides an overview of evaluation protocols used in international education projects with examples from planned rigorous impact evaluation designs to unplanned evaluation post program  implementation across Africa and the Middle East. We review standard designs used for evaluation, discuss the state of the art in terms of survey design and data collection, and analyze international data with respect to the impact of various interventions on education performance. The paper considers a wide variety of data sources, both primary and secondary, evaluates current approaches, and recommends methods for improving education interventions through superior evaluation methodologies. ","Wait time is the differences between the time a patient arrives in the emergency department (ED) and the time an ED provider examines that patient. This study focuses on the development of a negative binomial model to examine factors associated with ED wait time using the National Hospital Ambulatory Medical Care Survey (NHAMCS). Conducted by National Center for Health Statistics (NCHS), NHAMCS has been gathering, analyzing, and disseminating information annually about visits made for medical care to hospital outpatient department and EDs since 1992. To analyze ED wait times, a negative binomial model is fit to the ED visit data using publically released micro data from the 2009 NHAMCS.  In this model, the wait time is the dependent variable while hospital, patient, and visit characteristics are the independent variables. Wait time is collapsed into discrete values representing 15 minutes intervals.  The findings are presented. ","Query-based systems for disclosure-safe analysis without having direct access to the raw data housed behind firewalls are gaining popularity as an alternative to the traditional PUFs. NORC X-ID system for data de-identification provides two such tools-Input X-ID and Output X-ID. The Input X-ID treats the raw data for disclosure without using the traditional framework of quasi-identifying and sensitive variables. Through a query tool, it can provide summary statistics for small or micro-groups of individuals which are further subject to uncertainty by subsampling and calibration; and thus avoids the limitations of traditional PUFs due to potential increase in disclosure risk with the dynamic nature of intruder knowledge. The Output X-ID avoids the limitations of traditional remote analysis servers due to differncing attacks by potential disclosure. It imposes pre-specified analysis domain-level restrictions through a checklist. We show how both descriptive and analytic inferences can be conducted for X-ID data with generally simple modifications of existing procedures. ","Egocentric sampling comprises observation of a network of interest from the point of view of a set of sampled actors (egos), who provide information about themselves and their network relations (alters), but who often cannot disambiguate them. It is the only practical way to observe certain classes of networks, such as those of sexual partnerships. Although methods exist for recovering network features from such data, a unifying framework, such as exponential-family random graph (ERG) modeling, is lacking, and, so far, approaches to fitting ERGMs to such data have lacked a rigorous statistical foundation in general and measures of uncertainty in particular. In this work, we identify a subclass of ERGMs amenable to being estimated from such data, develop techniques for doing so, and introduce a technique for rigorously evaluating the uncertainty (i.e., standard errors) of these estimates. For ERGMs parametrized to be invariant to network size, we also describe a computationally tractable approach for fitting these networks. We demonstrate these techniques through a simulation study and apply them to the 1992 National Health and Social Life Survey (NHSLS) data. ","Hierarchical linear models account for different levels of aggregation that may be present in data from survey samples. Inferential procedures that take account of complex survey design features are well established for single-level (or marginal) models. However, the design weights that are required to reflect complex designs are rarely considered under hierarchical linear models. In this paper, we develop a pseudo-likelihood approach procedure to incorporate the design weights for hierarchical linear models model framed in finite population. The proposed pseudo-log-likelihood function is an unbiased estimator of the log-likelihood function when the entire population is sampled. A limited simulation study is presented to compare the proposed approach from models with and without the design weights. Results show significant reduction in the mean squared error using the proposed approach especially when the design weights are related to the response values. ","The Behavioral Risk Factor Surveillance System (BRFSS) is the world's largest on-going telephone health survey system.  Conducting interviews 50 states, the BRFSS surveys monitor state-level prevalence of the major behavioral risks among adults associated with premature morbidity and mortality. The weighting methodology was revised in 2011 to use raking as a means of incorporating more demographic variables, including telephone ownership, into the weighting.  Designed as a state level survey, there has been increasing interest in using the data to provide estimates at the national level.  One challenge is to  develop design variables for use in accurate variance estimation. We present the development of weights that support combining state-level data to produce statistical valid national estimates. This work updates research conducted by Iachan et al (1999). The paper presents methods, a summary of the resulting weights, and compares resulting estimates with those obtained from other national estimation systems. ","In this paper, a new notion of a \"quasi empirical\" Bayes estimate is developed for estimating the proportion of sensitive attribute in a population by making use of both a prior distribution of prevalence of the sensitive attribute in addition to the known prior distribution of an unrelated characteristic. ","Auxiliary information from marketing databases can be applied to an ABS frame to identify addresses likely to house the target population for a particular survey. If this information is sufficiently accurate, these data can be used to stratify addresses and efficiently target the eligible population. This paper evaluates the accuracy of auxiliary information based on data from the Evaluation of Public Education Campaign on Teen Tobacco (ExPECTT), an in-person household survey targeting youths ages 11-16. Addresses flagged as likely to have a member of the target population were assigned to a high likelihood stratum and those flagged as not likely as well as those without auxiliary information were assigned to a low likelihood stratum. Because the auxiliary information is untested and high coverage desirable, ExPECTT includes addresses sampled from strata with presumed high and low likelihood of being in the target population. In this paper, we explore demographic characteristics and key outcomes of youths in households from high likelihood addresses with those in the low likelihood stratum. We also comment on the impact of the auxiliary information on data collection efficiencies. ","Differential item functioning (DIF) occurs when people from different groups with the same level of latent trait (ability/skills) have a different probability of responding to an item or a bundle of items in a questionnaire or test.  DIF detection is an important step in the evaluation of the measurement bias in an instrument.  Simultaneous Item Bias Test (SIBTEST, Shealy &amp; Stout 1993) is a popular DIF detection method which can handle both dichotomous and polytomous items, and DIF in a single item and in a bundle of items.  In this paper, we focus on the effect size measure as defined SIBTEST, and derive the formulas for the effect size under the IRT models. The relationship between the SIBTEST effect size and other popular DIF effect size measures are discussed. The correctness of the formula is confirmed by simulation studies. ","The University of Michigan Dioxin Exposure Study obtained multiple biomarker outcomes (BMOs) from respondents willing to provide a blood sample. However all predictor data is available for respondents who refused to provide a biological sample in addition to a subset of predictors for respondents to a nonresponse follow-up (NRFU) who refused the main interview. BMOs were multiply imputed along with a subset of predictors from NRFU to help retain sample size. Environmental exposure models regressed the complete cases, the former plus respondents who refused to provide a blood sample, and the former plus imputed NRFU cases. Comparisons among these three data sets reveal that imputing BMO refusals to the main survey attenuates most coefficients relative to the complete case analysis (CCA) and greatly increases the instability of the estimates based on the coefficient specific fractions of missing information. Including the imputed NRFU cases does not markedly increase the attenuation or instability relative to the first comparison. These findings raise important questions about whether CCA with nonresponse weight adjustments provide a sufficient basis for inference to a population.  ","NORC engaged in data quality improvement activities throughout the recent ten month data collection period of the Survey of Consumer Finances.  These activities included weekly data review and ongoing instruction to interviewers.  The data review activities included data monitoring, data evaluation and reviewing three potential sources of explanation for data problems: call record entries, reviewing commentary recorded during the interview, and debriefing notes recorded after the interview.  The ongoing instruction was delivered in a variety of forms: weekly memos, targeted interactive self directed lessons delivered in electronic format, ad-hoc group meetings held via-teleconference; and during weekly one-on-one meetings with a supervisor.  We will describe the processes we used to identify data quality issues, our data quality improvement protocols, and the data quality measures over time.  ","In order to estimate the number of occupied households, US Census Bureau conducts many surveys. As a result, we get different estimates of the number of occupied households from these surveys. While each survey is useful, differences among the estimates they produce are sometimes very large. To resolve these differences, we propose in this study a hierarchical Bayesian method to obtain a more reliable estimate of the number occupied households by combining estimates from these surveys. Exploiting the repetitive nature of the surveys, we propose a time series model. We apply our method to the estimates from Current Population Survey (CPS)/Annual Social and Economic Supplement, CPS/Housing Vacancy Survey, American Community Survey, and American Housing Survey between 2002 and 2011 to produce a more reliable estimate of the number of occupied households. We implement our objective Bayesian method by Gibbs sampling. ","With its first presidential election in 2004, Afghanistan is still in the infancy of its democracy. Although a key component of building a strong democracy is citizen participation (Aristotle, n.d./1995), Afghan women give fewer opinions to political questions than men (e.g., \"Don't know\") (Heese &amp; Arthur, 2012). Despite gender-matched interviews, this behavior is not uncommon in Afghan culture. This research examines the frequency with which Afghan women give substantive survey responses using latent class analysis (McCutcheon, 1987). Using a probability-based sample of 1,000 Afghan women from the 2012 Gallup World Poll, four political questions are analyzed. Preliminary analyses indicate three classes: women with the highest probabilities of answering all questions (70.8%), those with moderate probabilities of answering questions (16.1%), and a third class with a mix of high and low probabilities (13.2%). This research then describes membership in the classes with key demographic variables. The results identify challenges related to data collection in this region and how those challenges undermine efforts to obtain good estimates on important issues. ","Analysis of survey data, no matter the sampling design, is naturally limited to observations from respondents who agreed to participate and appropriately completed the survey. Sample selection bias occurs when there is some inherent trait that is linked to both the variable of interest and the likelihood of completing the survey. In some cases, most notably involving demographic information, probability weighting can be used to adjust for the missing responses. However, other less well-defined traits are more difficult to deal with. In the context of consumer payments, it often makes sense to attribute payments to entire households, with members of the household carrying different degrees of responsibility for and knowledge about those payments. As a result, non-representative sampling from within households, such as a tendency to interview those who are more responsible for payments, could lead to biased results if unaccounted for. We use data from the Survey of Consumer Payment Choice to study the degree to which financially responsible household members are over-represented and estimate the potential effects on estimates of monthly payment use.   ","The adjustment of survey data for non-response is typically based on weighting class methods.  Weighting classes are formed using a set of core variables that are correlated with response behavior and with survey outcomes.  The underlying principle is that respondents are more alike within a class than across classes.  The choice of variables may be made using response propensity models (i.e., logistic regression models for the response indicator), or with a range of recursive partitioning, tree-based classification methods (e.g., CHAID or CART).  Propensity models may also be used more directly to generate propensity scores that are applied to adjust for response probabilities.  This research compares these methods using real and simulated data with origins in two kinds of multistage stratified sample surveys: samples of students within schools, and samples of patients within facilities.  Comparisons are made along bias, variance and mean squared error of key survey estimates. ","When face-to-face interviews are required in a probability sample, sampling statisticians often construct a clustered sample to minimize the cost of data collection. In this type of sample design, the primary sampling units are geographic clusters, allowing interviewers to visit areas that are close together, reducing traveling costs. However, in some subpopulations, the number of cases in the primary sampling units is sometimes too small to provide enough sample cases for analysis. In several projects at Mathematica, we have resolved this issue by implementing a dual sample design, where the sample design consists of a clustered component (with face-to-face nonresponse follow-up) and an unclustered component (with no face-to-face follow-up). In this paper, we discuss a sample where budget realities during data collection made the original sample design too costly to implement. In this case, the proportion of the design within each component had to be constantly adjusted during data collection. We discuss how data collection reports were used to anticipate final response rates within various subpopulations, and how the sample design was adjusted accordingly.  [presnt only Tu,We,Th] ","The design of most surveys contains information revealing geographic locality of the sampled items. Combined with the other sources, this information contains the risk of violating the confidentiality of survey data. We propose modification of existing methods for masking design information by constructing combined strata variance estimators. In our approach variance estimation error is minimized conditional on the realized PSU selection, which is usually suboptimal to the declared selecting PSUs with probability proportional to their size (PPS). We compare errors of variances estimated with new method versus conventional methods of combining strata proposed in literature.  ","NORC has recently developed two innovative statistical disclosure limitation (SDL) methods under the X-ID system. AL-PUF (input de-identification) is not based on the traditional identifying/sensitive variable (IV/SV) framework. This method uses summary stats for various analytic domains at aggregate, or micro-group, level and introduces uncertainty via subsampling and calibration. Disclosure risk is the probability of 'too precisely' estimating values for very small (~unique) domains. Data utility is the probability of adequately estimating values for analytically valid domains. Q-PUF (output de-id) is a non-traditional approach where only pre-specified analytic domains are allowed for queries. This prevents differencing attacks, which cause trouble for traditional approaches. Log-linear estimates of sensitive cells are used to judge how well they are being protected. Disclosure risk is the probability of too precisely estimating the values of sensitive cells and data utility is the probability of estimating non-sensitive cells (including complementary ones) with adequate precision. Replication methods are used to obtain empirical measures of risk and utility for the two methods. ","When potential survey respondents decide whether or not to participate in a telephone interview, they may consider what it would be like to converse with the interviewer who is currently inviting them to respond. This includes for example how the interviewer sounds, speaks and interacts. Using a corpus of 1380 audio-recorded survey invitations this presentation will discuss how vocal properties of interviewers and householders, such speech rate, pause duration, gap duration and vocal pitch affect householders' decision to participate in a survey. Of particular interest is the increasing convergence between interviewers and respondents speech rate and its positive association with the participation decision. Such convergence and other vocal properties are factors that later might lead to correlated response variance, often interpreted as measurement error due to interviewers.    ","Can statistics make a global impact and still meet the real needs of local people, and free of charge?  In response to this intriguing question of \"glocalization\" - a newly coined blend of globalization and localization - leading professionals of Statistics Without Borders and StatCom will discuss the glocal impact of pro bono statistical consulting which takes bottom-line account of local considerations.   A panel of academics and practitioners joining from SWB and StatCom will critically review what worked or not; demonstrate how to provide effective pro bono consulting with real examples, and recommend what next steps to take in order to make enduring glocal impact via a pragmatic vehicle of statistical consulting. ","Survey nonresponse and associated risks of nonresponse bias are major concerns for government agencies and other organizations conducting surveys. At the time of data collection, respondents may choose to decline survey participation (unit nonresponse) or participate selectively by responding to certain data items and not others (item nonresponse). Nonresponse analysis often requires modeling the probability that a given unit will respond to the survey based on known unit characteristics. Often, the choice of method and the intended use of the model is different in this context than when adjusting estimates for nonresponse. During this roundtable, we will discuss methods for modeling and identifying unit characteristics associated with nonresponse and nonresponse bias. We will emphasize methods designed to help understanding the effects these unit characteristics have on the response rate throughout the data-collection process. ","This paper discusses the challenges in building shared information technology systems for survey data collection. We discuss the pitfalls of supporting multiple and redundant data collection systems. We recount our interviews with a cross-section of colleagues in the statistical field and argue that to achieve the best outcomes, technology innovations in system and enterprise architecture must be intertwined with methodological innovations such as adaptive survey design.   ","Prevention of errors in surveys must always be the highest ideal, but in such a complex process as a survey there are limits on what is achievable, because of cost, the absence of strong instruments for control or the emergence of unforeseen outcomes.  Thus, effort must be devoted to identifying errors, remediating them, and designing better means of prevention or limitation where that is possible.  Editing is typically a key instrument of identification and remediation.  However, editing can consume very substantial resources and because the outcome is unlikely to be perfect, the very act itself introduces additional risks to data quality.  For these reasons, it has been argued (e.g., de Waal, 2013) that a selective approach to editing, focused as squarely as possible on the core analytical goal of a survey may be more appropriate than detailed review of all survey observations.  For surveys supporting multiple uses, particularly ones involving multivariate analysis, there may be a need for a somewhat broader focus, but a more efficient approach may still be possible in such cases.  This paper evaluates various approaches to selective editing, using various combinations of fully edited and unedited data from the 2010 Survey of Consumer Finances (SCF), a widely used survey covering household financial behavior and a variety of associated information.  The paper also explores the potential importance of contamination of the imputation process under selective editing.  While editing has its direct effect on individual data items, it also alters the set of information used in imputing the missing values that result from the unwillingness or inability of respondents to provide answers or from the resetting of values to missing during the editing process.  The results of the paper support a selective approach to editing and they indicate that any resulting contamination of imputation is relatively minor in the case of the SCF. ","The American Housing Survey (AHS) produces many estimates regarding the U.S. housing market. In this paper, we examine the current AHS weighting procedures and apply a newly developed alternative single stage generalized raking method, which adjusts nonresponse, coverage, and calibration simultaneously. We use housing unit totals and head-of-household distributions as calibration constraints in a minimization of the objective function. After adjusting our weights, we develop replicate weights and calculate variance estimates to examine the discrepancies between the current and alternative weight adjustment methods. ","The National Health Interview Survey (NHIS) is a multi-purpose health survey conducted by the National Center for Health Statistics (NCHS). Public-use microdata files and complex sample design variance estimation structures are available at the NCHS Internet web site. NCHS has restricted microdata files and separate variance estimation structures for internal use. The variance estimation structures are defined for the first year of a new sample design period, and then updated annually to account for new sample areas. The updates are done in a way to ensure consistent definitions over a sample design period, which is important when conducting analyses of NHIS annual data pooled across years.  We describe our methodology for defining the structures and updating them on an annual basis. ","As data collection agencies balance response rates, cost, and data quality, the use of paradata in the form of contact histories benefit response propensity models and have applications with respect to adaptive design. However, when interviewers record information regarding each contact attempt with a sample unit, these data violate most modeling assumptions because they are clustered and not randomly assigned. Focusing on the role of the interviewer, particularly with respect to case reassignment, we use data from seven demographic surveys collected over an eight-month period to examine the effects of interviewers on the response propensity of both responding and nonresponding sample units. Multilevel modeling compensates for both the nesting of the sample units within interviewers and nonrandom case assignment. This research includes case reassignments from one interviewer to another, a phenomenon largely ignored in previous research, and further explores the utility of a new indicator we developed, the scaled evenness of finding attempts, or SEFA. When modeling case reassignment, the characteristics of the interviewer who finished the case should be used. Whenever possible, difficult to contact cases should be reassigned to interviewers with a higher than average SEFA score and with fewer cases in difficult strata. ","In accordance with current Census Bureau plans, the Research and Testing Program for the 2020 Census is researching how to reduce the cost of the 2020 Census.  The largest and most costly operation, Nonresponse Followup is being restructured, using administrative records, adaptive design, and a reduced contact cycle.  However, in order to determine the impact on the eventual cost of the census, we have used 2010 census response and paradata to estimate the parameters needed to estimate the cost of various nonresponse followup options, in conjunction with other design changes to the census. These data are then available to decision-makers to begin the process of choosing the major options for the 2020 Census. In this paper, we will lay out the framework used to compare various options from a research perspective, the methodology used to complete the estimates, and future plans. ","The National Survey on Drug Use and Health (NSDUH) is sponsored by the Substance Abuse and Mental Health Services Administration and provides national, state and substate data on substance use and mental health in the civilian, noninstitutionalized population age 12 and older. The NSDUH is a continuous survey, with approximately 67,500 interviews completed annually. As part of the NSDUH imputation procedures, over 400 regression models are fit each year. These models are used to match each item nonrespondent with a \"neighborhood\" of similar item respondents in order to identify a donor. The response variables in these models are variables of primary interest to analysts. After the procedures are complete for each year, an imputation model database is populated which stores covariate-level information such as the p-values associated with the regression coefficients. This database is used both by staff working on the NSDUH imputation and by staff analyzing the NSDUH data. This paper illustrates how such a database can be used not only by those conducting the imputation, but also by those making decisions during the analysis of NSDUH data. ","This paper presents a dynamic dual-frame sampling approach for a national survey that selects monthly samples independently for each of the 50 US states and the District of Columbia.   Each state has a separate allocation to the cell and landline telephone samples that reflects the estimated share of the cell-only and cell-mostly segments.   The survey excludes the overlap set of cell numbers that also have landline access.  The design also oversamples listed households in the landline sample, and a subset of cell numbers flagged as active via Marketing Systems Group's Cell-WINS service.  We examine the gains on productivity resulting from the oversampling in both frames.  ","In 2013, the National Center for Health Statistics released linked mortality files linking about 2.5 million survey respondents to the 70 million death records in the National Death Index (NDI).  While these data are of substantive interest for studying health endpoints  (e.g., the effects of obesity on mortality outcomes), they also provide a rich data source for record linkage methodology research, and in particular, research incorporating measures of linkage quality into health studies.  As with all record linkage projects, some pairs of survey-NDI record matches are of much higher quality than others.  Intuitively, one would expect that the more individual components of the records match, the higher the probability that the two records are a true positive, and the fewer components match, the lower this probability and the higher the probability that they are a false positive.  In the context of an analytic example of the effects of socioeconomic status  on mortality using the National Health Interview Survey linked to the NDI, this research evaluates the quality of record linkage in the linked mortality files, and assesses whether such probabilities can be approximated. ","Great organizations have gone through many high points in their history. The cohort of statisticians that made up the \"class of 1940\" at the U S Census Bureau was responsible for one such high point. These were the people who came to conduct the Decennial Population Census in 1940. They gained success and changed the survey and census-going world, forever. not only as individuals; but perhaps, more importantly as a team.     ","Surveys provide critical data for evaluating the Patient Protection and Affordable Care Act (ACA). However, measuring health coverage is challenging and all surveys appear to undercount Medicaid enrollment. We examine the extent to which Medicaid enrollment is misreported in the American Community Survey (ACS) by comparing responses from the ACS linked to people shown to be enrolled at the time of the survey in the Medicaid Statistical Information System (MSIS). We find that in 2009, 78.4% of ACS respondents known to have Medicaid accurately report their coverage; about 12.5% indicate another type of insurance and 9.1% report that they are uninsured. The undercount varies between 2008 and 2009 and the magnitude of the difference varies across states. We examine the undercount across a variety of other characteristics potentially related to accurate reporting including household composition and interview mode, and discuss the role of the undercount in the level of bias to uninsured estimates. We find Medicaid reporting in the ACS is consistent with other federal surveys and the results should provide policymakers confidence in using the ACS data to evaluate the impact of the ACA. ","The National Agricultural Statistics Service (NASS) conducts a Census of Agriculture every 5 years, in years ending in 2 and 7.  For the 2012 Census of Agriculture, capture-recapture methods were used to adjust the Census of Agriculture for under-coverage, non-response, and misclassification of farms/non-farms. NASS's June Area Survey (JAS), which is conducted annually in June, was used as the independent survey in the capture-recapture survey.      ","Previous work by the author used Markov Latent Class Analysis (MLCA) to make aggregate estimates of the underreporting of household expenditure by category (e.g. clothes, furniture, and electricity) by exploiting the four interview, rotating panel design of the Consumer Expenditure Interview Survey (CE).  This analysis collapsed a few years of the CE into a pooled single panel.  Estimates from this analysis were shown to be consistent with both internal and some external indicators of measurement error.  However, there is no \"gold standard\" for expenditure reports and MLCA models require strong assumptions for identifiability making evaluation of model estimates difficult.  The current analysis uses MLCA on data from 15 years of the CE, in order to examine the reliability of measurement error estimates over time.  Both sequential and overlapping panels are used in order to assess the sensitivity of MLCA estimates to major survey revisions.  Finally, these estimates are compared to survey nonresponse and external benchmarks where available.   ","Small-area crime trends are often of interest, but neither of the two most widely used sources of crime data provide reliable county-level estimates. The Uniform Crime Reports (UCR) collect voluntary submissions of crime data from police agencies. UCR was designed for national estimates of reported crimes, and due to missing data does not provide stable county estimates. The National Crime Victimization Survey (NCVS) is a national survey which asks respondents about crimes over the past six months, including ones not reported to police. The NCVS public-use data does not have county identifiers.   Most research in combining these two sources has focused on using UCR data to augment NCVS estimates, typically using linear regression. The proposed method uses direct UCR county-level estimates from agencies using the NIBRS reporting system adjusted for bias using NCVS estimates for corresponding victim age, sex, and large area domains. The weight placed on each data source is allowed to vary based on the quality of information available. This strategy allows for reasonable estimation of crime rates in all counties with largely complete UCR data using only the NCVS public-use files.  ","The Survey of Prison Inmates (SPI) is being redesigned for its intended implementation in 2015 by the Bureau of Justice Statistics (BJS).  The goal of SPI is to provide estimates which describe the U.S. state and federal inmate population across topics such as criminal history, mental and physical health status, and socioeconomic status at the national-level, as well as at the level of particular subdomains (e.g., gender) and jurisdictions (i.e., states with large prison populations such as California, Texas, and Florida).  This paper discusses the methodologies and associated challenges encountered during the SPI redesign, including the determination of an optimal first-stage allocation scheme, utilization of both implicit and explicit stratification to select prison facilities across subdomains, and estimation of the required number of inmates to be sampled in order to achieve the desired precision.  Using a redesigned instrument and historical experience from the 2004 SPI to approximate design effects, a simulation study was conducted to determine the proportion of outcomes that would meet the study's precision goals while minimizing respondent burden on inmates and facilities. ","The Bureau is examining ways to reduce the costs of the 2020 Census while maintaining a high quality census.  One of the largest contributors to the 2010 Census cost was when enumerators had to be sent to interview and resolve housing units that did not return a mail questionnaire in the Nonresponse Followup operation.  One area of research has been attempting to utilize administrative record information from various federal and other sources to reduce the number of interviews to be done for the Nonresponse Followup operation.  This paper will present an overview of the current research being conducted and some initial research results of possible administrative records usages. ","Most of the nonresponse literature has been emphasized at the element level, that is, the nonrespondent is the ultimate unit in the sampling process. However, it is frequently found in survey practice nonresponse at earlier stages of the sampling process, especially on surveys of institutions, such as schools or establishments. One possible alternative when such event occurs is to substitute the nonresponding unit. Although such procedure has been extensively criticized in survey sampling literature, it is largely used in practice, especially in school-based surveys. Moreover, there are different ways such substitution can be implemented and it is not yet clear which one performs better. This paper aims to evaluate the impacts on the survey estimates under the presence of nonresponse at the primary selection unit level of a two-stage cluster sampling and the properties of such estimates when various forms of substitution are used. A simulation study is conducted to evaluate under which scenarios these substitution procedures are justified compared to other alternative strategies, such as sample size inflation and weighting. ","The Commercial Buildings Energy Consumption Survey (CBECS) is a quadrennial survey of commercial buildings, such as offices, retail stores, and other nonresidential structures. The survey collects information on energy use in buildings and their characteristics that are related to energy use. Finding a respondent who is knowledgeable about the entire building has always been a particular challenge for strip shopping centers, as most of the people on site are store managers who can only give information about their own establishment. To help rectify this, the 2012 CBECS used a new procedure in which the interviewers compiled a roster of all of the establishments in a strip shopping center building, and conducted interviews at a randomly chosen subset of the stores. The new procedure will produce a description of the establishments that were listed on the roster but not interviewed by imputing based on similar establishments that were interviewed. The interviewed and imputed establishment characteristics will then be aggregated into a description of the entire strip shopping center building.   ","Our paper examines the relationship between beliefs about cash and the sources and sinks of cash for a sample of 1200 individuals. The survey dataset is based on face to face personal interviews using a multistage geographic random sample of urban and periurban areas in Guadalajara, Mexico DF, Monterrey and Tijuana. Interview topics included basic demographic traits, employment, financial practices, beliefs about cash, sources of cash, cash access events and associated transaction costs; and payment choice for a set of sixteen bills and nine purchases. We estimate the prevalence of financial account ownership, the preferred sources and frequency of cash access, and the aggregate costs of access to cash. We examine the effects of financial inclusion on beliefs about the risks and necessity of cash; and the determinants of payment choice conditional on financial inclusion. The survey dataset is the second in a series of four country studies covering similar topics in the United States, Mexico, India, and Egypt. ","The Current Population Survey (CPS), sponsored jointly by the U.S. Census Bureau and the U.S. Bureau of Labor Statistics (BLS), is the primary source of labor force statistics for the population of the United States.  The official unemployment rate estimates are based on a composite estimator, known as the A-K estimator, which borrows strength from past survey data.  We extend the standard model-assisted method with estimated total for the auxiliary variables using models that are appropriate for longitudinal surveys.  We critically examine theoretical properties of the proposed method.  Using a Monte Carlo simulation experiment, we study different models for employment evolutions and study simulated properties of different estimators. We also compare these estimators when applied to the CPS data. ","The ethnocultural (EC) questions on the 2011 Canadian National Household Survey (NHS), which complements the Canadian Census, measured ethnic and cultural characteristics of the Canadian population. These characteristics are very closely related. For prior Canadian Censuses, the five topics of Immigration and Citizenship, Place of Birth of Parents, Aboriginal, Ethnic Origin and Population Group were processed sequentially through edit and imputation (E&amp;I). In addition to the less-than-optimal efficiency of separate processing, an unacceptable quantity of outlier combinations was present in the imputed data, necessitating a great number of manual post-E&amp;I fixes. For the 2011 NHS, the five separate EC topics were combined into one unified topic with the goal of simplifying the E&amp;I processing, improving the internal coherence of the imputed data and reducing manual intervention after imputation. This paper describes the challenges that were faced and the solutions developed in order to accomplish this task. ","Confidentiality is of primary importance for federal agencies, such as the National Center for Health Statistics (NCHS) when releasing microdata to the public. Although the data is initially de-identified, that is, stripped of personal identifying information (PII), such as name, address, social security number, etc., data users with malicious intent, also known as intruders, may be able to link records in the data to individuals through external data files or through observable traits. Records that have unique combinations of key variables are particularly vulnerable to disclosure. Failing to protect individuals from these breaches could possibly affect the reputation of the agency and may have consequences on the population's willingness to participate in future data collection efforts. Statistical disclosure limitation techniques, such as random data swapping, are used as an attempt to prevent breaches in confidentiality. For this presentation, random data swapping methods were explored through simulations and through real data analysis of public use data from the National Health Interview Survey (NHIS).   ","In cluster-randomized experiments--commonly used in the social sciences--a subset of clusters is randomly drawn from the population, treatment is assigned randomly across clusters, and individuals are sampled within each cluster.  The number of clusters sampled and the number of units sampled within each cluster is typically constrained by a budget allotted for the experiment.  We derive estimators for the average treatment effect (ATE) for cluster-randomized experiments with an arbitrary number of treatment categories and an arbitrary pretreatment blocking of clusters under the Neyman-Rubin potential outcomes model for response.  We demonstrate that sampling clusters with probability proportional to the number of units with a cluster will allow unbiased, location-invariant estimators of the ATE with smaller variance than those under simple random sampling of clusters.  We show that, given a budget allotted for an experiment, choosing the number of clusters and number of units sampled within a cluster to maximize the power of tests can be accomplished by solving an integer programming problem. ","The Schools and Staffing Survey (SASS) is undergoing a redesign and will be called the National Teacher and Principal Survey (NTPS) during its next administration, to be conducted during the 2015-16 school year. As part of this redesign, it is of interest to determine if multiple imputation methods or administrative records could replace or supplement the current hot deck imputation procedure. One research objective is to determine if the coverage and quality of the administrative records is sufficient to use as an alternative imputation source. The other objective is to explore additional suitable imputation techniques with and without using administrative records. This paper will discuss the direct assignment imputation method, in which administrative records will be used to assign values to missing data for the matching SASS record. Other imputation methods that will be explored include predictive mean matching imputation and propensity score imputation. To evaluate the proposed imputation methods we will look at several metrics that examine the bias of the predictive, distributional, and estimation accuracy of each of the methods. ","In recent years, the National Agricultural Statistics Service (NASS) began a research effort to address an undercount in the estimate of the U.S. number of farms derived from its annual June Agricultural Survey (JAS).  Misclassification of farm status was found to be a major cause of the undercount.  NASS has evaluated a host of measures and methods to assess, quantify, and account for this misclassification.  The approach derived from this process employs record linkage techniques, logistic regression, and NASS's annual list sampling frame.  The methods developed and the subsequent results are presented here.   ","Survey nonobservation errors are an increasing problem in surveys of all modes.  This paper examines the nonobservation errors for survey estimates with two different sampling designs: fixed sampling design and adaptive sampling design.  Both designs are assumed the same design features during data collection.  The difference is in how the auxiliary variables are used.  Fixed sampling design uses auxiliary information through post-survey adjustments to eliminate nonobservation errors.  The adaptive sampling design, utilizing those auxiliary information at the design stage to make sampling decision, reduces nonobservation errors at each data collection phase.  The nonobservation errors are studied by simulated hypothetical surveys under comparable auxiliary information and nonresponse mechanisms.  The simulation studies are based on real data from two large government surveys, NHIS and BRFSS.  The nonobservation errors, measured by biases and variances, for estimated means and ratios are investigated for the full population and for domains.  The results suggest the benefits of adaptive sampling design in reducing nonobservation errors.   ","Nonresponse is a ubiquitous problem in surveys around the world. Not only does nonresponse diminish the resulting sample sizes used for analyses, it can also introduce bias in statistical estimates obtained from the collected data. In practice, very little is known about the nonrespondents and whether they systematically differ from the respondents on key survey variables. In an effort to obtain more information about nonrespondents, we assessed the feasibility of asking nonrespondents to participate in a follow-up study conducted in Germany. In the study, nonrespondents to a telephone survey were randomized to one of two follow-up groups. In the first group, nonrespondents were mailed an abbreviated questionnaire and asked to answer 5 questions from the main survey questionnaire. In the second group, nonrespondents were mailed a consent form asking for consent to access their administrative employment records. We present results from both follow-up strategies to examine characteristics of those who choice to participate in the follow-up study and grant access to their administrative data.   ","The Current Population Survey (CPS) is a leading source for labor force data for the US. CPS produces such statistics as the national unemployment rate and other economic and employment statistics. CPS currently uses a hot-deck single imputation method to fill in any missing data. Single imputation methods do not incorporate the additional variation introduced using imputation into variance estimates, but multiple imputation methods can account for this added variation. To investigate the amount of variance underestimation, an iterative hot-deck multiple imputation method (Siddique, 2008) is used to impute the missing data from the 2012 CPS data. The results are compared to the current CPS method by comparing each methods' estimate of variance and examining the bias by determining the predictive, distributional, and estimation accuracy of the imputed data. Because the rates of item missing-ness are low for CPS, different rates of missing-ness will be imposed on the data to determine at what rate of missing-ness the added variance due to imputation is not significant. ","Small Area Estimation has gained importance day by day in survey sampling for effective allocation of government funds for economic, health and social planning. The design based estimators are unreliable due to unavailability of sufficient number of observations from each small area. Measuring variability of small area estimators under basic area level models with known sampling variances is a well studied problem in literature. But estimation of small area means as well as mean square prediction error of the small area means becomes tricky when sampling variances are assumed unknown in such models because the parameters are no longer identifiable. In this article we impose some additional structure on the sampling variances in the Fay-Herriot model, and resolve the identifiability issue. We prove consistency of parameter estimates, and establish results for the mean squared prediction error, and parametric bootstrap estimates for it. We implement our method under 3 different simulation setups and one real data. ","The National Hospital Care Survey (NHCS) is a new survey that aims to provide national estimates on utilization of health care provided (1)  in hospital-based settings, comprising  inpatient departments, emergency and outpatient departments (EDs and OPDs), and hospital-based ambulatory surgery locations (ASLs) and (2) in freestanding ambulatory surgery centers (ASCs). NHCS is designed to collect nationally representative data on utilization of hospital inpatient care from administrative claims databases submitted by a sample of hospitals. Data on sex, age and length of stay (LOS) were reported to be missing for about five percent of the discharge records in those databases. This research will explore hot deck imputation and model based imputation, including the sequential regression modeling approach, to impute the missing values.  Use of different available software, such as IVEware and SUDAAN 11, to perform the imputation will also be explored.  Evaluation of different models will also be performed for quality check on the imputed values.  ","A parametric bootstrap procedure is proposed for the mean squared error of the predictor based on  a unit level model. It is demonstrated that the proposed procedure has smaller bootstrap error than  a classical double bootstrap procedure with the same number of samples. Applications to a logit  model under different types of auxiliary information are discussed. ","RTI International has conducted Early Grade Reading Assessments (EGRA) in approximately 55 countries to date. This paper will focus on challenges of sample design and external factors faced by statisticians during the assessment, planning, and implementation processes.  Topics touched on include constraining factors (i.e. logistical, budgetary and timeline) and challenges during data collection and how these affect the sample design.  The statistician must be able to negotiate and compromise while still maintaining the statistical validity of the study.  RTI will focus on three case studies to demonstrate how challenges faced during the planning and implementation of EGRAs can always be overcome and result in high quality data and analysis for stakeholders. ","Data collection and analysis has grown considerably more complex in many areas. The research design literature has not kept up with the way in which research studies are implemented in many situations. In the design context, a principle concern is the sample size that should be used. Sample size considerations can be based on power, accuracy, or both simultaneously. To overcome the dearth of guidance on planning sample size in modern research designs, this presentation seeks to reconceptualize the way in which sample size is planned. Rather than an analytic based approach, many of which are approximate or does not exist, a general approach that would serve a multitude of approaches and goals would be beneficial. A unified framework is developed for sample size planning using a Monte Carlo study before the study is conducted, which solves issues of planning sample size in a most general way. The a priori Monte Carlo approach to sample size planning  allows any design to be evaluated for whichever approach to sample size planning is of interest, indeed, multiple approaches can be evaluated simultaneously, something few existing methods are able to accomplish.  ","Large government administered  surveys are designed to provide reliable estimates of finite population characteristics for large geographical regions such as the entire U.S. or each of the 50 states, but not for subpopulations and small geographical regions. Our investigation uses the 2010 Behavioral Risk Factor Surveillance System, BRFSS, to make inference at the county level for various population characteristics, such as totals, proportions, or quantiles, for health characteristics like the health insurance  rate or the obesity rate.    ","In the 175th year of the ASA, we are entitled to be looking backwards as a way of looking forward.  One way to do so is to consider recovering some of the survey research advances that are buried in the hardcopy ASA proceedings of the Social Statistics Section (SSS) before 1978, the year that the Survey Research Methods Section (SRMS) was born and the year it split from the SSS. All SRMS proceedings papers from 1978 onwards are already online. As a service to our profession, we are embarking on an effort to identify, scan, and put online a selection of  survey research papers published in the SSS proceedings prior to 1978. This talk focuses on a particularly seminal SSS session in the 1968 meetings held in Pittsburg, PA, to illustrate the significance and possibilities of this project. That session featured many luminaries, famous then and some even more now. Notably, Leslie Kish talked about his coinage of design effects, or DEFFs, and John Tukey built on Kish's idea and coined the term DEFT, which is the square root of DEFF.    ","In recent years, statistical organizations have developed increasing interest in the integration of customary sample survey data with information from other sources.  Examples of these sources include administrative records, commercial transaction records, and social media traces; and sometimes have labels like \"organic data,\" \"non-designed data\" and \"big data.\"  For some cases, methods for integration of these data sources can be based on extensions of previously developed methods for propensity modeling and for multiple-frame-multiple mode surveys.  This paper discusses these integration methods, with special emphasis on adaptive use of information on microdata quality and costs associated with a given data source.   ","Sample surveys are often used in the evaluation of social programs and require specific target minimum numbers of respondents to achieve statistical precision objectives for multiple subpopulations. To achieve the target number of completed interviews with the smallest sample size in each subpopulation, a carefully managed release plan is required for each study subpopulation and we often use random subsamples. The process uses the selection of an \"augmented\" sample (using implicit stratification) that is substantially larger than is expected to be required (using \"pessimistic\" response rates). The \"augmented\" sample is then randomly partitioned into small subsamples using an algorithm that preserves the implicit stratification. The initial sample release for data collection is then based on \"optimistic\" response rates.  Periodic assessments are conducted to determine if additional sample releases are needed and, if so, in which study subpopulation(s). In this paper we will describe the algorithm and provide guidance on how to manage a sample. This random partitioning can also be used to assess experimental data collection procedures. ","The National Health Interview Survey (NHIS) is an annual large scale national survey that collects individual health outcome data. As the NHIS is based on a complex survey design, analytical \"best practice\" involves accounting for the survey design features in analysis of the data. To expand analytical utility, the NHIS has been linked geographically to select EPA pollution data over the  years 1985 to 2005. This available EPA-linked data is only partially complete with respect to geographical coverage, and some analytical caution is advised since a \"missing at random\" distribution for linked pollutants cannot be assumed. Inferences about associations between population health outcomes and air pollution status may be biased if standard design-based analytical methods  are implemented. The present study focuses on investigating situations where such biases may occur and some possible analytical corrective actions. We suggest model-based alternatives for estimating associations between population health and air quality. The impact of bias and variance of the demographical components in the statistical weights, as well as clustering effects are examined.    ","The Medicare Current Beneficiary Survey (MCBS) produces nationally representative estimates of health status, health care use and expenditures, health insurance coverage, and socioeconomic characteristics of Medicare beneficiaries. It is a large-scale, national survey with a rotating panel design and two decades of history spanning more than 60 rounds of data collection. MCBS has experienced high response rates over its long history, but response rates have recently begun to decline, leading to a steady increase in contact attempts.    ","Bayesian methods can be used as an alternative to the traditional design-based methods to improve precision of small sub-domain estimates from complex surveys. Calibrated Bayesian methods use the design and weighting structure of complex surveys. We apply calibrated Bayesian methods to estimate state-specific vaccination coverage rates from the National Immunization Survey (NIS), a large telephone survey used to monitor childhood vaccination coverage among U.S. children 19-35 months. We use NIS data from 2010-2011 and use model-based distribution of predicted values of vaccination coverage as an informative prior to simulate posterior probability distributions. Estimates from these posterior distributions are then compared with the corresponding design-based official state-specific vaccination coverage estimates and their variances. We also compare the state-specific sub-domain Bayesian estimates by selected demographic characteristics with those obtained using small-area estimation methods. Evidence of improved precision would lead to consideration to apply calibrated Bayesian methods for small sub-domain estimates for future analyses. ","Although data linkage provides an opportunity to conduct analyses that pull strength from each contributing source, additional complexity can arise due to the incoherence of data features. The CDC's National Center for Health Statistics (NCHS), in collaboration with the Florida Cancer Data System (FCDS) and University of Miami, conducted a pilot linkage between the 1986-2009 National Health Interview Survey (NHIS) and 1981-2010 FCDS. This linked data might be used to examine risk factors and characteristics of Floridians who were diagnosed with cancer, controlling for other demographic, socioeconomic, and health factors. Due to residential mobility in and out of Florida, however, there are analytic issues in constructing an analytic sample, calculating appropriate sampling weights, and accounting for the longitudinal feather of both datasets. We will describe these issues and discuss methods that could be used to address them. This specific example will provide valuable insight into the complexities of analyzing national data linked to state-level data. ","The Census Bureau collects the Annual Survey of Jails for the Bureau of Justice Statistics.  Respondents have the option to fill out a paper questionnaire or respond online.  There are four versions of the questionnaire, with smaller facilities asked fewer questions.  The paradata captured with online submissions enable us to tell how often respondents change their answers.  We can now tell which questions pose the greatest challenges to respondents.  We can also get better estimates of how long it takes a respondent to complete each version of the questionnaire.  The Census Bureau has collected paradata for the Annual Survey of Jails through three annual collection cycles, so we can also see if the respondents have fewer difficulties with online reporting as they become accustomed to online reporting.  Does the facility's average completion time or average number of edit messages decrease during the second or third collection cycle?  The answers to these questions can help us improve the data collection process and enhance the quality of the respondent's survey experience in the future. ","Weighting adjustment and multiple imputation are two commonly used methods for missing data issues in surveys. In this talk, we compare results of weighting adjustment and multiple imputation for linked National Health Interview survey and Medicare data files (NHIS-Medicare).  We study mammography status based on Medicare claims from 1999 to 2004 for women 65 years and older each year. For NHIS-Medicare data files, mammography information is usually available for participants who consent to have their data linked and who are in the Fee-for-Service Medicare program, but not available for participants who were not linked and less consistently available for participants in the managed care programs, such as Medicare Advantage.  In our study, mammography and Medicare Advantage status are missing for NHIS participants not linked to Medicare; and mammography status is missing for some linked participants who have Medicare Advantage coverage. We conduct simulation studies and apply weighting adjustment and multiple imputation methods to the linked NHIS-Medicare files. ","This paper utilizes data from the Service Delivery Indicator Surveys, a recent data initiative led by the World Bank in Africa, to provide an overview of the learning experiences of primary school pupils in Nigeria.  The paper will combine data from classroom observations, teacher assessments and pupil assessments to explore how teachers allocate their teaching time and assess their quality of instruction in comparison to international benchmarks. It analyses the relationship between the quality of instruction and teacher characteristics and its impact on pupil learning outcomes to understand the link between teacher knowledge, behavior and pupil learning outcomes. ","Multiple imputation (MI) approaches like sequential-regression multivariate imputation and MI by chained equations often use a series of generalized linear models to create posterior distributions from which the imputed values are selected. In general, one must specify the correct type and functional form of the generalized linear model at some basic level for each variable imputed. Nonparametric multiple imputation (NMI), by contrast, eliminates much of the problematic specifications. NMI also incorporates a response propensity model in the formation of the posterior distributions and thus provides double protection against model misspecification. A Monte Carlo simulation evaluates the performance of NMI comparing it to MI using the MICE package in R. The evaluation criteria are bias, confidence interval length, and coverage of the 95% confidence intervals. ","Commercially-available address lists have been replacing traditional listings as the basis of sample design and implementation in recent years due to the potential for cost savings, with both the coverage properties of available lists and the cost of listing varying considerably based on various factors.  Research at NORC indicates that the proportion of the USA that requires in-field listing has changed substantially.  According to comparisons from the NORC National Master sample in both the 2000 and 2010 decades, which has listings for surveys across environments and geographies in the USA, the population requiring listing has shrunk from 28% to 15%.  While coverage of address lists is quite good overall, it suffers in areas with P.O. Boxes, rural routes, and other non-locatable addresses.  In addition to the United States Postal Service Delivery Sequence File (DSF), NORC has been involved in continuous evaluation of the NoStat File, particularly as it relates to vacant properties, drop points and rural routes.  Our paper summarizes previous research with a focus on determining the \"threshold\" of when it is necessary to augment the DSF to avoid coverage deficiencies.   ","All survey estimates are subject to measurement error - classification error in the case of categorical outcomes. This is especially true of sensitive outcomes such as sexual victimization. The National Inmate Survey (NIS), sponsored by the U.S. Bureau of Justice Statistics, is a nationally representative survey of inmates in prisons and jails which measures two types of sexual victimization - inmate-on-inmate and staff sexual misconduct with an inmate. This paper builds on the research of Berzofsky, Biemer, and Kalsbeek (2014) to present the results of a latent class analysis (LCA) designed to assess the measurement error in each type of sexual victimization. LCA uses multiple indicators of a construct embedded in the survey instrument to estimate the false positive and false negative probabilities in each indicator. Due to the rare nature of sexual victimization among inmates, our analysis combines data from the 2007-08 NIS and the 2009 NIS in order to achieve adequate precision in the results. One issue with LCA is how missing data for indicators and grouping variables are taken into account. Traditionally, if either type of variable was missing the model would use listwise deletion and remove the case from the model. Newer software, such as LatentGold, incorporates full information maximum likelihood (FIML) for dependent variables to utilize all records. This paper assesses the impact that the inclusion of cases with missing data have on the LCA estimates. Using MAR adjustments for missing data, we found evidence that inmates who do not respond to all indicators are more likely to be victims and more likely to not provide truthful responses for the items they do answer.  ","The National Survey of College Graduates (NSCG) is primarily a sequential multimode survey design that collects responses by internet, paper, and telephone. Cases are sampled out of the American Community Survey and are interviewed four times over an 8-year period using rotating panels. The NSCG has access to ACS data and, in later rounds, historical NSCG response data and paradata. This research explores using multinomial logit and probit models to predict final response mode using only the information available before data collection begins.    ","Correctly recalling where someone lived as of a particular date is critical to the accuracy of the once-a-decade U.S. Decennial Census. In the 2010 Census, all persons living in the U.S. were counted at the place they were living or staying as of Census Day, April 1, 2010. The data collection period for that census occurred over the course of a few months: February to August, with some evaluation operations occurring up to 11 months after Census Day in April. The assumption was that respondents could accurately remember moves and move dates on and around April 1st up to 11 months afterwards. Our research uses statistical models to investigate the validity of this assumption by comparing reports of move months in a U.S. Census Bureau survey with an administrative records database from the U.S. Postal Service containing requests to forward mail filed in March and April of 2010. We found some evidence that the length of time since the move affects memory error in reports of a move and the month of a move. Respondents were less likely to report a move when responding to a survey 10 to 11 months later than when responding to an identical survey either 2 to 3 months or 5 to 6 months later. However, the error in reporting a move did not differ when responding to a survey 5 to 6 months later compared to responding 2 to 3 months later. For movers, the analysis of the discrepancy between the reported move month and the NCOA record showed the length of time since the move had a similar effect on the error in the reports.  ","A compositional time series is a multivariate time series in which each of the series has values bounded between zero and one and the sum of the series equals one at each time point. This paper presents the state-space approach for modelling compositional time series from the Brazilian Labour Force Survey (BLFS) taking into account the sampling errors. The BLFS is a rotating panel survey in which the rotation pattern applies to panels of households. Within each rotation group, a panel of households stays in the sample for four successive months, is rotated out for the following 8 months and is sampled again for another four successive months. The survey collects monthly information about employment according to the International Labour Organization (ILO) definitions. The modelling procedure produces estimates for the vector of employed, unemployed and not in the labour force and also for the unemployment rate series with corresponding estimates for seasonals and trends. The model provides bounded predictions and estimates satisfying the unity-sum constraint while taking into account the sampling errors and the correlation structured implied by the survey rotation pattern. ","There are around 15,000 commercial banks, savings institutions, and credit unions (collectively, banks) in the United States that handle all of the deposit money and process virtually all of the noncash payments for consumers and businesses, summarizing the aggregate behavior of their customers. In our survey, a large representative sample of banks reported aggregate payment flows, along with the number of customers they served, complimenting the existing data. The reported information can be thought of as grouped sets of observations drawn from two independent bank customer distributions, and variations may help identify the customer distributions. Simple cross-sectional regressions produce reasonable allocations of payments to the different customer types, essentially estimating mean behavior, but do not use all the available information. Using an iterative Gibbs sampling approach, we develop estimates of the separate distributions of consumer payments and business payments using observed subsamples of the distributions with varying and known levels of mixing. The approach leads to a more complete description of payment choices being made by consumers and businesses. ","Many education surveys focus on data collected at schools, in classrooms or by assessing students. This paper will detail experiences from a household survey  called the Nigerian Education Data Survey. It will focus on the types of data available at the household level factors affecting attendance including distance to school, perceived school quality and household expenditure. Surveying households enables researchers to capture information on children who do not attend school and to information from the parent perspective. Household surveys are the best source of information about Net Attendance and Gross Attendance. These are important indicators related to access to school, a critical issue in developing countries. We will discuss the use of newer survey technology (tablets data collection and GPS) and how these can improve household data collection.  ","When selecting multistage samples for in-person household surveys, the final stage of sampling typically involves sampling dwelling units or addresses from lists of these units within sampled geographic areas generally known as segments. The use of address-based sampling (ABS) frames based on USPS-lists as the source of address lists is a cost-effective alternative to the traditional listing of dwelling units by field staff.  This paper discusses the implementation of an ABS frame for two in-person household surveys.   An Address-Coverage Enhancement (ACE) procedure, which involves the sampling of geography-based units in which field staff record potential off-of-frame addresses and the sampling of confirmed off-of-frame addresses for assignment to data collection, will be evaluated for its effectiveness.  Variables included on the frame will be evaluated for their usefulness in sampling, including the vacancy indicator and the educational institution indicator. Methods used to sample clustered units (called drop points) which are included on the frame will also be evaluated.   ","Data on a single individual or entity are often available from many different sources. Researchers seeking to combine information from such diverse sources must identify which records in one file correspond to the same individual in another. The difficulty of this task is compounded when confidentiality concerns mandate that records are stored without a unique identifier. Record linkage is the process of comparing information in such de-identified records in order to determine groups of records which correspond to the same entity and create a final linked data set to be used for inference. The linking process is often hindered by the possibility of errors in the record fields used for comparison. This work presents a Bayesian method for record linkage which accounts for potential errors in the linking fields while incorporating the uncertainty of the matching process into inference conducted on the final linked data set. An example with social science data is presented to illustrate the method. ","The NHIS is a multi-purpose face-to-face health survey conducted annually by the National Center for Health Statistics (NCHS). Historically, the NHIS has been designed to produce accurate statistics at the national level. The traditional design has been based upon a one-time static sample of coarsely defined geographical units, typically a probability sample of counties and block clusters, to define the areas for annual household sampling conducted over a 10 year NHIS design cycle. Recently, there has been a greater interest in structuring geographical flexibility into the NHIS. In particular, with recent attention given to state-level health measures,the NCHS wants the NHIS to have the ability to expand and contract the sample in targeted geographical areas in order to meet both estimation needs and budgetary conditions. Any flexible design structure must be cost-effective and implementable in a timely fashion with respect to field operations, and be amenable to creating the design features needed for data analysis for both annual and multiple years of data. In this work a proposed systematic sampling method which meets many of the goals of a flexible design is discussed. ","This paper will introduce the educational system in Qatar and the reform process it went through over the past decade. The basis for the school report cards used to support decision making by parents, teachers, principals and education officials. The Qatar National Education Database System resulting from a large scale data collection across all schools in Qatar. ","Interviewer administered household surveys with age specific target populations generally have three options for identifying age eligible households: (1) full enumeration of all household members and their ages; (2) directly asking the household informant if anyone in the household is in the desired age range; and (3) asking a few, brief questions about the age range of household members to isolate if the household is eligible without explicitly identifying the desired age range. Tourangeau et al. (2012) conducted a centralized telephone experiment examining these three methods and found that the full enumeration method produced the highest eligibility rates and lowest response rates, indicating a tradeoff between coverage and nonresponse errors when choosing between the three methods.  In an experiment sponsored by the Health and Retirement Study, we attempted to replicate these findings for an in-person screening of persons in a 6 year age cohort.  Our results showed trends that the full enumeration of all household members produced higher eligibility and similar response rates compared to the other two methods. Potential reasons for these findings will be discussed. ","A set of unweighted normal equations for a least squares solution assumes that the response variable of each equation is equally reliable and should be treated equally. When there is a reason to expect higher reliability in the response variable in some equations, we use weighted least squares (WLS) to give more weight to those equations. For an analysis of experimental or observational data, an inverse of variance is typically used for efficient estimates. For an analysis of survey data, sampling weights are typically used for unbiased and efficient estimates. There might be reasons for deviating from these weights - e.g., heteroscedasticity or extreme weights. Different weights can yield different point and interval estimates of the coefficients, affecting the interpretation of results. In other work, we considered the impact of different functional forms of weights on the WLS solutions. In the current work, we simultaneously consider sampling weights and inverses of variance for the WLS solutions, using data from the 2009-2010 National Health and Nutrition Examination Survey (NHANES), a periodic survey conducted by the National Center for Health Statistics (NCHS), Centers for Di ","We describe a method for sampling and processing cases that enables researchers to efficiently and collaboratively build sampling frames in a multi-site study. This method builds the sampling frames by using data from multiple data sources, thus introducing additional variables for stratification and/or clustering. It applies the No Personal Identification Disclosed (NOPID) approach (Jones, et al, 2011) to ensure respondent confidentiality, while attempting to maximize response rates and reduce measurement error. This method was used for The Patient Reported Outcomes Symptoms and Side-Effects Study (PROSSES) funded by the American Cancer Society-a study of cancer patient symptom experiences. To maximize accurate recall of symptom experiences during curative treatment, monthly cross-sections of new cancer patients were sampled and mailed recruitment materials on a monthly basis. PROSSES was conducted with 17 cancer centers dispersed across the United States, and achieved an overall response rate of approximately 60%. We will describe the key steps to successfully fielding PROSSES based on the NOPID method as well as the challenges faced, their solutions, and the lessons learned. ","The National Crime Victimization Survey (NCVS) is a nationally-representative survey of the non-institutionalized U.S. population aged 12 and older and utilizes a 7-wave rotating panel design. Prior to 2006, first wave interviews were not used in crime victimization estimation - these interviews were used only to provide a temporal landmark for survey participants to control telescoping, ensuring that the second- through seventh-wave interviews had bounded reference periods. Beginning in 2006, the Bureau of Justice Statistics (BJS) began using first-wave interview data to produce estimates as a way to cut costs without sacrificing estimate precision. This change is significant in many ways, not the least of which is its potential for the introduction of multiple sources of bias. This paper addresses how the authors approached analysis of mode effect bias - one of the potential sources of error in the NCVS resulting from this change. Since first-wave interviews are primarily conducted in-person and second- through seventh-wave interviews are primarily conducted over the phone, the potential for mode effect bias is high. Quantifying this potential source of error is important for understanding how incorporation of first-wave data affects victimization estimates in the NCVS. After accounting for respondents' level of exposure to the NCVS, the difference in victimization rates between in-person and telephone groups is shown to be non-significant, suggesting the apparent mode effect is actually a symptom of respondent fatigue. ","We exploit the panel dimension of the Canadian Financial Monitor (CFM) data to  assess the causal impact of retail payment innovations on cash usage. The CFM  collects data on methods of payment and cash usage since January 2010. The  availability of multiple years of observations should improve our understanding of whether retail payment innovations such as contactless cards and stored value cards reduce cash usage. We estimate a semiparametric panel data model that accounts for general forms of attrition. We find that retail innovations decrease cash usage on average by about three percent. ","Imputing compositional data is challenging because imputations must obey the restrictions in the data while remaining strictly non-negative. This problem becomes increasingly complicated when the data has a nested compositional structure. We propose predictive ratio matching (PRM) as a general imputation method for compositional data. PRM imputes (nested) compositional data by iteratively updating the pairwise ratios in the data. The proposed method yields plausible inference and imputations, while keeping the intricate compositional structure, data distributions and relations intact. ","Disclosure control of tables where the study variable, and thus the cell values, may take negative values poses a particular problem. The common sensitivity rules like the dominance rule and the p% rule do not apply. To solve this, it has been suggested to transform cell values (e.g. add a constant or take absolute values) in order to make all values positive and facilitate the use of the common sensitivity rules. With this approach, it is assumed that the risk scenario for variables that may take negative values is similar to variables that only take positive values. We use an empirical study to illustrate how the common sensitivity rules perform in different situations and we initiate a discussion of the sensitivity of data that may take negative values and discuss the need for a different approach to determining the sensitive values. Ideas for modified measures of risk are presented.  ","Models for dealing with missing outcomes are necessarily based on restrictive assumptions when the missing data are nonignorable. In order to avoid the often unrealistic normality assumption for the hypothetically complete outcomes, and to avoid choosing arbitrary sensitivity parameters, we adopt a pragmatic Bayesian methodology for estimating regression parameters using multiple imputation. This method is based on fully conditional specification that imputes the missing outcomes and remodels the missingness mechanism in an alternate fashion. The proposed method requires correct specification of the form of the missingness mechanism, up to an unknown parameter that is estimated from the data. The simulation shows that the method is insensitive to failure of the normality assumption, and clearly improves upon the selection model and multiple imputation under missing at random for the cases investigated. ","Group Quarters (GQ) enumeration in the U.S. Census involves collecting demographic data from such places as correctional facilities, skilled nursing facilities, college residence halls and military barracks. They include harder-to-access populations who live or stay in a group setting but are not usually related to each other. We analyzed the results of the 2010 Census to investigate the extent to which in-facility administrative records were possibly used for enumeration by major types of GQ. We discuss findings relative to merits and limitations of using administrative records for GQ enumeration in the Census.  ","The American Community Survey (ACS) is an ongoing survey by the U.S. Census Bureau that provides yearly demographic data.  Recently the Census Bureau released an online API to give public access to several parts of the Decennial Census and data from the 1, 3, and 5 year aggregates if the American Community Survey (ACS).  We will discuss how to use the API to access these datasets, give examples and also discuss some of the shortcomings of the current implementation ","From its founding in 1839, until very recently most Presidents of the American Statistical Association have been men. In this paper we divide the History of ASA into 11 periods. The first and longest ended in 1914, just 100 years ago. All these early founding Presidents were men. Indeed, all the ASA Presidents were men until 1945. In fact in each of the 10 year periods 2045-2054, 2055-2064, 2065-2074, 2075-2084, 2085-2094, and 2095-2004 most were still men. Only in the period, beginning with 2005 to the present, have the two genders been on a rough parity. In our JSM talk attention will be confined to just three periods: We will start with John Koren, ASA President in 1914, at the start of the \"Great War\", continue with Frederick Mosteller ASA President in 1968, whose JSM presidential talk was the first for one of us. Hard to end but we chose Bob Hogg (1988) and George Box (1978) together because of their humor. The period after 2004 is omitted as perhaps too familiar and not yet historical enough. ","From its founding in 1839 there have, until very recently, been few women Presidents of the American Statistical Association (ASA). In this paper we divide the History of ASA into 11 periods. The first and longest ended in 1914, just 100 years ago. In that period there were no women. In the following 10 year periods: 1915-1924. 2015-2034, 2035-2044, 2045-2054, 2055-2064, 2065-2074, 2075-2084, 2085-2094, and 2095-2004 altogether there have only been seven women ASA Presidents. In the recent period, beginning with 2005 to the present, there have been equal numbers of women and men, five of each. In our JSM talk attention will be confined to just three periods and four women: Helen Walker our first female ASA president will be where we start. Then we will talk about Margaret Martin, who just died at 100 and was president in 1980, ending up with Janet Norwood (1989) and Barbara Bailar (1987), both very much alive, who will be treated together. Women ASA Presidents after 2004 are omitted as perhaps too well known for this audience and not yet historical. ","In this paper, we revisit the problem of estimating the proportion of people in a population who possess one or both of a pair of sensitive characteristics as considered by Lee, Sedory and Singh (2013: Statistics and Probability Letters). The method in that paper allowed  the seeming magic of obtaining estimates of three parameters while taking a pair of responses per respondent. The method proposed here results in the seemingly black magic of allowing the estimation of the same parameters from a single response per respondent of the sample. Bias and variance expressions of the proposed estimator are derived and compared to the Lee, Sedory and Singh (2013) estimators using simulation studies. ","Project TALENT is a large, nationally representative longitudinal study conducted by the American Institutes for Research from 1960 to 1974. Among the participated students, nearly 40,000 students were from immigrant families. (That is about 10% of the participating students.) Many of their parents immigrated to the United States during or after world war II. One of our interests is to analyze differences between students from immigrant families and non-immigrant families in family background and student performance in the 1960 base year survey. Another interest is to investigate how immigrant status affected students' future education, career path and personal life following graduation from high school. ","In this paper, a fictitious story, \"How Amy Predicts Her President\",  is introduced to motivate the research considered. In the course of the story we propose a new class of estimators in dual frame survey sampling that makes use of a power transformation. The estimator proposed by Hartley (1962, 1974) is shown to be a special case of the proposed class of estimators.  The mean squared error of the proposed estimator is derived and compared to that of the Hartley estimator. A suggestion is given for improving the Fuller and Burmeister (1972) estimator along similar lines. Lastly, the work is extend to the case of multi-covariates. Note that we make no use of any known parameter of auxiliary information as in the ratio estimator due to Cochran (1940). In this regard the proposed class of estimators is different from the existing estimators in the literature of dual frame survey sampling. We show theoretically that the proposed class of estimators is always more efficient than the pioneer Hartley (1962, 1974) estimator.  The results are also justified through extensive simulated numerical situations ","The National Crime Victimization Survey (NCVS) currently utilizes a 7-wave or time-in-sample (TIS) design. The Bureau of Justice Statistics commissioned a Panel Design Study to evaluate the effects of changing the NCVS from a 7- TIS design to a 5-TIS, 3-TIS, or 1-TIS design.  The study utilized a set of simulations to mimic different panel designs. The simulation assumptions were constructed using NCVS data from 1999 to 2011, and included assumptions about sample sizes, costs, response rates, household replacement, type of interview, demographics, and victimization propensities.  After determining which variables are associated with victimization reporting, we simulated samples with different panel designs and computed summary victimization propensities and standard errors by several characteristics of interest. Simulations considered both keeping cost constant and the number of interviews constant across the different panel design options.  In this poster we show the impact of changing the number of panel waves on property and violent victimization rates, in terms of point estimates, variability, sample sizes, and costs, by several population characteristics. ","Many variables in surveys contain natural orderings that should be respected in the estimates.  For instance, the National Compensation Survey estimates mean wages for many job categories, and these mean wages are expected to be non-decreasing according to job level. In this type of situation, isotonic regression can be applied to give a constrained estimation satisfying the monotonicity.  We combine domain estimation and the pooled adjacent algorithm to construct new design-weighted constrained estimators. Under some conditions on the sampling design and the population, the estimators are shown to be design consistent and asymptotically normal. The delete-d Jackknife method is suggested for variance estimation given that the constrained estimator is not necessarily smooth; simulation studies show that as long as d is chosen sufficiently large, the jackknife appears to estimate the variance well.  ","The International Tobacco Control project (http://www.itcproject.org/) conducts longitudinal surveys to monitor Tobacco use in some 20 countries worldwide. In Africa, face-to-face surveys are conducted in Zambia, Kenya and Mauritius.  In a fourth country, Nigeria, the survey did not proceed past the planning stages due to an unexpected event. I will discuss practical challenges encountered with sampling in these four African countries. ","The Current Population Survey (CPS) is a monthly household sample survey consisting of eight rotation groups so that each selected household will be interviewed for 4 consecutive months and another 4 consecutive months after resting 8 consecutive months. A composite type estimator is adopted in the CPS for the estimation of the monthly population total, which combines sample information from the current month survey and previous months using the fact that 75% households have data for two consecutive months. There are two tuning parameters, A and K, in the composite estimator to decide how to combine the available information, and thus this estimator is called the AK composite estimator. However, the current choices of the tuning parameter values were determined by some empirical studies without theory support. In this paper, we derive a formula of the mean squared error of the AK composite estimator, and show that this formula is a quadratic form of A for each fixed K. Using this result, we propose an easy-to-use method of choosing the tuning parameters A and K. Our method is data-driven, i.e., we propose a method to estimate some population quantities in the mean squared error formula using observed data. ","The Brazilian national household sample survey is carried out annually since 1967, except in Census years. It is a key source of socioeconomic information about the Brazilian population. This paper presents some evidence that the survey suffers from differential nonresponse by sex and age. It then discusses alternatives to compensate for the nonresponse, and presents some results of a study carried out to investigate alternative calibration weighting methods. Calibration weights are obtained using a raking ratio type distance function with population projections classified by (sex x age) and by (region). The calibrated weights lead to improved estimates of some selected key indicators. ","We propose test procedures for assessing the goodness of fit of a  postulated multinomial distribution incorporating effects of survey  design. We examine the problem from the perspective of Fourier  analysis, function estimation, and order selection. Simulations are  used to evaluate the practical utility of the proposed methods. ","Survey methodology for sensitive questions is of great interest to psychologists, social scientists, and statisticians. In this poster session, we consider a questionnaire that asks three binary sensitive questions of each individual participating in the survey. We regard the responses as dependent Bernoulli random variables and estimate the probabilities of \"yes\" responses to the three questions using Bayesian methods. Our Bayesian models complement the likelihood methods introduced by Klotz (1973) and Bonney (1987).  We formulate a Bayesian logistic regression for these dependent dichotomous responses and consider Bayesian sample size determination.   ","Population health outcomes have demonstrated substantial variations at a variety of geographic levels: from local community to state. Most health surveys have been designed to monitor national or state population health. In order to meet the data needs at local levels in public health practice, small area models have been applied to national health surveys to generate local small area estimates of population health outcomes. But very few model-based small area estimates have been evaluated via external validation. Thus, we applied multilevel logistic models with 2011 BRFSS and poststratification with 2010 census data to generate county-level COPD and current smoking, diabetes and obesity prevalence estimates for 115 Missouri counties. We compared them with those direct survey estimates of the 2011 Missouri County-level Study, which has a sample size of 400 or more for each county. Our model-based estimates are within the 95% CIs of the direct survey estimates in at least 95 counties. The external validation confirmed that the multilevel regression and poststratification approach could generate both reliable and accurate county-level estimates for the majority of Missouri counties. ","The Tobacco Use Supplement to the Current Population Survey (TUS-CPS),conducted by the Census Bureau and sponsored by the National Cancer Institute(NCI), is a key source of national and state-level data on smoking and other tobacco use in the U.S. household population. However, policy makers and cancer researchers often need county-level data to evaluate tobacco control programs, and the TUS-CPS does not have a large enough sample at the county level to support estimates with adequate precision. In such case, estimates derived through small area estimation (SAE) techniques may be preferable. Through collaboration between the Census Bureau and NCI, we propose model-based county-level estimates for several different smoking-related variables for all U.S. counties using a Bayesian framework through a Markov Chain Monte Carlo (MCMC) simulation. We applied extensive model selection and diagnosis techniques to choose the best set of auxiliary variables from a pool and the best fit models from a few candidate models. Our small area models generate a new set of estimates with improved precision over the survey-based estimates. This paper describes the methodology used and also demonstrates the accuracy of the model through data exhibits. ","This project is focused on generating partial synthetic datasets for households, with the application for decennial census household synthesis. Extensions of nested Dirichlet Process model are developed to allow two-level clustering of households and individuals in households. Both household-level variables and individual-level variables can be modeled, and the model provides good data utility in terms of recovering the marginal, bivariate distributions of the variables in the original dataset, as well as within household structures. A data augmentation method to account for the relational structural zeros in a household dataset is developed. Risk measure computation methods based on computing the posterior probability of one record being identified given the synthetic datasets and other information available to the intruder, are developed.           ","The Current Population Survey (CPS) is a complex, multistage household probability sample that produces monthly labor force estimates in the U.S.  Adults in a household are interviewed for four months in a row, left out for eight months, and then included for four more months.  New households are added each month.  This 4-8-4 rotation design produces overlap in the sample from month to month, quarter to quarter, and year to year. Several weighting steps are used to adjust the ultimate sample to be representative of the population.  A composite estimator, called the AK estimator, is used to produce estimates of labor force levels and change. Seasonal adjustment is used account for regular fluctuations in the labor force variables.  This presentation reports on studies of alternatives to the 4-8-4 rotation design and enhancements to the AK estimator.  Rotation designs that are described as 3-9-3, 2-10-2, or 6-0 are compared to the 4-8-4 design.  Potential enhancements to the AK estimator allow for specific parameters for areas of aggregation below the national level and shrinkage of estimates through hierarchical modeling. ","We consider the problem of constructing confidence intervals for a finite population proportion that is estimated using the unrelated question randomized response technique.  If we apply one of the standard methods for creating confidence intervals, we get intervals that may be empty or misleadingly short.  If we obtain intervals using the likelihood ratio method, our intervals are never empty or misleadingly short, but tend to be longer than necessary.  We develop a hybrid method and demonstrate that it performs well in terms of length, in terms of the likelihood ratio, and even by Bayesian criteria.  We apply the method to a dataset from the randomized response literature.  ","With advances in machine learning based analysis of textual data (Roberts et al., 2013) and in web survey features to facilitate collection of higher quality open-ended data (Smyth et al., 2009), it is increasingly easier to combine open-ended responses with closed-ended ones to enrich the quality of quantitative analyses. However, understanding the reasons for nonresponse to open-ended items in web surveys and their relationship with attitudinal measures has received little attention (see Reja et al., 2003) and is a first step in effectively utilizing these data. Second, properties of open-ended responses including sophistication, attitude strength and uniqueness can be coded and correlated with closed-ended knowledge or attitude strength items to identify limitations in these measures. With data from an Indiana University School of Public Environmental Affairs web survey of 2,087 US adults from GfK's probability-based KnowledgePanel, we model nonresponse in open-ended items on attitudes toward road financing alternatives as a function of demographic and attitudinal covariates and correlate properties of the open-ended responses with closed-ended measures to assess validity. ","We propose a model for analyzing recurrent event data obtained from a complex survey design. The survey design consists of a finite population that is stratified; where each stratum contains primary and secondary sampling units. The models assumes the form of a stratified Cox model where the baseline intensities differ across the strata.  A partial maximum likelihood estimator of the regression parameter and Nelson-Aalen type estimators of the baseline cumulative intensities are given. Asymptotic properties of the estimators are established and the statistical analysis of a real data set is used to illustrate the model. ","Over the past decade, the need to monitor and measure effectiveness of policies at different geographical levels has grown extensively. Often local authorities require information at very small regional levels, while most sample surveys at Statistics Netherlands are designed to provide statistics at the national and larger regional levels, using traditional design-based estimation procedures. Sample sizes are generally too small to provide sufficiently precise estimates at low regional levels. In such situations, model-based estimation techniques can be used. Statistics Netherlands' Social Statistics Division started the implementation of a small area estimation program to accommodate the demand for detailed regional statistics on labor force, crime and public safety. The availability of strong correlating register variables that are known for the entire population is crucial to improve the efficiency of small area estimates. This paper summarizes the experiences and results obtained with setting up this program and assessing the validity and plausibility of the results. The Dutch national crime victimization survey (NCVS) is used as an illustration. ","A major obstacle that hinders medical and social research is the lack of reliable data due to people's reluctance to reveal confidential information to strangers. Fortunately, statistical inference always targets a well-defined population rather than a particular individual subject and, in many current applications, data can be collected using a web-based system or other mobile devices. These two characteristics enable us to develop new data collection methods with strong privacy protection. These new technologies hold the promise of removing trust obstacle, promoting objective data collection, allowing rapid data dissemination, and helping unrestricted sharing of big data.    ","With the growing need for health data at an increasingly granular and customized geographical level, new challenges are posed to the established small area estimation approach. Data in fine geographic areas are sparse, and it is cost-prohibitive to increase the sample for the areas of interest in the survey design. In addition, spatial dependence is possible and may not be appropriately modeled with linear relationships. In this study, we propose an approach to address these specific issues by combining a generalized linear mixed model and a non-parametric smooth trend on socio-demographic contextual variables and geo-coordinates. Conceptually, we model the socio-demographic-adjacency as well as the geo-adjacency of the health indicators. The model could be fitted using existing procedures for generalized linear mixed model. We provide a method for computing small-area prediction mean squared error through replication methods which takes survey design into account. An example of the application is demonstrated using data from California Health Interview Survey, the largest state health survey in the U.S.   ","This is the second of two presentations that will provide a brief chronological record of the American Statistical Association (ASA) from its modest beginnings in Boston in 1839 to its present status as a worldwide professional organization with approximately 19,000 members and a headquarters in Alexandria, Virginia. Both presentations will emphasize the past 30 years of the Association. Together they will impart information about the ASA, including its leadership; its chapters, sections, and committees; and the services provided by the Association, such as meetings, publications, membership services, education, accreditation, and advocacy. ","The offer of incentives plays a large role in the success of recruitment of research subjects.  We conducted an experimental study, using an Internet panel, to assess the impact of incentive amounts on respondents' stated willingness to participate in a hypothetical qualitative research effort. Subjects were randomized to one of five incentive conditions, ranging from no mention of an incentive to a $75 incentive. We measured respondents' self-reported willingness and interest to participate in the qualitative research effort.  Our primary hypotheses were: H1: Larger monetary amounts will result in higher willingness to participate and H2: All incentives will result in higher willingness to participate than no incentive at all.  We used a two-part model to compare different incentive amounts, including no incentive and non-monetary incentives, controlling for demographic and other factors.  Willingness to participate increased with increasing incentive amount. There was no difference in willingness to participate among those offered $0 versus a non-monetary incentive. We will present the findings to provide guidance on optimal incentive levels for qualitative research participants. ","The Study of Attitudes and Factors Affecting Infant Care Practices (SAFE Study) collected data on sleep position, bed sharing, pacifier use, and other infant care practices from a national probability sample of about 3,000 mothers of 2 to 10 month old infants. SAFE employed a two-stage sample with 32 hospitals sampled using a composite size measure to oversample minorities. Hospitals unable to participate were replaced with a randomly selected hospital from that stratum. In each of 3 cycles, hospitals were assigned targets for sampling and enrollment of Hispanic, Black, and all other race mothers. Enrolled mothers completed a short baseline interview, and then were asked to complete a follow-up survey by web (66%) or telephone (34%) to achieve each cycle's goal of at least 1,000 completed surveys, 250 each from Hispanic and Black mothers. In this paper, we present solutions to the operational challenges this design posed for frame building and stratification; sample allocation and minority oversampling; hospital recruitment and maintenance over the three year study; mother selection and enrollment; and maximizing response rates for follow-up surveys. ","This is the first of two presentations that will provide a brief chronological record of the American Statistical Association (ASA) from its modest beginnings in Boston in 1839 to its present status as a worldwide professional organization with approximately 19,000 members and a headquarters in Alexandria, Virginia. Both presentations will emphasize the past 30 years of the Association. Together they will impart information about the ASA, including its leadership; its chapters, sections, and committees; and the services provided by the Association, such as meetings, publications, membership services, education, accreditation, and advocacy.. ","Survey sampling is fundamentally an applied field.  During this roundtable, we will discuss techniques long used by experienced survey statisticians with little or no references in the literature.  Although we will use the textbook \"Practical Tools for Designing and Weighting Survey Samples\" (Valliant, Dever, &amp; Kreuter 2013) as the basis for discussion, participants are encouraged to share their experiences. This roundtable will benefit (1) students seeking a more in-depth understanding of applied sampling; (2) survey statisticians searching for practical guidance on sampling and weighting; and (3) other survey practitioners who desire insight into the design and implementation of survey samples. ","The annual sample of Medical Expenditure Panel Survey (MEPS) is selected from the responding households to the prior year's National Health Interview Survey (NHIS). The selection probability in MEPS varies considerably due to variations in both NHIS selection probability and response propensity. The resulting high variability in the MEPS base weight contributes to the variance of MEPS estimates. Since 2010, a probability proportional to size (PPS) subsampling from NHIS to MEPS has been introduced where the size measure is used as the NHIS household weight. The purpose of the PPS subsampling is to reduce the variation in MEPS base weight and thereby increase the precision of the MEPS estimates. This talk will present the results of an evaluation of this PPS subsampling scheme on the precision of MEPS estimates.  ","The Medical Expenditure Panel Survey Household Component (MEPS) is an annual two year panel survey of Households sponsored by the Agency for Healthcare Research and Quality and conducted by Westat.  The survey collects data on household characteristics, insurance coverage, healthcare use and expenditures.  The MEPS survey is a subsample of responding households to the National Health Interview Survey (NHIS).  Current non-response and post-stratification steps for MEPS are carried out in traditional sequential steps.  This project investigates the possibility of using calibration through SUDAAN'S proc WTADJX to perform non-response adjustment and post-stratification on the MEPS weights at the household level.  The results from this calibration approach will be compared to the current approach empirically and by simulation.  The simulation will measure bias in the methods by using values on the non-respondents to MEPS created from a combination of NHIS information and modelling.  Since the bias can be measured in this way, then the total Mean Squared Error for estimates under the two approaches can be compared. ","When designing a sample, estimates of expected precision are commonly made to help determine sample size.  These calculations require specifying one or more of the following: type I error, power, population variance, design effects, finite population corrections, R2 for covariates, and the effect size.  Sometimes, an earlier study can provide some of these, but often one must rely on educated guesses. Even when other studies are available, it is not straightforward to derive the various components of variance. In this paper, we decompose the variance and design effects for several key child outcomes from two rounds of the Head Start Family and Child Experiences Survey (FACES), in the hope that they can be used to help design samples for similar multistage samples of preschool-age children.  This clustered sample involves selecting Head Start programs, then centers, classrooms, and children. Working backwards from the observed total variance for these outcomes, we first factor out the design effect due to unequal weighting, and then decompose the design effect due to clustering. ","In order to maximize participation in the Survey of Consumer Finances (SCF), the base monetary incentive offer is increased during the field period for many reluctant families. About 25 percent of responding SCF families accepted a monetary incentive that was larger than the $50 base incentive. Though the benefits and costs of offering a base monetary incentive are well-studied, the impact of then offering an increased incentive is less well-known. This paper focuses on how these increased incentive offers influenced participation in the 2013 SCF after factoring in, among other things, how the case had been worked up to that point.    The results shown here indicate that the increased incentive offer increased participation for families that are least likely to participate in the survey (as rated by field staff), and amongst families in high-income areas. A high level of data quality is maintained even with increased incentives. Finally, the data shows a burst in participation in the week after the increased incentive offer is made which quickly fades in subsequent weeks.   ","The National Highway Traffic Safety Administration (NHTSA) launched a new project to overhaul the agency's National Automotive Sampling System (NASS). This presentation is an overview of this data modernization project. We shall explain the congressional driver of this project, the existing data collecting systems that are impacted by this project, the major components of this project, and the sample design challenges we faced during this redesign.  ","In order to improve accuracy in the Survey of Consumer Finances (SCF), a feature was  included in the survey software to allow the interviewer to make comments-both  question-specific and general comments-about their interaction with the respondent.  These comments are then used during the editing process to fix the data so that it properly  reflects the respondent's situation. This paper focuses on how these comments drive edits,  and improve data quality.  Examining the data, particularly the heavily edited pension and income sections, reveals  that a high percentage of variables require editing, and a high percentage of these edits are  in fact motivated at least partially by these comments. Respondent confusion due to  complicated survey questions and complicated respondent situations can both be  ameliorated by the use of these interviewer comments, making them a valuable part of the  survey process   ","Confidence intervals based on ordinary least squares may have poor coverages for regression parameters  when the effect of sampling design is ignored. Standard confidence intervals based on design  variances may not have the right coverages when the sampling distribution is skewed. Berger and  De La Riva Torres (2012) proposed an empirical likelihood approach which can be used for point  estimation and to construct confidence intervals under complex sampling designs for a single parameter.  We show that this approach can be extended to test the significance of a subset of model  parameters and to derive confidence intervals. The proposed approach is not a straightforward extension  of Berger and De La Riva Torres (2012) approach, because we consider the situation when  the parameter is multidimensional and the parameter of interest is a subset of the parameter. This  requires profiling which is not covered by Berger and De La Riva Torres (2012). The proposed  approach intrinsically incorporates sampling weights, design variables, and auxiliary information.  It may yield to more accurate confidence intervals when the sampling distribution of the regression  parameters is not normal, the point estimator is biased, or the regression model is not linear. The  proposed approach is simple to implement and less computer intensive than bootstrap. The proposed  approach does not rely on re-sampling, linearisation, variance estimation, or design-effect. ","The Hartley-Rao-Cochran ( RHC ) sampling design (Rao et al., 1962) is a popular unequal probability sampling design. We show how empirical likelihood confidence intervals can be derived under this sampling design. Berger and De La Riva Torres (2012) proposed an empirical likelihood approach which can be used for point estimation and to construct confidence intervals under complex sampling designs. We show how this approach can be adjusted for the RHC sampling design. The proposed approach intrinsically incorporates sampling weights and auxiliary information. It may give better coverages than standard methods even when the sampling distribution of the parameters of interest is not normal. The proposed approach is simple to implement and less computer intensive than bootstrap. The proposed approach does not rely on re-sampling, linearisation, variance estimation, or design-effects.   ","Poststratification is a standard approach to account for differences between survey samples and its target population by incorporating population distribution of variables into survey estimates. However, classical poststratification estimators can have unacceptably high variance because of sparse or empty poststratification cells. Instead, we proposed a multilevel penalized-spline poststratification model (MPPM), in which in the first level of model a distinct mean and variance are assumed in each poststratum, and in the second level of model the cell mean is further assumed to follow some distribution with mean as a spline function of inclusion probability. This multilevel modeling not only facilitates the estimation of cell mean in sparse or empty cells but also gains efficiency in the survey estimates by modeling the association between survey outcomes and the inclusion probabilities for cases in different poststratification cells. We compared the MPPM to the classical method using simulation studies and showed its application in a survey studying the mental health problems of Reserve and National Guard Service members returning from the conflicts in Iraq and Afghanistan.  ","This research generates coverage estimates by socioeconomic characteristics from the American Community Survey (ACS).  By using ACS results, we identify the areas that are poorer, less educated, more mobile, and less employed.  The 2010 Census Coverage Measurement (CCM) program evaluated coverage of the 2010 Census and produced components of census coverage results that included estimates of correct enumerations, erroneous enumerations, and omissions of the national household population.  We repeat the 2010 CCM estimation methodology to produce component estimates for these challenging areas.  We could then use the results of this work to help focus research with the goal of improving coverage in these areas.  Specifically, this paper integrates the five-year ACS estimates at the block group and tract level. ","With many surveys now embracing adaptive survey design methods, new challenges have arisen in applying optimization methods to meet the requirement to produce automated real-time decisions to adapt survey design strategies across sample cases simultaneously.  Chesnut (2013) describes a mode-switching process for switching sample cases from Internet to mail using integer programming with the objective to maximize timeliness while controlling cost, response, and sample representativity.  Building on this work, this paper presents the application of an integer programming solution using the opt model procedure in SAS\u00ae to automate the mode-switch decision process. We discuss the use of indicator variables to enable linear representations of our objective function for maximizing timeliness and the constraints for cost, response, and sample representativity in the integer programming environment. While this allows for a complete representation of the optimization problem at hand, the numerous constraints pose a challenge for computing a solution. We discuss alternative linear representations of our constraints using linear approximation methods to enable computed solutions.  ","The survey methodology focus is changing from retrospective analyses towards pre-emptive corrective procedures. Of course, these procedures are not limited to data collection:  examples include imputation, adjustment cell weighting, and calibration. In this paper, we consider imputation procedures, which can be used to account for either unit nonresponse or item nonresponse. We propose using a proxy pattern-mixture (PPM) model analysis to evaluate the predictive power of different sets of covariates used in regression imputation models or to determine imputation cells for one or more outcome variables, obtaining the fraction of missing information (FMI) obtained via maximum likelihood estimation from separate PPM models fit to the same data sets. Our variable selection approach is most like that of S\u00e4rndal and Lundstrom (2010), in that we estimate the FMI and look for the point at which changes in the FMI level off and further auxiliary variables do not improve the imputation model. We illustrate our proposed approach using empirical data from the Service Annual Survey and from the Ohio Medicaid Assessment Survey. ","Data collection efforts often focus on maximizing survey response. However, increasing the response rate does not necessarily improve the quality of the estimates. For example, the respondents and nonrespondents might differ systematically on key survey characteristic(s). In this case, without successful data collection strategies to obtain data from underrepresented subpopulations, additional collection efforts may not improve the quality of the estimates. Using empirical data from two surveys, we examine two indicators, each measuring a separate property of the respondent sample: the R-indicator, which measures deviation from missing-completely-at-random; and the balance indicator, which measures the deviation of the respondent-based mean from the full sample mean for selected items. Examined in conjunction with the weighted volume response rate, which estimates the population coverage, these two indicators signal when the response set has stabilized but is not a random subset of the full sample. Thus, the current data collection strategy is at phase capacity, and new collection strategies -- targeted to specific subpopulations, are needed to achieve a balanced response. ","In this paper, we will examine domain estimation with the use of auxiliary information, when combining survey data with administrative records. Two competing approaches are considered: calibration weighting and probability-weighted linear prediction. When there is a domain indicator among the calibration targets, these two approaches will produce the same results. But what if there isn't?  Comparisons will be made between the validity (bias) and reliability (variance) of these two methods through a simulation study based on the 2010 US birth data file. A bias test will be proposed to determine whether or not the bias of a domain estimate derived from the weighted prediction method is significantly different from zero. If it is not, the variance of this domain estimate can be measured and compared with the variance of the corresponding domain estimate derived using calibration weighting. These rival methods are also frequently used when there is a two-phase sample and the calibration targets for the final sample are computed from the first-phase sample.  We will discuss the additional complications in variance estimation caused by the existence of two sampling phases. ","Measurement error occurs when there is a discrepancy between an observed measure and its true value. Depending on the nature of the data collection, this discrepancy may be due to the imprecision of the instrument used. However, survey interviewers can also have an influence on measurement error since, in their presence, interviewees can be prone to give socially desirable answers that may differ from the truth. In this research we investigate interviewer effects on measurement error by applying multinomial multilevel models. We use data from the 2010 Norwegian sample of the European Social Survey (ESS) linked to administrative data. Our analysis is based on the joint distributions of variables of interest present in both sources of data. Preliminary findings suggest that the measurement error variable varies by interviewer. ","For practical convenience reasons, survey datasets are often disseminated to both internal and  external secondary users with imputed values in place of the missing observations that occur during  the data collection and processing stages. However, the released dataset generally leads to incorrect  analyses if standard complete-data methods are applied directly without taking into account the  imputation models. To alleviate this problem the survey statistician may, firstly, obtain some key  results in any manner that is considered appropriate, and, secondly, calibrate the disseminated data  so that these results can be reproduced by relevant standard complete-data procedures. In this paper,  we discuss an approach that allows us to control both the first- and second-order properties of  the imputed data and can be applied to complex surveys. We illustrate the implementation of the  proposed approach with a numerical example. ","It is common in surveys to probe don't know (DK) responses in order to reduce item nonresponse. There is a risk, however, that too much probing will lead to measurement error. This negative impact needs to be balanced against the potential benefit that probing will elicit meaningful responses from initial DK respondents and thus improve overall data quality. This paper will consider how modeling may be used to disentangle the different possible impacts of probing, positive or negative, in a multi-item survey setting where missing data occurs on all items. In such a setting, we suppose that item nonresponse may be affected by a latent response propensity as well as a latent variable of interest.  We use data from an experiment on the use of probing in the innovation sample of the European Social Survey in three European countries in 2014. Multi-item scales include attitudes to recipients of welfare benefits, where DKs may arise from the sensitivity of the questions, and subjective judgements about national levels of welfare need, where DKs may arise from the complexity of the task.      ","One major criticism against the use of synthetic data has been that the efforts necessary to generate useful synthetic data are so intense that many statistical agencies cannot afford them. We argue many lessons in this evolving field have been learned in the early years of synthetic data generation, and can be used in the development of new synthetic data products, considerably reducing the required investments. The final goal of the project described in this talk will be to evaluate whether synthetic data algorithms developed in the U.S. to generate a synthetic version of the Longitudinal Business Database  (LBD) can easily be transferred to generate a similar data product for other countries. We construct a German data product with information comparable to the LBD - the German Longitudinal Business Database (GLBD) - that is generated from different administrative sources at the Institute for Employment Research, Germany. The ultimate goal of the project is to provide access to multiple synthetic datasets similar to the SynLBD at Cornell to enable comparative studies between countries. The Synthetic GLBD is a first step towards that goal. ","Multiple Imputation (MI) (Rubin 1978, 1987) is a general-purpose approach to allow for statistical analysis of incomplete data. While survey data sets were among the first data types which MI was applied to, the first MI algorithms had difficulties in dealing with nominal-scale variables which occur frequently in survey data. The currently implemented methods typically use an underlying multinomial logit or probit model, but there is some indication that semi- or non-parametric methods might be more robust to model misspecification. Predictive Mean Matching (PMM) (Rubin 1986; Little 1988) is a nearest-neighbor approach that is used within MI algorithms for metric-scale data. We propose several PMM variants for unordered categorical variable types, and compare them to parametric MI algorithms based on the results of an MC study using simulated and empirical data.  ","Statistical matching is a technique of integrating two or more data sets when information available for matching records for individual participants across data sets is incomplete.   Statistical matching can be viewed as a missing data problem where a researcher wants to perform a joint analysis of variables that are never jointly observed. A conditional independence assumption is often used to create imputed data for statistical matching.     ","The selection of an appropriate sampling frame is dependent on multiple factors, including the target population, frame coverage, mode(s) of data collection, anticipated response rates, and impact on data collection costs. For household telephone surveys, two sampling frames are typically considered: dual-frame random digit dialing (RDD) and address-based sampling (ABS). Aligning Forces for Quality: Assessment of Consumer Engagement (AF4Q) is a survey of chronically ill consumers of healthcare residing in targeted geographic markets, ranging in size from single counties to entire states. Three previous waves of AF4Q were based on traditional (landline only) and dual-frame RDD designs. Because of the error in mapping wireless phone numbers to small geographic areas, the wireless phone sample led to both inefficiencies and undercoverage. To mitigate these concerns, an ABS design was implemented in three AF4Q markets for the second round of data collection. We discuss the challenges associated with each sampling method and compare key sampling and data collection measures for the RDD and ABS designs. ","For the 2020 Decennial Census, we are looking into reducing the cost of Nonresponse Follow-up by using administrative records as a substitute for field follow-up for housing units that do not respond themselves by, for example, mail or the internet. Enumeration using administrative records is referred to as ADREC Enumeration. In many cases, with the sources we currently have available to us, administrative records have no information on Hispanic Origin or the available information may not be accurate. With these existing sources, the imputation rate for Hispanic Origin for ADREC Enumeration would be very high. Missing data on Hispanic Origin (binary variable, Hispanic or Non-Hispanic) is ignorable if true origin status is independent of whether the origin status of a person could be resolved without imputation. It may be that those for whom data is not available are more likely to be Hispanic. This paper describes a potential non-ignorable nonresponse procedure and an ignorable nonresponse procedure using methodology adapted from Little and Rubin (1987). Both the ignorable and non-ignorable procedures are applied for two simulations using ADREC Enumeration in place of NRFU.  ","The Monthly Wholesale Trade Survey (MWTS) provides estimates of the dollar value of sales and inventories for wholesale businesses in the United States. In this longitudinal survey, missing values for sales and inventories in any month are imputed via a ratio adjustment applied to data from the prior month. In this article, we describe ongoing research to evaluate the performance of the current imputation method and to investigate possible alternatives. Using information from the MWTS and the sample frame, we generated an artificial population of wholesale businesses with two years of monthly sales and inventory data. We repeatedly drew samples from this artificial population, imposed patterns of nonresponse on the samples, and filled in the missing values by two methods: the current ratio-based procedure and a new model-based multiple-imputation procedure. Preliminary results from this simulation are challenging to interpret because, apart from missing data, the inferential procedures (complete-data point estimates, variance estimates and interval estimates) do not behave as large-sample normal theory suggests they should. Based on these results, we recommend further research on improving the quality of the complete-data inferences, using methodologies that are better suited for stratified sampling from populations that are highly skewed. ","The challenges of constructing confidence intervals for a binomial proportion and the deficiencies of  the popular Wald interval in achieving its nominal coverage - particularly when the true proportion  is close to 0 or 1 or when the sample size is moderate - have been well documented (Brown et  al. 2001, 2002). The problem is further compounded when inference is based on complex survey  data. Yet intervals resembling the Wald interval are often applied to complex surveys, an example  being the confidence intervals used in the American Community Survey (ACS). In the literature,  confidence intervals designed for binomial proportions with modifications based on the design effect  are often used for complex surveys. Here, we adopt this approach and study the coverage and  expected length properties of 7 different intervals. We focus on how phenomena such as clustering,  stratification, misspecification of variances, and patterns of variation of stratum expected sampling  fractions and stratum survey attribute-proportions, affect coverage. A simulation study examines  the effect of such factors. ","One of the important reasons advanced for calibrating household surveys to population totals is the imperfect coverage of target frame populations by operational frame lists. Calibration is an implicitly model-based operation, the theoretical justification for which depends on relatively simple assumptions about the correctness of the frame list. Essentially, one assumes that this list is nearly perfect except possibly for random omissions similar to those in the pseudo-randomization model for unit-level nonresponse. The author is not aware of previous studies extending the design consistency of calibration to more complicated models of frame deficiencies, although such models are formulated and studied at the Census Bureau for other purposes. In this research, we formulate a simple set of models allowing dynamic transitions of units on an ideal frame list into and out of the operational frame and describe the provable design-based properties of calibrated survey-weighted estimation methods under these models. The theoretical results will be illustrated via simulation and on CPS 2012 first-month-in-sample data.    ","Adaptive design strategies for data collection can increase the quality of response data under a reduced survey budget. In this framework, the U.S. Census Bureau is investigating nonresponse subsampling strategies, including a systematic sample of nonrespondents sorted by a measure of size, for usage in the 2017 Economic Census. Design constraints include a mandated lower bound on the Census unit response rate, along with targeted industry-specific response rates. This paper presents research on allocation procedures for subsampling nonrespondents, given a systematic subsample. We consider two approaches: (1) equal-probability sampling and (2) optimal allocation with constraints on unit response rates and sample size with the objective of selecting larger samples in industries that have initially lower response rates. Using the Annual Survey of Manufactures (ASM) sample as our original population, we present a simulation study that examines the cost, variance, relative bias, and unit response rates for the proposed allocations, assessing each procedure's sensitivity by varying the program-level sampling rate, the response mechanism, and the nonresponse adjusted estimator.   ","In this paper, we examine reporting patterns for the 2012 Economic Census to inform future data collections.  The economic census is a quinquennial survey of U.S. business establishments in the eight major business trade areas: manufacturing, construction, mining, retail, services, wholesale, finance-insurance-real estate, and utilities-transportation. Information collected includes employment labor costs and output, assets, expenditures, inventory, and other industry-specific items.  For the 2012 Economic Census, data were primarily collected using two-self-administered modes: mail-out/mail-back and electronic.  Electronic data collection was through one of two custom-built Census Bureau software products, depending on the size of the business.  Available data for this analysis include 2007 and 2012 Economic Census data, both of which include establishment responses to the survey and survey process data.  We examine establishment-reporting behavior by sector.  We also use multinomial modeling to understand business characteristics that predict which establishments are most likely to switch (or not switch) response mode.  Here, we present our initial findings, and hope that they might be used to influence discussions on improving data collection efficiencies, including tailoring survey contact strategies.  ","The use of administrative records - data collected by governmental or non-governmental agencies in the course of administering a program or service - for household enumeration may be one way to significantly reduce Census costs, particularly in nonresponse follow-up (NRFU).  Administrative records suffer the complications of big data in that they are collected for purposes not related to Census enumeration; yet they contain a wealth of information relevant to Census enumeration.  This work investigates different classification techniques for determining which administrative records are sufficiently reliable to use to achieve a Census enumeration that maintains data quality but reduces costs.  In addition to the cost/quality tradeoff associated with using administrative records, we seek a methodology for using administrative records that strikes a balance between predictive power and model complexity.  In this research, we compare the use of logistic regression and machine learning techniques for extracting and synthesizing the most important enumeration information from a set of governmental and non-governmental data sources. ","Advances in computing processing and the availability of open source statistical platforms have facilitated the development of new graphical tools to analyze nonresponse patters in survey sampling analysis. In this paper we introduce one of these tools called a b-plot that can be used to study, among other things, the relationship between the response propensity and the variable of interest. The use of the b-plot is demonstrated by revisiting a previous paper that studies the influence of multiple auxiliary variables and nonresponse.  In addition to expanding the current research on the relationship among these factors, we highlight the issues with those findings and how these additional tools could have been useful in identifying them. ","For large-scale national surveys, gains in cost reduction and increased timeliness must be weighed against the quality of the data produced. Recent developments in adaptive design and data monitoring indicators offer promising tools to assess the tradeoff between quality, cost, and timely data production. The Survey of Doctorate Recipients (SDR), a biennial longitudinal survey of U.S.-trained doctorates in science, engineering, and health, used to collect data for more than 40 weeks in order to achieve a high response rate. In its 2013 survey cycle, the SDR shortened its data collection period by 40 percent. The compressed data collection schedule was implemented by reducing the timing between follow-up contacts while maintaining all of the past contact strategies. We analyzed and compared the weekly progress of the 2010 SDR with the 2013 SDR with respect to representativeness of the response set, non-response bias, data quality at the item level, and level of bias and precision in estimating key survey outcomes.  The cost and efficiency of data collection were also tracked and compared over time to yield information on optimizing the design of data collection strategies. ","The 2013 Census Test modified existing Census Bureau systems to study an adaptive approach to case assignment for a sample of housing units in Philadelphia between October and December of 2013. A test goal was to uncover operational \"lessons learned\" related to modeling daily response propensities for sample cases and automating case assignments. We used frame information and contact history paradata to model daily response propensities for sample cases. Case management systems prioritized seven cases with the highest response propensities per CAPI interviewer, and interviewers were trained to work these \"high priority\" cases each day. This paper discusses lessons learned. Topics include: systems testing (e.g. how do we construct systems tests that test response propensity models and automated case assignment?); systems in operation (e.g. what interviewer behavior and systems anomalies need to be anticipated?); case dispositions (e.g. how should we treat problematic cases such as demolished units?); and case reassignments (e.g. how should incomplete reassigned cases be handled?). The paper concludes with a brief discussion of propensity model revision for a 2014 Census Test. ","Small area estimation is an important area in survey sampling because of the growing demand for better statistical inference for small areas in public or private surveys. Some traditional methods for small area problems borrow strength through linear models that provide links to related areas, which may not be appropriate for some survey data. We propose a stepwise Bayes approach which borrows strength through an objective posterior distribution. This approach results in a generalized constrained Dirichlet posterior estimator when auxiliary information is available for small areas. The objective posterior distribution is based only on the assumption of exchangeability across related areas and does not make any explicit model assumptions. The form of our posterior distribution allows us to assign a weight to each member of the sample. These weights can then be used in a straight forward fashion to make inferences about the small area means. Numerically, we demonstrate in simulations and applications that the proposed stepwise Bayes approach can have substantial strengths compared to traditional methods. ","Missing values are a common problem in surveys and imputation potentially reduces the nonresponse bias. However, commonly used statistical software packages don't account for hierarchical data structures (e.g. students within schools) in their imputation routines. The consequences on post-imputation analyses are seldom considered.    ","Work with sample surveys often makes extensive use of measures of size.  Two prominent examples are the use of \"probability proportional to size\" sampling; and use of size measures in adjustment of survey weights through, e.g., ratio estimation, post-stratification or calibration weighting.  However, many survey applications use size variables that are imperfect approximations to the idealized size measures that would produce optimal efficiency results. This paper explores the effects that errors in size measures may have on the efficiency of some standard design-estimator pairs.  Principal emphasis is placed on numerical results of a simulation study that uses size measures and economic variables available through the Quarterly Census of Employment and Wages of the Bureau of Labor Statistics. ","The American Time Use Survey (ATUS) is designed to measure how people spend their time. The ATUS sample is drawn from households completing their final month of interviews for the Current Population Survey (CPS). Because the CPS collects a wealth of demographic information about respondents, this design provides information about ATUS nonrespondents. Measurement error, due to either forgetting an activity or mistakes estimating the time or duration, may also contribute to bias in the ATUS survey.  This paper focuses on nonresponse bias and measurement error.  A propensity score model is used to examine differences in time-use patterns between those who are likely to respond and those who are reluctant, and to assess the extent of nonresponse bias.  The two processes (forgetting and duration recall) will be explored by looking at the large number of zeros in many time-use categories, with the zeros serving as indicators of possible forgetting, and unusual durations conditional on remembering indicating possible recall error.  ","There is considerable interest in producing public-use data that allow analyses that reproduce a few analyses from original, confidential microdata. Several methods/tools facilitate the production of synthetic (or partially synthetic) data with valid analytic properties and allow provision of public-use data with reduced re-identifidation risk. ","In surveys on income, respondents tend to round their answers with unknown degree, resulting in \"heaps\" in the distribution of the observed values. In this talk we illustrate the substantial impact that rounding can have on important measures derived from the income variable such as the poverty rate. To obtain unbiased estimates, we propose a two stage imputation strategy that estimates the posterior probability for rounding given the observed income values at the first stage and re-imputes the observed income values multiple times given the rounding probabilities at the second stage. A simulation study shows that the proposed imputation model can help overcome the possible negative effects of rounding. A slight overestimation of the sampling variance of non-parametric poverty rate estimators is outweighed by a significant bias reduction. We also present empirical results based on the household income variable from the German household panel study \"Labor Market and Social Security\". ","We consider the problem of analyzing interval censored data comparing cumulative incidence functions in the presence of competing risks. In this paper, we propose a method based on multiple imputation, which will impute the exact event time for interval censored data and take advantage of standard estimation methods for right censored data. Simulations are carried out to examine the performance of the proposed method. We analyze data from the National Children's Study to estimate cumulative risks of transition between Probability of Pregnancy Statuses and to examine the effect of major demographic variables. ","We propose a model-based extension of weighting design-effect measures for two-stage sampling when  calibration weighting is used. Our proposed design effect measure captures the joint effects of a nonepsem  sampling design, unequal weights produced using calibration adjustments, and the strength of the  association between an analysis variable and the auxiliaries used in calibration. The proposed measure is  compared to existing design effect measures in an example involving household-type data. ","Wright (2012) provides an exact optimal allocation of the fixed overall sample  size n among H strata under stratified random sampling that minimizes the sampling variance of an estimator of a total subject to the constraint that the total sample size is n. The exact optimal allocation avoids the need to round the optimal sample sizes for the strata to integer values, as is the case with Neyman allocation. Neyman allocation with rounded integers does not always lead to the optimal allocation, that is, an allocation that minimizes the sampling variance subject to the constraint.    ","The National Survey of College Graduates collects paradata in each of its three modes - Internet, mail and telephone.  With these paradata, historical survey responses, and a rich set of frame data, we construct a model to predict response mode during data collection. This dynamic model updates response mode estimates daily by incorporating information obtained throughout data collection, and allows survey managers to switch modes in an informed and adaptive way.  We discuss model development and evaluation along with potential future applications in the 2015 National Survey of College Graduates.  ","A clinical follow-up study to the National Survey on Drug Use and Health (NSDUH) collected information on specific mental disorders among adults that could be used to provide national and state estimates of serious mental illness.  Specifically, a nationally representative subsample of adult respondents to the NSDUH was interviewed by trained clinicians over the telephone using a psychiatric diagnostic interview between 2008 and 2012.  In order to estimate the prevalence of mental health disorders among adults in the U.S., weights were created for the clinical subsample. The weighting procedures included a nearly pseudo-optimal \"poststratification\" to non-mutually exclusive control totals from the NSDUH interview. This use of data from the entire NSDUH sample in weight creation resulted in estimates with increased accuracy. Both the nearly pseudo-optimal poststratification and improved standard error measures for the resulting estimates were completed using the WTADJX procedure in SUDAAN 11.    ","The goal of various clean-up methods is to improve the quality of files to make them suitable for economic and statistical analyses.  To fill-in missing data and 'correct' fields, we need generalized software that implements the Fellegi-Holt model (JASA 1976) to preserve joint distributions and assure that records satisfy edits.  To identify/correct duplicates within and across files, we need generalized software that implements the Fellegi-Sunter model (JASA 1969).  The goal of the clean-up procedures is to reduce the error in files to at most 1% (not currently attainable in many situations).  In this presentation, we cover methods of modeling/edit/imputation and record linkage that naturally morph into methods of adjusting statistical analyses in files to linkage error.  The modeling/edit/imputation software has four algorithms that may be each 100 times as fast as algorithms in commercial or experimental university software.  The record linkage software used in the 2010 Decennial Census matches 10^17 pairs (300 million x 300 million) in 30 hours using 40 cpus on an SGI Linux machine.  It is 50 times as recent parallel software from Stanford (Kawai et al. 2006) and 500 times as fast as software used in some statistical agencies.  With skilled individuals and this fast software, a group of national files can be cleaned up and used in preliminary analyses in 3-6 months. ","In recent years, non-probability samples are being selected in more and more surveys. This may be driven by lack of an accessible frame or non-existence of a suitable frame for probability sampling. More often cost and practicality are factors. When the eligible population is rare, it has to be identified through screening a general population. In such surveys, the use of probability samples can be very expensive because of high screening costs to locate eligible cases. Low response rates may also make the results unreliable because of bias due to nonresponse and high variance. We examine the use of non-probability samples in such surveys. A major concern about the results from non-probability samples is the unknown bias in the estimates. We consider and present steps that could be taken before and after selection of a non-probability sample to minimize bias. We also consider methods of making the non-probability sample close to one of the possible samples that a probability design could produce. We investigate the possibility of assigning a measure of precision to the estimates from non-probability samples and discuss the pros and cons of such a metric. ","Korn and Graubard (1998) suggested various modifications to make scatterplots more informative for sample survey data. These included the notion of sampled scatterplots - plotting a subsample of the data such that the resulting subsample is approximately a simple random sample. Hinkins et al (2009) used inverse sampling to construct regression diagnostic scatterplots, and illustrated how multi-panel plots of many inverse samples could be used to account for the loss of information when subsampling. One criticism  of this approach is that the multi-panel displays may be harder to view; a smaller set of plots, or ideally a single plot, may be better. We explore using the ideas of Flury and Tarpey (1993) to order a large collection of curves. Using this technique on smoothers through scatterplots of survey data subsamples will provide a way to order scatterplots and therefore reduce the number, e.g. a minimum, median and maximum plot, or some other alternative based on order statistics. ","Nonresponse and coverage problems affect every household survey, but it can be difficult to determine the impact on estimates.  The Current Population Survey is a monthly panel survey, and each panel is included in the survey 8 months.  The panel aspect of the survey expands the possibilities for analyzing nonresponse and coverage.  For example, panel data on relationships of household members to head-of-household can be examined to determine if relationship structures systematically change during the eight months.  Another possibility is to link panel microdata across 8 months.  The linked microdata can be used, for example, to measure differential response (and presumably nonignorable nonresponse) among households with different relationship structures.  Results from analyses of panel data are presented and suggestions made for further research.  ","In this talk, we will discuss combining information from multiple sources. In particular, we will consider combining a categorical survey with marginal information from another source. In practice we may have information on the margins of certain variables, such as gender, educational status, or age group from the Current Population Survey. We will incorporate such marginal information using a Bayesian latent class model that can capture complex interactions. We will present empirical examples and show how our method can apply to multiply imputing missing data caused by nonignorable unit nonresponse. ","For many years the American Medical Association (AMA) Masterfile has served as the standard source of sample frames for physician surveys. Its primary advantages are high coverage and some useful auxiliary variables for sampling and estimation. However, contact information provided is often out-of-date, and some physicians have no contact data provided, adding to survey nonresponse. An alternative source for the frame was used to conduct a recent survey of physicians in the specialties of Family Medicine and Internal Medicine. The frame was derived from the National Provider Identifier (NPI) file of individual health care providers maintained by the Centers for Medicare and Medicaid Beneficiaries (CMS) as part of the National Plan and Provider Enumeration System (NPPES). The results are promising in terms of coverage provided, and contact information was more up-to-date than previously experienced with the AMA file.  Nevertheless, there were some drawbacks as well. This paper discusses these findings and draws comparisons to surveys where the AMA file was used as the sample frame.  Some strategies for enhancing the usefulness of the NPI file are also considered. ","NHTSA's new National Automotive Sampling System (NASS) is expected to address a large number of research questions and analytic objectives, and therefore requires a multi-purpose study and sample design. In addition, the new NASS consists of multiple modules, with future funding levels and precision requirements unknown and subject to change for any given module. A multivariate sample design optimization system was designed and built for NHTSA to address these design requirements, parameters and constraints. The system offers two options: A) Minimize cost subject to variance constraints; B) Minimize a weighted sum of variances subject to cost constraints. This paper presents the development and architecture of the sample design optimization system, a description of its outputs, the iterative process of reviewing those outputs, and the ability to modify design parameters and constraints. ","One of the primary objectives of the new National Automotive Sampling System (NASS) sample design is to update and improve the previous NASS' primary sampling unit (PSU) sample. For probability proportional to size (PPS) sampling, finding a composite measure of size (MOS) that is closely related to the multiple outcome variables of interest will reduce the variability of the estimates. In order to achieve this, external information from multiple sources was considered for both the new NASS' General Estimates System (GES) and the Follow-on Passenger Vehicle (FOPV) modules. The external information was used to develop and evaluate multiple composite measures of size against key outcome variables for each module. A MOS was then selected on the basis of correlation with outcome variables and the anticipated variance. The MOS for the secondary sampling units (SSUs) based on obtained crash counts is also presented. The selected MOS was also used to define a minimum MOS for GES PSU formation, while a different minimum PSU MOS variable was used for the FOPV module. ","Survey research often covers a range of topics in one large survey and related questions are usually grouped to form sections in the questionnaire. With a computer-assisted survey interview instrument, the time spent to complete each section of the questionnaire may be measured. Our interest is to understand the completion-rate variability due to interviewers.    ","Kish (1965) lists four frame problems that can occur with any sampling frame and he describes potential remedies for them. Missing elements may be handled by the use of a separate frame or by some form of linking procedure, such as the half-open interval; clusters of elements by a take-all procedure or by subsampling with reweighting; blanks or foreign elements by simply ignoring the sampled element; and duplicate listings by unique identification or by weighting. This paper describes these problems and potential remedies in relation to the use of the United States Post Office computerized delivery sequence files of residential addresses as a sampling frame for address based sampling of households and persons for in-person household surveys.  ","Undercoverage plaques many frames - housing units are missed by listers or do not appear on the postal service list; persons with tenuous connections to households are not captured in rosters; persons hide their eligibility during screener interviews. The literature on undercoverage suggests several methods for improving the coverage of such frames, via a missed housing unit procedure, or detailed probes about household members, or disguising the target population in survey questions. However, each of these solutions introduces additional costs into the survey process. In this way, survey designers face a coverage-cost trade-off. In addition, there is increasing evidence that the cases found via these coverage-improvement measures are disproportionately nonresponders to the survey request. Thus there appears to be a coverage-nonresponse trade-off as well. Together these points raise the question of how much effort we should put into increasing coverage, when such efforts increase costs and nonresponse? This presentation will review empirical evidence for these trade-offs and search for clues to the mechanisms underlying the connection between nonresponse and undercoverage. ","Follow-up of nonrespondents in business surveys is a time and resource intensive activity. Unlike household surveys where respondents are more-or-less equivalent, business surveys have influential units for whom a response is absolutely necessary in order to not adversely affect level estimates. An extensive simulation study was undertaken to compare different methods of performing follow-up for business surveys.  For a fixed budget, is it best to follow-up all nonrespondents or only a sample of nonrespondents? If it is better to follow-up a sample of nonrespondents, what is the best way to select this sample? Should the sample be selected using simple random sampling (SRS), stratified SRS or probability proportional to size (PPS) sampling? The different methods were evaluated via a simulation study by comparing their Monte Carlo mean square errors. ","The Consumer Expenditure Survey implements a statistical disclosure limitation process known as  top-coding in the public used microdata release to conceal sensitive and identifiable information in  order to protect the households confidentiality. This process replaces, for example, the high (low)  end households annual income by the average of all high (low) end households annual income in the  microdata for public users. Top-coding can numerically affect the utility of the microdata, especially  for analyses that are sensitive to the high (low) end of the distribution. For instance, parameter  estimates and confidence intervals can both be biased by this process. In this study, we investigate  the impact of top-coding on CE microdata utility for multiple regression models used to analyze  the relationship between certain expenditures and household income after adjusting demographic  characteristics. We conduct a bootstrap re-sampling study and implement a data utility measurement  based on a modified form of Kullback-Liebler divergence to evaluate the effects of top-coding on  the utility of the CE microdata. ","As substitutes for individual-based information, area-based socioeconomic status (SES) measures have demonstrated their usefulness in population-based research on cancer health disparities. However, the uncertainty in such measurement estimates due to survey sampling is usually ignored. This methodological limitation could have affected on the relative SES rankings of small areas, consequently distorted the estimation of the relationships between certain SES measures and cancer health outcomes. Because the American Community Survey is based on a smaller sample of the U.S. population than the Decennial Census Long Form Survey, such impact could be larger. To address this challenge, we explore the use of a Monte-Carlo approach to quantify the precision of estimating the quantile rankings of census tracts for a composite SES index that is constructed using several census tract-based SES measures based on factor analyses. We further assess the amount of uncertainty in estimating the rates of cancer incidence for SES quantile groups attributable to the uncertainties in ranking census tracts. Preliminary results show that compared to the traditional approach where area-based SES estimate ","It is well-established that interviewers learn behaviors both during training and on the job. However, how this learning occurs has received little empirical attention. There are two competing hypotheses about what happens during field data collection - one is that interviewers learn behaviors from their previous interviews, and thus change the measurement situation in reaction to the behaviors previously encountered (i.e., a contagion model). The second hypothesis is that interviewers encounter less cooperative respondents as the data collection period progresses, and thus change the measurement situation in reaction to these types of respondents (i.e., nonresponse propensity affecting the measurement error situation). This paper will examine these two hypotheses using data from CATI surveys, paradata, and behavior codes. Findings will have implications for interviewer training and monitoring. ","The National Survey of College Graduates (NSCG) has been conducted by the Census Bureau for the National Science Foundation (NSF) since the 1960s. It is the nation's only source of detailed statistics on the science and engineering (S&amp;E) labor force. The NSCG uses a rotating panel design and selects its sample on a biennial basis from the American Community Survey to allow both cross-sectional and longitudinal analysis of education, employment, and demographic characteristics of the S&amp;E labor force. Under this design, the NSCG data is collected and released on a biennial or triennial schedule. The 2010 survey cycle marked the initial use of the ACS as a sampling frame for the NSCG. We examine the feasibility and desirability of using small area estimation methods for estimating characteristics of the S&amp;E labor force in small domains.  Small domains are defined crossing levels of two or more variables in the primary analysis domains.  ","The sampling weight in CES is determined at the time of sample selection. It depends on a unit's State, industry, and size class. However, the population of businesses is highly dynamic. Establishments constantly grow or contract; sometimes they also change their industrial classification or geographical location. Even the number of population units is not fixed but continuously changes over time. A unit may change its size class at the time of estimation or the content of the original stratum may change. Under such circumstances, application of the original survey weights may increase volatility of survey estimates. In this paper we investigate if the survey estimates can be improved by adjusting the original weights. ","Statistics New Zealand is in the midst of implementing its 2010-2020 Strategic Plan that will transform how the agency functions. The \"administrative data first\" philosophy is a critical component in the transformation process and the Goods and Sales Tax (GST) data is a key dataset for transforming business surveys. GST can provide data which can be used in sub-annual surveys to replace directly surveyed units or to improve in editing or in calibration. These processes could potentially reduce response burden and collection costs plus improve quality. However, the use of administrative data poses major challenges. Due to late filings, GST data are not all available on time during the production cycle and the data covers a melange of varying and overlapping time intervals. We propose a calendarization method based on interpolating the cumulated flows with splines that provides data with standardization time intervals and short-term forecasts. The methodology improves the timeliness and quality of the GST data and increases the willingness of the survey programs to embrace tax data. ","Nearly 7 million U.S. citizens move from their state of residence each year.  Determining the causes of why individuals and families leave their home has been a topic of debate among scientists beginning with Ravenstein's papers on migration in the 1880's.  Recently migration research has experienced an increase of interest due to the readily available amount of data being produced from many governmental agencies including the Internal Revenue Service and the U.S. Census Bureau, as well as the desire of policy makers to have an accurate prediction of population for funding allocation.  Historically, models have predicted migration flows without accounting for sampling variability, enforcing the constraint of a system in equilibrium, or making use of the inherent spatio-temporal structure that exists within migration flow data.  We develop a Bayesian hierarchical model to fit the 1988-2010 yearly IRS migration flow data.  Using covariate information and novel empirical basis functions for areal data with spatially and temporally varying parameters, we are able to predict the state-to-state migration flows one year ahead, while simultaneously accounting for uncertainty in the data. ","Social network analysis (SNA) is used in a broad set of applications.  Most of the statistical research in this field has been focusing on modeling the adjacency matrix.  Not much of the discussion focuses on missing covariates associated with actors or nodes.  In wireless service companies, such as AT&amp;T and Sprint, call detail records (CDR) define connections in the social network. The companies have missing data problems of various kinds.  Demographic information on subscribers based on self report is frequently missing. So the question is how to make use of the data to do inference about the social network structure when there are missing labels on the customers.  We describe the EM algorithm for use with various social network models with missing data.  Extensions are made enabling Bayesian analysis and multiple imputations through data augmentation.   We explore the possibility of applying our methods in large, complex networks containing millions of edges, or links, between nodes, such as is found in the CDR data from wireless service companies. ","Unsupervised learning methods such as topic modeling or k-mean clustering can provide techniques for organizing, understanding and summarizing text data without using any manually labeled records as training data. It uses annotations to organize text and discover latent themes in documents without target attributes. We explore using unsupervised learning to classify open-ended survey question responses. By grouping similar responses together, we construct a class of \"topics\" and reduce the exploration of open ended text information to common categorical analysis. We present topic modeling and k-mean clustering examples using different survey data. The resulting topic categories are described by sets of keywords. ","Due to the vast amount of information available to the public, releasing data with fine geographies, while protecting the confidentiality of data subjects' identities and attributes, can be a challenging endeavor. Oftentimes, data stewards resort to aggregating subjects into small areal regions, such as census tracts, or swapping sensitive attributes between subjects. Though such measures have been shown to reduce data privacy risks, this comes at the expense of data quality. Our goal is to protect the privacy of the data subjects at fine scales of geography without compromising the validity of the resulting inference. To accomplish this goal, we propose a fully Bayesian approach for generating synthetic data that maintain both the statistical properties and spatial dependence structure of the original data. We illustrate our approach through simulation as well as a real-data example. ","The American Community Survey (ACS) is an ongoing survey administered by the U.S. Census Bureau that provides timely information on several key demographic variables. Specifically, the ACS produces 1-year, 3-year, and 5-year \"period-estimates,\" and corresponding margins of errors, of various random variables recorded over the U.S. (e.g., median age, race, median income, veteran status, etc.). The 1-year, 3-year, and 5-year period estimates are defined on different levels of geography depending on the population size. Despite the availability of different choices for the estimate's spatial support, it would be useful for ACS users to be able to specify their own spatial support.  We propose Bayesian spatial change of support methodology for count data that allows users of ACS to define their own spatial support. We demonstrate the effectiveness of our approach through a simulated example as well as through an analysis of data from the ACS. ","In surveys, sample size planning is important in achieving precise estimates at a low cost. However, this issue is not adequately addressed for group testing data obtained from a three-stage sampling process. In this study, we obtained the optimal allocation of localities, fields, pools per field and pool size in a three-stage group testing survey. Analytical expressions were derived for optimal values of localities, fields and pools per field for a given pool size. However, only computational solutions were possible when we also calculated the optimal values of pool size. These optimal values were obtained under the assumption of equal locality and field sizes. To handle the unequal sample size case, we derived the relative efficiency (RE) of unequal versus equal locality and field sizes to estimate the proportion. By multiplying the sample of localities and fields obtained assuming equal cluster size by the inverse of the corresponding REs, we adjusted the sample size required in the context of unequal localities and field sizes.   ","The new National Automotive Sampling System (NASS) sample design uses a multivariate optimization method to solve for the sample sizes at the first, second, and third stages, with the considerations of the operation cost and the variance of the variables of interest. The calculation of variance called for building a Police Jurisdiction (PJ) level sampling frame that includes population crash count by Police Accident Report (PAR) stratum, population count of the key estimates, both for the new NASS General Estimates System (GES) and the Follow-on Passenger Vehicle (FOPV) modules by utilizing counts by PAR classification reported to the state by the PJ. Multiple Linear Regression models were developed for estimating other county level estimates from PAR classification counts using the current PSU level data of GES and FOPV. These models were then applied to the PAR counts provided by the population of PJs within sampled PSUs. Special error terms were added to the models in order to add variability / noise across counties. The PJ list contains all PJs in a sampled PSU who report PARs to the state, and the list was further geocoded to map to Census geographic files to associate urbanicity with each PJ. ","The new National Automotive Sampling System (NASS) sample design requires a flexible and scalable PSU sample to be able to respond to future budgetary constraints and data precision needs. The future sample for the NASS could have between 16 primary sampling units (PSUs) and 101 PSUs for the General Estimates System (GES) module, and between 16 PSUs and 96 PSUs for the Follow-on Passenger Vehicle (FOPV) module. We describe a mechanism for drawing a PSU sample that allows the PSU sample size to change in the future while maintaining a high level of PSU stratification. Conditional probabilities are formulated that allow the PSUs in the largest PSU sample to be subsampled as needed to meet future budgetary constraints. ","Address lists originating from the United States Postal Service (USPS) have been used as sampling frames in various survey modes for more than a decade. Surveys that might otherwise have opted for random digit dial (RDD) sampling with telephone interviewing are now using these address lists as sampling frames.   Multi-stage area probability sample surveys with in-person data collection are using these address lists in place of the traditional address listing process. The lists are only available to survey researchers through third-party vendors who append auxiliary information to the lists which can be used in the sampling process in addition to the delivery information from the USPS. Together, these data have become known as address-based sampling (ABS) frames.  This paper gives an overview of ABS frames including the USPS- and vendor-provided information available and its utility for survey research.    ","This paper describes the application of the replication variance estimation method to the sample data drawn from a balanced sampling design in the Programme for the International Assessment of Adult Competencies (PIAAC) Cycle 1 Round 1 study. One of the participating countries, France, used the cube's method to select a balanced sample of the first stage sampling units, Interviewer Action Areas (IAAs), within administrative regions. To be consistent with other participating countries, the variance estimation should use a replication method with 80 replicate weights. Most existing methods for constructing replication weights are not valid for the balanced design. Within each stratum we applied a method outlined by Fay (1984) to generate replication weights such that the replication variance estimator produces algebraically equivalent results to the linearization variance estimator proposed by Deville and Tille (2005). Across strata, the replication weights were allocated in a block-diagonal fashion to reduce the correlation. ","Complex statistics are usually difficult to predict in Small Area Estimation (SAE). Elbers et al. (2003) have proposed an empirical semi-parametric method for dealing with poverty indices in SAE. This method, commonly called the ELL method, consists of drawing from the empirical residuals to reconstitute the entire census. After predicting the census, any complex statistics is easily obtained. ELL method has poor MSE performance in many situations even though bias is usually small. Later, Molina and Rao (2010), proposed an empirical best predictor assuming the nested error linear regression model with normally distributed errors. As expected, this estimator can perform poorly when the model errors are not normally distributed.  We relax the normality assumption by allowing the errors to follow a skew-normal distribution. Skew-normal is particularly interesting because it contains the normal distribution as a special case and at the same time it allows departure from symmetry. In this paper, empirical best predictors are derived assuming skew-normal errors and their performance in terms of MSE is studied relative to the normality-based and ELL predictors.  ","Customers have a wide variety of choices in selecting a method of payment in modern society due to advancements in technology.  We investigate the method of payment habits of banking customers using item response models. We consider three binary item response models used in the literature within an empirical Bayesian framework. These models capture the heterogeneity and complexity of customer perception on methods of payment in different capacities, with different features. For this reason, model assessment methods need to be developed for better inferential purposes. We introduce an assessment criterion based on predictive simulations and illustrate the approach using graphical summary measures. The approach is further highlighted using empirical financial data. ","The importance of explicitly accounting for spatial correlation in univariate small area estimation has recently been acknowledged.  However, methodology for simultaneously and explicitly modeling within-area correlation and between-area spatial correlation in multivariate small area estimation is relatively under-developed. In this research, we develop hierarchical Bayesian methodology to flexibly handle explicit multivariate spatial dependence in the Fay-Herriot framework of models. The effectiveness of our approach is illustrated through simulation as well as data from the American Community Survey. ","This talk describes a method to improve the efficiency of small area demographic and economic estimates from the American Community Survey (ACS).  Small area estimates from the ACS are terribly imprecise however it is possible to improve the precision of estimates via spatial (or temporal) aggregation.  However, geographic aggregation of tracts can lead to significant information loss and a dilution of spatial patterns.  The method presented here is a novel spatial optimization algorithm that improves small area estimates by intelligently grouping small areas while minimizing information loss and preserving spatial patterns.  The algorithm allows the post-processing of public use data to to a user specified level of precision.   ","Statistics anxiety and mathematics anxiety are widely researched constructs (Cruise, Cash, &amp; Bolton, 1985; Fennema &amp; Sherman, 1976; Pan &amp; Tang, 2005; Richardson &amp; Suinn, 1972). Few researchers have discussed both of them simultaneously; further, they have argued that these are distinct variables (Baloglu, 2004; Zeidner, 1991). Most students and educators, however, perceive the phenomena of mathematics anxiety and statistics anxiety as the same. This paper includes a brief literature review on reasons for this misconception followed by a comparison of the constructs of mathematics anxiety and statistics anxiety with respect to undergraduates' gender, college year, STEM and non-STEM fields of study, and mathematics background. This will add support to the fact that these are two distinct constructs and therefore must be dealt with in different ways. ","In 2011, the National Immunization Survey (NIS) began using a dual-frame landline and cell-phone sample design to monitor vaccination coverage rates among children 19-35 months. As of 2012, coverage of the cell-phone sampling frame has increased to 92.9% of all age-eligible children in the NIS. Thus, a single-frame cell-phone sample design could be a viable option for the NIS, but such a sample design would not cover children living in landline-only and phoneless households. In order to adjust for the noncoverage of children living in landline-only and phoneless households, we propose two distinct adjustment methods. The first option is to identify a subset of cell-phone households with sampled children who have similar socio-demographic characteristics as those from landline-only and phoneless households and use a weighting class based approach to adjust for the noncoverage. The second option is to adjust for the noncoverage by using socio-demographic characteristics in a raking adjustment step.  We implemented the proposed methods with 2012 NIS data and compared survey estimates generated under a single-frame cell-phone sample design with similar estimates from the current dual-frame sample design. ","Based mainly on 1990 Census data, Waksberg et al. (1997) examined the effectiveness of disproportionate stratification to oversample persons in areas with greater concentrations of a minority population in order to yield a specified effective sample size for that minority in a national survey.  The areas were defined using block and block group Census information. The effectiveness depends on both the degree of the minority's geographical concentration and on the relative costs of the full survey data collection to the screening costs. This paper updates the Waksberg et al. findings using 2010 Census data and compares the findings across the two censuses.  In addition, we extend the application to surveys of individual census regions and Core Based Statistical Areas (CBSAs).  Oversampling more than one minority population in a survey is also discussed.  ","We propose an alternative of weighted generalized estimating equations for incomplete longitudinal data exposed to non-ignorable and non-monotone drop-out. We introduce the generalized method of moments type estimator combining weighted generalized estimating equations of dynamic model and the mean score functions of response model which are obtained in each time point. The proposed mean score function of response model is a new approach of maximum likelihood estimation that is obtained without specifying the dynamic outcome model. It is less sensitive to failure of the assumed outcome model. We present a limited simulation study and apply our method to the analysis of Korea Work Place Survey (WPS) panel data. ","Samples in cell phone surveys are subject to substantial geographic misclassification that can add bias or variance to survey estimators. The National Immunization Survey (NIS) uses a dual-frame sample to monitor vaccination coverage rates among children. Because NIS selects samples in all areas of the country, geographic misclassification does not create bias in an overall estimate for any given Estimation Area (EA). However, completed interviews from respondents located within a given EA may actually have been sampled in two or more EAs due to geographic misclassification, and therefore can have different probabilities of selection, which in turn can increase the design effect. Using data from the 2012-2013 NIS, we examine a two-step strategy to control the variance of the estimator of the vaccination rate.  First, we determine a minimum number of completes that must come from the sampling frame of the target geographical EA and actually be located in the EA. Second, we calibrate the weights to the known numbers of children actually living in the given EA. We demonstrate the extent to which this method can reduce the increase in variance due to geographic misclassification.   ","Weight trimming can be used to reduce sampling variance and the impact of influential cases.  However, it also introduces bias into the survey estimates.  In surveys with an emphasis on comparing estimates across countries or states, it is desirable that the impact on bias be consistent. This was the case for the Programme for the International Assessment of Adult Competencies (PIAAC), an international adult literacy survey.  To limit the number of cases trimmed and help achieve a comparable increase to bias across countries, a modification was made to the common method of trimming weights that are over k times the median.  Rather than using a constant value of k, the factor was based on a function of the coefficient of variation (CV) of the country's weights.  This paper will describe the trimming procedure for PIAAC and evaluate the resulting effect on bias and variance. ","This presentation describes our evaluation of sampling results in the Programme for the International Assessment of Adult Competencies (PIAAC) Cycle 1 Round 1 to help guide the development of efficient sample designs for the second cycle. We used intra-class correlation coefficients estimated at each sampling stage to evaluate how to reduce the clustering effect. We also reviewed the survey outcome (proficiency scores) to identify effective stratification and sorting variables. For the next cycle, initial sample sizes may need to be increased to take into account the effect of the sample design components (cluster sizes, stratification, variation in weights, multiple imputation) on the resulting design effects (DEFFs) observed in Cycle 1 (or an expected DEFF due to design improvements since Cycle 1) so that the quality of the resulting estimates is comparable across countries. In addition, sample designs may need to be modified to reduce the overall expected DEFF. ","Technical standards and guidelines (TSGs) were written for participating countries to follow in the first round of the Programme for the International Assessment of Adult Competencies (PIAAC). A challenge was to reduce the quality variation across the 24 countries involved, while ensuring that the TSGs were adhered to. This became complex since countries were given options to choose their sample design, and to conduct weighting. The sample designs differed in several ways, including the number of selection stages, and the use of a population registry or not. Quality control (QC) forms and files standardized and facilitated the process, prior to, during and after data collection.  This presentation gives examples of the QC forms that were implemented, the QC files that were requested, and examples of problems that were caught. Results of this work were considered when making a decision to publish countries' data in an international report. ","Within the context of probability-based sampling from a finite population, a number of schemes have been studied to maximize or minimize the overlap between two sample selections while maintaining the required probabilities of selection for each. For example, in redesigning a personal-visit survey, it may be desirable to overlap the sampling of primary sampling units between the old and new designs. Optimum solutions to many overlap problems require mathematically and computationally complex approaches, but Ohlsson proposed simpler methods involving permanent random numbers applicable in some situations. Although not optimal, the methods are easily implemented and typically realize much of the gain achieved by the optimal solution. Ernst extended Ohlsson's methods for sequential methods such as Durbin/Brewer method, by a probabilistically correct retrospective assignment of permanent random numbers.  This paper presents an extension of the Ernst approach when the first sample was selected by drawing more than one unit per stratum systematically and illustrates its efficiency with a simulation study. ","Especially in designing the samples for first-time surveys, there is uncertainty about the response rates and eligibility rates. To address this uncertainty, a large sample may be initially selected, then randomly separated into a main sample and a reserve sample. In this cost-efficient approach, a reserve sample will be on hand for release if needed. In multi-stage samples there are options of drawing a reserve sample at different stages. This paper discusses the issues to consider when choosing the sampling stage for reserve samples and discusses options for drawing reserve samples at various stages. We also look at other issues related to the selection and release of reserve samples, such as the timing of selection and release, amount of release, etc. ","This paper focuses on measuring disclosure risk when missing data exists among key identifying variables. It is well known that combining identifying variables together can lead to the identification of an individual. Records that are unique in the sample based on a set of identifiers may not be 'true' uniques if there exists at least one other record that is a match on a non-missing subset of variables, because it is unknown if the true values match among the missing subset of variables. Therefore, there is some protection from missing values due to the uncertainty about their true values, and it is unclear how much protection is provided by the missing data items. In addition, available software handles missing data differently when measuring disclosure risk. In this paper we describe an approach to help gauge the impact of missing data on disclosure risk. We conduct an empirical investigation on public use data, as well as a simulation to evaluate further.  ","Immunization Information Systems (IIS) and the National Immunization Survey (NIS) are used by immunization programs to track provider-reported vaccinations, describing vaccination coverage at the individual, provider, clinic, local, state, or national levels, depending on the system. Leveraging the IIS population-based structure to increase efficiencies of the NIS random digit dial (RDD) survey is increasingly important as immunization programs and survey environments evolve. The NIS-IIS sample-frame pilot project included 4 states; with state samples of IIS records of children 19-35 months fielded as a list sample, utilizing the standard NIS household survey and follow-up provider-record-check model. In this paper, we will examine the impact of enhancing the NIS sample-frame from the current dual-frame approach (landline and cell phone RDD) to an alternative sample-frame that would include the IIS. A core set of estimates for vaccination coverage and sampling error for the alternative approaches will be compared to the traditional NIS estimates. Options for constructing efficient optimal combinations of the sample-frames including landline, cellphone, and IIS will be discussed. ","Assuming  standard regression mixed model with  unequal error variances and  cluster  sampling of households m.s.e. efficiency of BLUP estimator,as compared to ratio-synthetic estimator, is evaluated. A sample of given number of households is selected in two stages,clusters and households, from a population of geographic strata with two domains: domain of interest Ui and complementary domain Uc (see Ghangurde P.D.; JSM (2013)). Sample size in Ui is increased in steps of number of households and that in Uc is decreased in steps of the same number of households. In a household survey BLUP estimation was done using  a   quantitative auxiliary variable assumed &gt; 0  (e.g. annual household income ).   Efficiencies were obtained for several harmonic means, total sample sizes and sample sizes in Ui and Uc.  The results show that ratio of harmonic mean to sample size in Ui and ratio of total sample size to that in Ui essentially determine efficiency of BLUP as compared to ratio-synthetic estimator; ratio of harmonic mean to sample size in Uc is not important .Data on efficiencies obtained by using assumed values of harmonic means and sample sizes in Ui and Uc are presented  and  analyzed.    ","The Census Bureau is incorporating adaptive design techniques in multiple surveys and the Decennial Census in an effort to maintain or increase data quality while controlling costs. Model-based use of paradata including contact history, interviewer observations, and cost are beginning to guide data collection decisions. This talk mentions the wide-ranging adaptive design efforts underway at the Census Bureau, and then provides a case study of the National Survey of College Graduates (NSCG).    ","This overview of adaptive survey design reviews the objectives of the approach, its historical antecedents, its status in a variety of survey organizations and its promise for the future of survey practice.  The paper draws on the plans and experience of survey organizations in different countries and in the governmental, commercial and academic sectors.  It considers the \"building blocks\" of adaptive design - systems, data resources, modes of data collection - as well as varieties of adaptive interventions and outcome measures.  It identifies impediments and  assesses progress in the development of adaptive design capabilities in the field.  It seeks to provide a framework for understanding and tracking the progress of the adaptive design approach.      ","Paradata are data about the process by which survey data are collected.  This paper explores the ways in which Governments (GOVS) Division can use paradata to improve our questionnaires.  It looks at paradata from both web-collected surveys and paper survey forms.  The 2011 Government Units Survey was collected using the Centurion web instrument, which enables the collection of paradata as respondents are completing their questionnaires online.  Paradata such as keystrokes, time stamps of movement, and navigation patterns throughout the questionnaire were captured and analyzed.  Some of the things analyzed include the frequency of response changes, time spent on each question, and survey break-off points.  Additionally, paper responses to the 2011 Annual Survey of Public Pensions were examined to determine if there were questions that would benefit from further definitions or explanations, questions that were particularly troublesome for respondents, or break-off points on the questionnaire.  Respondent call records and emails were also used to identify these problem questions. ","Statisticians within the Governments (GOVS) Division are entrusted with maintaining the Governments Master Address File (GMAF) a comprehensive frame of government entities. The GMAF is being re-engineered, which allows an opportunity to build paradata into the frame processing.  Complete coverage is imperative as this repository acts as the sampling frame for all of the division's surveys.  The availability of lists of local governmental units varies significantly from state to state and poses challenges in frame coverage.  Historically, we have recorded changes in governmental organization (mergers, reductions, reincarnations, and births) through a combination of legislative research, the Government Units Survey, and Quality Improvement Program trips.  In this paper, we hypothesize that an influx in population may predict the births of governments, specifically special districts.  We examine population counts and changes logged by the last four decennial censuses by state/region against that of historical governmental units to test this theory. ","Reviewing paradata and monitoring response dashboards are two tools used to help reduce nonresponse. After the 2007 Census of Governments:Finance Component, we developed a dashboard that monitored response in the very early stages of processing.  In 2012, we added the use of a score function to the dashboard.  As part of our modernization and re-engineering efforts, a new nonresponse follow-up application was also developed. This paper shows how we incorporated the use of paradata into the dashboard score function and subsequently incorporated the new score function into the nonresponse follow-up application to target units with the highest score function in an effort to increase response rates. ","The Governments Division of the U.S. Census Bureau conducts the Annual Survey of Local Government Finances (ALFIN). The ALFIN provides statistics about the financial activities of state and local governments across the country. We currently use calibration to estimate these finance statistics. Calibration methods adjust sampling weights so that the adjusted weight totals agree with reliable known totals, e.g., census totals (or census counts) obtained from the Census of Governments. In previous cycles of the ALFIN, survey analysts used decision-based estimation, a technique that performs hypothesis tests that allow combining strata when possible to reduce the variance and improve the accuracy of survey estimates. In this evaluation, we develop a design-based Monte Carlo simulation experiment in which we draw repeated samples from the 2007 Census of Governments data using the ALFIN sample design. We compute the decision-based, calibration, and Horvitz-Thompson estimates that use the generated sample and the 2002 Census of Governments data as auxiliary information. We then compare mean squared errors of these estimators.  ","In planning the estimation methodology for the Quarterly Summary of State and Local Government Tax Revenue (QTax), we decided to use additional data captured during the process of data collection, paradata, to improve the estimates.  QTax is comprised of three components: local property tax, state tax, and local non-property tax.  In this paper, we focus on local non-property taxes. These taxes include Individual Income, Corporation Net Income, and General Sales and Gross Receipts. We have paradata information on which governments responded each quarter. In our research, we used the paradata to improve sample design and the estimates. From the paradata, we developed a response propensity model to adjust the survey weights due to nonresponse.  In this paper, we discuss how to use the paradata in our models and calibration estimators with adjusted weights and census calibration totals to produce the estimates that agree with the totals from the Annual Survey of State and Local Government Finances and from the Census of Governments. ","The Quarterly Survey of Selected Non-Property Taxes (F-73) is a sample survey conducted by the Governments Division of the U.S. Census Bureau that collects data on local government revenue from non-property taxes.  During its 2009 sample design, F-73 experienced a response rate that was lower than the U.S. Census Bureau's standard.  In 2013, the Governments Division redesigned the sample and narrowed the scope of the survey from 11 taxes to three to try to reduce respondent burden, increase the response rate, and increase data quality.  In this paper, we describe how paradata, which are data about the data collection process, were used in part of the sample redesign to select units with a high estimated response propensity.  We also describe evaluation simulations that were performed to validate the redesign and fine-tune parameters. ","Adaptive and responsive survey designs have focused mostly on reducing nonresponse error in single mode surveys. However, to date, the most influential and most debated  survey design feature is the survey mode. The quality - cost differential between modes is very strong and worldwide statistical offices have gradually begun to redesign their surveys to mixed-mode designs. Given the large differences in costs and given large anticipated differences in mode-specific survey error, i.e. the mode effect, the survey mode is the main design feature to be adapted to the respondent. Such designs need to extend their scope to measurement error and they need to minimize mode effects with respect to some benchmark.  In this paper, a number of approaches is presented to account for mode effects in adaptive survey design. For surveys with only a few key variables, the optimization can be directed  specifically at the minimization of the corresponding mode effects. A case study is presented linked to the Dutch Labour Force Survey.   ","The Governments Division of the U.S. Census Bureau uses small area estimation techniques for several of its surveys. The Annual Survey of Public Employment and Payroll (ASPEP) yields estimates of the number of federal, state, and local government civilian employees and their gross payrolls. The ASPEP sample design is based on state and type of government as strata from which a proportional-to-size sampling design is applied.  Estimation of government totals at the state and functional level, e.g., air transportation, public welfare, hospitals, etc. are produced.  This estimation motivates the small area estimation methodology that enables the production of reliable estimates in the level of aggregation small cells where direct estimators show some limitations. We used Empirical Bayes (EB) models to estimate the totals for the cells. At the state and national level aggregates,the totals obtained from the direct estimates are reliable due to big data. Furthermore, we obtain other reliable totals, Decision-based estimates, from which we benchmark on. In this paper, we show how to use the EB estimation, and then benchmark the estimates to the direct estimates and Decision-based totals. ","Abstract:  The Governments Division of the U.S. Census Bureau employs small area estimation techniques for the Annual Survey of Public Employment and Payroll (ASPEP).  ASPEP provides statistics on the number of federal, state, and local government civilian employees and their gross payrolls.  Different small area estimators can be produced using the ASPEP data and auxiliary information from the preceding Census of Governments. We develop a design-based Monte Carlo simulation experiment in which we draw repeated samples from the 2007 Census of Governments data using the ASPEP sample design.  We compute a wide range of estimates that use the generated sample and the 2002 Census of Government data. We then compare simulated design-based biases, variances, mean squared errors and coverage probabilities of these estimators.  We repeat the experiment using the 2012 Census of Governments data in order to understand if these properties change over years. The estimators covered under our simulation study include Horvitz-Thompson, Structure PREserving Estimation (SPREE), traditional composite and empirical Bayes methods.     ","Address based sampling and random digit dialing designs are two of the most commonly used probability based sampling methods to obtain data aimed at answering questions in a broad spectrum of fields including social science, public health, and medicine and beyond.  Enhancements to samples can be made in order to facilitate better frame construction including the appending of information on home ownership or presence of children or age of head of household to create strata that could be meaningful for a given study.  These variables can also be used as part of a tailored design or in part for nonresponse and weighting adjustments after data collection is completed.  The degree to which these appended variables can be used for various aspects of the survey process from sampling, to recruitment to weighting depends in part on their availability for all sampled or population units and on their accuracy at the time of appending.    In this presentation we provide a broad overview of the types of variables that can be appended to address based samples with particular focus on vendor related, rather than public use, variables.  We describe appending rates for a key set of socioeconomic and household variables, and discuss their variability across vendors.  We discuss how survey researchers can properly evaluate the quality of these data and make recommendations about their potential uses for ABS sampling designs.   ","This paper quantifies the efficiency gained from incorporating consumer marketing databases in an address-based sample design.  We discuss how the relationship among match rate, accuracy rate and population prevalence influence sample efficiency in terms of costs, design effects and effective sample sizes. Using results from multi-mode and in-person studies we estimate record match rates and accuracy rates of key demographics variables and evaluate their effect on sample efficiency. We will also provide the preliminary results and present methods to improve the accuracy of data appends. ","The National Survey of Children's Exposure to Violence (NatSCEV III) documents changes in the incidence and prevalence of children's exposure to a broad array of violence, crime and abuse experiences. NatSCEV III used a multiple frame design that included an address based sample (ABS) frame to construct a nationally representative sample of households with children under 18 years old. The availability of ancillary demographic information matched by a sample vendor to some proportion of the ABS sample addresses suggests the feasibility of increasing sampling efficiency by using an optimal allocation among matched and unmatched strata based on the expected proportion of households with children. The two waves of mailing in NatSCEV III incorporated an adaptive design where demographic data collected in the field during the first wave were used to assess the accuracy of the vendor-appended ancillary demographic data, and to develop an optimal stratified allocation for the second wave. Findings and conclusions about the viability of this approach for NatSCEV III and other national surveys of households with children will be presented. ","Address-based sample (or \"ABS\") designs are attractive for population studies due to both the possibility of contacting households with multiple modes as well as high coverage rates.  One limitation of ABS is higher costs in situations where the target population has low eligibility, making approaches such as random-digit dial telephone plus cell (or \"dual frame\") more efficient.  We employed age- and race-targeted lists to enhance an address frame based on the United States Postal Service (USPS) computerized delivery sequence file (CDSF) in a study targeting low-incidence households.  Our paper presents results that indicate which factors are more easily captured, and the associated coverage/hit-rate trade-offs in different environments.  We also discuss the impact of utilizing lists from multiple vendors simultaneously, and the kinds of households present or absent from one or more targeted list.   ","Survey data collections have traditionally been judged by the response rate. In terms of improving the response rate, every case has the same value. An optimal approach to maximizing the response rate would be to always attempt to interview the \"easiest\" remaining case. However, this strategy might lead to interviewing very similar cases. In this way, as a guiding indicator, the response rate might distort the survey data. The ultimate goal is to control nonresponse bias, but this bias is never observed in practice. Researchers are investigating proxy indicators that may be used to control the risk of nonresponse bias. Unfortunately, the field of survey methodology knows very little about how other indicators might perform as guides to data collection. One possible set of indicators are drawn from regression diagnostics. These diagnostics can be used to identify influential data points. Exploring the covariate space near these influential points may reduce our uncertainty about these regions. Under this approach, not all cases have the same value. We are currently running some exploratory experiments that prioritize currently active cases that are similar to observed cases that are ","Calibration weighting involves a mild adjustment of probability-sampling weights that forces the weighted totals for a set of calibration (benchmark) variables to equal values determined using more complete information from the frame, the target population, or a larger sample.  Its use can increase the efficiency of survey estimates as well as adjust for frame coverage errors and unit nonresponse.This short course is composed of three modules of increasing complexity.  The first module  provides a broad overview of the topic.  It assumes knowledge of survey sampling at the level of Lohr's Sampling Design and Analysis. The second  module discusses the roles of the linear prediction models and probability-sampling theory in calibration weighting.  A treatment of optimal and pseudo-optimal calibration is followed by a discussion of double protection from potential biases due to unit nonresponse or coverage errors.  The potential use of calibration weighting when nonrespondents are not missing at random is introduced.  Some familiarity with derivatives and linear algebra is needed.  The third module discusses large sample variance estimation for which previous exposure to asymptotics would be helpful, although concepts are stressed over rigorous proofs. A motivating example grounds the theory throughout by displaying the numerical impact of alternative approaches.  ","In the 2013 Methods-of-Payment (MOP) survey, sampling units were selected through an approximate stratified random sampling design. To  compensate for non-response and non-coverage, the observations are weighted  through a raking procedure so that the weighted sample is representative of  the population with respect to control variables. The variance  estimation of weighted estimates must take into account both the  sampling design and raking procedure. We therefore propose using bootstrap  resampling methods to estimate the variance. We produce replicate raking  weights for the questionnaire (SQ) portion of the survey using the bootstrap  resampling method and use them to compute the variance of weighted  estimates. We find that the variance is considerably different when  estimated through the bootstrap resampling method than when estimated  through Stata's linearization method, where the latter does not take into  account the correlation between the control variables and the outcome  variable. ","The Federal Reserve Payments Study collects information from surveys of payment networks and depository institutions, providing a longitudinal panel for two types of establishment populations. Both network and depository institution surveys are designed to enable national estimates of aggregate totals from response data, but discrepancies are sometimes large. We propose an approach to reconcile differences while improving the reliability and consistency of estimates. ","For surveys targeting a broad, heterogeneous population, such as that of U.S. consumers, good sample coverage is important to obtaining accurate inferences. Generally, a sample of respondents comes from only one source. However, administering the same survey to two samples generated through different recruitment methodologies potentially allows for better coverage across and within population strata. In this presentation, we discuss our experiences with assimilating responses to the Survey of Consumer Choices from two panels: RAND's American Life Panel and USC's Understanding America Study. Relative comparisons of the panels as well as strategies for assimilating responses from each to estimate population parameters are presented. ","The 2012 Diary of Consumer Payment Choice (DCPC) is a survey of U.S. consumers produced by the Federal Reserve Banks of Boston, Richmond, and San Francisco. Benford's Law, sometimes known as the First Digit Law, can be applied to the dollar amounts of over 12,000 consumer payments measured in the DCPC. The analysis shows that the distribution of the first digits of the dollar amounts follow Benford's Law reasonably well. In addition, the distribution of first digits in the DCPC dollar amounts is similar to other analyses done by the US Bureau of Labor Statistics on data from the Consumer Expenditure Survey. A generalized version of Benford's Law can be used to find the expected distribution of the nth digit. Using techniques described in the generalized Law, DCPC respondents who reported rounded dollar values can be identified. The analysis will determine if those respondents who round and those who do not round report different mean dollar values for their payment activity. ","Using shopping diary survey data we show that changing payment patterns is a challenging task; even when consumers have fallen in love with the debit card they find it hard to divorce from cash. While seven out of ten Dutch consumers report to prefer using the debit card, only seven out of twenty actually mostly pay by debit card. The likelihood that reported preferences and actual behaviour do not match increases with income, education and age. Consumers with payments in cash-intensive sectors, where the wide acceptance of the debit card is a relatively recent phenomenon, are more likely to overestimate debit card usage than other consumers. The likelihood of a gap also increases with the amount of cash that consumers carry with them and decreases with the average transaction size. Our findings indicate that persistent habits are an important explanation why the substitution of cash by debit cards took place at a slower pace than was expected. ","American Trends Panel is a probability panel with RDD recruitment developed by by Pew Research Center and Abt SRBI. Panel surveys have been conducted on different modes in different waves, including web for most panel members, and mail or phone for those who do not have access to the Internet. We analyze the results of the July 2014 wave (Wave 5) that included a comprehensive, large-scale mode-of-interview experiment that randomly assigned respondents to telephone and web modes, with approximately 1,500 respondents in each mode. To quantify the contributions to the mode effects of the different question characteristics in the 75-question instrument, we build a cross-classified mixed model with effects of person and question characteristics to identify the properties of survey questions that make them susceptible to mode effects, as well as the demographic groups that tend to exhibit mode effects. We discuss how the decomposition of the total survey error and explained variance helps identifying the properties of the questions that are associated with the mode effects, such as question format, topic, and the potential impact of social desirability.   ","There seems to be a consensus in survey methodology that cell phone interviews are longer than landline interviews within the same study (Brick, et al., 2007; Lynn &amp; Kaminska, 2011; and Vicente, Resis, &amp; Santos, 2009). Several reasons have been hypothesized to explain the differences in length. Among these are satisficing, multitasking, respondent distraction, connectivity issues, or the type of questions related to the telephone service. The available literature is limited and previous studies are based on surveys conducted several years ago. In the meantime, there have been many changes in phone technology, cell phone use, and culture. Additionally, all of these studies were based on short surveys of 15 minutes or less. However, an initial analysis of the 2013 2014 California Health Interview Survey (CHIS) finds little difference in interview length between the two modes.    ","Multi-mode surveys have been in use for a long time as survey managers seek to use collection procedures that produce the best possible data within existing constraints of time and budget (de Leeuw, 2005). Especially, Web or CATI surveys (laptop, tablet, pad, PC) become more and more popular due to people's willingness to adopt advanced technologies in their daily lives. Consequently, such multi-mode designs lead to a confounding of selection effects and measurement effects (measurement errors) caused by mode differences. This research will investigate the mode effects by developing propensity models of R-Indicators. R-indicators are designed to measure the similarity between respondents and the original sample or survey population. They are used to measure representativeness of respondents and to identify which subgroups are over- or under-represented. In this research, three multi-mode surveys are analyzed using the same propensity model. ","Finding response patterns is key to increase survey completion. The ACBS follows up on the BRFSS by calling BRFSS respondents identified with asthma. Using the 2012 and 2013 ACBS disposition files, we computed the rate of eligible BRFSS respondents who agreed to a call-back (AR) and the ACBS response rate (RR) based on the American Association of Public Opinion Research formula. We also examined the difference in RR between landline (LL) and cellphone (CP) samples and assessed the impact of lag days between interviews on ACBS completion. BRFSS CP samples agreed to a call-back more often than LL samples (AR: 75.5% vs. 70.9%). However, when contacted for ACBS, CP RR was lower than LL RR (43.4% vs. 47.0%), except for adults aged 25-44 years. Among them, CP RR was 1.6% higher than LL RR. The difference in RR (3.6%) between ACBS LL and CP responses was smaller than the difference in corresponding BRFSS RR (11.8%, 2013 data). The RRs for both ACBS LL and CP responses were highest if call-back was within 2 days of BRFSS interviews (92.3% and 88.8%). As lag days increased, the ACBS RR decreased. The CP RR showed a sharper drop; after 2 weeks, the RR gap between LL and CP reached 12.0%. ","The Army Study to Assess Risk and Resiliency in Servicemembers (STARRS) is a study of mental health risk and resilience among military personnel. One component of this large study -- the Pre- and Post-Deployment Study (PPDS) -- followed a set of about 9,000 soldiers before and after deployment. The final wave of this study (T3) used a mixed mode design to interview individual soldiers. The design included a web survey followed by telephone attempts for nonresponders. In a mixed-mode design such as this, one important design decision is when to switch from web to telephone. A longer duration for the web mode may reduce costs. On the other hand, a longer duration for the web mode may lower overall response rates. The Army STARRS PPDS T3 survey tested the impact of the timing of the mode switch using four different durations for the web mode. This paper will examine differences in cost, response rates, and key estimates across the four experimental treatments. We do not find differences in estimates or response rates, but costs do differ across the four treatments. ","As part of research on Web survey method for Residential Energy Consumption Survey (RECS), the U.S. Energy Information Administration (EIA) sponsored the 2014 students' survey practicum in the University of Maryland's Joint Program in Survey Methodology (JPSM). Under a self-administered survey called the Home Energy Use Survey (HEUS), the practicum conducted several experiments on participatory incentives and questionnaire instruments. Here, we integrate both experimental treatments and contact procedures as survey treatments. A survey treatment is a composite treatment, for example, consisting only of advance letter and first invitation letter for a Web survey. A composite treatment takes place over time and composite treatments build up over time, depending on when responses come in, if at all, and what modes (Web or paper) the respondents choose. Possible response modes also depend on a survey treatment-in the example of survey treatment above, a response by the Web form is the only possible response mode. In this paper, we examine the relationships between survey treatments and response modes in terms of counts and rates of response over survey time period as well as over survey treatments. ","In November, 2014, the Defense Research, Surveys, and Statistics Center (RSSC) conducted for the Federal Voting Assistance Program (FVAP) a survey of active duty military members. To improve response rates, RSSC designed a randomized experiment to assign sample members to either of two modes of data collection (web or telephone). Historically, response rates to web surveys of active duty military have had low response rates, and an earlier phone survey produced better response rates, especially for typically difficult to survey young military members. Response rates to the web survey were 10% while the phone produced 20% response rates. The successful integration of the web and phone surveys was critical, and we examined the data to assess mode effects. Unweighted estimates from the phone survey were significantly different from data from the web survey. The determination whether to use the phone data in production estimates hinged on whether we could attribute the mode differences to either improved sample balance (sample composition) from the web survey, or improved measurement. ","Prior to the explosion in the cell-phone only (CPO) population, Keeter's method was used to adjust for noncoverage in landline RDD telephone surveys. Approximately 98% of adults and children live in a household with access to either cell phone or landline telephone service (NHIS, 2014) and more than 40% of households are CPO. Only a small proportion of the U.S. population (~4%) lives in a landline only telephone household and phoneless population remained smaller being less than 3%. Use of Keeter's method has been complicated by the reduction in the size of the landline sample in dual frame surveys and the fact that interruption in telephone service does not apply to cell phone sample cases. Furthermore, the sample of landline cases with interruption in landline services has decreased to such minimal size that it cannot represent the phoneless population. We use data from 2012 NIS, and 2012-2013 NHIS to explore the potential bias resulting from noncoverage of the phoneless population, discuss alternate methods to compensate for noncoverage in dual-frame telephone surveys, and present results relative to bias reduction and impact on the variance of survey estimates. ","Address-based samples allow auxiliary data to be linked to addresses via geographic coordinates. These variables are used for nonresponse adjustments and follow-ups. One common design, an ABS Sample with Phone Follow-Up, sends mailings to all addresses, and then makes phone calls to addresses with phone numbers appended and sends mailings to those without. Differences in completion rates and respondent characteristics have been reported for addresses with and without phone appends, but it is not clear whether this is related to initial differences in having an appended phone number. In this paper we identify variables related to phone append status. Cross-validated accuracy, error rates and variable importance measures are presented from various machine learning models predicting whether an address has a phone append (about 45% of addresses) from over 500 candidate variables appended to a sample of 1 million records randomly chosen from an ABS sampling frame. The results will allow researchers to understand how initial phone append status is related to various socio-demographic and economic variables, and could potentially affect coverage and nonresponse biases. ","Address-based sampling frames generated from the U.S. Postal Service Computerized Delivery Sequence file (CDS) are considered by many to be the gold standard source for a national household sampling frame. The literature to date suggests that the CDS provides nearly complete coverage of the national household population. Prior research has shown that the under-coverage is not random and can vary by geographic area, survey mode, and address type, all of which can introduce significant bias to survey outcomes.     ","The National Immunization Survey (NIS), a dual-frame landline and cell-phone survey, monitors vaccination coverage among children 19-35 months and teens 13-17 years in 56 state and local areas. Given the increasing prevalence of cell-phone use and declining prevalence of landline-only households among young families with children, a single-frame cell-phone sample design may be a viable option for the NIS. We used 2012-13 NIS data to evaluate the area-level impact of using a single-frame cell-phone sample design. We compared the single-frame and dual-frame sample designs in terms of vaccination coverage estimates and key survey indicators. We identified factors associated with significant differences in estimated vaccination coverage rates between the dual-frame and single-frame sample designs. Potential factors that we considered included cell-phone-only rates in the area and geographic inaccuracies associated with the cell-phone sample. Finally, we assessed whether changing to a single-frame design has similar impact on estimated vaccination coverage among children 19-35 months and teens 13-17 years. Findings will provide guidance for future NIS surveys. ","Health surveys estimating child wellness often face challenges efficiently identifying households with certain characteristics, including having individuals in a specific age range. Companies specializing in creating targeted lists, such as Marketing Systems Group (MSG), can compile information from various data sources to potentially-flag households believed to have children of specific ages. Such a task is challenging for reasons including the time-lag necessary for births to appear in vital records and the fact that children are not direct purchasers as are adults and therefore may not yield salient consumer data. Marketers have access to large quantities of transactional information, which may correlate with the presence of children and serve as alternative indicators. We examine the utility of such information to indicate the presence of young children in households in LA County, based on screening data from a multi-mode ABS sample. Our study reveals whether these \"signals\" can positively predict the presence of young children. We examine the sensitivity and specificity of these indicators and evaluate how useful they could be in identifying such households in the future. ","In telephone surveys, ported numbers are those telephone numbers originally assigned as a wireline that are subsequently converted to wireless service. These ported numbers are sampled as a part of a landline sample and identified as wireless during the post-sampling processing. Ported telephone numbers, as of 2013, represent a small but growing proportion in landline samples, and there is little research on their characteristics. In some survey operations, ported numbers are excluded from data collection, while in others they are dialed as a part of a cell phone sample. In the California Health Interview Survey (CHIS), a representative dual-frame random digit dialed (RDD) telephone survey of the California non-institutionalized population, ported telephone numbers have been sampled and dialed as a part of the cell phone sample since 2007.  In the 2013 CHIS, sampling and subsequent processing identified over 10,000 ported telephone numbers. This paper analyzes the results of dialing these ported telephone numbers and compares their respondents' characteristics to those from both the landline and cell phone samples to assess potential bias if the ported numbers are excluded in data collection. ","The National Health Interview Survey (NHIS) has undergone a sample redesign at ~10 year intervals after each decennial census. The current NHIS sample design began in January 2006 and will run through the end of 2015. Planning for the 2016 sample redesign began in 2012 when an interagency group (Census Bureau, National Center for Health Statistics (NCHS)) developed a redesign milestone schedule. A major change, relative to the last three NHIS redesigns, is the sample address source. The current method of field listing all sample areas is no longer affordable for the next NHIS sample design, so NCHS will be using addresses purchased from one or more commercial vendors (supplemented with a limited amount of field listing). We describe the research undertaken to guide the implementation of the sample redesign. ","Interviewer skills for obtaining cooperation from sampled households require flexibility, tailoring to the respondent, and maintaining interaction. On the other hand, administration of survey questionnaires requires reading questions exactly as written, nondirective probes, and a clear set of regimented behaviors during the interview. That is, interviewers are required to be flexible during recruitment, but standardized during measurement. These skill sets may be at odds. This paper will examine behavioral differences in the survey interview between interviewers who are more successful at gaining cooperation and those who are less successful. We examine whether question misreadings, probing, feedback, disfluencies, and clarifications differ for interviewers with higher versus lower cooperation rates. We use the Work and Leisure Today Survey (n=450, AAPOR RR1=4.7%), including survey data, paradata, and behavior codes. Preliminary analyses indicate that interviewers with higher cooperation rates deviate more from the question wording, introducing (major) changes to the question stem or response options more often than interviewers with lower cooperation rates. ","We estimate the magnitude of three sources of error in a mobile Web survey. 1390 members of the Longitudinal Internet Studies for the Social Sciences (LISS) panel were asked to answer the same questions, once using a smartphone and once using a personal computer (PC). We use the PC Web survey as a benchmark, and deviations from the benchmark are regarded as error. To estimate coverage errors in the mobile Web survey, we compared those with their own smartphones (71%) to the full sample. To estimate nonresponse errors, we compared those who responded on their smartphone (73%) to the covered sample. Finally, to estimate measurement differences, we compared how the same people responded when using their smartphone and when using their PC. We find large non-coverage error errors relative to nonresponse and measurement errors. Furthermore, the non-coverage errors were not consistently offset by the other sources of error. This suggests that limiting Web surveys to mobile Web users only is risky for general population surveys. This research is a first step towards understanding the effect of mobile-only Web surveys on data quality using a total survey error framework. ","We compare nonresponse and measurement error bias across two modes of data collection--telephone and web. In an experimental setting we randomly assigned respondents to either telephone or web mode (n=3,482). Because the sampled persons were selected from German administrative records, record data are available for all sample units allowing us to disentangle nonresponse and measurement error.    ","In this paper, we attempt to demonstrate the coverage impact of disallowing smartphones, the futility of relying on warnings and recommendations about device usage, and we demonstrate how a well-chosen \"mobile friendly\" interface design can minimize the impact of non-response and measurement errors in web surveys that allow smartphone participation. We propose that in order to minimize Total Survey Error, researchers should allow smartphone participation on most surveys but simultaneously work hard to minimize measurement and non-response error by focusing on survey content, question types and interface designs. The paper draws on a review of literature, recent data from studies conducted at the University of Michigan's Survey Research Center demonstrating the growing trend of smartphone response to web surveys, and the results of experiments conducted by Market Strategies International -- the first tests approaches to discouraging smartphone survey taking on a customer service feedback survey; the second compares response distributions for smartphone and non-smartphone survey participants. The experiment compares multiple \"mobile friendly\" design alternatives to learn which des ","In face-to-face survey data collections, interviewer observations recording selected features of sampled units represent a promising, cost-effective source of paradata that can be used for nonresponse adjustment. Recent research indicates interviewers can successfully record features of sampled units correlated with key survey measures and response propensity. However, these observations can be error-prone, limiting the effectiveness of nonresponse adjustment. Furthermore, no existing study has identified additional sources of interviewer variance of observation accuracy. This study performed analyses of justifications for interviewer observations in the National Survey of Family Growth (NSFG) on two key features of all sampled households: the presence of children under age 15 and expected probability of household response. Cluster analyses suggest that unique subgroups of NSFG interviewers do exist based on the observational strategies used. Multilevel models comparing the accuracy of the observations among the identified interviewer subgroups suggest that alternative strategies employed by the interviewers are more or less effective for improving the quality of the observations. ","The State Inpatient Databases (SID) contain the universe of the inpatient discharge abstracts from hospitals in the U.S., thus providing a unique platform for a broad range of research in healthcare and medicine. As with any large scale data collection effort, the SID have a moderate amount of missing data in several patient-level variables. This study aims at identifying appropriate imputation methods for SID. To accomplish this aim, we compare six imputation methods (i.e., complete case analysis, mean imputation, marginal draw method, hot deck, joint multiple imputation (MI), conditional MI) through simulations. These simulations consider missing observations in continuous, binary, ordinal, and nominal variables. We report root mean square error and bias of the imputed values for continuous variables; the corrected imputed proportion for discrete variables. Simulation results indicate that the differences between these methods are marginal for continuous and binary variables. Conditional MI has the highest correctly imputed proportions for ordinal variables. Hot deck and conditional MI show similar performance and are superior to other methods for nominal variables.  ","The Survey of Occupational Injuries and Illnesses(SOII) is an establishment survey that provides annual estimates for the incidence count and rate of employer-reported work-related injuries and illnesses. Results of the survey are published by industry for the nation and participating states. Low response rates for some industries within a state result in many of the state industry-level estimates not being published because of quality and/or confidentiality concerns. The SOII sample is stratified by state, ownership, industry, and size.  The number of sample units from each sampling stratum is currently determined by the Neyman allocation, which is intended to minimize the expected sample variance of the estimator for total recordable cases given the fixed sample size.   Our goal for the study is to develop a new sample allocation to increase the publishability of estimates at the state industry level while constraining the variance for the fixed sample size. In this paper, we explore a method for assigning sample allocation that aims to maximize the number of publishable cells while constraining the variance of the estimator for total recordable cases. ","The Current Population Survey (CPS) is a rotating monthly panel survey of households that provides comprehensive information on the U.S. labor force. The collection mode of the CPS is either by personal visit or telephone interview. In this study, we focus on data collection during the first four months of household participation in the eight-month survey. The first month's visit is conducted by a Census field representative (FR), most commonly using computer-assisted personal interviewing (CAPI). Over the next three months, some household's data may continue to be collected by the FR either in-person or over the telephone, while other households are transferred to a centralized telephone call center for collection by computer-assisted telephone interview (CATI). Assignment to CATI is based on a number of criteria determined by the FR, the Census regional office, and outcomes from the previous month. In this study, we examine which demographic variables and outcomes are associated with the CATI assignment propensity. In addition, we investigate which variables are associated with the propensity of success in CATI. ","A lot of research has focused on the influence of internal establishment characteristics on nonresponse; however, there is little known about the influence of regional characteristics on establishment nonresponse. This paper focuses on exploring the relationship between establishment nonresponse and regional demographic, economic, and social characteristics of an establishment's location. We use hierarchical linear modeling to control for regional characteristics that may have an added influence on nonresponse beyond the internal establishment characteristics we have identified in previous models such as establishment size and industry (Earp, Toth, Phipps, &amp; Oslund, 2012). The variables we consider include regional characteristics as they relate to county level data (including population age, income &amp; poverty, education, banking, crime, health, taxes, government spending, etc.) according to the Census Bureau's USA Counties summary data. We also consider household data on trust of the federal statistical system using the Census Daily Tracker questions administered during 2012. ","Currently the U.S. Census Bureau is conducting research on ways to use administrative records to reduce the cost and improve the quality of the 2020 Census Nonresponse Followup (NRFU) at addresses that do not self-respond electronically or by mail. In previous censuses, when a NRFU enumerator was unable to contact residents at an address, he/she found a knowledgeable person, such as a neighbor or apartment manager, who could provide the census information for the residents, called a proxy response. The Census Bureau's recent advances in merging federal and third-party databases raise the question: Are proxy responses for NRFU addresses more accurate than the administrative records available for the housing unit? Our study attempts to answer this question by comparing the quality of proxy responses and the administrative records for those housing units in the same timeframe using the results of 2010 Census Coverage Measurement (CCM). The assessment of the quality of the proxy responses and the administrative records in the CCM sample of block clusters takes advantage of the extensive fieldwork, processing, and clerical matching conducted for the CCM. ","Studies have shown that air quality is associated with population health.  Health and air data are collected by two independent data systems, the National Health Interview Survey (NHIS) and the Air Quality System (AQS), respectively.  To overcome the limited spatial-time coverage of AQS monitor data, an extensive model-predicted universe of spatial-time air measurements was created.  These approaches were developed by the Environmental Protection Agency (EPA) and adopted for use in public health by CDC's Environmental Public Health Tracking Program, National Center for Environmental Health (NCEH).  From this universe, air measurements for PM2.5 and ozone were linked at the census tract level to the NHIS sample over years 2001 to 2010 for those areas within the contiguous-US. As this linkage is complete, air/health association analyses on the NHIS can be performed using suitable design-based or model-based methods in contrast to analyzing a partial air/heath linkage with the original AQS air data.   This study is somewhat exploratory with the needs of a typical NHIS data user in mind.  The study attempts to determine some of the operating characteristics of the linked data and presents suggestions for design-based and model-based analyses. In particular, some basic spatial-time associations are explored.   Some thoughts on next steps are also discussed at the end of the paper. ","Statistical organizations are increasingly exploring the integration of sample survey data with information provided by alternative data sources, e.g. commercial transaction data. One issue is that these data sources may not satisfy survey sample design requirements (e.g. probability sampling). Thus, the quality of the statistical products produced from the integrated data will depend on the mechanisms that link the data sources to the target population of interest. One technique that may be used to produce high quality survey products from integrated data sources is pattern-mixture models (Little and Rubin 1993). Pattern-mixture models have traditionally been used in surveys with nonignorable nonresponse. In those applications, response indicators define subpopulations of missing data patterns and inferences are made conditional on those patterns. Applying these models to data integration problems seems reasonable since subpopulations can be defined by indicators of the unit being captured or represented by one or more of the alternative data sources. As such, we explore the extent to which pattern mixture models can be used to integrate survey data with alternative data sources. ","The size and scope of linkage projects are increasing as probabilistic record linkage becomes a standard method to integrate administrative and survey files based on personal identifiers.   Thanks to recent improvements in computers and software, Statistics Canada is currently undertaking multiple, large record linkage projects, including the Justice Re-contact Project, which follows pathways through the justice system and measures the time between police contacts.    ","Panel conditioning can refer to either changes in respondent behavior due to participation in repeated interviews or changes in their reporting of behavior. Difficulties arise when distinguishing between the two forms: Do changes over time reflect true change or do they reflect change in reporting (or both)? Using administrative data linked to a German panel survey on labor market outcomes, changes in the take-up of labor market programs are analyzed. These linked data allow us to distinguish between the two types of panel conditioning. We use propensity score weighting to estimate the average treatment effect of participation in several waves of the panel survey. Results show that respondents participate in more labor market programs than a control group of unselected (but eligible) persons. We interpret these results as evidence that the survey induces changes in behavior among respondents. These results will be of interest to all researchers who collect or analyze panel survey data. ","In response to the growing reliance upon administrative records to generate national estimates of key indicators of interest, federal statistical agencies have been expanding and enhancing activities to assess and improve the quality of data collected through administrative records systems. Given that administrative records are often collected across different agencies or local reporting units, encountering missing data at both the individual and aggregated levels is inevitable and must be addressed when developing estimates. The Federal Bureau of Investigation's (FBI) National Incident-Based Reporting System (NIBRS) is a system designed to collect data from administrative records to be used for research and statistical purposes. It was developed as an expansion to its Uniform Crime Reporting (UCR) Program to improve the quality of crime data collected by law enforcement by capturing detailed information on each single crime occurrence. In this paper, we present an imputation method developed to handle missing data in NIBRS by leveraging other relevant external data sources. Given the hierarchical structure of NIBRS, this particular method addresses missing data occurring at multiple levels, including: a) incident level, due to item missing within each incident; b) agency level by month, due to some agencies reporting only for a partial year; and c) agency level by year, due to some agencies not submitting data to NIBRS for an entire year. The proposed method will be applied to a study in the Bakken region of the United States that utilizes the NIBRS data to examine how crime and law enforcement changed in the region as oil production increased from 2006 to 2012. A variance estimation method was also developed to evaluate the uncertainties in our estimates introduced by this imputation technique. In a broad sense, this research also can be viewed as an example of how to handle missing data in hierarchically structured administrative records.  ","The National Immunization Survey (NIS) tracks state and national vaccination coverage, while state public health departments use Immunization Information Systems (IIS) data to track vaccination coverage at the individual, provider, and clinic, local and state levels. Leveraging the IIS population-based structure to increase efficiencies in the NIS is increasingly important as survey costs escalate and response rates decline. We report on a pilot study, sponsored by the Centers for Disease Control and Prevention, in which IIS in four states were used as a sampling frame for children 19-35 months. Data were collected in two steps for IIS sample children: (1) the standard NIS household phone interview of the child's parent/guardian and (2) the standard NIS mail survey of the child's vaccination providers given parent/guardian consent. We assess the impact on estimated vaccination rates derived from the IIS pilot sample to rates produced by the NIS random digit dial (RDD) sample and to rates produced using various multi-frame IIS plus RDD approaches. We also examine the use of IIS vaccination totals for calibration purposes in weighting the standard NIS RDD sample. ","The National Crime Victimization Survey (NCVS) in the U.S. has provided estimates of violent and property crime for over four decades. Until recently, the survey has had almost exclusively a national focus. As recommended by a National Academy of Sciences panel, the Bureau of Justice Statistics (BJS) has undertaken a number of efforts to expand the geographic utility of the survey results. This paper is an outgrowth of research to provide small area estimates of key crime rates from the NCVS for states, large counties, and large metropolitan areas. The estimates are based a modified version of a time-series model proposed by Rao and Yu to take advantage of strong area-level correlations in the estimates over time. A multivariate version of the model was used to provide estimates for components of the crime rates by type of crime and by relationship to the perpetrator. BJS plans to make the estimates available to users on their website. This paper describes a hybrid model representing a further extension. These methods have potential applications to other situations in which the underlying characteristic exhibits strong stability over time. ","The U. S. Census Bureau's Small Area Income and Poverty Estimates (SAIPE) program provides estimates of poverty within school districts, counties and states for different age groups. Currently, the SAIPE program uses the current year's data for estimation of small area means. However, there is potential for improvement of the mean squared error of point predictions of the current year by using multiple years of survey data. In addition, incorporating data from multiple annual surveys allows for inference on the change in small area means over time. There are two main goals of this paper: the first is to investigate potential gains in efficiency using multiple years of survey data for estimation of a small area parameter. The second goal is to estimate the increase or decrease over time of a small area parameter, and to construct valid credible intervals for the change over time. An example using state-level SAIPE data is presented. ","The Annual Survey of Public Employment and Payroll (ASPEP), conducted by the Government Division of the U.S. Census Bureau, provides statistics on the number of federal, state, and local government civilian employees and their gross payrolls. Different small area estimators can be produced using the ASPEP data and auxiliary information from the preceding Census of Governments. We develop a design-based Monte Carlo simulation experiment in which we draw repeated samples from the 2007 Census of Governments data using the ASPEP sampling design and compute a wide range of estimates that use the generated sample and the 2002 Census of Government data. We then compare simulated design-based biases, variances, mean squared errors of these estimators. We repeat the experiment using the 2012 Census of Government data in order to understand if these properties change over years. The estimators covered under our simulation study includes: Horvitz-Thompson, SPREE, traditional composite, and empirical Bayes and hierarchical Bayes methods. Lastly, we present different benchmarking approaches and compare their performances. ","Small Area Estimation has gained importance over the years in survey research methods for the purposes of effective allocation of government funds for economic, health and social planning. The direct survey based estimators are unreliable due to insufficient number of observations from each small area. The way to overcome this problem is typically to use model assisted predictors. Estimating mean square prediction error of these estimators is a crucial step to determining the quality of these predictors and is the first step towards developing other inference techniques. The situation gets very complicated when we consider predicting small area means of a bunch of correlated attributes that have possibly been measured over time as well. In this article, we work on developing theories and methods for the prediction of small area means, the estimation of their mean squared prediction errors and subsequent second order bias correction of these estimators in multivariate and multivariate-time series version of basic area level models under the assumptions of known and unknown sampling variances. We present simulation results and discuss applications of our developed methods. ","AskCHIS Neighborhood Edition disseminates small area health estimates based on California Health Interview Survey for zip codes, cities, legislative districts and counties in California. Census tract-level auxiliary data and population data are used to provide information for granular areas such as zip codes where survey samples are scarce or even non-existent. A semi-parametric model with penalized splines over auxiliary data and random effects at sampling strata-level is used. Due to the common form of minimization criteria, fitting of penalized spline can be converted to solving for coefficients of mixed models. A Jackknife method is used for estimating variance and prediction mean-squared error of the modeled estimates. We conduct an empirical comparison to examine the validity of the proposed approach. ","Estimates from the Current Employment Statistics (CES) Survey are produced based on the data collected each month from the sample of businesses that is updated once a year. In some estimation cells, where the sample is not large enough, the Fay-Herriot model is used to improve the estimates. Under the current approach, the model combines information from a set of areas and is estimated independently every month. Given the design of the survey, it may be beneficial to borrow information not only cross-sectionally but also over time. This paper explores the feasibility of applying such a model. The results are evaluated based on historical \"true\" employment data available on a lagged basis. ","A spatial hierarchical Bayesian model based on a Generalized Dirichlet distribution is introduced to construct small area predictors of proportions in several mutually exclusive and exhaustive land cover classes. The standard survey estimators are judged unreliable at the county level due to small sample sizes, and the hierarchical model is an effort to obtain more efficient predictors. At the first level, the design based estimators of the proportions are assumed to follow the Generalized Dirichlet distribution (GD). After proper transformation of the design based estimators, beta regression is applicable. We consider a logit mixed model for the expectation of the beta distribution, which incorporates covariates through fixed effects and spatial structure through a conditionally autoregressive (CAR) process. In the application, the survey data are from the National Resources Inventory, a longitudinal monitoring survey, and the covariate is derived from the Cropland Data Layer (CDL), a land cover map based on satellite data. In a design based simulation study, the Bayesian estimators have smaller relative root mean squared error than design based estimators. ","In computerized adaptive testing (CAT), items (questions) are selected in real time based on the already observed responses, so that the ability of the examinee can be estimated as accurately as possible. This is formulated as a non-linear, sequential, experimental design problem with binary observations that correspond to the true or false responses. However, most items in practice are multiple-choice and dichotomous models do not make full use of the available data. Moreover, CAT has been heavily criticized for not allowing test-takers to review and revise their answers. In this work, we propose a CAT design that is based on the polytomous nominal response model and in which test-takers are allowed to revise their responses at any time during the test. We show that as the number of administered items goes to infinity, the proposed estimator is strongly consistent for any item selection and revision strategy and asymptotically normal when the items are selected to maximize the Fisher information at the current ability estimate and the number of revision is smaller than the number of items. We also present the results of a simulation study that supports our asymptotic results. ","The popularity of conditional dynamic models for network panel data has greatly increased over the recent decade; however, there are few practical heuristics for what level of temporal sampling -- e.g. weekly, daily or hourly snapshots -- of the evolving dynamic network is required for modeling and estimation purposes. We investigate these models through simulation so as to ascertain when these models are appropriate and when these models should be avoided. Here, we use parameters from empirical cases in the literature to inform a series of simulation based studies where we generate synthetic datasets which we then subsample at well chosen time intervals to test the model's ability to recreate the known parameters and macro-level network properties of interest. Beyond purely testing for parameter degradation we also examine information metrics and a series of predictive checks as proposed in the statistical network literature. ","The paper describes the construction of cross-sectional weights for the second wave of the Panel on Household Finances survey, the German part of the euro area Household Finance and Consumption Survey. Its first wave was conducted in 2010/2011 and its second in 2014, the last including both the respondents of the previous wave and a refresher sample. The construction of cross-sectional weights in a panel survey poses certain challenges as changes in the participating households, changes in the population and panel attrition need to be taken into account. To adjust the weights of the panel households we use a 'base weight' approach as described for example in Verma, Betti and Ghellini (2006). The proposed method relies on the construction of person weights, from which household weights are derived. Non-response adjustments follow, which are carried out separately for the panel, split and refresher samples. At the final stage the three components are merged and their weights are calibrated together. ","The Census Bureau is currently evaluating a redesigned Current Population Survey's Annual Social and Economic supplement instrument. The Census Bureau also produces estimates of state and federal taxes based on responses from the CPS ASEC supplement. This paper compares tax estimates from the redesigned instrument sample to those from the classic instrument sample. The paper also will compare these estimates to other sources including IRS benchmarks. ","Commercial crowdsourcing service has gained prominence for obtaining machine learning labels recently. As the crowdsourcing workers are paid for each label they provide, it is desirable to reduce total budget by selecting proper workers adaptively. In this talk, we investigate properties of optimal adaptive sequential designs for worker selection that have minimal Bayes risk. This paper combines several major techniques in statistics and applies them to design optimal crowdsourcing procedure. In particular, we employ techniques in adaptive testing, sequential probability ratio test, stochastic control and empirical Bayes. ","Dugoni and Brown (2014) presented initial work exploring a new three stage approach to developing code frames for open ended questions in survey research. This approach combines current approaches to software assisted content analysis with expert rater review and statistical, psychometric analysis to generate a code frame for the coding of verbatim (text) responses to open-ended survey items. This study applies the same approach to the analysis of longer verbal protocols by re-analyzing verbal protocols collected by Dugoni and Ilgen (1981). This set of archival data was constructed using structured interview techniques and classified into categories using a technique known as Q-Sort to develop the classification scheme prior to the development of the computer assisted techniques used by Dugoni and Brown. The use of the three-stage approach was found to be effective with the larger data sets yielding comparable results to earlier results with short responses to single items. The present study also supports the usefulness of this approach with archival data. Suggestions for modifications of the technique for archival re-analysis and structured protocols are discussed. ","All the SRMS proceedings papers from 1978 onwards are already online. Last year, at the 175th year of the ASA, we made a proposal to identify, scan and put online the survey-related papers scattered in the hardcopy ASA proceedings of the Social Statistics Section (SSS) before 1978, the year that the SRMS was born when it split from the SSS. With the enthusiastic support of the SRMS, the authors are delighted to report that all these old SSS papers, including the non-survey research papers, are available online. We believe that this effort will help to recover some of the survey research advances that are buried in the hardcopy proceedings. This talk will provide an overview of the project, discuss the process to digitize the papers, display the contents of the product, and demonstrate how users can conduct searches. ","By incorporating sampling weights, complex survey data can be summarized to obtain unbiased estimates of population parameters. Inexpensive auxiliary information may be available for the entire population. In recent years, nonparametric model-assisted methods have been proposed that incorporate auxiliary variables more efficiently than misspecified parametric models. However, careful consideration on including only relevant variables in the model is needed to obtain the most efficient estimator. Previous literature on variable selection may not perform well when the model is misspecified and the data is selected from a finite population with unequal weights. In this paper, we propose an information criterion when the superpopulation model is assumed to have an additive form and the data is collected from a finite population at a single stage with unequal sampling weights. Our proposed method is simpler to implement and is demonstrated through simulation to select the correct variables more often than a previously proposed method. An application of the method to the NHANES data is provided. ","Classification with high-dimensional features are commonly encountered in many scientific problems in biology, genetics, medicine, and so on. When the number of features is ultra high, a fast and effective dimension reduction is needed or desired to capture important signals, filter out noises, and down-scale the data set without information loss, before a refined and more computationally expensive analysis. In this paper, we study the problem variable screening in multicategory classification problems. A variety of screening procedures are considered, including likelihood-based and LDA procedures, along with screening methods based on pairwise classification.  These tools are thoroughly evaluated and compared at various scenarios, and then applied to cancer classification. ","This project is concerned with feature screening method for time-varying coefficient models with ultrahigh dimensional longitudinal data. While some covariates are truly associated with the response mean function, there are other covariates that are potentially responsible for the variation in the response. We propose a two-step screening method that identifies important fixed effects in the first step and selects random effects in the second step. We study its sure screening property, and examine the finite sample performance via Monte Carlo simulations under generalized linear model framework for both continuous and categorical response. In an empirical analysis of a genetic dataset, we advocate a two-stage approach by first reducing the ultrahigh dimensionality for both fixed and random effects to moderate sizes using the proposed two-step procedure, and then applying model selection techniques to make statistical inference on the coefficient functions, variance components and covariance structure. ","For functional time series with physical dependence, we construct confidence bands for its mean function. The physical dependence is a general dependence frame, and it slightly relaxes the conditions of m- approximable dependence. We estimate functional time series mean functions via spline smoothing technique. Confidence bands have been constructed based on a long-run variance decomposition and a strong approximation, which are satisfied under mild regularity conditions. Simulation experiments provide strong evidence that corroborates the asymptotic theories. Additionally, an application to S&amp;P 500 index data demonstrates a non-constant volatility mean function at a certain significance level. ","Canonical correlation analysis (CCA) is a classical statistical technique to measure associations among two sets of random variables, with applications to several fields like multimodal signal processing and machine learning. In this talk, we first provide a direct and general formulation of Kernel CCA. We next present theoretical results for estimating the canonical correlation directions and the associated projection operators. Our results are based on certain concentration inequalities for the sample covariance and sample cross-covariance operators. ","The goal of full Bayesian methods for modeling survey outcomes at the unit level is to provide more precise estimates with an accurate assessment of uncertainty. This should be accomplished without resorting to asymptotic assumptions based on small sample sizes. The way to achieve this is by using all available, relevant data and carefully model the uncertainty within both the population model and due to sample selection. The difficulties of this approach, in part, are due to the need to model many details not directly relevant to the outcome of interest, lack of data for model checking, and missing information. The purpose of this discussion is to bring together the experiences of researchers who are working in this area to share approaches---both the successes and the failures. ","The small area distribution of a continuous random variable is the monotone non-decreasing function defined by the proportion of units (individuals, households, businesses) within the small area that have values for this variable that are less than or equal to the argument of the function. Such distributions underly virtually all small area characteristics that are of interest in official statistics, including the increasingly important work on inequality assessment. Standard methods for estimating this function depend typically on access to unit level survey data from the small areas of interest. However, in many applications this is not possible, for example poverty mapping where data confidentiality restrict access to unit level survey data with small area identifiers, or where the agency carrying out the small area analysis does not have the resources to analyse unit level data, as in many developing countries. In this paper we explore model-based methodology where published tabulated data corresponding to estimated small area counts can be used to make inferences about small area distributions. Applications to estimation of widely used poverty measures will be presented. ","Small area estimation of count data has become a research topic of widespread interest due to the ever-increasing need to produce more precise estimates for undersampled/unsampled geographies. This problem becomes more exacerbated when one acknowledges that many data sources also report related variables of interest that are referenced at different levels of spatial aggregation and by time. Moreover, the resulting multivariate spatio-temporal (areal) dataset can be extremely high-dimensional. Thus, to provide a coherent set of small area (domain) estimates in a computationally efficient manner, we propose a model that takes advantage of this highly complex dependence structure. Specifically, we propose a Poisson multivariate spatio-temporal mixed effects model that uses extremely effective dimension reduction to model high-dimensional multivariate spatio-temporal count data. We illustrate our method through simulation as well as an analysis of Quarterly Workforce Indicators (QWI) published by the US Census Bureau's Longitudinal Employer-Household Dynamics (LEHD) program. ","We examine alternative approaches to borrowing information over time in small area estimation with the goal of improving on estimates from the model of Fay and Herriot (1979) when this model is applied to only current data. We focus on the case of a moderate to large number of areas and a small number of time points, and consider two situations distinguished as models with strong covariates versus models with weak or no covariates. Alternatives considered include autoregressive and random walk dependence structures, as well as a bivariate model applied to current estimates and an average of past estimates. Theoretical calculations indicating how much improvement might be expected out of borrowing information from past data for the alternative models are compared to results from empirical examples. ","The Ohio Army National Guard Study is a panel survey examining the prevalence and risk factors of psychiatric disorders among National Guard service members. A sample of 2616 soldiers from a population of 10778 completed the first wave of the study between 2008 and 2009. Survey samples often differ from the target population due to nonresponse and under-coverage. To estimate the population prevalence of psychiatric disorders, we propose a logistic post-stratification model, in which administrative data on population race, sex, rank, age, and marital status are used to improve the survey estimates. The stratum specific prevalence rate is modeled using a partially linear model, with the post-stratification weight modeled by a penalized spline to allow nonlinear association with the outcome. Further, post-stratification variables that are related to outcome are included in the model as linear predictors. Simulation study shows that the proposed method is more efficient than the classical post-stratification or raking estimators in population proportion estimation. The proposed method also allows the estimation of prevalence of psychiatric disorders in small sub-populations. ","Survey weighting adjusts for differences between the collected samples and the target population. However, classical weights have lots of problems. Extreme values of weights will cause high variability and blow up the estimates. In practice, weighting construction requires arbitrary choices about selection of weighting factors and interactions, pooling of weighting cells and weight trimming. The general principles of Bayesian analysis imply that models for survey outcomes should be conditional on all variables that affect the probability of inclusion, which are the variables used in survey weighting. We would like to incorporate these weighting variables into the model for survey outcomes under the framework of multilevel regression and poststratification at much finer levels. Our procedure will yield the model-based weights after smoothing. We will use Stan for computation and illustrate the performances via the application of the New York Longitudinal Survey of Poverty study. ","Skewed data are common in sample surveys. We consider two types of non-normally distributed data, including skewed data and zero-contaminated skewed data. For the skewed data, we propose a skew-normal penalized spline model by assuming a skew-normal distribution given the sample selection probability and modeling the location and scale parameters of the skew-normal distribution as a penalized spline function of the selection probability. To model the zero-contaminated skew-normal data, we consider a two-stage approach by first modeling the probability of positive values using a probit penalized spline regression model (Chen, Elliott, and Little 2010) and then using a skew-normal penalized spline model for positive values. We assume that the probability of selection is known for all units in the population. Using a fully Bayesian approach, we can obtain the posterior predictive distributions of the non-sample units in the population, and thus the posterior distributions of the population quantities, such as means and quantiles. We compare our proposed estimator with alternative methods using simulations based on artificial data as well as a farm survey data. ","Space plasma simulations are known to generate vast amounts of numerical data. In recent times,with greater availability of computer power, the proportion of data generated has increased exponentially posing new challenges in its analyses. Simulations can be scaled up, but matching methods for analyses have not been developed at the same pace, except in the industry setting where companies utilize big-data techniques to build advanced analyses engines. Although many of these methods need developers working with low-level big-data programming, today there exist powerful out-of-the box solutions that can be easily employed. In this paper, using SAS Visual Analytics as one of these tools, we will demonstrate how existing statistical tools and analytical platforms can be used to analyse simulations of the Sun in a novel way. In ours case, simulations will generate roughly 660mb/time unit and hundreds of them are needed to provide an adequate analyses. Application of our method thus allows instant analyses and prevents the need to guess \"when\" and \"where\" the interesting physical phenomena occur, thus effectively getting rid of data which has little or no scientific value. ","Propensity modeling has been extensively used in telecommunication companies to optimize marketing outcomes in cross sale and up sale campaigns, retention tactics, recruiting strategies, etc. By identifying characteristics of customers who are most likely to respond, propensity models make no discrimination between the potential responders that are going to take actions regardless of campaign and those only take action after being treated by campaign. Uplift models, on the other hand, specifically target those that can be influenced by promotional marketing operations. While both modeling methodology are proven useful in many use cases, uplift models are found superior in terms of maximizing return on investment, especially for cross sale and up sale targeting. In this paper we examine the pros and cons of both modeling methodology vs. different scenarios of marketing need, and present empirical suggestions on best practices tactics. ","We introduce a graphical diagnostic called the Torgegram for characterizing the spatial dependence of a variable on a stream network. The Torgegram consists of four component empirical semivariograms, each one corresponding to a particular combination of flow-connectedness within the network and model type (tail-up/tail-down). We show how an overall strategy for fluvial variography can be based on a systematic analysis of the Torgegram. ","The Continuous Sampling Plan (CSP) was designed in the 1940's for monitoring the quality of a production line. CSP is used by the Australian Department of Agriculture for monitoring the biosecurity compliance of incoming goods at international borders. CSP amounts to a sample design: it specifies the probability with which consignments should be inspected, but the probability is a function of the inspection history. The probability of contamination of a unit arriving at the border is valuable information for inspection policy. We develop and then assess the performance of maximum-likelihood estimators of the probability of non-compliance under CSP. We show that ML estimators of the probability of non-compliance at the end of a CSP cycle are biased, and we provide expressions for the main contribution of the bias. We then construct bias-corrected estimators and confidence intervals, and evaluate their performance in a numerical study. Although the methodology is presented in the context of border inspections, it can be applied in many more settings. ","Many procedures for generating latent factor scores can be found in the literature. One major division is the use of differential weighting versus unit weighting of indicators. Simulation studies show that the two methods highly correlate; however, unit weighting appears to yield results that are overall more generalizable across research studies. The literature does not appear to provide a theoretical rationale for why this might be the case. This presentation posits that just as the mean of a sample of participants' scores is often the best estimate we have of a population parameter, the mean of several standardized, manifest indicators is often the best estimate we have of a latent trait.  ","For likelihood-based inferences from data with missing values, Rubin (1976) showed that the missing data mechanism can be ignored when (a) the missing data are missing at random (MAR), in the sense that missingness does not depend on the missing values after conditioning on the observed data, and (b) the parameters of the data model and the missing-data mechanism are distinct; that is, there are no a priori ties, via parameter space restrictions or prior distributions, between the parameters of the data model and the parameters of the model for the mechanism. Rubin described (a) and (b) as the \"weakest simple and general conditions under which it is always appropriate to ignore the process that causes missing data\". However, these conditions are not always necessary. Also, they relate to the complete set of parameters in the model, but we argue that it would be useful to have definitions of partially MAR (P-MAR) and ignorability (IGN) for a subset of parameters of substantive interest. We propose such definitions, and apply them to a variety of examples where the missing data mechanism is missing not at random, but partially missing at random or ignorable for a parameter subset. ","A bachelor's is considered one of the basic ways to improve an individual's increase of long-term earnings. For many fields, a bachelor's is required to be taken seriously as a candidate for desirable employment, even entry level. The amount of debt to obtain a degree also has major impacts on future earnings, savings, and investment in retirement. Analyses have looked at initial income in recent graduates and described the harsh impacts of lower income for women and minorities on repaying what was assumed equal debt upon graduation. This analysis questions the assumption of equal debt burden among demographic groups. A discrepancy in debt burden between demographic groups indicates disparities and disadvantages in achieving similar financial success. This study focuses on two major outcomes:   Average student debt per borrower and proportion of each demographic group receiving loans for those bachelor graduates from a 4-year college in Virginia. Outcomes will be compared between men, women, Caucasian and Minority subgroups using Bayesian methods and cluster modeling at the college level. We will evaluate our hypothesis by calculating credible intervals around the outcomes. ","A popular tool for analyzing product choices of consumers is the well-known conditional logit discrete choice model. Originally publicized by McFadden (1974), this model assumes that the random components of the underlying utility functions of the consumers follow independent Gumbel distributions. However, in practice the independence assumption may be violated and a more reasonable model should account for the dependence. In this research we use the Gaussian copula with compound symmetric correlation matrix to construct a general multivariate model for the joint distribution of the utilities. The induced dependence on the utilities and the choice probabilities are studied using analytic expressions and simulations. For regression with consumer and product specific covariates, we derive expressions for the likelihood function, score functions and the Fisher information. We use numerical methods and computer code to obtain the maximum likelihood estimates and standard errors. Comparison of our model with other competing methods and practical applicability is illustrated using real world consumer preference data. ","There are a growing number of surveys recruiting and collecting data from participants solely through online methods, without any contact via telephone, mail, or in person. A concern for any such survey is the assurance of effective antifraud measures such that no one person is able to enter the study multiple times. The purpose of this paper is to share an antifraud method from an innovative study, the National Cancer Institute's QuitTXT Smoking Cessation Evaluation (QuitTXT). This study included online recruitment with a non-probability sampling method, data collection via Web and cell phone, and monetary incentives. In QuitTXT, antifraud efforts centered on detecting duplicate phone numbers, email addresses, and IP addresses to prevent a single individual from enrolling into the study multiple times. We developed procedures during the recruitment phase of the study, along with a post-recruitment method for detecting and eliminating multiplicity when lapses in the screener antifraud methods occurred. This retroactive fraud detection is useful for any study recruiting and implementing a study entirely online, in particular for studies without a rigorous antifraud screener process. ","This study validates the near-surface temperature and precipitation output from decadal runs of eight atmospheric ocean general circulation models (AOGCMs) against observational data from NCEP/NCAR reanalysis temperatures and GPCP precipitation data. We modeled the joint distribution of these two fields with a parsimonious bivariate Matern spatial covariance model, accounting for the two fields' spatial cross-correlation and smoothness values. We fit 30 year seasonal averages from each AOGCM to a statistical model on each of 21 land regions. Both variance and smoothness values agree for both fields over all latitude bands except southern mid-latitudes. Our results show temperature fields rougher than precipitation fields, while both are increasingly rough with increasing latitude. Models predict rougher fields than observations for the tropics. The estimated spatial cross-correlations of these two fields are quite different for most models in mid-latitudes and southern hemisphere. Model correlation estimates agree well with those for observations for Australia, at high northern latitudes, and some tropical regions, but elsewhere little consistent agreement exists. ","Four-parameter logistic regression (4-PL) is a common statistical model used in bioassay to determine the potency of a sample relative to the reference standard. Due to both the natural characteristics of biological substances and the complexity of biological reactions, it is normally required to confirm similarity or parallelism of the dilution response curves before calculating the relative potency. However, sometimes the laboratory may omit the confirmation step, since confirming parallelism is not straightforward for 4-PL models. In this presentation, we evaluate the impact of parallelism evaluation on the bias and variability of relative potency estimates with different assay variability and weighting using simulated data and real data. The results show that at the early stage of assay development it is strongly recommended to evaluate parallelism with a proper weighting function. But once the assay system is well-controlled, there is little impact to the bias or variability in the relative potency estimates if parallelism is simply assumed even for approximately parallel curves, partial curves, or even curves with hooked effects.  ","Ruggedness or Robustness tests are used to demonstrate that a response such as the result of a chemical determination is unaffected by selected external factors when they vary over a defined range. Most published work on ruggedness testing relies on the graphical display of the computed effects in unreplicated or fractionally replicated two level factorial experiments.   The present work extends graphical analysis to the case where three factors are examined at three levels each. The method is applicable to higher order three level designs.  It is well known that the treatment sum of squares in a 3 cubed factorial experiment may be decomposed into 13 two degree of freedom components. If the factors have no effect, the square root of the 13 mean squares, will follow a Weibull distribution with a scale parameter equal to the standard deviation of the response and a shape parameter of 2.   To test whether the factors have an effect on the response one may, among several options, examine a Weibull plot of the rms values of the two degree of freedom components ","The tedious process of inspection of the individual members in a large population can often be reduced greatly by examining only a sample of the population and rejecting the whole lot if the proportion of defectives in the sample is unduly large. The hypergeometric distribution is a useful model often used to model the number of defective items in such cases. Sometime the distribution of the number of defective items in a sample from a finite population can deviate from the well-known hypergeometric distribution due possibly to the nature of defects being modeled or the nature of the units being studied. For this purpose, we propose a modified version of the well known hypergeometric distribution called the COM-Hypergeometric distribution. This new distribution has the limiting distribution form of COM-Poisson distribution (Conway and Maxwell, 1962) and COM-Poisson-Binomial (CMPB) (Borges and Balakrishnan, 2014 and Shmueli et.al,2005). We study some limiting properties and some other characteristics of the proposed model. We have also developed statistical inference for analyzing some existing data sets and compare the results obtained with other models. ","The most widely used version of the spatial scan statistic is the circular scan. However, circular windows are not always adequate to correctly describe the true solution when the cluster has a irregular shape. Furthermore, this methodology does not provide any measure of the relevance of each region to the most likely cluster. Based on a recently proposed tool, the intensity function, we build a more accurate way of defining the uncertainty in the delineation of spatial clusters using Item Response Theory (IRT) models for adjusting the probability that each region belongs to the anomaly. IRT is used in situations in which there are latent characteristics of interest - such as the proficiency in a given area or the extent of depression of a given individual - and we use observed variables to measure how much of the latent trait an individual possesses.  We propose the use of the 2PL model to predict the probability that a given region of the map belongs to the real cluster. We use a bootstrap framework to obtain the necessary data and fit the model. Our tests show that the IRT approach is able to accurately pinpoint the true cluster. Simulations and applications will be presented. ","In survey sampling practice, unequal sampling weights (the inverse of the selection probabilities) can be both beneficial and deleterious. Extreme variation in the sampling weights can result in excessively large sampling variances when the data and the selection probabilities are not positively correlated. In addition, extreme variation in the weights can result from unplanned subsampling, nonresponse adjustments, or post-stratification. In some survey situations, the survey statistician may impose a trimming strategy for excessively large weights. Because of the weight trimming, the survey statistician will usually expect an increased potential for a bias in the estimate and a decrease in the sampling variance. The ultimate goal of weight trimming is to reduce the sampling variance more than enough to compensate for the possible increase in bias and, thereby, reduce the mean square error. In this presentation, I will discuss current methods used to identify the appropriate trimming values and provide guidance on selecting the final trimming level, which may be different from the values suggested by the algorithms. ","The correct specification of dimensionality is fundamental to the selection of appropriate measurement models and valid interpretations of parameter estimates. Thus it is critical to detect model misspecification in terms of dimensionality based on model fit indices. This simulation study investigates the sensitivity of model fit indices to bifactor model misspecification and evaluates structural coefficients bias if misspecification is not detected. Design factors include data structure (9 items with 3 group factors; 36 items with 3 group factors; 36 items with 12 group factors), group factor loading magnitude (0.3, 0.4, 0.5, 0.6), general factor loading magnitude (0.3, 0.4, 0.5, 0.6, 0.7), and type of misspecification (ignoring the general factor; ignoring the group factors). Overall the chi-square statistic outperforms RMSEA and SRMR. When the general factor is ignored, all fit indices are insensitive to misspecification and structural coefficients are negatively and severely biased. Implications for applied and methodological researchers are discussed.  ","Traditional weight adjustments for survey sampling error are often constructed through multiple stages, where design weights are based on the inverse of the probability of selection, and in a separate stage nonresponse adjustments are derived from weighting cells or classes, or based on model-deduced response propensities. More recent efforts by Little and Vartivarian (2003) have advocated the use of propensity models that incorporate both design information, as well as variables that are, ideally, related to both nonresponse and the survey outcome. There is often a third stage of adjustment that involves calibration to known or reliable population totals. It would be useful to incorporate this calibration stage into a propensity model containing the design information and variables related to response behavior. This can be accomplished via a latent constructs that are constrained (by totals or proportions) to the external information being used. By simultaneously estimating the response propensity under calibration and incorporating design variables, additional variance due to adjustment would be minimized. ","In 2015, the American Housing Survey is selecting a new sample cohort of housing units. The prior cohort was selected in 1985 and was interviewed every other year until 2013. With this new sample comes the opportunity to reexamine and revise the weighting methodology. The 2013 methodology included two sets of ratio adjustments that were combined in a raking procedure: one for known totals of housing units and another for population distributions. The adjustment for population distributions used the concept of the principal person to define the distributions. In our paper, we discuss the results of our research into two main questions. First, can we improve the principal person methodology of the current ratio adjustments and replace it with a calibration weighting adjustment to population totals? Second, can we combine population and housing unit ratio adjustments into one calibration adjustment? Here we examine whether one calibration adjustment can be employed to adjust for housing unit characteristics that include both housing unit and population characteristics. ","Analysts typically employ calibrated weights to improve their inferences. Calibration weighting can adjust for differing probabilities of response or coverage among the elements in the sampled population or it can reduce the impact of purely random error sources. Final weights may have undergone several calibration-adjustment steps. Each adjustment can increase the variability of the weights, which results in a decrease in the precision as conventionally measured. Relatively new software such as SUDAAN's WTADJUST procedure allows analysts to produce more accurate precision measures by calculating estimates during a calibration-weighting step rather than after. This means that calibrated weights need no longer be treated as if they were the original design weights (i.e., the inverse-selection-probability weights) in standard-error estimation. Using recalibrated sub-national estimates of victimization rates from the National Crime Victimization Survey, a nationally representative survey of persons in households, sponsored by the Bureau of Justice Statistics, we show how much this improved measure of precision can reduce estimated standard errors. ","The National Agricultural Statistics Service (NASS) conducts a Census of Agriculture every 5 years, in years ending in 2 and 7. For the 2012 Census of Agriculture, NASS used capture-recapture methods to adjust the Census for under-coverage, non-response, and misclassification of farms/non-farms. After these adjustments, the weights were calibrated and integerized. Calibration was conducted to ensure that state and national totals were unbiased for variables where administrative data were available. The integerization process rounded weights but did not change marginal totals. NASS researched alternative calibration methods applied to the Census. Here the constraints and limitations of those methods are discussed. ","There are two basic methods for the adjustment of non-interviews. In both you adjust the interviews for the non-interviews by applying a factor that is sometimes described as the inverse of the probability of completing an interview. One way to calculate the factor is directly as the ratio of the weighted count of eligible units (both completed interviews and non-interviews) with the weighted count of completed interviews. This is often done within cells to reduce bias, where cells are groups of similar units.    ","This paper introduces new estimators for population total and mean in a finite population setting, where ranks of population units are available before selecting sample units. The proposed estimator selects a simple random sample and identify their ranks in the population. Selection of the sample can be performed with- or without-replacement. The population ranks of the selected units of with-replacement samples are determined among all population units. On the other hand, the ranks of the selected units of without-replacement samples are identified in two different ways: (1) The rank of a selected unit is determined sequentially among the remaining population units after excluding all previously ranked sample units from the population; (2) The ranks are determined among all units in the population. By conditioning on these population ranks, we compute the inclusion probabilities, construc three new estimators, develop a bootstrap re-sampling procedure to estimate the variances of the estimators, and construct percentile confidence intervals for the population mean and total. The new estimators provide a substantial amount of efficiency gain over their competitors. ","Most of the existing research about the choice of missing data method for non-normal data has been carried out using binary data. This study however uses ordinal data to compare the different approaches of listwise deletion, mean imputation, and multiple imputation to determine how informative each method will be within an ordinal multinomial logistic model. Imputing categorical variables which are non-normal is challenging and it still is unclear which approach should be preferred (Lee et al., 2012). Considering the type of missing data (MCAR, MAR, or MNAR) is also important in determining how to handle missing values. In this study, after learning about the type of missingness by applying a logistic regression, an ordinal multinomial logistic regression is fitted to the ordinal data and within that model, different approaches of missing data are performed to evaluate the appropriateness of missing data handling procedures.  This comparison is done by applying these methods to a dataset on the length of stay for people with severe mental illness at a live-in healing community in North Carolina, which includes longitudinal ordinal and multinomial data containing missing values. ","There is a long history of devleopment of methodology dealing with missing data in statistical analysis.  Today, the most popular methods fall into two classes, Complete Cases (CC) and Multiple Imputation (MI).  Another approach, Available Cases (AC), has occasionally been mentioned                                     in the research literature, in the context of linear regression analysis, but has generally been ignored.  In this paper, we revisit  the AC method, showing that it can perform better than CC and MI, and we extend its breadth of application.                   ","A key component of a proteomics experiment is the estimation of relative protein abundance from peptide level measurements. Two statistical features of this inferential step are matched pairs data and non-ignorable missingness. Software often estimates proteins using a complete case analysis of peptide ratios. While missing data models usually fail to match peptides across samples. Here we develop the first statistical model that accounts for both critical features. Our simulation analysis shows that models based on average intensity suffer large losses to accuracy with basic ANOVA estimates having an average MSE 371% higher than median ratio estimates. In turn the method of medians, which ignores missing data, has an average MSE 35% higher than estimates from our M5 model. Analysis of tumor data reinforces these relationships and shows how our M5 model improves depth of discovery by enabling a 22% increase in the number of proteins estimated. Our assessment of models based on average intensity suggest that they should be categorically avoided. When compared with the method of medians, our model provides an alternative with improved accuracy and enhanced depth of discovery. ","Individual covariates are commonly used in a capture-recapture model as they can provide important information for the population size estimation. However, in practical applications, some covariates may be missing and that can lead to unreliable inference if the records with missing data were just ignored. This study considers the estimation problems when some covariates are missing at random. When some covariates are missing, the naive complete-case approach is shown to underestimate the population size. We develop methods for estimating regression parameters and population size based on regression calibration, inverse probability weighting and multiple imputation techniques without any distributional assumption about the covariates. A simulation study was carried out to investigate the effects of missing covariates and to evaluate the performance of our proposed methods. A data of bird species yellow-bellied prinia collected in Hong Kong was analyzed for illustration. ","We present a technique for using calibrated weights to incorporate whole-cohort information in the analysis of a countermatched sample. Following Samuelsen's approach for matched case-control sampling, we derive expressions for the marginal sampling probabilities, so that the data can be treated as an unequally-sampled case-cohort design. Pseudolikelihood estimating equations are used to find the estimates. The sampling weights can be calibrated, allowing all whole-cohort variables to be used in estimation; in contrast, the partial likelihood analysis makes use only of a single discrete surrogate for exposure. Using a survey-sampling approach rather than a martingale approach simplifies the theory; in particular, the sampling weights need not be a predictable process.   Our simulation results show that pseudolikelihood estimation gives lower efficiency than partial likelihood estimation, but that the gain from calibration of weights can more than compensate for this loss. If there is a good surrogate for exposure, countermatched sampling still outperforms case-cohort and two-phase case-control sampling even when calibrated weights are used. Findings are illustrated with data from ","We show how to recover marginal treatment estimates from a transitional incremental model with martingale random effects working assumption by using the generalized SWEEP (G-SWEEP) estimator.    Conditioning on intermediate outcomes is often used in settings with generally non-ignorable dropout to facilitate unbiased estimation in such cases. Estimates recovered from such a strategy are conditional on the past outcomes. Combining this with path analysis method that can recover marginal treatment effects from the conditional ones yields a method that effectively adjusts for non-ignorable drop-out and can be used to unbiasedly estimate total effect of the treatment.   The application of the G-SWEEP estimator to incremental longitudinal data performs an implicit path analysis and recovers the effect of treatment at baseline which is generally masked by conditioning on intermediate outcomes. ","Compliance in randomized clinical trials (RCT) is typically measured using self-report, pill counts and blood levels. In a longitudinal RCT the question arises as to how to define or analyze compliance when it may change over time. Intent-to-treat (ITT) is the primary approach to analyze longitudinal RCT. In this approach, all available data for randomized patients are included, regardless of compliance. In the per-protocol approach, only patients who were compliant with the protocol should be included. However, both ITT and per-protocol approaches may introduce bias in the longitudinal setting under various relations between compliance and response. A simple method is to define compliance on an observation level, rather than a patient level. A simulation study shows that results are varying among the different approaches to define and analyze compliance. In summary, if compliance is not related to outcome, or even moderately related to outcome, adjustment is not necessary in ITT analysis. If compliance is highly related to outcome, the adjustment is strongly suggested even when using the ITT population. ","Accounting for multistage survey sample design features when generating datasets for multiple imputation is a non-trivial task. Thus, multiple imputation often ignores complex sample designs and assumes simple random sampling when generated imputations, even though failing to account for complex sample design features is known to damage inference. Here we extend a recently-developed weighted finite population Bayesian bootstrap procedure (Dong et al. 2014) to generate synthetic populations conditional on complex sample design data that can be treated as simple random samples at the imputation stage, obviating the need to directly model design features for imputation. We develop two forms of this method: one where probabilities of selection are known at the first and second stage of the design, and the other, where only the final weight based on the product of the two probabilities are known. We show via simulation study this method has advantages in terms of bias, mean square error, and coverage properties over methods where sample designs are ignored, with little loss in efficiency even when compared with correct fully parametric models. ","Hot deck imputation is popular for handling item nonresponse in survey sampling.  In hot deck imputation, imputed values are taken from the respondents in the same imputation cell, where imputation cells are used to approximate the imputation model. We extend the fractional hot deck imputation of Kim and Fuller (2004) to the case where the imputation cells are not defined in advance. The proposed method of fractional hot deck imputation is performed in two steps and has a structure similar to that of two-phase systematic sampling. The proposed hot deck imputation method is applicable to multivariate missing data. A replication method is used for variance estimation. Results from two simulation studies are presented.  ","Calibration weighting can be used to remove bias when unit nonresponse is a function of one or more survey variables. This is done by allowing the model variables in the weight-adjustment function to differ from the variables in the calibration equation. An extension of calibration weighting allows there to be more calibration variables than model variables. Rather than equating the two sides of a calibration equation, the difference between the sides is minimized in some sense. This paper discusses some ways of doing that. A promising solution results instead from an alternative version of the calibration equation. A helpful insight into choosing calibration variables for given model variables follows.   ","Calibration is a technique developed in sampling survey literature. Its application in missing data analysis has attracted considerable research interests recently. We will discuss how calibration, combined with the empirical likelihood method, can lead to many desirable properties when analyzing incomplete data. Especially, the robustness against model misspecification can be significantly improved, resulting in the so-called multiply robust estimators. These estimators are consistent if any one of the postulated parallel parametric models is correctly specified. ","Nonresponse is frequently encountered in survey sampling. To correct for unit nonresponse, weighting adjustment using nonresponse propensity score is used. In cluster sampling, the missingness indicators are often correlated within clusters and the response mechanism is subject to cluster-specific nonignorable (CSNI) missingness. Based on a parametric model for the response mechanism incorporating the CSNI missingness, we propose a method of nonresponse weighting adjustment under cluster sampling. The proposed method provides a consistent estimator of the mean or totals when the study variable follows a linear mixed model. A consistent variance estimator based on Taylor linearization is also proposed. Numerical results, including a simulation and a real data application, are also presented. ","Space plasma simulations are known to generate vast amounts of numerical data. In recent times,with greater  availability of computer power, the proportion of data generated has increased exponentially posing new  challenges in its analysis. Simulations can be scaled up, but matching methods for analysis have not  been developed at the same pace, except in the industry setting where companies utilize big-data  techniques to build advanced analysis engines. Although many of these methods need developers working  with low-level big-data programming, today there exist powerful out-of-the box solutions that can be  easily employed. In this paper, using SAS Visual Analytics as one of these tools, we will demonstrate  how existing statistical tools and analytical platforms can be used to analyze simulations of the Sun  in a novel way. In our case, simulations generated roughly 660mb/time unit and hundreds of them were  needed to provide an adequate analysis. Thus, application of our method allows instant analysis and  prevents the need to guess \"when\" and \"where\" the interesting physical phenomena occurs, thus effectively  getting rid of data which has little or no scientific value.   ","Propensity modeling has been extensively used in telecommunication companies to optimize marketing outcomes in cross sell and up sell campaigns, retention tactics, recruiting strategies, etc. Uplift modeling, on the other hand, is less familiar territory due to its complexity1. Net lift models are reported as superior in terms of maximizing return on investment by some practitioners2,3, while others cautioned on its trade-offs and limitations4. Little was reported, however, on the comparison of these two techniques with respect to skewed data. Our research shows that for highly skewed data, while the net lift model produced much improved incremental sales rates compared to the traditional propensity model, the propensity model outperformed the net lift model in terms of number of incremental sales, due to its much larger segments.  ","Metabolic syndrome is a complex, polygenic condition comprised of a cluster of risk factors that can result in chronic diseases including cardiovascular diseases, cancer, arthritis, and Type II diabetes. Risk factors for metabolic syndrome include abnormal lipid levels, hypertension, insulin resistance, abdominal obesity, and genetic susceptibility. Genes that may predispose individuals to metabolic syndrome are not fully identified. Osteopontin (OPN) is an inflammatory cytokine, ubiquitous in body tissues, that regulates tissue repair and energy metabolism.     ","This roundtable will discuss the American Association for Public Opinion Research (AAPOR) Big Data Task Force report (release Feb. 2015). There is great potential in Big Data, but there are some major challenges that have to be resolved before its full potential can be realized. The AAPOR task force report gives examples of different types of Big Data and their possible use for social science research. Big Data has a number of advantages when compared to survey data. An obvious advantage is that these data already exist in some form, and therefore research based on Big Data does not require a new primary data-collection effort. Primary data collection is usually expensive and slow. The task force concludes that surveys and Big Data are complementary data sources. It also describes the Big Data process and discusses its main challenges. Because there is no clear legal and ethical framework for the collection and subsequent use of Big Data, it is important for AAPOR to work with other organizations such as the American Statistical Association to resolve potential ethical and legal issues and unlock the great potential of Big Data for the public good. ","In the era of big data, sampled data is still important in controlling the usage of storage and logging resources as well as in creating smaller data sets for which analyses may be run quickly or for which more complex analyses become feasible. We present methods for constructing simple random samples in a distributed and streaming fashion as well as constructing the largest possible combined simple random sample from a set of simple random samples. ","Analysts at large web firms are often given the task of analyzing and processing tremendous amounts of data in a quick, iterative fashion. Oftentimes, this involves formulating a sequence of hypotheses to test, each of which queries the same data multiple times but analyzes a different stratum. This presents challenges when each query must be answered in a short amount of time and when computational resources are constrained. Using subsampled data is one way to reduce both the time and computational cost while still being able to provide statistical insights about the data. Furthermore, when the subsampled data is small enough to store in memory, subsampling can greatly increase the set of software packages that may be used for analysis. However, drawing a useful subsample can be problematic when the data is severely skewed, with a few strata dominating the others in size. This work proposes a novel streaming method for drawing a stratified sample from a stream where the memory budget is constrained, the data may be very skewed, and the number of strata of interest is potentially very large. ","Respondent driven sampling (RDS) is a social network link-tracing sampling technique for studying marginalized or hard to reach populations. Previous literature models the referral process as a random walk on the social network. However, participants in RDS are incentivized to refer between three and five future participants. To account for the branching structure of the process, this talk studies the RDS process as a Markov chain *indexed by a tree*. Several results follow. An exact formula is given for the variance of the popular Volz-Heckathorn estimator (VH). There is a critical threshold that relates the referral tree to the social network and past this threshold, VH is not square root N consistent. ","Respondent-driven sampling (RDS) is a popular methodology for sampling hard-to-reach populations to estimate what proportion have a certain condition, for example HIV. In this approach, participants in the study refer people they know into the study; however, many participants fail to refer future participants. If enough fail to make referrals, then the referral process dies and the researchers fail to attain the target sample size. We invoke tools from branching process theory to study this failure. Specifically, modeling the referral process as a multi-type Galton-Watson process, we study the extinction of the process and propose estimators for the extinction probability. We use these ideas to explore sample size calculations for RDS studies. Using these tools, we examine extinction in previous RDS studies.   ","The network scale-up method enables researchers to estimate the size of hidden populations, such as drug injectors and sex workers, using sampled social network data. It offers advantages over other size estimation techniques, but the basic scale-up estimator depends on problematic modeling assumptions. We propose a new generalized scale-up estimator that does not suffer from these problems. The new estimator can be used in settings with non-random social mixing and imperfect awareness about membership in the hidden population. Further, the new estimator can be used when data are collected via complex sample designs and from incomplete sampling frames. However, the generalized scale-up estimator also requires data from two samples: one from the frame population and one from the hidden population. In some situations these data from the hidden population can be collected by adding a small number of questions to already planned studies. For other situations, we develop interpretable adjustment factors that can be applied to the basic scale-up estimator. We conclude with practical recommendations for the design and analysis of future studies. ","This research is focused on the selection of predictors for non-response analysis and weight adjustments for school survey data. Weighting classes for non-response adjustments are formed using a set of core variables that are correlated with response behavior and with survey outcomes, with the intent of minimizing the potential non-response bias (balanced against acceptable increases in weighting variance). The choice of variables to use when defining weighting classes is determined by measures of variable importance for predicting response. This study compares the use of response propensity models estimated using mixed effects logistic regression and a range of recursive partitioning methods for evaluating variable importance and the resulting bias reduction achieved through non-response weighting adjustments. Mixed effects models are run using SAS Proc Glimmix that allows modeling of random effects (e.g. school, class) and works best for modelling response indicator variables that follow exponential distributions. ","The National Adult Education and Training Survey (NATES) was a 2013 pilot study sponsored by the National Center for Education Statistics to evaluate whether high-quality data on adult education, training, and credentials for work could be collected using a mailed household survey. A unique aspect of the NATES pilot was its nonresponse follow-up study, in which a random subsample of nonrespondents to the mailed survey was selected for in-person interviews using a shortened questionnaire. This paper uses data from the nonresponse follow-up interviews to estimate the extent of unit nonresponse bias in key NATES estimates. Base-weighted estimates are compared to nonresponse-adjusted estimates in order to assess the efficacy of standard weighting class adjustments at mitigating the observed bias, using a set of commercially purchased household-level auxiliary variables appended to the NATES sampling frame. Associations between the auxiliary data and key NATES outcomes are observed in order to further assess the potential utility of commercial auxiliary data at correcting for nonresponse in future administrations. ","The Defense Research, Surveys and Statistics Center (RSSC) is responsible for surveys that involve all military services (Army, Navy, Marine Corps and Air Force). The results of these surveys are often mandated by Congress and results are used by policy offices to evaluate current policy and when necessary update policy. Response rates from these surveys are generally low when surveying the Active Duty, Reserve Component and their spouses. Strong and accurate administrative data has proven to be quite beneficial for adjusting for non-response, but in order to also further prove the accuracy of the estimates,, RSSC conducts non-response bias analysis.    ","Measures of nonresponse bias can provide information on survey quality through response rates and models of nonresponse propensity. This study uses contact history, demographic information, and prior responses to model longitudinal nonresponse bias in the Current Population Survey (CPS) estimates of labor force participation. Previous research into patterns of nonresponse rates in the CPS show seasonal and rotation group effects (Dixon, 2007 JSM proceedings). Similar patterns are examined for the measures of bias in labor force measures using nonresponse propensity models. These patterns may supplement the current data quality monitoring for the CPS as well as provide insights into potential sources of bias. ","Unit nonresponse in face-to-face household surveys using probability samples can affect the quality of survey estimates. Although researchers can use a variety of tools for reducing unit nonresponse rates, most efforts in this area have focused only on the design features to improve survey participation such as number of calls, prenotification, incentives, or mode switch. However, those tools can be ineffective when controlled access situations or facilities (e.g., high-rise apartment buildings with locked central entrances or gatekeepers) are encountered. In these situations a different strategy or effort will be required to bring the sampled person into the respondent. In this paper, we present how using a special strategy based on an administrative system or process can increase response rates in a rare population survey on the use of illicit drugs in South Korea, one of the societies becoming increasingly security- and privacy-minded. In addition, we show the results on survey quality improvement in using the strategy. ","A method to monitor survey outcomes during fieldwork is proposed. It assesses nonresponse bias using call record data by comparing estimated and true distributions of specific survey variables at each call attempt using dissimilarity indices. These indices are compared with other indicators, such as response rate, R-indicators and coefficients of variation. Results for data from waves 1 and 2 of the UK Household Longitudinal Study show that survey estimates tend to stabilise after around 5 call attempts and, in some cases, at levels that depart significantly from the corresponding true values even after high response rates are achieved. The study demonstrates that most indicators in current use, although adequate to assess nonresponse bias after data collection, are not effective in capturing nonresponse bias during the call process. The study concludes that dissimilarity indices and coefficients of variation have the best properties. This research has direct implications to responsive and adaptive survey designs. Focusing on simply achieving high response rates is not cost-effective and considering alternative indicators is crucial to guide real-time data collection strategies. ","Diminishing survey response rates are threatening the reliability and generalizability of survey results. Maintaining high participation is particularly important for longitudinal studies such as the National Children's Study (NCS), where attrition tends to increase over time. For the NCS, address quality impacts communication with participants, completion of mail surveys and in-person specimen collections, and linkage to existent data used to reduce response burden. For these reasons, an accurate participant mailing addresses is crucial. In this paper we test the use of Delivery Point Validation (DPV) codes and Residential Delivery Indicators (RDI) in an NCS mailing.     ","Respondent-driven sampling (RDS) is often used to investigate populations for which a probabilistic sample cannot be efficiently generated. RDS has become a common approach for recruiting hard to reach populations. Because RDS is not a probability sample design, standard design-based estimation approaches are not applicable. To obtain survey based estimators, researchers make a number of assumptions about the recruitment process and the underlying social network in order to calculate inclusion probabilities. We borrowed from the spatial statistics literature to improve the prediction of prevalence estimates at the regional level. Our proposed approach can be used in the identification of spatial patterns of the population of interest. This is possible by attaching participant geo-reference info to the data (e.g. latitude-longitude of the recruiting center, mailing or surrogate address of participants). This approach can be useful to evaluate the spatial distribution of the sample; to detect and characterize clusters of cases. We illustrate this approach with data collected by FHI360 in a study of black population at risk for HIV in Durham, NC. ","The June Area Survey is conducted by the National Agricultural Statistics Service each year to estimate agricultural activity in the USA. The survey design is a labor and time intensive process. We develop a sampling design that creates a permanent area frame, stratifies the frame using multivariate auxiliary data to meet specification criteria on estimates, and then spatially balance the sample using the local pivotal method with rotation sampling over time. The new sampling design reduces cost in sampling frame construction and stratification compared with the current method. The coefficients of variation for total estimates are reduced in the new design for all crops in Indiana and North Carolina and most crops in South Dakota for the same sample size. ","RTI International is currently performing a multi-national survey requiring the identification of residences that are accessible to interviewers and provide an appropriate demographic cross section. To meet this challenge, RTI designed a grid based sampling method that uses inputs from multiple Geographic Information System (GIS) layers and available spatially defined demographic data.  Compiling the geospatial and census data for the sampling process has proven particularly complex due to the inconsistency of readily available data across countries and the variability of administrative structure. The methodology, while transferable to all countries, needs to be elastic to allow tailoring for spatial and temporal differences between areas.  In addition, RTI has researched the availability and effectiveness of geospatial data layers, to improve the design for operational reasons. Strategies used include ways to minimize the inclusion of non-residential areas, plan for difficult to access areas, and account for exclusions due to security. These enhancements have the potential to greatly increase data collection efficiency and reduce the need to supplement the sample during field work.   ","Multi-stage cluster designs are employed in area probability household surveys. Samples of primary sampling units (PSUs) are selected at the first stage; within a PSU, smaller geographical areas are selected at subsequent stages; and households are selected at the final stage. Many area probability household surveys use single counties or a combination of contiguous counties is used as PSU. The Census Public Use Microdata Areas (PUMAs) are defined for the dissemination of Census public use microdata and American Community Survey (ACS) data. We used the PUMAs as PSUs in two national surveys. In this paper, we describe the benefits of using PUMAs as PSUs instead of counties. We also present the results of two simulation studies designed to address concerns (the field costs and the coverage of major metropolitan areas) with using PUMAs as PSUs.  ","The National Agricultural Statistics Service (NASS) conducts the annual June Agricultural Survey (JAS), which is based on an area frame. Segments of land comprise the sampling units for the JAS. Finding and interviewing all farm operators can be challenging and costly, especially in previously unenumerated segments. In highly-cultivated land areas, names and addresses obtained from the Farm Service Agency often provides good starting information to identify operators within the selected land area. However, in areas with small-scale agriculture, screening to identify farm operators is often time-consuming, expensive, and subject to misclassification. In 2012 and 2014, geo-referenced county assessor parcels were intersected with the sampled JAS segments to reduce prescreening costs, raise efficiency in data collection, and reduce misclassification of farms. Controlled experiments were conducted to evaluate the usefulness of the county assessor's data in the identification of more agricultural tracts. This paper presents the results of these controlled experiments. ","The Consumer Expenditure Survey (CE) is a nationwide household survey conducted jointly by the U.S. Bureau of Labor Statistics and the U.S. Census Bureau to investigate how Americans spend their money.  Every ten years the survey updates its sample of geographic areas around the country as well as its sample of households in those geographic areas based on the latest decennial census to ensure the sample accurately reflects shifts in the American population.  This paper describes CE's latest sample design that will be used over the next ten years (2015-2024), including research that went into its decisions.  Topics include the coordination of CE's household sample with other household surveys conducted by the Census Bureau, and a new annual sampling methodology used by all Census Bureau household surveys. ","Calibration of weights allows auxiliary information available for a whole population or cohort to be used in estimation for a subsample, without any additional model assumptions: the resulting estimators are design-consistent when the sampling weights are correctly specified even if a misspecified model is fitted. If the model is also correctly specified, more efficient estimators are typically available, but these are not typically design consistent. I will show that the efficiency gain of these estimators is not robust to minor model misspecification, and argue that calibration estimators are preferable when the sampling probabilities are known, with double-robust estimators being preferable when the sampling mechanism must be modelled. ","This talk consists of two parts. We first provide an overview on methods for obtaining range-restricted calibration weights, with main focus on efficiency and computational simplicity. We then explore several issues with design-based   inferences using range-restricted calibration weights, including variance estimation for descriptive population parameters, survey weighted estimating equations, and the pseudo empirical likelihood based inferences. ","Weighted estimating equations are frequently used to estimate parameters in surveys and randomized experiments in settings where there is missing data due to nonresponse. The weights are generally related to the reciprocals of the \"propensities\" or conditional probabilities of response for individual subjects in terms of subject-level covariates which are known or observed even for nonresponding subjects. The consistency properties for the resulting estimating equations depend crucially on the \"Missing at Random\" (MAR) property for outcome variables in terms of the covariates used to construct the weights. Yet the MAR property may hold only when propensities reflect covariates observable simultaneously with outcome-variable responses. This talk describes settings in which such propensities may be used to adjust for nonresponse, consistently and sometimes efficiently, when population-level information about the full set of covariates is known or estimated from external data sources. The methods will be illustrated using data from the American Community Survey.    ","We consider improving the efficiency of the weighted likelihood estimator (WLE) under two-phase stratified sampling in a general semiparametric model. Our proposed method uses observations not sampled at the second phase. This idea has been examined in various ways both in biostatistics and survey sampling. Estimating weights in the WLE is a representative method in biostatistics where two-phase sampling is often treated as a special case of missing data problems that assume independence for mathematical convenience. Calibration and post-stratification are standard methods in general complex survey where observations are dependent through sampling. We prove that these existing methods are asymptotically equivalent under certain transformations of variables. Moreover, we show that these general techniques may not guarantee efficiency gain in our sampling scheme. Our method is tailored to two-phase stratified sampling, and necessarily makes improvements on the WLE in this sampling design as well as stratified Bernoulli sampling. We illustrate our theoretical results in a simulation study and real data example. ","Motivated by the data obtained from the BodyMedia FIT device, we take a functional data approach for longitudinal studies with continuous proportional outcomes. The functional structure depends on three factors. In our three-factor model, the regression structures are specified as curves measured at various factor-points with random effects that have a correlation structure. The random curve for the continuous factor is summarized using a few important principal components. The difficulties in handling the continuous proportion variables are solved by using a quasilikelihood type approximation. We develop an efficient algorithm to fit the model, which involves the selection of the number of principal components. The method is evaluated empirically by a simulation study. ","The first regression tree method, AID, was developed more than fifty years ago specifically for analysis of survey data. Numerous algorithmic advances have appeared since and the methodology is popular in many areas of business, science, and industry. Applications to complex sample surveys, however, are relatively few. This paper aims to demonstrate new ways of analyzing survey data using the GUIDE algorithm. ","In the computation of survey weights to be used for the analysis of complex sample survey data, an adjustment for nonresponse is often an important step in reducing bias. These adjustments depend upon estimated response propensities, which are traditionally obtained through empirical response rates within weighting classes or through logistic regression modeling. In this paper, we discuss possible benefits of using regression trees and random forests for estimating response propensities in surveys, and describe how these models might be used to reduce nonresponse bias. We review issues for their use with complex surveys such as the effect of survey weights and clustering, pruning criteria, and loss functions, and we explore the sensitivity of results to these conditions. ","We introduce a recursive algorithm to represent an arbitrary regression tree as a linear function of the splits. This representation is particularly useful for applications involving survey data. For instance, this representation makes the splits more interpretable when doing nonresponse analysis and aids checking for potential model misspecification and estimating the conditional sample variance. We demonstrate the added interpretability and usefulness with applications on Bureau of Labor Statistics survey data. ","Adaptive survey design researchers and practitioners tailor data collection features to sample units to maintain data quality while reducing cost or improve data quality within a fixed budget. Examples of treatments include how sample units are contacted, the level of incentive a sample unit receives, or whether further attempts on a unit should be made. To tailor treatments, survey designers use data known about sample units before data collection begins as well as response data and operational paradata collected during data collection.    ","The National Survey of College Graduates (NSCG) allows for static adaptive design, defined by Bethlehem, Cobben and Schouten as \"An adaptive design where the strategy allocation depends on linked data only\" (2011). In other words, contact strategy tailoring depends only on data known before data collection starts and does not depend on paradata collected during data collection. Functionally, this means making changes to the data collection between survey rounds based on information learned during previous rounds, rather than dynamically during the data collection period. The NSCG combines a survey sponsor open to embedded experiments with continuous monitoring of and learning about the respondent population throughout rounds of data collection. This allows the NSCG to regularly make changes that can improve data collection across various dimensions including: maintaining data quality, increasing timeliness, and controlling costs.   In 2013, the National Survey of College Graduates (NSCG) included an incentive timing experiment, which consisted of sending prepaid monetary incentives to influential sample persons at one of four times during the twenty-three week data collection period ","Respondent-driven sampling (RDS) is a sampling mechanism that has proven very effective to sample hard-to-reach human populations connected through social networks. A small number of individuals typically known to the researcher are initially sampled and asked to recruit a small fixed number of their contacts who are also members of the target population. Each subsequent sampling waves are produced by peer recruitment until a desired sample size is achieved. However, the researcher's lack of control over the sampling process has posed a number of challenges to producing valid statistical inference from RDS data. For instance, it is often assumed that participants recruit completely at random among their contacts. However, it has been observed in practice that participants sometimes systematically recruit with a higher probability individuals with a specific characteristic. Existing literature suggests that RDS prevalence estimators are greatly sensitive to this assumption. The main contribution of this study is to propose a Model-Assisted approach to correct current RDS estimators for the bias introduced by preferential recruitment. ","Respondent-Driven Sampling (RDS) is a method for sampling hard-to-reach populations. It samples population members via their social networks by having sample members recruit their contacts into the sample. Estimation of population characteristics with RDS data is challenging due to the unobserved population network, and multiple estimators are currently used. Past work has focused on point estimation, and no evaluation of currently used variance estimators exists. We evaluate the performance of RDS variance estimators via simulations of RDS on synthetic networked populations. The networks and RDS sampling processes are based on 40 surveys of injection drug users from CDC's National HIV Behavioral Surveillance system. In these simulations, average design effects (DEs) are lower and average 95% confidence interval (CI) coverage rates are higher than suggested in previous work, with average CI coverage of 93%. However, DE and coverage vary across the 40 sets of simulations, suggesting that the characteristics of a given study should be evaluated to assess performance. We also find that simulation results are sensitive to parameters such as sampling with replacement. ","Survey researchers implement adaptive survey designs that tailor data collection features to sample units in an effort to maintain data quality while reducing cost or improve data quality within a fixed budget. To tailor treatments, researchers use data known about sample units before data collection as well as response data and operational data. Adaptive survey designs also require metrics to assess data quality, data collection progress, and field procedures as well as tools including reports and graphs to monitor those metrics throughout data collection. This presentation discusses the monitoring of field procedures during the 2015 Survey of Income and Program Participation, a longitudinal survey administered by the U.S. Census Bureau. We discuss specific metrics, including interviewer workload, interviewer effort, and field management. We detail why monitored these metrics and how we developed the reports to do so. We end with a discussion of how these monitoring systems could be used to implement an adaptive design intervention in the next round of SIPP data collection.    ","Over the past decade, there has been increasing interest in responsive and adaptive survey designs. These designs use paradata and, in some cases, survey response data gathered throughout the data-collection period, to determine whether and when to modify survey design features. The reasons for changing survey protocol vary -- some survey managers want to maximize response rates for a fixed level of effort, some want to reduce respondent burden or harassment, and others want to reduce costs while maintaining sample balance. While these goals are varied, many adaptive survey designs require estimation of response propensity models. Furthermore, most of the large, national surveys conducted by the U.S. Census Bureau collect a common form of paradata with the Contact History Instrument (CHI). This paper explores the use of these paradata in the construction of response propensity models for several surveys that vary widely in their content and design features. ","We propose a Bayesian phase I/II dose-finding trial design that simultaneously accounts for toxicity and efficacy. We model the toxicity and efficacy of investigational doses using a flexible Bayesian dynamic model, which borrows information across doses without imposing stringent parametric assumptions on the shape of the dose-toxicity and -efficacy curves. An intuitive utility function that reflects the desirability trade-offs between efficacy and toxicity is used to guide the dose assignment and selection. We also discuss the extension of this design to handle delayed toxicity and efficacy. We conduct extensive simulation studies to examine the operating characteristics of the proposed method under various practical scenarios. The results show that the proposed design possesses good operating characteristics and is more robust to the shape of the dose-toxicity and -efficacy curves than other phase I/II designs. ","It has been shown that using Contura Multi-Lumen Balloon (MLB) breast brachytherapy catheter to deliver accelerated partial breast irradiation in patients with early-stage breast cancer achieved statistically significant improvements in dosimetric capabilities compared to central-lumen single-dwell and multidwell treatments (Arthur et al 2013). However, seroma is a common toxicity after this surgery. In a clinical trial, 341 patients received MLB treatment, out of whom more than 32% patients developed seroma after surgery. One of the goals of this clinical trial is to reveal the relationship, if there is any, between the formation of seroma and dosimetric parameters. Logistic regression is used to examine the relationship, and possible predictors include age, treatment target, maximum skin and rib dose, percentage of prescribed dose, and volume of tissue receiving 100, 150, and 200 percent of prescribed dose. Initial investigation shows that maximum skin dose and volume of tissue receiving 200 percent of prescribed dose are potentially associated with formation of seroma. Detailed analysis with the nature of association will be presented in this report.  ","Propensity score (PS) and disease risk score (DRS) are popular covariates balancing techniques for observational studies. However, few has been reported on the performance of prognostic propensity score (DRS-PS) method which combines the two techniques together in covariate adjustment setting. We conducted simulations to evaluate the performance of three DRS-PS models, a probability-based PS models, and an inverse probability treatment weighting using PS model (IPTW).  We observed, in general, IPTW method out-performed other methods models, particularly in the scenarios with rare outcome, lower exposure rates and high relative risk (RR). Among three DRS-PS models, the full-cohort DRS-PS method usually performed better than the other two methods. It sometimes even out performed PS and IPTW. DRS-PS performance generally improved when outcome and exposure rates increased, and RR decreased. IPTW and the full-cohort DRS-PS method usually had better mean squared errors than PS and other two DRS-PS methods.   Users need to be aware of outcome and exposure rate before choosing an appropriate method to estimate RR.   ","The application of sample size calculation and power analysis in preclinical discovery research is not a straightforward task. Unlike its clinical counterpart, where sample size calculations are standard procedure, the utility of sample size calculations is less clear in the preclinical world. Here we present a selection of case studies of sample size calculation and power analysis application in preclinical discovery research in the biotechnology field. We include both in vitro and in vivo studies in a variety of therapeutic research areas. We will discuss challenges faced by statisticians advising in the field, including working within 3R principles (Replace, Reduce, Refine), educating scientists on the relevance of sample size calculations and addressing multiple testing considerations, resource limitations, historical precedence, and the reliance on a p-value measure of success. ","In conventional group sequential designs both conservative and relaxed alpha spending functions are used with sequential monitoring of clinical data. This is an effort to adjust alpha over time with an idea developed through a stepwise approach within each interim look capturing statistical information in an efficient scheme under a stratified study design. This alternative approach considers observed and expected efficacy as realized through a process over time. Thus, informative B-values over time will allow to extrapolate efficacy of the study-end with a proposed stopping rule within each stratum and the whole study, with an ongoing dynamic adjustment of type-I error. In order to evaluate the comparative utility of this proposed alternative approach with standard O'Brien and Fleming and Pocock methods, a simulation will be performed with a hypothetical stratified clinical trial design.     ","In phase III confirmatory trials, it is typical to have both a primary endpoint and a secondary endpoint to characterize the treatment effect for a regulatory claim with two active doses of study drug compared to placebo. Rigorous statistical methods are available to provide strong control -of the study wise type I error rate. Typically, the secondary endpoint is under powered compared with the primary endpoint. We conducted a set of simulations in this setting to compare the performance of several popular methods (including graphic approach, truncated Hochberg method, matched gate keeping method, and gatekeeping testing via adaptive alpha allocation etc.), and then provide recommendations for this type of study design. Other eligible designs are evaluated as well to evaluate the potential savings in sample size. ","In the presence of missing data in clinical trials, sensitivity analyses should explore the potential impact of the missingness on the reliability of key results. One approach, coined a tipping point analysis in recent literature, is to vary assumptions about the missing outcomes in the experimental and control arms in order to identify scenarios under which there is no longer evidence of a treatment effect. Then, the plausibility of those assumptions can be discussed. Previous tipping point approaches (e.g., Yan et al., 2009; Liublinska and Rubin, 2014) relied on single or multiple imputation of outcomes in patients with missing data. We introduce a simple, alternative tipping point approach in which inference on the treatment effect is based on the observed data and the sensitivity parameters, with minimal assumptions and no need for imputation. The sensitivity parameters to be varied are the mean differences between outcomes in dropouts and outcomes in completers on each of the two treatment arms. We derive the asymptotic properties of the proposed statistic and illustrate the potential utility of this approach in the regulatory setting. ","Beginning in 2017, the Economic Census will begin using the North American Product Classification System (NAPCS) to produce economy-wide product tabulations. This marks a major departure from the current collection method that explicitly links products to industry. Motivated by this collection change, the U.S. Census Bureau conducted a study to investigate methods of treating missing product data in the Economic Census, with the goal of recommending a single imputation method to produce product data in all trade areas that is statistically defensible and operationally practical. The validity of an imputation method is highly dependent on the nature of both the reported data and on the nature of missing data (e.g., factors that contribute to response). This paper presents an exploratory data analysis of empirical data from selected industries with common products under NAPCS at the national industry level that explores these factors, describing the methods and presenting the results. The collective results were used to recommend candidate imputation methods, to develop imputation cells, and to inform the subsequent evaluation study by providing realistic response propensity models. ","The Economic Census collects general items from business establishments such as total receipts, as well as detailed items such as product data. Although product data are an essential component of the Economic Census, they vary by establishment and across trade areas, can be difficult to collect, and are characterized by low item response rates. Beginning in 2017, the Census Bureau will begin using the North American Product Classification System (NAPCS) for economy-wide product tabulations. Under NAPCS, products are no longer linked to industry, so we seek a single imputation method for all products. We present two regression models for these data: Ratio imputation and sequential regression multivariate imputation (SRMI). The ratio estimator uses a simple linear regression model with total receipts as the single predictor and product receipts as the estimated value. The SRMI method proposed by Raghunathan et al. (2001) imputes missing values consecutively by fitting a sequence of regression models to estimate product receipts conditioning on observed and imputed variables. We present the methodologies, implementation, and application to missing product data imputation. ","Failure to treat missing data in survey or census data may introduce bias in the tabulated totals. The Census Bureau is seeking a common adjustment method for more than 6,000 products collected in the Economic Census. Unlike the general statistics items collected from every eligible establishment such as annual payroll and total receipts, product information varies by establishment and administrative data are not available for substitution or validation. Moreover, product data are characterized by poor item response rates, few available predictors, additivity constraints within establishment, and definitional rules such as mutually exclusive products in some cases and required products in others. Little historic data are available for modeling. Hot deck imputation provides a flexible approach to dealing with missing data that retains multivariate relationships without making explicit parametric model assumptions. Instead, hot deck methods impute missing values using reported values from a similar unit. In this paper, we examine the merits of random and nearest-neighbor hot deck imputation using empirical and simulated data for selected Economic Census products. ","In preparation for the 2017 change to the North American Product Classification System (NAPCS), Economic Census staff was tasked with determining a single imputation method to treat missing product data collected from all trade areas. To objectively compare four proposed imputation methods, we conducted a simulation study to obtain two evaluation measures: imputation error (IE), to measure the accuracy of the overall estimate, and the fraction of missing information (FMI), to measure the precision of the imputed estimate. For the \"cook-off,\" we generated complete pseudo populations by applying each imputation method to missing sample data, inducing product nonresponse in each population, and applying each imputation method to the missing data. Nonresponse was induced independently in each pseudo population, yielding 50 replicates. Each imputation procedure was multiply-imputed within replicate. Imputation methods (\"treatments\") are evaluated within trade area using the averaged IE and FMIs. This evaluation approach is generalizable to other programs with similar missing data problems. ","Recent research has attempted to decompose interviewer variance into its potential sources, using the Total Survey Error framework. These sources include sampling error variance, or a breakdown of interpenetrated assignment of sampled cases to interviewers; nonresponse error variance, or variance among interviewers in the types of cases recruited; and measurement error variance, or variance among interviewers in mean response errors. No research to date has attempted to compare this decomposition for different interviewing techniques. In this study, we will perform a descriptive decomposition of the interviewer variance associated with conversational (CI) and standardized (SI) interviewing. We will present results from an experimental study mounted in Germany, where a national sample of employed individuals was measured on a variety of complex employment items by interviewers randomly assigned to use either CI or SI. This study featured an interpenetrated area sample design, enabling estimation of interviewer variance, and the presence of administrative information on the sampling frame enabled estimation of interviewer variance in the various sources of error for each technique.  ","The Manufacturing Energy Consumption Survey (MECS) is a national sample survey that collects energy-related building characteristics, energy consumption, and energy expenditures of manufacturing businesses in the United States every four years. The U.S. Energy Information Administration (EIA) sponsors the MECS and the U.S. Census Bureau collects the data. For the 2014 MECS, EIA proposed implementing a modified estimation model of energy use in the United States. In order to provide the data necessary to make the modifications to the estimation model, EIA developed seven questions specific to the flow of energy through asphalt plants and seven questions specific to the flow of energy through petrochemical plants. Cognitive testing with petrochemical plants and asphalt plants revealed not only potential measurement errors related to these seven questions, but also the existing paper questionnaire. This paper highlights the findings from cognitive testing and discusses their implications not only to the 2014 MECS, but also to questionnaire design best practices for business surveys. Specifically, we focus on lessons learned on using the correct terminology, appropriate questionnaire formatting, and being cognizant of question order effects. Additionally, the paper outlines the steps EIA and the U.S. Census Bureau are taking to reduce measurement error in the 2014 MECS and future data collection cycles.  ","This paper presents a quality assessment of a critical subpopulation within a national survey. The National Crime Victimization Survey (NCVS), sponsored by the Bureau of Justice Statistics, is a survey of U.S. households which measures non-fatal crime victimization counts and rates as well as characteristics of victimizations. The survey interviews all persons 12 years and older living in sampled households and uses the same survey instrument for all respondents. Juveniles (those 12 - 17 years old) have a high potential for various types of non-sampling error including nonresponse and measurement error. Sources of these non-sampling errors may include parental monitoring affecting the interview, cognitive ability to understand the instrument, and lack of availability to participate in the survey. Moreover, if participation rates are low among juveniles, the precision of the estimates may be inferior to adult estimates. This paper conducts a secondary analysis of the 2006 - 2012 NCVS to assess the quality of juvenile estimates and discusses the reasons for potentially lower quality estimates. Descriptive analyses of paradata and survey data are presented and discussed. ","In panel surveys, respondents are asked to report the same information several times, e.g. for a different reference period or to update their previous answers. Survey designers must decide whether or not to provide respondents with their previously reported data (PRD). Research on household surveys suggests that the impact of giving prior information can have differential effects, depending on the stability, saliency and complexity of the question topics. However, there is not much known about the impact of PRD in establishment surveys. This study seeks to provide information to allow survey designers to make better decisions about the use of PRD in their survey. Building on prior research, we explore the impact of PRD on data quality in two ways. First, we compare data from the Occupational Employment Survey (OES) with Quarterly Census of Employment and Wages (QCEW) data that were used as PRD during the OES interviews. Second, we code interviews for the Agricultural Labor Survey to classify respondent reactions when PRD is used. This research will determine the impact of PRD on the response process and data quality, allowing survey designers to make informed decisions. ","For complex surveys, one of the most effective tools for reducing nonsampling survey error is interviewer comments. In the Survey of Consumer Finances (SCF), these comments have been particularly useful in explaining unusual respondent situations, allowing editors to alter case data after the interview has been completed in order to restore the data to the state they should have been in had they been correctly gathered originally. However, this method of error reduction is also extraordinarily time consuming, requiring months of careful analysis by multiple editors. Furthermore, in order to even become a qualified editor, extensive training is required, necessitating even more time. Given these twin issues, any method for speeding up the training and editing processes while still maintaining the data quality improvements they generate is worth exploring. With this goal in mind, a system was designed to incorporate survey data, interviewer comments, and a series of data checks into a single, easy-to-use program interface. Perhaps most helpfully, the program also generates financial summary sheets-such as a household balance sheet and an income statement-for the quick identification of anomalies. Using this system, data editors can, at a glance, understand the basic fundamentals of a case, identify potential problems, make corrections, and then check to see that these corrections did not create further issues. The program also serves to encapsulate knowledge about the survey that previously had to be memorized during the training process. This program, named the Editor Assistant (EA), was fully employed for the 2013 SCF and was used to swiftly train three new editors and speed up the editing process. The four- to five-month reduction in required editing time-compared with previous years-has been credited in large part to the EA. ","Survey respondents often round their answers to questions about quantities such as number of cigarettes smoked, annual income, or amount of time or money spent on tax preparation. This rounding can create heaping at particular values. For example, a person might report an income of $50,000 when the actual value is $47,332. This will cause a histogram of the data to have a higher value at $50,000 than in neighboring bins. In some cases, however, there may be a true spike causing the heaping: if many persons use a tax preparation software package that is priced at $40, some of the data heaping at $40 may represent the true value rather than a rounded quantity. We develop a likelihood-based approach for estimating the underlying distribution of a quantity of interest when the values are heaped, using a mixture model to capture components of true spikes as well as rounding.    ","The USDA's National Agricultural Statistics Service (NASS) launched a research effort to identify the causes of significant discrepancies in reporting of land related variables (in particular total farm acres operated) between the 2012 Census of Agriculture (COA) and June Agricultural Survey (JAS). NASS conducts the JAS (a probability-based sample survey of U.S. farm operators) annually and the COA (a complete enumeration of U.S. farms and ranches) every five years. JAS records were matched to corresponding ones from the COA (both unedited and edited), with those having absolute acreage differences exceeding a preselected threshold categorized as discrepancies and subjected to further investigation. The degree of influence of explanatory variables such as type of farm, number of operators and average drought level during the JAS data collection period on percentage and size of discrepancies associated with total acres operated was evaluated using descriptive statistics and logistic regression.     ","Variance computation requires the presence of two or more units from each stratum in the population. However, in sampling practice, the final data set may include \"singleton\" strata (i.e. with single sample observations). To enable variance estimation in the presence of singletons, statisticians must apply fixups. Some, if not all, of the currently available statistical software include fixups which statisticians may select to enable the software to approximate variances when singletons are present in the data set. However, anomalies in variance results produced by some software fixups have been observed for some singleton situations. Some examples of those singleton situations and the resulting anomalous variances are presented in this paper along with descriptions of manual fixups applied to eliminate the observed anomalies. ","New mobile communications technologies provide a unique opportunity for innovation in public health surveillance. Smartphone Web access is immediate, accessible, and confidential, a combination of features that could make it ideal for ongoing surveillance. The purpose of this study is to evaluate the process and outcomes of conducting a population-based survey through Web surveys with smartphone respondents. The study consists of an initial telephone interview to identify smartphone users followed by 2 weekly surveys completed via smartphone. We will determine the following: 1) technological feasibility: whether and under what circumstances smartphones can be used to collect population-based public health and behavior data; 2) quality of the data: evaluating response bias, coverage bias, outcome rates through comparisons of data collected by smartphones vs. landline &amp; cell phones from another survey; 3) cost effectiveness: how much smartphone data collection costs compared to more conventional data collection approaches. As mobile communications continue to evolve, a better understanding of how smartphones can be used to collect data is critical to public health surveillance. ","The standard population mean estimator for post-stratification is undefined when there are empty post-strata. In this paper, we develop a class of unbiased estimators that can be applied even with empty post-strata and that take into account the finite population correction factor. We first work under an assumption of equal post-stratum sizes, showing that which estimator is optimal in a given case depends only on the fraction of the total population variability that is in-stratum variability. We then extend the estimators to work in the more general case of unequal post-stratum sizes. In this more general case, we provide exact, simulation-based, and Taylor-series-based methods for determining how to appropriately weight each in-stratum mean. Our comparisons with existing estimators show that small but consistent efficiency gains are available from using the proposed estimators. ","Low income families are hard to reach and it takes extra effort to identify, and entice these households to surveys. The goal of the research was to test Address Based sample frame attributes and mixed mode survey methods for reaching and identifying households that met low income thresholds and determine the extent of their civil legal needs. This study is important to understanding the circumstances of low income households and expand public support for civil legal aid. Mixed mode survey methods (telephone, mail, and web) and experimental testing of differing amounts of token cash incentives, high and low valued lotteries, and payment of $20 for completion are used to draw respondents to the survey. This study is significant as it experimentally tests new methods of using ABS sample targeting low income census tracts and survey techniques, using a combination of upfront cash incentive with promise of payment for survey completion, for effectively garnering participation for low income individuals. It also tests the effectiveness of a novel income eligibility question. Pilot study results show that methods and survey modes make a difference. Cash incentives and lotteries were an i ","It is understood that survey modes have inherent biases that effect data quality. Mixed mode studies are often conducted to create a balance between these effects; however, when popular survey modes changed and new ones are created the modes used can differ. The effects of these differing modes confound the observations for a given year. For data that is collected over time in which the mode of data collection changes, it is necessary to tease the temporal effects of the variable of interest from the impact of mode. We will examine approaches to adjust data collected using old modes to what would be expected under a new mode of data collection. ","The NHIS is a multi-purpose face-to-face health survey conducted annually by the National Center for Health Statistics (NCHS). Historically, the NHIS has been designed to produce accurate national-level statistics on an annual basis. The traditional design has been based upon a one-time probability sample of coarsely defined geographical units containing population dwellings to be sampled over a 10 year NHIS design cycle. The current NHIS objectives, however, are broader in scope than in the past. Now, the 10 year survey objectives also include the ability to achieve accurate estimates for state and targeted minority populations, but on an \"as needed\" basis. Budgetary and operational constraints have presented challenges in achieving an optimal design. As a compromise, the final redesign is structured with a nationally-focused design as its core, but the design contains large reserve samples that can be used to achieve state or minority objectives.  A recent focus on state-level heath care has placed a priority on the allocation of sample to achieve state objectives.   In this paper we present the state allocation strategies to be implemented in 2016. ","Parameter estimation for generalized linear models (GLM) with missing covariates is commonly encountered in practice. If we assume a fully parametric model for the joint distribution, Monte Carlo EM algorithm based on parametric fractional imputation of Kim (2011) can be easily applied to obtain the maximum likelihood estimates for the regression parameters. In the semi-parametric approach, no parametric model assumption is made on the conditional distribution of the missing covariates given the other observed covariates. Instead, we use Kernel-based nonparametric regression estimator to obtain a nonparametric distribution of the conditional distribution. The resulting estimator is called semiparametric maximum likelihood estimator (SMLE) and can be implemented by semiparametric fractional imputation. Some asymptotic properties of the SMLE are discussed. Results from a limited simulation study are also presented. ","Interviewer-respondent (I-R) interactions are dynamic phenomena at their core, making dynamic systems (DS) theory an obvious framework for their study, yet DS theory and methods are rarely applied to I-R interactions. This study uses utterance-level coded data from audio recordings of phone interviews conducted by the Reuters/University of Michigan Surveys of Consumers (SCA). Recordings were transcribed into interviewer and respondent utterances, and rater judgments of affect were applied. Using GridWare v1.15a, a DS theory-based software (www.statespacegrids.com), trajectory plotting, attractor states identification, dynamic systems parameter estimation are demonstrated. The paper also motivates a graphical and intuitive understanding of cross-utterance I-R dynamics using the software. Initial results show patterns of affect dynamics varying by responses to income, interviewer gender, and across interviews conducted by the same interviewer. Potential for DS theory and methods to inform and guide the study of I-R interactions is briefly discussed. This research was sponsored by the Charles Cannell fund in Survey Methodology and the U.S. Census Bureau Dissertation Fellowship. ","Healthcare for persons with disabilities is an important issue for public policy makers. The Affordable Care Act will have benefits for people with disabilities since insurance companies will not be able to deny coverage based on pre-existing conditions. This paper focuses on health insurance, access to health care, and utilization and cost of services, of adults with and without disabilities. Several measures of disability will be used including limitations in basic and complex activity limitations. Both descriptive and multivariate methods will be used. This analysis will use pooled data from the Medical Expenditure Panel Survey, a nationally representative sample of the US non-institutionalized population, for years 2007-2012. Preliminary data show higher mean health care expenditures in 2012 for those with basic ($13,802) or with complex ($15,337) activity limitations, compared with those with neither ($3,433). Preliminary data also show adults with basic and adults with complex activity limitations were both more likely than adults with neither to report problems in communications with their providers. ","The Occupational Requirements Survey (ORS) is an establishment survey conducted by the Bureau of Labor Statistics (BLS) for the Social Security Administration (SSA). The survey collects information on vocational preparation and the cognitive and physical requirements of occupations in the U.S. economy, as well as the environmental conditions in which those occupations are performed. Calibration training is a type of refresher training that compares interviewer performance against predetermined standards to assess rating accuracy, inter-rater reliability, and other measures of performance. This paper will review the results of three separate calibration training sessions that focused on a data collector's ability to identify the presence or absence of physical demands and environmental conditions based on visual observation (assessed by watching job videos), assign Standard Occupational Classification (SOC) codes, and code Specific Vocational Preparation (SVP), which is a measure of the lapsed time required by a typical worker to reach average performance. Information obtained from these sessions was used to help evaluate training and mentoring programs, as well as to provide input into quality assurance procedures. However, the three calibration training sessions described in this paper generally showed minimal impact on performance measures used in the sessions. ","In this work, we consider inferences in linear mixed models when the observed values are transformed by the Box-Cox transformation, namely the Box-Cox transformed linear mixed models (BC-LMM) for positive valued and clustered data. We propose a simple and consistent estimating method for the transformation parameter. We also provide a procedure for estimating the whole parameters in the proposed model. Based on these estimators, we propose an empirical predictor of a linear combination of both fixed and random effects and second-order accurate prediction intervals for measuring uncertainty of the predictor. As an application of the proposed methodology, we focus on small area estimation. We investigate the proposed procedure through simulation experiments and empirical applications. ","The National Crime Victimization Survey (NCVS), sponsored by the Bureau of Justice Statistics (BJS) and conducted by the U.S. Census Bureau, is a multi-mode, rotating panel design survey of households that produces nationally-representative criminal victimization estimates for major types of crime in the United States. Two broad methods exist for calculating design-consistent variance estimates for surveys with complex sample designs like the NCVS - Generalized Variance Functions (GVFs) and direct variance estimation techniques such as Taylor Series Linearization (TSL). For the NCVS, GVFs created by the Census Bureau were designed to produce variance estimates at the national level, but their accuracy at the subnational level has not been evaluated. We assess the accuracy of GVF estimates within subnational areas based on geographic identifiers on the NCVS Public Use Files (i.e. Census region, population size, and urbanicity) by comparing them with TSL estimates. Our analysis found that TSL and GVFs do not provide consistent variance estimates within these subnational areas and thus GVFs should not be applied below the national level. ","This paper discusses the methods used to develop a generalized variance function (GVF) to properly compute standard errors (SEs) for recidivism rates for a cohort of inmates released from prison in 2005. GVFs use model-based parameters in a relatively simple function to calculate the SEs for an estimate while properly taking into account the complex survey design without requiring the use of statistical software. Thus, GVFs provide users a computationally easy method to calculate SEs for estimates from a survey with a complex design. The Bureau of Justice Statistics (BJS) has developed an online tool for estimating recidivism rates over time for a cohort of inmates released in 2005. The cohort of inmates was selected using a stratified design. Due to computational constraints the online tool could not use a statistical software package to dynamically compute requested SEs. Therefore, BJS wanted to incorporate a GVF into the tool to produce SEs along with the recidivism estimates. We discuss the models considered and the challenges associated with developing three parameter GVFs for each estimate type for this population. ","It is often the case that surveys of persons and households collect income data along with other demographic and socioeconomic questions. When income level is not the primary focus of the survey, it may be used in domain estimation or as a covariate in multivariable analyses. In these instances, it is common practice for income to be collected in a categorical form, with nonstandard category boundaries that vary from one survey to another. Though these categories may be appropriate for their originally-intended purposes, they often are not ideal for analyses not considered when the survey instrument was developed (e.g., for determining a household's percent of the federal poverty level). This paper describes a method for estimating a continuous income measure based on observed categorical responses with arbitrary category boundaries. The authors present this method in general terms and provide validation results both from simulation and comparison with federal benchmark surveys. ","There is increasing interest in understanding the long-term influences of early-life factors on late-life outcomes. Most studies of older adults rely on retrospective reports of events that occurred earlier in life. Yet we know little about the quality of these reports. We address this using data from the Health and Retirement Study (HRS), an ongoing panel study of people age 51+ in the U.S. Participants are interviewed every two years, alternating between telephone in one wave and in-person the next. At the end of each in-person interview, they are asked to complete a self-administered questionnaire (SAQ) on psychosocial topics. The 2006-2012 SAQs included a set of questions on important life events and experiences, including several from childhood. We distinguish between two sets of events-those that are more factual and those that are more qualitative--and examine changes in reports of these events over four years and factors that predict these changes. The magnitude of change differed between these two sets of events. Demographic and physical and mental health factors are significant predictors of change in reports, though not always in a consistent or expected way. ","This paper describes a statewide dual-frame RDD survey weighting procedure developed in SAS using the Raking SAS Macro with weight trimming (Izrael, 2009). The AAPOR Cell Phone Survey Task Force Report states, \"There is no consensus regarding how RDD cell phone samples should be weighted, especially when combining them with RDD landline samples.\" During the winter of 2014, a dual-frame, stratified random sample, including cell and landline telephone users, was utilized to ensure coverage of a statewide population. A total of 12 sub-sampling frames were created by using six non-overlapping strata representing six geographic regions and two frames, cell and landline, for each region. The weighting process consisted of developing design weights for selection probability, eligibility, non-response, multiplicity due to multiple phones, respondent selection procedure, and frame overlap. Raking and trimming techniques were used to adjust the sample to reflect the statewide telephone service benchmarks and demographics of interest. This paper presents the weighting methodology along with estimates at each stage, the limitations of the procedure, and the methodological lessons learned. ","While data management in general is a multi-step process, when working with multinational surveys the challenges intensify. Dealing with questionnaires undergoing some adaptation from country to country, accounting for updates and improvements to the data extract process, formatting and database structure, and time constraints are just some of the challenges that need to be handled toward generating datasets suitable for cross-country comparisons and meta-analysis. To best deal with these issues, we have created a team-based approach that allows all team member to be versed in the entire process and thus capable of better Quality Control. Building a process for handling fast-track multi-country data management and documenting such a process have been a critical key for success that we would demonstrate as a model for adaptation for multi-national surveys. A major consideration while working with such surveys is to facilitate the meta-analysis step by contrasting and combining results from different countries to possibly identify patterns within the study results. The sooner meta-analysis can begin, the sooner its results can be used to possibly affect future aspects of the study. ","The main objective of sampling is to obtain a representative sample for an unbiased and efficient estimate within a budget constraint. In a balanced sample, according to Yates' definition (Yates, 1971), the mean value of the balanced factor in the sample is equal to the mean of the factor in the population. In this study, a balanced sample is not a purposively selected sample but a randomly selected one. Another important reason for a balanced sample is to protect the inference against a model misspecification (Royall &amp; Herson, 1973a; Royall &amp; Herson, 1973b). In this work, we propose and demonstrate a practical balancing method which would be a small modification to currently practiced design-based sampling procedures for small and large-scale surveys. We demonstrate practicality of our approach with a simulation of sample selection from 3,143 U.S. Counties for estimates of the total and mean population sizes in 2010 with Census 2000 count and State indicator as auxiliary variables. Our simulation study indicated that a balanced sample was good for reducing bias regardless of the particular sorting method. Rather than selecting a random sample from an ordered frame, we should try to find a balanced sample for an unbiased estimate. ","Correct specification of the model used for small area estimation is important in order to obtain valid predictors of the target quantities and of the prediction of the mean squared error. By using recursive residuals, we construct misspecification tests for the two linear mixed models in common use for small area estimation; the area-level model and the unit-level model. We propose simple formulas for the recursive residuals that do not require repeated estimation of the variance components, and use them to form tests with asymptotic t distribution. The proposed tests are most powerful against nonlinear effects of the covariates. Simulation results reveal that under appropriate sorting of the sample observations, the tests possess the correct size under the null hypothesis and good power in detecting misspecification of the linear predictors. ","This roundtable provides background on some of the applications and methods developed for record linkage. Record linkage is the science/art of finding duplicate entities within and across files using non-uniquely identifying fields such as name, address, date-of-birth, etc. It describes applications and provides specific examples. It notes substantial efficiencies and cost savings. ","An informative sampling design assigns probabilities of inclusion that are correlated with the response of interest and often induces a dependence among sampled observations. It is well-known that model inference performed on data acquired under an informative sampling design will be biased for estimation of the joint distribution of model parameters supposed to generate the population from which the sample was drawn. Known marginal inclusion probabilities assigned through the sampling design may be used to weight the likelihood contribution of each observed unit in the sample with the practical intent to ``undo\" the design for inference about the population. This paper extends a theoretical result on consistency of the posterior distribution at the true generating distribution to the weighted ``pseudo\" posterior distribution used to account for an informative sampling design. We construct conditions on known marginal and pairwise inclusion probabilities that define a class of sampling designs where consistency is achieved, in probability. The method is applied to time-indexed differences in establishment level employment counts between different survey instruments. ","Survey data often consist of a large number of categorical variables, some of which are ordinal. Item nonresponse is common in these settings, and can be dealt with using multiple imputation. We distinguish between variables which are fully observed, those which are missing but to a small degree, and those which have a large proportion of observations missing. It is these latter variables we are most concerned with modeling flexibly, in order to provide imputations which preserve the complex relationships in the data. We specify a joint model for the set of variables which are not completely observed, using a flexible Dirichlet process mixture specification for the unordered categorical variables and the latent continuous variables which drive the ordinal responses. This approach allows for capturing complex distributional features and dependencies, leading to a powerful and practical imputation engine. ","In survey statistics, small area methods and models are used to produce estimates (for instance, average salaries) for geographical areas or population subgroups for which the sample is too small to support direct estimation. One well known model is the Fay-Herriot model, which can be interpreted as a linear mixed effects model in which normality for random effects is assumed. Because random effects are not observed, it is difficult to check the assumption of normality (or any other parametric assumption). In this presentation, we consider extensions of the Fay-Herriot model in which the default normality assumption for the random effects is replaced by a non-parametric specification. We explore the estimation of individual area means as well as the distribution of their ensemble. Viability of the approach and the effects are investigated using the National Survey of Recent College Graduates to estimate average salaries for different demographic subgroups. ","In this talk we are concerned with the most traditional scenario of record linkage, which consists of linking two disparate datafiles containing overlapping information on a set of individuals or entities, and it is assumed that each entity is recorded maximum once in each datafile. Most statistical techniques currently in use are derived from a seminal paper by Fellegi and Sunter (1969) who formalized procedures that had been used earlier by other researchers. These techniques usually assume independence in the matching status of record pairs to derive estimation procedures and optimal point estimators (e.g. Fellegi-Sunter decision rule). We argue that this independence assumption is unreasonable and target instead a bipartite matching between the two sets of records coming from the two files as our parameter of interest, and derive estimation procedures and point estimators accordingly. We demonstrate the improvements of our approach over traditional methodologies in a number of realistic simulation studies. ","Statistical agencies and other organizations that collect and process  data are often faced with data files that contain faulty values. When these errors result in inconsistencies agencies usually correct them through a process known as edit-imputation. The dominant paradigm, due to Fellegi and Holt (1976), separates the task into an error localization and an imputation phase, and is based on finding the minimal set of changes needed for the records not be inconsistent. While this approach has the advantage of minimizing changes to the original data, it can produce biased estimations, as it ignores the distribution of the data during error localization. In this talk I introduce a new procedure for edit-imputation of categorical data based on joint modeling. This model includes a flexible representation for the underlying true values, with support only on the consistent responses; a model for the location of errors; and a model for the observed faulty data. Estimation is performed simultaneously using MCMC sampling. Through challenging data-based simulations I show how this method can deliver superior results than those obtained from the application of the Fellegi-Holt approach. ","In population-based household surveys, for example, the National Health and Nutrition Examination Survey (NHANES), individuals related by blood are often sampled within households. Therefore, genetic data collected from these population-based household surveys are often correlated due to two levels of correlation. One level of correlation is caused by the multistage geographical cluster sampling and the other is caused by biological inheritance among participants within the same sampled family. In this paper, we develop an efficient HWE test considering the weighting effect induced by the differential selection probabilities in complex sample designs, as well as the two levels of clustering (correlation) effects described above. The developed tests account for various complex designs for selecting households and individuals within households, and are evaluated via Monte Carlo simulation studies. The results show that the developed HWE tests maintain the nominal levels, and are more powerful than existing methods (Li et al. 2011) under various (non)informative sample designs that depend on genotypes, family relationships, or both, especially when within-household sampling depends o ","We consider the use of two-phase study designs in the context of small area estimation, where the goal is to reconstruct (unobserved) populations total by strata of interest. We describe a Bayesian hierarchical model that includes both spatial and non-spatial random effects to account for confounding by location. The methods are illustrated using birth data from North Carolina. For these data, we construct several survey sampling schemes and two-phase designs in order to compare the efficiency of standard survey approaches with two-phase approaches in both parameter and small area estimation. We show that the count reconstruction can be carried out accurately based on a fraction of the original data. ","Investigations commonly use data that were originally collected for a different primary research goal. When novel biomarkers are investigated, it is natural to leverage existing data together with stored specimens to yield new exposure data. Limited availability of specimens and financial resources may require targeting a subset of patients for new analyses; use of outcome dependent sampling (ODS) designs can provide a cost-effective and efficient way to conduct substudies (Schildcrout et al. (2013)). To date statistical methods have focused exclusively on using only data from the subsample for final analysis, but research in the univariate setting (Weaver and Zhou (2005)) suggests that analyzing unsubsampled individuals, in addition to those on whom the biomarker has been ascertained, may contribute to improved estimation of target regression parameters. This talk will focus on the use of ODS sampling designs and analysis in the longitudinal setting with continuous outcomes. We examine the potential advantages of these designs/analyses from a likelihood perspective, and propose some alternatives to this approach. Results will be illustrated using a longitudinal cohort study.     ","Atherosclerosis Risk in Communities (ARIC) study used stratified sampling to select participants within the main cohort for biomarker studies of the risk of Coronary Heart Disease (CHD). Previous studies showed one inflammation marker high-sensitivity C-reactive protein (hs-CRP) was an independent predictor of increased risk of CHD and this biomarker acted additively on the baseline risks of CHD. To make efficient use of the data and perform individual risk prediction, we developed a new method for fitting additive hazards model to two-phase sampling data. Our method jointly estimated the regression coefficients and the baseline hazard function in the model so that we are able to make risk profiles for individuals with different levels of hs-CRPs. Our method further improved inference and prediction precision by calibration techniques. These risk profiles made upon the standard risk factors and the new biomarker information that is only available for selected samples may help physicians to discover new patients at high risk of CHD. Our new tools are not restricted to a particular two-phase study and they were implemented in the R software for immediate use. ","The 2014 midterm election had the lowest turnout in decades. Several states have recently introduced new voter ID laws, and Texas has one of the most stringent in the nation, allowing one of only 7 types of picture ID. The Dallas County Elections office maintains voter registration files that include the address and voting history (i.e.voted or not) of every registered voter in Dallas County. We have obtained these files for analysis and geocoded the addresses to match the census geography in the Census Planning Database (PDB). Then we calculated by tract the current voter registration rate (which can be obtained by utilizing information from the PDB) and the voting rate among registered voters for the last several midterm elections. Our research addresses the following questions:  (1)Is the the change in voting rates in recent elections (including 2014) by block-group in Dallas County related to characteristics of the block-groups available from the PDB?  (2)Can we predict the block-groups where those who are hard to register or hard to turn out reside?  (3)Do the communities in Dallas County that are hard to count and difficulty to turn out to vote have similar characteristics? ","Examining disparities in resources on the census tract-level is currently a public health priority. The Modified Retail Food Environment Index (mRFEI), provided by the CDC, incorporates two food environment metrics, \"food deserts\", areas with no access to healthy foods, and \"food swamps\", areas in which the quantity of unhealthy food options overwhelm healthy ones. We assess the association between the census tract racial make-up and food environment. Multiple logistic regression models are fit, controlling for census-tract level covariates from 2008-2012 ACS estimates, as well as state. Percent black is significantly associated with food swamps, with an absolute increase of 14.4 percent black living in food swamps (p&lt; 0.01). Percent Hispanic is associated with food swamps, with an absolute increase of 9.1 percent Hispanic living in food swamps (p&lt; 0.01), but inversely related to food deserts (absolute difference -6.8, p&lt; 0.01). After adjustment, all associations remain significant. The strong association between the census tract-level racial make-up and food swamps shown here will allow for targeted interventions to census tracts where these disparities exist. ","The modifiable areal unit problem refers to introduction of statistical bias in hypothesis testing when spatial processes are aggregated over arbitrary areal units. Aggregation of a spatial process over different areal units can result in varying conclusions, so that the results of the analysis are dependent on an arbitrary choice of areal unit. In spatial epidemiology, this problem is of particular concern as the spatial resolution of disease incidence available for analysis is generally limited due to privacy concerns. Our goal is to explore the extent of the modifiable areal unit problem by comparing results of a simple analysis conducted at the census tract and census block level. We explored the spatial trend in aggregate household income (AHI) from US Census Bureau's 2014 Planning Database, and performed a cluster analysis to identify areas of spatial clusters of low aggregate household incomes. We also explored the spatial trend in AHI after adjusting for potentially correlated demographic covariates. Spatial trend of the clusters in both of these analyses is compared across census tracts and census blocks to explore how choice of areal unit of analysis impacts results. ","The objective of this analysis is to identify the determinants of poverty in United States. First we design an interactive and data driven web-based approach to make easy the data exploration for anyone, using this tool we can identify which factors can be relevant in the community poverty. After the graphical exploration, we fit several statistical models to quantify the importance and behavior of the variables that could help to explain the poverty on different regions. ","Repeated measures are of interest to marketers (Lessne &amp; Hanumara, 1988). Many marketing issues deal with the study of change in marketing variables based on analyses of repeated measurements, or at different levels of an independent variable. Growth analyses of demographic variables such as population demographics are of utmost importance, especially in the United States, which is fast becoming an older nation. Traditional statistics in studies of demographic change analyzing longitudinal data use Ordinary Least Squares regression pooled across repeated measurements (Steenkamp &amp; Baumgartner, 2000), and ANOVA; which are not appropriate, because such longitudinal data are autocorrelated (Timm, 1980). So, data from the AGing Integrated Database (AGID) was used to run growth curve models using AMOS 20.0. Results suggested that for the baby boomer population, a two-factor unspecified LGM was the best fit. This showed a linear growth in the overall baby boomer population. Among the Hispanic population, a three-factor polynomial LGM was the best fit. A chi-square difference test and acceptable values of fit suggested a possibility of quadratic growth in the Hispanic population. ","This study looks at the density distribution of standard errors (SE) of item stratum-index area level percent change using CPI-U historical data in order to understand anomalous behavior. Many attributes can be determined about the SE, such as: the shape of the SE density distribution; whether the overall central tendency and overall variability of the SE distribution are smaller, larger, or similar from one year to the next, or from one month to the same month of the following year; whether the distributions tend to shift over time or stay stationary. The SE of basic level price changes are used to produce the underlying distributions for further examination. SE for May 2006 in particular were investigated as it was the month when the SE of 12-month CPI-U percent change reached its largest value of 0.19. Non-parametric methods and categorical analysis techniques are employed to assess the May 2004-May 2008 datasets. An art of compelling data visualization is produced by combining multiple pieces of information in a single graph in order to demonstrate an optimal and coherent visual of comparisons. ","In 2013, D3 Systems Inc. conducted a national face-to-face survey to better understand the telephone owning population in Egypt. While this study's primary purpose was to inform methodological decisions in designing subsequent computer-assisted telephone interviewing, additional information such as telephone usage habits was collected of respondents. In February of 2015, D3 Systems conducted a follow-up study by contacting these respondents again over the telephone. We present some of our initial findings and discuss results focused on possible bias resulting from the mode of surveying.  ","Rising costs, growing non-response and uncertain frame coverage stimulate growing interest to use web surveys instead of representative samples for estimation of general population characteristics. Previous research has demonstrated that weight adjustment by propensity score constructed on demographic variables may be insufficient for balancing volunteer panels of web surveys selected in nonrandomized fashion. This may result in biased estimates. Assuming systematic dependence of propensity score and target variable on unobserved variable $U$, we derive dependence of bias on the model parameters associated with $U$. Direct analytic dependencies were calculated for limiting cases of model parameters using Taylor series expansions. Better understanding the reasons for biased estimates will stimulate development of methods for bias reduction. ","Travel indexes are part of the Consumer Price Index (CPI) produced by Statistics Canada. The Travel Tours and Traveller Accommodation indexes are currently being reviewed to maintain their relevance and propose solutions to some challenging aspects including response burden, sampling strategies and collection methods. In-person collection is the traditional method to collect data for the CPI, but information on prices in the travel industry has proven to be more and more difficult to gather in the traditional way. Furthermore, online sales make up a constantly increasing share of the travel industry market. Online collection seems to be a promising method to collect representative data on the travel industry. This paper will give an overview of the introduction of online collection for the travel indexes, the obstacles encountered and overcome and the challenges that we are still experiencing. ","The Consumer Price Index (CPI) estimates the change in prices over time of the goods and services U.S. consumers buy for day-to-day living based on price quotes selected from probability samples and aggregation weights derived from the Consumer Expenditure (CE) Survey. Recently, there has been speculation about the percentage of the CPI-U standard error that is due to the variability of its cost weights. Cost weights are the building blocks of the CPI that are simply the product of CPI indexes and CE aggregation weights. In this study, we draw simulated samples of CE reports to produce simulated cost weights; we then use the simulated cost weights to find a functional form of how the CPI-U all U.S. - all items standard error changes as CE sample sizes change using stratified random groups (SRG) variance methodology. ","Editing survey data can be both challenging and rewarding. The National Ambulatory Medical Care Survey, a large nationally representative survey of physician office visits, had significant changes in methodology in 2012. We needed to edit the data efficiently and understand how the 2012 data compared to previous data. This involved analyzing different types of data (i.e. character, numeric, categorical, continuous) that needed differing methods of assessment. A partially automated SAS program was created to compare two years of data to 1.) Identify category response changes that exceed a specified threshold for categorical variables, 2.) Identify differences in the mean/median that exceeded two standard deviations for continuous variables, and 3.) List a user specified number of ranked character values. Some examples of the findings were: there were less missing values for height in feet in 2012 than in 2011, potential errors in decimal places were identified for a lab value, and most verbatim diagnoses matched for more than half of the top 12 values. Several valuable lessons were learned from creating this program; they will also be shared. ","We address the problem of identifying underground anomalies (e.g. holes) based on gravity measurements. This is a theoretically well-studied yet difficult problem. In all except a few special cases, the inverse problem has multiple solutions, and additional constraints are needed to regularize it. Our approach makes general assumptions about the shape of the anomaly that can also be seen as sparsity assumptions. We can then adapt recently developed sparse reconstruction algorithms to address this problem. The results are extremely promising, even though the theoretical assumptions underlying sparse recovery do not hold for gravity problems of this kind. We examine several types of sparse bases in the context of this gravity inverse problem and compare and contrast their relative merits. ","The Producer Price Index (PPI) uses industry specific strata in their sample design. The PPI samples each of its industries independently. Establishments are typically selected at the six-digit NAICS level, though some industries are sampled as high as the three-digit level. Since PPI designs their samples on an industry by industry basis, each sample may have strata based on attributes of that industry. In the initial development of the variance estimation method, which makes use of the bootstrap method, replicates were selected over the whole of the industry. The purpose of this paper is to describe a PPI variance estimation method which takes into account the industry specific stratification. ","The topic is related to the Central Limit Theorem. In this paper, we study the self-normalized moderate deviations for centered independent random variables with finite third or higher moments. With these moment conditions, we obtain the exact self-normalized tail probabilities for all x in the range of [0, o(n^(1/4))]. This is an extension of the results in Jing, Shao and Wang (2003) where at most finite third moment is assumed. In particular, if the centered independent random variables have some third moment conditions, the self-normalized moderate deviation probabilities hold uniformly in a range of x which is related to the moments with order between 3 and 4. Furthermore, it is proved that the range [0, o(n^(1/4))] is optimal under these third moment conditions. At the end, we show the necessity of these third moment conditions in obtaining the self-normalized moderate deviation probabilities for x outside the range of [0, o(n^(1/6))]. ","While there is great demand for detailed occupational information, such as employment and wages by gender, age, tenure, and turnover by occupation, this information has not previously been available from an establishment survey. The Occupational Employment Statistics (OES) survey conducted a test to see if employers were willing and able to report additional demographic information about employees, in addition to occupations and wages, which are already collected by OES. This new information could greatly expand the range of data products offered by OES. To determine if it was possible to uniformly collect extra data items and to assess the impact on regular OES data collection, the Bureau of Labor Statistics designed and conducted a three-wave field test. This paper will describe the field test and discuss the results and feasibility of collecting the new data items. ","Wage inequality, as measured by the national 75th/25th percentile wage ratio, grew 6.6% between 2006 and 2014. Changes in occupational wage and composition contribute to this rising inequality in three ways: changes in overall wages of occupations, shifts in national employment shares of occupations, and changes in wage distributions within occupations. This article applies a modified Shapley decomposition to data from the Occupational Employment Statistics (OES) program at the Bureau of Labor Statistics (BLS) to quantify how these factors have contributed to the increase in wage inequality. The paper shows that all three occupational components contribute to the rising inequality, and demonstrates that it is essential to account for all three factors when discussing inequality. ","We present an extension to filtering techniques to estimate trends of multivariate time series. The proposed method is based on a vector signal-plus-noise representation of Penalized Least Squares that requires only the first two sample moments, and introduces an index of smoothness. This index allows setting in advance a desired amount of smoothness to achieve. It is also a function of the correlation between the noises of the series and the sample size. Our proposal arises from a statistical solution to a multivariate GLS problem. Such a solution leads to an index of smoothness that is applicable in the general multivariate case, but we pay special attention to the bivariate situation. Here we show the closed-form expressions for calculating trend estimates with their corresponding variance-covariance matrices, and present the proposed algorithm for smoothing bivariate time series.   We discuss the results on simulated data and a real application (Mexican and U.S. GDP data). ","Civil unrest events (protests, strikes) unfold through complex mechanisms that cannot be fully understood without capturing social, political and economic contexts. Modern cultures use a variety communication tools (e.g., traditional and social media) to coordinate in order to gather a sufficient number of people to raise their voice. To accommodate dynamic features of social media feeds and their impacts on insurgency prediction, we develop a dynamic linear model based on daily keyword combinations. In addition, due to the large number of so-called n-grams, we employ a sparseness encouraging prior distribution for the coefficients governing the dynamic linear model. Included in the predictors are significant sets of keywords extracted from Twitter, news, and blogs. We include volume of requests to Tor, a widely-used anonymity network, economic indicators and two political event databases (GDELT and ICEWS). Insurgency prediction is further complicated by the difficulty in assessing the exact nature of the conflict. Our study is further enhanced by the existence of a ground truth measure of conflicts compiled by an independent group of social scientists and experts on Latin America. ","The National Center for Education Statistics (NCES) recently unveiled several new online data training modules and data tools to facilitate easy access and use of its data. This session provides guidance and advice on using these online resources to navigate a myriad of NCES data sets. The session will cover new additions to the Distance Learning Dataset Training System (DLDT), which provides information about NCES data and the tools that users need in order to find published reports, explore and acquire data, create data files, and conduct analyses in selected statistical software packages. The session will also summarize web gateways for NCES data users and introduce a resource document offering detailed instructions on using selected online data tools. ","For a variety of reasons, many statistical agencies perform  seasonal adjustment. Of the multiples different choices for  seasonal adjustment methodology and software, a large number of  agencies, including the U.S. Census Bureau, choose to use  X-13ARIMA-SEATS. The \"big picture\" ideas behind seasonal  adjustment are well understood and easily explained. However,  once it comes time to perform a seasonal adjustment, an  inexperienced user can become easily overwhelmed with the  options and diagnostics of most seasonal adjustment software.  This, coupled with the associated learning costs, yields many  people who could benefit from a better understanding of the  X-13ARIMA-SEATS process.    ","To meet the strategic goals and objectives for the 2020 Census, the Census Bureau is exploring ways of leveraging technology and different response modes and contact strategies to optimize self-response enumeration. As part of the 2014 Census Test, eight different experimental panels were tested to compare different contact strategies for optimizing response rates. The unifying theme underlying each of the strategies was the goal of encouraging households to respond online using the Internet. These approaches included having respondents signup in advance to receive email or text messages in lieu of mail materials as a contact method for response, Internet response without a unique User ID, email invitations to respond to the census, and automated voice reminders. This paper presents the findings from the research. ","We designed an experiment in the 2013 Health and Retirement Study (HRS) Internet survey to test context effects on reporting of prescription drugs. The module included questions on whether the respondent was currently taking drugs for a series of common conditions and the number and names of drugs currently taken. The order of the question sets was randomized. Also, a random half of respondents were prompted at the beginning of the module to consult medication resources before answering the questions; the other half received no prompt. We hypothesized that asking about drugs taken for common problems would lead respondents to recall more drugs and that prompting them to use resources would result in higher use of such resources and better reporting of drugs. We found that question placement did not result in a significant difference in the number of drugs reported, but asking about drugs taken for specific medical conditions before asking general question about the number and names of drugs did lead to higher reporting of several of the conditions. Encouraging respondents to look at medication labels had a modest on the desired behavior. ","Much of the research into item nonresponse for income questions has focused on in-person, mail, and phone surveys, which often have missing rates of 20% or even higher (Moore, Stinson &amp; Welniack, 1999). We explore factors related to income nonresponse in online panel surveys, in which respondents' willingness to answer sensitive questions may be affected by their long-term relationship with the panel. For this research, respondents were randomly assigned to one of three versions of a household income question to test various ways of presenting the response options to a close-ended question. We examine correlates of missing income data and report the results of the experiment, identifying the version with the lowest rate of missingness. The results demonstrate that the inclusion of a \"don't know\" response and how it is presented have significant implications for the rate of missing. In addition to reporting the nonresponse rate for each of the experimental conditions, we investigate the validity of the data collected under each condition. ","The recent growth in the number of participants who take surveys on mobile devices can be seen as an opportunity to increase coverage and reduce non-response, but it is essential that online surveys be designed with mobile display in mind to improve data quality and minimize device-specific effects. We conducted a study using GfK's KnowledgePanel, the largest online probability-based panel in the US to inform recommendations for a mobile-first study design. Rather than rely on 'accidental mobile' participants with their attendant self-selection bias, participants were randomly assigned to complete our survey on one of three devices (desktop/laptop, smartphone, or tablet). We had 4,555 completed surveys with an average completion time of about 18 minutes. We will present the factors that affect completion and substantive response differences, including device, survey design, and demographics, behaviors, and attitudes of participants. We will also discuss observed differences in response that appear to occur because of device used and not due to sample differences as well as provide recommendations for designing online surveys to accommodate mobile respondents. ","In self-administered surveys, grids have evolved to efficiently measure multiple concepts using the same graded response format across a number of different targets, with responses typically in columns and the rating targets to evaluate in rows. Generally, grids allow us to assess multiple targets more quickly than if each target is presented separately. However, grids have a number of issues, and in particular affect respondent experience on mobile devices - measurement in online surveys is generally limited by screen size. In a series of 3 studies with over 80,000 respondents in total, we present experiments that examined how reducing the number of grid response categories affects the measurement of a variety of concepts. We found that response formats with fewer response categories take less time to complete, are easier to complete on mobile platforms, and show as much validity as formats with more response categories. In addition, they also appear to detect smaller differences between rating targets than those with more response categories. This higher level of differentiation was unexpected, and we explore some reasons for this occurrence. ","School-level nonresponse is a serious threat to the validity of estimates from statewide student surveys that are used for alcohol, tobacco, and other drug use (ATOD) planning and assessment. Yet, there has been limited research to understand the district or school-level decision to participate in a substance use survey and to determine strategies that can increase participation. We drew on the Willimack, Nichols, and Sudman (2002) model of business survey participation to identify features of the school as an organization and survey design features such as the timing or topic that may drive this decision. We conducted a study of nonresponse in the Indiana Youth Survey that involved four components: (1) assessment of the characteristics of nonrespondents using matched data from the Department of Education, (2) surveys of staff conducting other statewide substance use surveys, (3) in-depth interviews with principals and superintendents and (4) a web survey of 296 principals and 102 superintendents. Findings revealed a varied set of factors involved in participation suggesting the need for a tailored survey design and multiple strategies to reduce barriers to participation.  ","The ability to conduct surveys using opt-in Web respondents has raised concerns about whether these samples are valid. Probability sampling theory is not applicable because the units are not subject to being sampled with a known and non-zero probability of selection. Frameworks have been proposed for Web opt-in surveys, but these generally have features that are not well suited for general-purpose surveys. This paper proposes a model-based framework for making inferences from non-probability samples that we refer to as a compositional approach. The paper outlines the assumptions required for making inferences from these types of samples, and suggests some evaluation measures for assessing the assumptions.  ","The enterprise of online research with non-probability samples continues to grow. Still, concerns about its trustworthiness linger. For this reason, researchers have attempted to develop models that replicate probability sampling's positive effects. Such efforts have met with mixed success. To complicate matters, some of the more promising ones have depended on small sample sizes, a small number of samples, or both. Accordingly, there is little consensus on what variables to include in sampling or weighting models when the aim is to minimize bias. Here, we rely on seventeen samples of one thousand, provided by seventeen agencies. We then apply search-based optimization methods to these data to identify the best combination of variables to include in sampling or weighting models. The resulting evidence suggests these models, compared to standard ones, can reduce bias by a third. This learning  may be important for science, practice, and policy. ","While the primary concern for surveys using opt-in Internet panels is possible bias from self-selection, variance estimates are still useful regardless of whether the estimates are unbiased. Consistent asymptotic variance estimates can be obtained under weak conditions. These are compared to empirical estimates of sampling variability, along with estimates of the components of mean square error due to bias and sampling variability. The results are also applicable to other types of non-probability samples, such as low-response telephone polls.  ","Surveying rare groups in a time-sensitive frame can be cost-effectively done using non-probability online panels. Although an unattractive proposition for a probabilistic researcher, it can be a practical alternative to costly traditional surveys. Abt SRBI has used non-probability sources to track national influenza vaccination rates for two such rare groups: pregnant women and health care personnel. Each sample is weighted to their respective set of available national benchmarks. A concern with the estimates based on these samples is the likely bias due to no known frame selection probabilities and the exclusion of those who do not use the internet or join online panels. Another concern is not being able to assign a measure of precision to the estimate. One approach to addressing these concerns is to match the online sample with a probability sample from the same population groups. The National Health Interview Survey of the general population is the best such available source. We demonstrate a propensity score method of matching based on Internet use. The matched sample estimates may help quantify the bias and lead to a method of assigning a generalizable measure of precision. ","The Bureau of Labor Statistics (BLS) is working with the Social Security Administration (SSA) to establish the Occupational Requirements Survey (ORS), an establishment survey that collects information on the vocational preparation and the cognitive and physical requirements of occupations in the U.S. economy, as well as the environmental conditions in which those occupations are performed. In preparation for the first ORS production sample, we studied several potential sample designs and assessed each for practicality and coverage of the population. In addition, we considered how and if the ORS sample might be coordinated with the sample of BLS's National Compensation Survey (NCS). In this paper, we evaluate each sample design option, describe the sample design that was selected for the first ORS production sample, and outline plans for monitoring the effectiveness of the selected sample design. ","The Occupational Requirements Survey (ORS) is an establishment survey conducted by the Bureau of Labor Statistics (BLS) for the Social Security Administration (SSA). The survey collects information on the vocational preparation and the cognitive and physical requirements of occupations in the U.S. economy, as well as the environmental conditions in which those occupations are performed. This paper provides an overview of the ORS Review Program including information on the review processes, systems, and tools. The review process ensures that data are coded correctly and that documentation is sufficient. Review takes a variety of forms, such as edits in the computer system to catch erroneous data and queries that look for unusual data. One review process targets specific elements that an employer reports, while a second review process looks at all the data an employer provides. This paper discusses how all these review processes work together to ensure the quality and transparency of the data.  ","The Bureau of Labor Statistics (BLS) Consumer Expenditure Survey (CE) is exploring options for an Internet-based expenditure diary accessible via a computer or mobile device as a step towards the CE's redesign plan released in 2013. With the introduction of a web interface, the CE will need to incorporate data security requirements for respondents logging in to the survey. In doing so, it will be important to strike a balance between essential security requirements to maintain respondent privacy and trust while making it easy enough for respondents to access without getting frustrated. This paper summarizes the findings related to respondent login procedures from a series of tests conducted using multiple versions of a web diary design. A web diary has security needs distinct from one-time web-based surveys. Respondents will need to access the diary multiple times over the reporting period, multiplying the burden created by login credentials. The focus of this paper will be on the mobile interface as it entails a number of unique device specific characteristics that make logging in more challenging such as smaller screen sizes and distinct keyboards. ","The Bureau of Labor Statistics (BLS) fielded the Green Technologies and Practices (GTP) Survey in 2011 and 2012.  During survey development, respondents repeatedly advised BLS that they would be more inclined to respond to a green survey if they could do it in a green manner, so an online method of response was developed for the survey.  Internet collection benefits the environment by reducing printing and USPS delivery, as well as saving time and taxpayer money by allowing for earlier responses, respondent key-entry of data, and online editing of survey responses.  A much lower than anticipated Internet response was recorded during the first production survey in 2011, and when plans began for the next round of data collection beginning October 2012, steps were taken to \"push\" internet collection to a targeted group of sample units.  This paper summarizes the procedures and reporting results for the two GTP surveys conducted in 2011 and 2012, and discusses the alternative mailing strategies employed in the 2012 survey to boost the Internet reporting rate. ","The 2013 Business R&amp;D and Innovation Survey (BRDIS) included an experimental design to test the impact of removing the questionnaire booklet from the initial mail packages on response. The letter/experiment group received a letter with instructions for completing the survey online and a questionnaire booklet was not included in the mail package. The questionnaire/control group received a questionnaire booklet in the mail  package that included instructions for reporting online consistent with past BRDIS cycles. At the end of the experiment, the weighted check-in rate for the letter/experiment group was 58.9% compared to 63.8% for the questionnaire/control group. The difference in check-in rates decreased from 4.9% to 2.3% after the 2nd nonresponse follow-up in which both groups were sent questionnaire booklets. Large companies(defined as having at least 500 domestic employees) exhibited higher weighted check-in rates in the letter/experiment group after all processing was complete. Companies reporting online reported significantly larger amounts of R&amp;D expenditures (the key variable of interest to BRDIS) than those reporting on the questionnaire booklet. ","The census of Agriculture, Forestry, Fishery and Animal Husbandry is an important official survey  in Taiwan. However, enormous survey cost and effort for the data processing are required for such  a nationwide census, hence this census can only be conducted every five years. Therefore, certain  annual agriculture survey is necessary for the realization of the current related industry information,  so that proper policy can be formulated timely. A sampling strategy for the Taiwanese primary  farm household survey is constructed in this research. A stratified random sampling design, in  which the optimal stratum boundary and allocation are carried out based on the 2010 census data, is  proposed for the purpose to enhance the estimation precision and investigate certain subpopulations  of interest. The result indicates that the performance of the proposed stratified sampling is much  more advantageous than simple random sampling without replacement and other stratified design  with optimal allocation and equal within-stratum size stratification boundary under a comparable  total sample size.    ","The U.S. Census Bureau must develop new methods to reach households for the 2016 National Survey of Fishing, Hunting, and Wildlife-Associated Recreation (FHWAR) while keeping the address-based survey within budget. Telephone contact rates were a major problem for the previous FHWAR survey (2011). As a result, the Census created a pre-screener test to help contact households and potential wildlife-related activity participants through self-response. The15,000 household pre-screener test was implemented in two vastly differently states, New Jersey and South Dakota, with a new sample design using historical knowledge and administrative data aimed to locate wildlife-related activity participants. The test included an embedded experiment between a paper only questionnaire, a web questionnaire, and a paper/web option. The pre-screener test results were reasonably positive with response rates between 21 and 32%. There were little differences between the rural and urban state in response rates, which is a beneficial finding for the National Survey. The test and research findings indicate the pre-screener will likely produce higher contact rates thus saving time, money, and resources. ","The final sampling weights are the result of various steps of adjustments to the design weights such as frame integration, nonresponse and calibration. Evaluating the Design Effect at each of these adjustments sheds light on their impact on the precision of survey estimates. In this paper, we focus on the Canadian Community Health Survey (CCHS), which uses a complex survey design with multiple frames and multiple stages of selection, and empirically examine the effect of different steps of the weighting process on the overall variability in estimates. The results suggest that the use of unequal person-selection probabilities and the nonresponse adjustment have the most negative impact on the design effect of the CCHS while calibration decreases the design effect and improves the precision of the estimates. ","Faced with declining response and increased risk for nonresponse bias, surveys have increasingly relied on post-survey adjustments to ensure the accuracy of estimates. In particular, researchers are exploring the potential of paradata, including interviewer observations in face-to-face surveys, to correct for nonresponse bias. In 2013, the National Health Interview Survey (NHIS) added 15 interviewer observations collected during the first personal visit attempt. Examples include the physical condition of the sample unit and evidence of smokers. Using 2013 data, we assess the utility of these observations for nonresponse adjustments. Of specific interest is whether adjustments based on these measures represent improvements over current procedures. Using a set of key survey estimates, we address the following questions: Are substantial shifts in estimates observed when the observational measures are used in the adjustments? What differences are observed in the variance of survey estimates by adjustment method? We discuss the implications of our results for NHIS weighting procedures and the utility of observational measures for post-survey adjustments more generally. ","A series of multi-national surveys based on complex geo-sampling probability-based designs were conducted with the goals of ensuring adequate coverage within each country and facilitating meta-analysis across countries. Such a complex design, where researchers are trying to balance between standardization across countries with different administrative structure and adaptation within each country, is susceptible to increased design effects.     ","Nonresponse and raking adjustments can adversely impact the coefficient of variation of survey weights and standard errors for survey estimates. In this study, we investigate a raking approach that attempts to mitigate the impact on standard errors when weights are raked. Raking involves minimizing the distance between raked and pre-raked weights while also requiring the sum of the weights agree with population totals. In addition to this requirement, our modified approach also requires that the distance between the raked weights and mean of the raked weights within each stratum be minimized. Using 2012 and 2013 National Immunization Survey data, we compare our approach with the traditional raking approach in terms of standard error and mean squared error. Preliminary results indicate that at the national-level, the proposed method yields similar estimates as the official NIS estimates (absolute difference less than 0.26 percentage points), but decreases the standard error by 0.03 percentage points. In comparison, the unweighted estimate decreases the standard error by 0.20 percentage points, but has a difference of up to 2.15 percentage points compared to the official estimate. ","TATAMS is an ongoing three year longitudinal cohort of youth (grades 6, 8, 10) attending schools in five counties within the largest cities in Texas (Austin, Dallas-Ft Worth, Houston, San Antonio). We seek to understand the diversity of nicotine products used by youth and the impact of tobacco product marketing on use behaviors. We used a multistage probability-based study design to yield representative estimates from these counties. The sampling frame was built using school data from the Texas Education Agency (public and charter), the Texas Private School Accreditation Commission, and the National Center for Education Statistics. A random sample of 12 or 50% of all the point of sale outlets, whichever is greater, that sell tobacco within a 0.5 mile radius around each school was selected. The baseline survey collected data using tablets at schools. The five additional semi-annual surveys will be collected using a website or CATI. We provide an overview of the design and methods, sampling weights, reweighting for unit nonresponse, analysis plan, and monitoring considerations of this cohort, including its strengths and limitations. Ninety four schools consented to participate. ","Efficiency of ratio-synthetic estimator compared to BLUP estimator, both based on cross-sectional data, depends on ratios of harmonic means of auxiliary variable  values of sample units to sample sizes in respective domains, domain of interest Ui and complementary domain Uc. Assuming equal unit error variances in model-based  domain estimation BLUP estimator of domain mean or total is less efficient when compared to ratio-synthetic estimator(Ghangurde,P.D.; JSM(2014)).In this paper   efficiencies of ratio-synthetic estimators based on time series data as compared  to the same based on cross-sectional data are analyzed for assumed values of   correlation coefficient in AR(1)model for sampling errors in the regression model assuming 5 and 10 time-points or occasions with diagonal covariance matrix  for sampling errors. These efficiencies are not affected by small area domain effects in the mixed model assumed to derive BLUP estimator (pages 159-62;   Rao,J.N.K.(2003)). Empirical results can be similarly obtained for ratio-synthetc estimators in household surveys with rotation sample designs. ","Missing data are frequently encountered either by chance or design. A naive analysis with complete cases can lead to biased estimation and inefficiency. Imputation is a process of assigning values to the missing items with the objective of reducing bias and improving the efficiency. In survey, fractional imputation (FI) is attractive in three fold. First, it constructs imputed values with fractional weights which facilitates full-sample estimators. Second, it allows consistency among different users. Third, FI procedure furnishes good estimates of distribution function and the resulting fractionally imputed data meet the goal of multiple use. In more general disciplines, the FI method can be a substitution for a computationally difficult or intractable expectation step in the EM algorithm. We demonstrate the empirical relevance of FI using simulation designs, compared to the multiple imputation method. ","The Current Population Survey Annual Social and Economic Supplement (CPS ASEC) serves as the data source for official income, poverty, and inequality statistics in the United States. The Census Bureau has used a \"hot deck\" procedure to impute missing income values since 1962. This paper implements an alternative model-based methodology, sequential regression multiple imputation (SRMI), to impute missing income values in the CPS ASEC. SRMI offers several potential advantages over the current hot deck method, including 1) greater flexibility to add additional covariates and 2) using multiple imputation to account for uncertainty in the imputation process. We implement a baseline SRMI with data from the CPS ASEC and then augment this with tax records on earnings from the Social Security Detailed Earnings Records (DER) file. We compare imputed income values from SRMI to those from the hot deck procedure along several dimensions including the mean, median, variance, and common official statistics derived from income (poverty and inequality). ","Surveys usually suffer from non-response, which decreases the effective sample size. Some form of random imputation is typically used to handle non-response if we wish to preserve the distribution of the imputed variable. A possible drawback lies in an increased variability due to the imputation variance. Several approaches have been proposed for reducing the imputation variance. One of them consists in using some form of balanced imputation, where donors or residuals are selected at random in such a way that the imputation variance is eliminated (Kalton and Kish, 1981, 1984; Deville, 2006; Chauvet, Deville and Haziza, 2011).     ","Each percentage point increase in the mail response rate to the Decennial Census saves the federal government about $85 million. The Census Bureau's 2014 Planning Database contains low response scores (LRS) by geographic area. These are summary scores that reflect an area's predicted mail response rate. The populations in geographic areas with similar LRS can have different characteristics such as the proportions of non-English speakers, proportion of transient population, or the number of 18-24 year olds. These characteristic differences may require very different outreach techniques in order to boost response from mail. Using the 2014 Planning Database, this research will investigate effective methods for incorporating the LRS into a screening process to identify geographic areas for further review. We will examine how the low response score can be used in a supervised learning context to explicitly identify the most salient characteristics for describing the geographic areas, and suggest tailored outreach programs based on a particular area's characteristics. This research will be conducted with an emphasis on answering the question: Why? In so doing, it will use the reproducibl ","Topological Data Analysis (TDA) is an attempt to apply topological concepts of 'shape' to data clouds by finding clusters, holes, tunnels, etc. Specifically, the well known statistical technique, cluster analysis, is a very simple special case of TDA. The main method unique to TDA is to encode the persistent homology of a data set in the form of a parameterized version of a Betti number which is called a persistence diagram or Betti barcode. I propose to use currently available packages in R such as diffusionMap, randomForest, ggplot2, and phom(persistent homology) to apply TDA to the Census Bureau's 2014 Planning Database at the tract level. The initial step in the proposed TDA research will be to apply statistical 'lenses' such as random forests, distance matrices and possibly Principal Components Analysis to produce metrics that can be used as proximity measures. The low dimensional structure produced by these metrics can be viewed through plots such as from a diffusion matrix. Finally, the 'persistent' structure will be investigated using Betti barcodes. ","New York City park managers work with limited information when responding to natural disasters. They base decisions largely on institutional knowledge, finding it difficult to incorporate the big data generated from residents using the City's 311 system to report damaged and downed trees. Even worse, when used, analysts interpret call patterns as reflective of need without accounting for variability in individuals' underlying propensity to use 311.     We are working with City agencies to investigate alternative, highly-interpretable models for predicting the propensity of individuals to call 311 in the hours and days following major storms. The result will leverage socio-demographic information from the decennial census and a New York-specific \"tree census,\" to help the City predict and understand the relationship between the number of requests for tree service in a given census tract and the actual need. This work will allow public managers to make better decisions by incorporating 311 information into the decision of where to send work crews after the next storm.    ","The concerns on survey errors and the rise in survey costs are exacerbated by the increase in survey nonresponse. To optimize the cost-error tradeoffs we propose, in a multi-phase survey, to enhance the representativeness of respondents and the prediction of missing responses through adaptive sampling and imputation. The adaptive sampling continuously benchmarks a focal survey to a high quality benchmark survey. The nonresponse imputation builds upon the Census Planning Database (PDB). Models predicting sample characteristics are fitted to the data from the benchmark survey, which shares the same contextual information (PDB) as the focal survey. The PDB bridges the prediction of missing data with the data from respondents and the benchmark. With better prediction of nonresponse, estimation of nonresponse errors and decision on nonresponse follow-up become a data-driven strategy. The 2010 PDB, ACS and CPS are used to illustrate the proposed method. The method is evaluated by various cost and error decisions, including comparison to the conventional fixed sampling design that uses post-survey weighting and nonresponse follow-up. ","While Fully Conditional Specification (FCS) or chained equations is a popular procedure for performing multiple imputation under the Missing at Random (MAR) assumption, it has lacked a principled approach for handling data that are Missing Not at Random (MNAR). By considering the full conditional distributions implied by a specified joint distribution for the substantive variables and their missingness indicators, we demonstrate how the standard FCS procedure can be extended to impute under general MNAR missingness mechanisms. Assuming that this joint distribution follows a loglinear or conditional Gaussian model, we examine the finite sample properties of the extended procedure in a range of simulation studies. We place our results in context with recent work on the theoretical properties of the standard FCS procedure. Finally, we use the procedure to assess the robustness of two recently published analyses of data from the Avon Longitudinal Study of Parents and Children to departures from MAR: these analyses explored exposure to a parental suicide attempt before age 11 and intelligence measured at age 8 as potential risk factors for self-harming behaviour at age 16, respectively. ","This paper presents a comparison of two full information maximum likelihood (FIML) approaches to traditional imputation approaches for addressing item nonresponse in a complex survey. Item nonresponse is an issue researchers using public use files for survey data often encounter. There are several techniques for dealing with item nonresponse in categorical data. A relatively new technique for handling missing data is FIML which incorporates all response patterns during the estimation process rather than ignoring cases with missing values. In 1982, Fuchs' proposed a FIML method to handle nonresponse missing at random (MAR); Fay (1986) expanded the FIML method to handle nonresponse not missing at random (NMAR) by incorporating item nonresponse indicators into the model. The National Survey of Drug Use and Health (NSDUH) is an annual national and state level survey that collects information on the use of tobacco, alcohol, illicit drugs and mental health in the U.S. Several variables on the NSDUH undergo a weighted sequential hot deck imputation (WSHD). Using data from the NSDUH, these two FIML approaches are compared with listwise deletion (i.e., MCAR) and WSHD imputation techniques.  ","In government survey applications, zero-inflated count data often arise, sometimes with item nonresponse.  We consider the problem of imputing missing counts.  We assume that the observations/items are missing at random.  We also assume the zero-inflated data is a mixture distribution: one component from a distribution degenerate at zero and one from a Poisson distribution.  Both components may depend on covariates, which are always observed.  We formulate a model for bivariate zero inflated count data and propose a Bayesian imputation scheme for imputing missing items by assigning priors to unknown regression parameters.  Using the predictive distribution of missing items given observed items, one can impute each missing item as a random draw from the predictive distribution.  We use Markov Chain Monte Carlo to generate imputed values of missing items.  Multiple imputations are computed by running the chain long enough to produce multiple realizations of the missing item values.  To obtain (nearly) independent draws, the chain must be thinned.  We will illustrate its potential with a simulation study and with an analysis of part-time Employment data from the Annual Survey of Public Employment &amp; Payroll (ASPEP).  ","This paper presents an analysis to determine the best approach to impute household income in a large, national survey. The National Crime Victimization Survey (NCVS) is a multi-stage, rotating panel design of households sponsored by the Bureau of Justice Statistics. The NCVS is designed to allow estimation of annual counts and rates of criminal victimization for both the population as a whole as well as subgroups of interest. Due to the strong relationship between socioeconomic status and criminal victimization, household income is a key characteristic often chosen to partition the population. Like many other surveys, the NCVS suffers from a high rate of missing data on household income and, while weighting is used to adjust for unit nonresponse, nothing is currently done to address item nonresponse. Failure to properly account for this missing data can lead to a loss of power and potentially biased estimates. We evaluate several potential approaches to imputing missing income data in the NCVS and assess each option on several criteria including consistency, variability, and usability. Final imputed results are also compared to the ACS for external validation of the chosen method. ","In this paper, we propose a new method of imputation for imputing missing values by making use of sensible constraints on a study variable and auxiliary variables which are correlated with the variable of interest. The resultant estimator based on these imputed values is shown to lead to the regression type method of imputation in survey sampling. Further, when the data are missing completely at random (MCAR), the resultant estimator is shown to be a consistent estimator and has asymptotic mean squared error equal to that of the linear regression method of imputation. ","Real world data collection must take into account participant burden - for example, in studies such as ours, the amount of time to answer a survey must be short. One way to avoid reducing the number of items, while minimizing response time is to create planned missing data patterns. Data for the current study was taken from a larger tobacco cessation study for young adults where surveys were administered in bars. As such, only 2/3 of participants were given certain items. We focus on the 13-item Social Prioritization Index (SPI) for this analysis. We examined how latent class analysis (LCA) can be used to examine class membership based on the SPI using two imputation approaches. First, we used important variables in the dataset to impute all of the SPI items, and ran LCAs on each of the ten imputed datasets. Second, we ran the LCA on the original non-imputed dataset, and then used the class information and other variables available in the raw data to impute class membership probabilities for each participant. The optimal number of classes across all the LCAs appeared to be five. As such, for each individual we had a 5 (class) by 20 (imputation) matrix where each cell of the matri ","Multiple imputation is a commonly used method to deal with incomplete data sets and is used by researchers on many different analytical levels. Imputation substitutes missing data with some values instead of discarding the entire case from the analysis. Multiple imputation based on log-linear modeling is a useful way to solve missing-data problems for categorical variables (Schafer 1997), though log-linear models have limitations especially when dealing with large data sets with more than a few incomplete categorical variables because of sparseness issues. . The latent class model is a plausible multiple imputation tool to solve this problem (Vermunt 2008), as is hot-deck imputation (Rubin 1987). The latter does not rely on model fitting for the variable to be imputed, and thus is potentially less sensitive to model misspecification than are imputation methods based on a parametric model, such as with regression imputation (Andridge &amp; Little 2010). In this study, we are proposing a restricted latent class model as a multiple imputation tool and comparing it with the unrestricted latent class model and the hot-deck imputation method. ","This paper presents a simulation study of some properties of size-based probability sampling with unequal unit-level costs, subject to constraints on aggregate costs.  Principal emphasis centers on the distribution of realized sample sizes; and on the distribution of estimation errors for a ratio estimator of per-unit population means.   ","Two-phase sampling is a long-standing sampling method. It identifies subpopulations of interest in the first phase of a survey, from which a random subsample is selected in the second phase for further data collection, using the new information to further stratify or to narrow the survey population to a particular subgroup. It is also used to randomly subsample survey nonrespondents for more intensive follow-up. In this context, the phase-one nonrespondents are considered a subpopulation that is identified after data collection efforts have been completed with the initial mode and protocol. The more intensive phase-two data collection protocol is generally more expensive to implement than the first and is expected to have a greater success rate. However, budgetary constraints generally limit how many nonrespondents data collectors can attempt to contact using this more expensive protocol. Hansen et al.'s (1953) work provides optimal values for the fraction of phase-one nonrespondents to be subsampled for phase two (1/k) and for the initial sample size (M) in a two-phase sample with a subsample of proportion 1/k. However, these calculations assume that phase-two methods result in 100 percent response, which is not often the case in real-world scenarios. In this paper, I derive new optimal values for M and k under the more realistic scenario in which not all phase-two attempts result in a response. ","Neyman allocation of the sample under stratified random sampling is among the top major advances and most widely used methods in probability sampling theory because it minimizes sampling variance. Unfortunately, (1) Neyman allocation does not usually yield integer solutions; (2) rounding to integers  generally does not guarantee minimum sampling variance unless the sample size is increased; and (3) it can result in a sample size n_h for stratum h that exceeds the overall size of the stratum N_h. This paper presents  simple exact optimal allocation algorithms that yield integer solutions which minimize the sampling variance and avoid the possibility of calculating n_h where n_h &gt; N_h.    ","Statistical learning is both a theory and a group of algorithms for machine learning. These algorithms detect and learn patterns in data that, in turn, can be used to predict outcomes. These methods have become very popular in recent years and have been successfully applied in many fields. One criticism is that these methods are mainly used as black boxes without a good understating of how they work. Clark (2013) developed a novel approach to optimal sample allocation in stratified design based on a statistical learning approach (SL). However, the SL sample allocation is not without problems when compared to simpler allocations. In this paper we expand the research on the SL approach and re-evaluate its performance. We describe the mechanism behind the SL sample allocation for situations not previously considered. ","Textbooks provide guidance on the general principles and desirable properties of defining sampling strata. This paper reviews some basic exploratory methods for determining stratification variables, including principal component analysis, cluster analysis, correlations, regression analysis, and decision trees for reducing the set of potential variables. Although all stratification methods use auxiliary variables available for the entire frame, the decision tree, regression, and correlation approaches also use prior outcome data, which may be available for just a sample of units. The principal component method combined with cluster analysis, on the other hand, focuses on relationships among stratification variables. Using both principal components/cluster analysis and decision trees, we stratify primary sampling units for the Residential Energy Consumption Survey and compare the resulting strata. ","A single primary sampling unit (PSU) per stratum design is a popular design for estimating the parameter of interest. Although, the point estimator of the design is unbiased and efficient, an unbiased variance estimator does not exist. A common method for the variance estimation of this design is based on collapsing or combining two adjacent strata, but the attained estimator of variance is not design-unbiased. If in some situations an unbiased estimator of variance is needed, the 1 PSU per stratum design with collapsed stratum variance estimator cannot be a good choice, and some statisticians prefer a design in which 2 PSUs per stratum are selected. In this talk, we first compare a 1 PSU per stratum design to a 2 PSUs per stratum design. Then, we propose an empirical Bayes estimator for the variance of 1 PSU per stratum design. To protect the over-shrinking in Empirical Bayes, we investigate the potential of the constrained empirical Bayes estimator. Using a simulation study, we show that the empirical Bayes and constrained empirical Bayes methods outperform the classical collapsed stratum variance method in terms of empirical relative man squared error. ","In official statistics, data collected from scientific sampling methods are considered as the \"gold standard\" from which statistically valid descriptive and analytic inferences can be made. While Big Data, being digital trails, are generally inexpensive to collect, they invariably suffer from different levels of coverage bias, representational bias and measurement errors. On the other hand, Big Data, where properly harnessed, can be used to provide statistical products more frequently, with more details at the small area or small domain levels, and cheaper, than those provided from censuses and surveys. The challenge to the official statistician is to find an efficient and effective way to harness Big Data, in such a way that the official statistics derived such sources continue to provide the high level of reliability available from survey data. In this talk, we shall present a methodology in which survey data and Big Data are integrated to formulate a State Space model for predicting the finite population parameters of interest to the official statistician. We shall illustrate the methodology using a Big Data source. ","Combining information from different source is an important practical problem. The source of information can come from a probability sampling with direct measurement, from another probability sampling with indirect measurement (such as self-reported health status), from auxiliary area level information, or from a non-probability sample (such as administrative records from a particular program). Such problems has been addressed in the context of area level small area estimation. In this talk, we are interested in combining information from three sources. The first source is the June Area Survey (JAS). In JAS, the sample observations are obtained from a probability sampling. The second source is from the Farm Service Agency (FSA) data. The FSA data is obtained from a voluntary participation of certain programs. The third source is from the classification of the Cropland Data Layer (CDL).     ","The USDA's National Agricultural Statistics Service conducts multiple surveys for major crops, including  winter wheat, during a growing season. These surveys are designed to capture the current status of crops at  state, regional and national levels. Each of the surveys also provides an indication of end-or-season yields.  A Bayesian hierarchical model gives improved yield forecasts by combining these indications from three  different types of surveys together with auxiliary data. Modeled state forecasts are benchmarked against  a regional forecast and rigorous measures of uncertainty are provided. Advantages of this model are the  flexibility for inclusion of new sources of auxiliary information and the incorporation of expert judgment  while retaining reproducibility and estimability of standard errors. The model for winter wheat is shown to  perform well over a wide variety of conditions, as illustrated by data from the 2012 crop year. ","Small area estimation has been extensively studied in the literature under unit level linear mixed models. In particular, empirical best linear unbiased predictors (EBLUPs) of small area means and associated estimators of mean squared prediction error (MSPE) have been developed. However, EBLUPs can be sensitive to potential outliers in the data. To deal with outliers, robust EBLUP methods have been developed in the framework of linear mixed models. In this research, we relax the assumption of linear regression for the fixed part of the model and replace it by a weaker assumption of a semi-parametric regression. Approximating the semi-parametric mixed model by a penalized spline mixed model, we develop robust EBLUPs of small area means and bootstrap estimators of MSPEs. We investigate the empirical properties of the proposed estimators using simulations.    ","The Survey of Income and Program Participation (SIPP) is designed to make national level estimates of changes in income, eligibility for and participation in transfer programs, household and family composition, labor force behavior, and other associated events. Used cross-sectionally,  the SIPP is the source for commonly accepted estimates of disability prevalence, having been cited in the findings clause of the Americans with Disability Act. Because of its sample size, SIPP is not designed to produce highly reliable estimates for individual states nor any estimates for  individual counties. The American Community Survey (ACS) is a large sample survey which is designed to support estimates of characteristics at the state and county level, however, the questions about disability in the ACS are not as comprehensive and detailed as in SIPP. We propose combining the information from the SIPP and ACS surveys to improve estimates of disability (as defined by SIPP) by a generalized regression predictor. Both the Bayesian and Frequentist approaches will be explored.  ","Demand for reliable estimates for characteristics of small areas  has considerably increased worldwide due to a growing use of such estimates in formulating national policies and programs, allocating government funds, planning regional development, and making decisions at the local level. However, cost and operational considerations rarely make it possible to get a large enough sample at the small area level to support direct estimates with adequate precision for all domains of interest. Model-based methods are used to produce reliable estimates. We consider an adaptation of the Fay-Herriot model for the area-level data where one covariate is measured with error. We consider structural measurement error model and a semi-parametric approach to produce accurate prediction intervals for small area means. We replace the normality assumption of the sampling error and the normality assumption of the measurement error of a covariate by heavy-tailed distributions. Estimating the unknown measurement error density nonparametrically, we develop both point estimates and prediction intervals of small area means. We present an expansion of the coverage error of the proposed prediction intervals.   ","The Fay-Herriot model has received great appreciation among model-based survey researchers in the past few decades. The classical Fay-Herriot model assumes normality for the random small area effects, which may not be appropriate in various circumstances, particularly in the presence of outliers. We discuss different cases where the normality assumption for random effects may lead to poor small area estimates and subsequently propose two robust extensions of the Fay-Herriot model. In this paper we focus on a hierarchical Bayesian approach in order to build the models. The performance of these models is examined through an extensive simulation study. ","Many generalized linear mixed models (GLM) for small-area estimation problems use only one parameter for model variance. The Fay-Herriot model and its many successors adopted this modeling structure for its relative simplicity in obtaining a theoretical derivation on the uncertainty of the estimator. However, this is also a major limitation for a small-area model. We study a small area model with a mixture of unknown variances. The standard Expectation-Maximization technique is applied to obtain empirical Bayes model estimates. We illustrate our findings through simulations and American Community Survey data. ","In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments' confidentiality. Agencies potentially can manage these risks by releasing synthetic microdata, i.e., individual establishment records simulated from statistical models designed to mimic the joint distribution of the underlying observed data. Previously, we used this approach to generate a public-use version---now available for public use---of the U.S. Census Bureau's Longitudinal Business Database (LBD), a longitudinal census of establishments dating back to 1976. While the synthetic LBD has proven to be a useful product, we now seek to improve and expand it by using new synthesis models and adding features. This paper describes our efforts to create the second generation of the SynLBD, including synthesis procedures that we believe could be replicated in other contexts. ","We describe and analyze a method that blends records from both observed and synthetic microdata into public-use tabulations on establishment statistics. The resulting tables use synthetic data only in potentially sensitive cells. We describe different algorithms, and present preliminary results when applied to the Census Bureau's Business Dynamics Statistics and Synthetic Longitudinal Business Database, highlighting accuracy and protection afforded by the   method when compared to existing public-use tabulations (with suppressions). ","When a statistical agency releases survey data to the public, the agency is responsible for disseminating high-quality data while protecting the privacy of respondents. As collected, data often contain missing, inconsistent or implausible values. Agencies prefer handling those values by imputation process and editing process followed by disclosure limitation process. To date, the three processes have been largely disconnected, and the impact of each data processing to final inference with released data is often unclear. In this study, we suggest a multiple imputation approach for simultaneously handling missing and faulty data and then generating synthetic data, leveraging a nonparametric Bayesian model. More specifically, the synthesizer generates synthetic data that preserve joint distributional features of the original data and lead to final inference that appropriately reflects the uncertainty introduced by imputation, editing and synthesizing processes. We apply the method to generate synthetic public use datasets for the 2007 U.S. Census of Manufactures. ","Organizations collect private data of great interest to public researchers, which means finding ways to release information about these data while maintaining the confidentiality of study participants is paramount. Synthetic data, generated from models built on the true data, is one of the favored methods for doing this, but it is typically difficult to give theoretical bounds on privacy loss due to the release of these data. We explore the privacy properties of several synthesis models (including differential privacy levels). We also explore the properties of various \"fidelity measures\", which gives researchers an indication of how close the results of a query on the synthetic data would be to the results that would come from using the true data. ","Panelist turnover forecasting is the key to successfully managing the size and the distribution of a survey panel and is crucial for budget control and operational efficiency. Traditional predictive modeling approaches, such as logistic regression and decision tree, enable us to predict whether panelists will turnover within a certain period, but as the period gets shorter, the prediction error will increase and go beyond a reasonable level. This paper proposes a survival analysis approach to forecast the lifetime function of panelists with differing demographics. In particular, a discrete time survival model was developed to allow for flexible time interval selection, and a time-varying seasonal covariate was adopted to control the seasonal differences of turnover behavior. The model was applied to the Nielsen Audio PPM (Portable People Meter) panel data, and was implemented to support production decision-making including sample selection volumes. The estimated parameters indicated that panel turnover is influenced by multiple demographic factors and displayed a strong seasonal pattern. The out-of-sample forecast accuracy measured by Mean Absolute Percentage Error was 6%. ","Each time an established periodic survey introduces a newly redesigned field instrument, there is a risk of introducing between-year measurement error that can interrupt the continuation of statistical trend.  Regardless of improvements to the measures of interest, a redesigned questionnaire introduces potential instability to the validity of trend estimates for outcomes of interest between survey years.  This paper will discuss sources of measurement error specific to redesigned survey instruments, describe methods for evaluating estimate comparability between survey years in the event of a redesign, and provide a solution for adjustment to correct for measurement error, illustrated by select examples from the 2011 Police-Public Contact Survey (PPCS). ","The Major League Baseball season runs March through September. Each of the 162 games is broadcast over the radio. Nielsen estimates the individual game audiences using electronic measurement in rolling panels. Every day, the number of panelists who wore their meter that day is assigned a sample weight, and their radio exposure data is recorded.    ","Aging research often involves unavoidable incomplete data measured or observed longitudinally on individuals. Inappropriate handling of the presence of nonresponse or attrition may result in biased or inefficient inference. In this study, we attempt to fill in an extension to the situation in which there are non-ignorable or no sequential ignorable missing bounded outcomes, such as MMSE scores (range: 0 to 30), observed longitudinally incorporating with a missing categorical covariate at baseline, such as ADLs. The primary objective is to tackle and alleviate the complexity of model estimation for normal or skew-normal censored distributed outcomes through MCMC sampling techniques combined with a nonparametric Bayesian imputation for incomplete covariates. We apply the proposed method to a data set arising from H-EPESE study to assess the effect of physical activity on cognitive functioning in late life. ","The Medicare Current Beneficiary Survey, sponsored by the Centers for Medicare and Medicaid Services in partnership with the Center for Medicare and Medicaid Innovation, is a continuous, in-person, longitudinal survey of a representative national sample of the Medicare population. The survey was designed to aid in administering, monitoring, and evaluating the Medicare program. Historically, the survey samples have been obtained using a multistage sample design in which beneficiaries were selected within ZIP-code areas, which in turn were selected within 107 primary sampling units.  This paper reports on a redesign of the second stage of sampling implemented in 2014. Instead of using ZIP-code areas as second stage units, we selected a set of Census tracts, thus reducing the burden of maintaining the second stage units and allowing for easier merging of survey data with aggregate-level environmental data. The paper also reports on an expansion of the sampling frame being implemented in 2015 to include persons who are age 64 and will become eligible for Medicare in the coming year, thus permitting the release of data products up to a year earlier than in the past. ","The Medicare Current Beneficiary Survey is an overlapping panel survey conducted in three rounds a year that samples Medicare Enrollees using enrollment data to identify respondents and then links their responses to Medicare claims data in order validate self-reported data and to help adjudicate the source of payments for a variety of medical events and services. The claims match process, while simple in the sense that the survey and claims share a beneficiary number, in practice is made complicated by that the claims must be allocated to the correct event at the correct time. This sorting and matching process relies on survey-reported information about the payor or insurer and the health care provider requesting payment to provide initial information for matching. The reliability of survey-reported information clearly rests on the nature of the instrument and the accuracy with which the interviewers report that information. In this paper, we discuss the use of insurance and health care provider lookup functions built into the MCBS CAPI questionnaire that share with the CMS claims files underlying data bases. These lists include the National Plan and Provider Enumeration Sys ","The Medicare Current Beneficiary Survey (MCBS) is the most comprehensive and complete survey available on the Medicare population and is essential in capturing data not otherwise collected through CMS operations/administration. The MCBS collects information that plays a critical role in the monitoring and evaluation of key provisions of the Affordable Care Act (ACA), such as analyzing and monitoring trends in health disparities. In response to Federal initiatives to enhance the collection and reporting of key demographic data, the MCBS is undergoing several enhancements to more accurately capture demographic information and be consistent with standards set for HHS sponsored population based health surveys. This includes new oversamples of Hispanic and Asian beneficiaries, new items on limited English proficiency and sexual and gender minority status as well as modified items on race, ethnicity, and disability status. This paper will discuss these innovations and their impact on health disparities research using the MCBS. ","The Medicare Current Beneficiary Survey (MCBS) is a continuous, multipurpose survey of a representative national sample of the Medicare population. Currently, access to the MCBS requires entering into a Data Use Agreement with CMS and purchasing the files. One of the primary reasons that CMS has not offered the MCBS as a free, downloadable, beneficiary-level public-use file (PUF) is because the data include both survey and administrative information, such as Medicare claims, which increases disclosure risk. However, a MCBS PUF will allow for greater access to the data and would likely result in significantly more research being conducted using the survey. This paper will discuss the creation of a MCBS microdata PUF using methods to ameliorate disclosure risk, including an evaluation of the trade-off between disclosure risk and analytic utility. This paper will also discuss the content of the MCBS PUF, such as beneficiary measures, access to care measures, and characteristics of the health care system, allowing for research to monitor priority areas of improving the quality of care, increasing access to care, and strengthening population health. ","Total survey error (TSE) is a very valuable paradigm for describing and improving surveys, but it can be improved. One key limitation is that TSE was formulated to apply to a single, standalone survey. Yet most survey research combines and compares surveys. TSE can be extended to cover these multi-survey utilizations. TSE needs to be thought of as heavily involving the interaction of error components and the concept of comparison error should be used to extend TSE to cover multiple-surveys including trend analysis, comparative studies, and longitudinal panels. This extension of TSE will greatly improve the design of multi-surveys in general and of comparative (i.e. cross-national/cross-cultural) surveys in particular. Likewise, using TSE can greatly advance the analysis of comparative data by using it to assess and adjust for difference in the error structure across surveys. A comprehensive TSE typology should be used whenever comparative studies are designed and also whenever secondary analysis of comparative studies is carried out. In particular strict application of the TSE paradigm can help to achieve the goal of functional equivalence cross-nationally/culturally. ","Many developed countries have high quality census data or population registers that can be used to build sampling frames. In other countries, census data is out of date or traditional sampling methods are impractical or dangerous. Problems may occur at the first design stage, in which clusters are selected with probability proportional to size, due to out of date or unavailable census data. Problems can also arise at later design stages, such as persons or households selected within the clusters, because no register data are available or listing households within the cluster is not feasible. This chapter describes the options that are available to samplers in such situations. Techniques to be discussed include: random geographic cluster sampling and nighttime lights at the first stage; and reverse geocoding, random walk, respondent-driven sampling, and quota sampling at a subsequent stage. For each method, we describe the statistical properties and note the pros and cons. Throughout, we suggest the best sampling techniques as ones that minimize interviewer discretion and contain built-in opportunities for verification of interviewer performance. ","The diffusion of affordable technology in developing and transitional countries is facilitating new approaches to data collection and quality control monitoring. This includes the collection of rich paradata, with immediate access to survey and process data (including call records). Self-administered modes such as audio computer assisted self-interview (ACASI) are being used in new contexts as are the use of digital recordings, GPS, digital photography and digital fingerprinting, among other examples. These advances bring both challenges and opportunities. The use of technology can also disrupt traditional organizational structures and models as well as information flow about data production, quality and costs. This presentation will trace these developments with examples from a number of surveys, with an emphasis on studies conducted in developing and transitional countries. The presentation will also look at regional trends as many of the challenges differ by research and technical infrastructure, number of languages, and cultural traditions. Finally, we will look take a look ahead to developments and trends in this area. ","Quality control is needed during all the phases of a survey lifecycle. Surveys implemented in diverse cultures are no exception; the diversity of space, time, infrastructure, and expertise makes quality control critical. Many cross-cultural surveys are interviewer-administered and are conducted in less than ideal circumstances where interviewing traditions differ, multiple contractors are hired to implement the work and interviewer workloads vary greatly. Under such conditions, interviewers become an important source of survey error (variance/bias), and close monitoring becomes essential.   This presentation addresses interviewers as an important source of error in surveys conducted in diverse cultures. It discusses some of the cutting-edge methods that have been implemented to monitor interviewer behavior and reduce interviewer error in a number of surveys including two panel surveys in India, a national mental health survey in the Kingdom of Saudi Arabia, a cross-cultural attitudinal survey and the European Social Survey. The presentation concludes with a discussion on future directions on quality assurance and control in diverse cultures.  ","Aggregate Level Public Use File (AL-PUF) methodology is an SDL method developed at NORC. It relies on low levels of aggregation for producing arbitrary domain-level summary statistics and subsampling to introduce uncertainty. This results in imprecise estimates for very small domains, for disclosure protection, and precise estimates for large domains, promoting analytic utility. This paper demonstrates how AL-PUF can be described as a generalization of the commonly used SDL methods of k-anonymization and micro-aggregation. In AL-PUF, partitioning of data into small aggregates is similar to k-anonymization and computation of summary statistics for each analytic domain within each aggregate, is like micro-aggregation. Subsampling introduces uncertainty in summary statistics and the resulting data is amenable for standard analysis which is not possible for usual PUFs requiring deterministic suppression to satisfy k-anonymization. For AL-PUF, these traditional methods are used as a starting point and then adjusted to address utility issues. These modifications result not only in more utility, but also quantifies risk and utility allowing the balance to be thoughtfully managed. ","A new estimator for estimating the proportion of a potentially sensitive attribute in survey sampling has been introduced by forcing a constraint of independence of expected responses on the odds ratio. The proposed estimator has been compared to the estimator proposed by Odumade and Singh (2009) with equal protection to all of the respondents. The asymptotic properties of the proposed estimator are investigated through exact numerical illustrations for different choices of parameters. A nonrandomized response approach has been suggested. A scope for further research has also been pointed out.   ","In this paper, we show that the three estimators proposed by Lee, Sedory and Singh (2013) attain the lower bounds of variance at equal protection of respondents while using their proposed randomized response techniques. A comparison of range restricted estimate with an unrestricted estimate is also given. The findings are supported with a real data example. ","Many data masking techniques have been developed in the literature. In this paper, a detailed statistical analysis is carried out about the security properties of a class of them called random matrix masking (Ting et al., 2008 and Wu et al., 2014). Specifically, we quantify how much information is known by public users about the original data X using its posterior distribution given the masked data AX, XB, or both. The security analysis relies on a new definition of uniform distribution on the set of orthogonal matrices. We show that: (1) When A is a random orthogonal matrix, then X|AX is uniform over possible domain; (2) When A is a random orthogonal matrix and B is an invertible matrix with entries from U[0, 1], then X|(AX, XB) is uniform over possible domain. ","In this study, we extend Christofides model to estimate proportions of individuals with each of multiple possible attributes by means of only one randomization device. Under the extended model, a respondent reports the difference between an integer associated with his or her attribute and a random integer. A simulation study examines choices of the distribution for the random integer. The technique is illustrated using data from the 2012 Family and Gender Module of Taiwan Social Change Survey to estimate the proportions of individuals of different sexual orientations, and results are compared with results of direct inquiry from the same survey. ","Randomized response techniques (RRTs) are proposed in survey sampling as a solution to the problem of social desirability bias (SDB) when dealing with sensitive questions. RRTs reduce the SDB by providing privacy protection for respondents. However, their variances are inflated with respect to the direct questioning (DQ), i.e., the RRTs provide unbiased estimators in exchange for less precision with respect to the DQ. The success of RRTs heavily depends on the assumption that both the interviewers and all respondents fully understand and correctly apply the RRT procedure. More importantly, the sensitive question of interest may not be considered as really sensitive by most of the respondents, in which case using an RRT instead of the DQ will inflate the variance of the estimates. In this study, we propose a two-stage sampling design where one can accurately estimate the prevalence of the sensitive characteristic under study without paying the penalty of the inflated variance by choosing between the proposed model and the DQ. With the proposed model one can also estimate the probability of cheating in the population. We support the theoretical results with various simulations. ","Every three years the Survey of Consumer Finances (SCF) is conducted to collect family finance data using a lengthy survey instrument. The survey uses monetary incentives to encourage participation. During the last quarter of 2014, NORC conducted an experiment on behalf of the Federal Reserve Board to determine if offering larger incentives would encourage early participation, in Census tracts with income in the top quintile, in three US cities. Half of the respondents received a $5 bill with their initial letter. Three hundred randomly selected addresses in each city were randomly assigned to one of three initial incentive groups: $50, $100, and $150. The addresses in each initial incentive group were randomly assigned to one of two additional second phase treatments: an escalated ($75, $150, or $250, respectively) incentive or no escalation. These second phase treatments were offered to those respondents who initially refused to participate.   We will present data on the effectiveness of pre and post-paid incentives in encouraging early participation in a high burden survey with respondents in higher income communities. ","As part of a recent reorganization of its data collection activities, the Census Bureau created a new management structure. In addition to consolidating twelve Regional Offices to six, it created several new supervisory positions, like the Field Supervisor (FS) and the Survey Statistician - Field (SSF). The FS directly supervises interviewers within a geographic area, and the SSF manages multiple FSs within their collective area. At inception, only the SSF was granted rights to reassign cases to interviewers across FS areas. Initially, managers in some Regional Offices extended these rights directly to the FS. A uniform policy to reassign cases across FS areas was implemented in all Regional Offices in November 2014. This research assesses this change in management of case reassignment, specifically attempting to determine the effectiveness of the policy of allowing case reassignment by the FS early in the data collection process. After including covariates known to increase the level of effort necessary to resolve a case, as well as those affecting interview completion, multilevel models and logistic regression with random intercepts can use paradata to determine whether this modification of reassignment rights is changing data collection outcomes. ","The Census Bureau measures demographic, social, and economic characteristics of the United States population through the American Community Survey (ACS). Starting with the 2005-2009 ACS 5-year estimates, the ACS annually provides these data for small geographies and small race groups. The focus of this evaluation was to assess the reliability of the 5 year estimates for five main race groups: White, Black, Asian, American Indian and Alaska Native (AIAN) and Native Hawaiian and Other Pacific Islander (NHOPI); for specific AIAN tribal groupings; and for detailed Asian (e.g., Chinese), and detailed NHOPI race groups. For the analyses we used the 2006-2010 ACS 5 year Selected Population Tables. We used the coefficients of variation of estimates of a broad range of characteristics as measures of reliability. The ACS sample was designed to assure certain levels of reliability for estimates of geographic areas. A key point the study established was that the reliability of estimates of small race groups was comparable to that of geographic areas of similar population size. Further, we found that race groups and tribal groupings of similar population size had similar levels of reliability. ","The use of response models to group or stratify members of a target survey population into homogeneous response groups, informed by auxiliary data characterizing the sample units, is a well-established practice to reduce bias and improve reliability of survey estimates. More recently, adaptive survey design methods have extended the use of propensity models to inform tailored design changes. This paper explores the use of various Internet response models to stratify the American Community Survey (ACS) sample frame to enable a tailored initial assignment of the mail and Internet self-response modes. These models include traditional logistic regression as well as some of the more recent machine learning techniques - decision trees, random forests, boosting, support vector machines, and K-nearest neighbors. To inform the models, we augment the ACS sampling frame using administrative records. Using data from the April 2011 ACS Internet Test, we establish that offering mail or a choice of mail and Internet self-response modes are viable options for members of the low Internet response stratum. ","The Census Bureau uses American Community Survey (ACS) data to produce, on an annual basis, one-year, three-year, and five-year estimates, for time periods defined by calendar years. The data for these estimates are collected in yearly samples that are equally distributed across monthly panels, but monthly response patterns are not equal. Numerous external and internal stakeholders have expressed interest in estimates that are based on shorter subannual time periods, such as monthly estimates. The variable monthly response patterns mean that annualized weights, from the pooled samples, cannot be used to produce subannual estimates. This paper presents results of a pilot project that attempts to optimize the existing weighting methodology to produce monthly estimates of health insurance coverage using data from the monthly ACS samples. If successful, these methods could be used to study trends in fast changing characteristics or possible seasonal patterns in the data in a way that is not currently possible with annualized weights. ","The American Community Survey (ACS) multiyear housing unit (HU) weighting process includes a model-assisted step that uses a generalized regression model (GREG) to calibrate the HU weights using covariates derived from administrative records. The Census Bureau discovered that the variance estimates of some 5-year estimates at the national level may be larger than the corresponding 1-year estimates. The ACS standard error estimates are calculated using the successive difference variance estimator, which measures both sampling error and ratio estimation bias. One initial tool considered is calculating an alternative variance estimator targeting only sampling error. Some evidence exists that using this alternative variance estimator provides a possible explanation for this concern. This paper evaluates this concern using 2006-2010 ACS 5-year data. ","Multilevel Regression and Poststratification (MRP) has been shown to be a flexible and valid approach to generate small area estimates (SAEs) of health outcomes for local levels in the United States such as census tracts and counties. We applied tract-level poverty to county-level regression coefficients in model prediction. We examined whether this cross-level inference produces any substantial bias in SAEs. In this study, we fitted a multilevel prevalence model using 2012 Behavioral Risk Factor Surveillance System data and generated county-level SAEs of uninsured adults aged 18-64 years via two multilevel prediction models with county-level versus census tract-level poverty and compared them with estimates from the 2012 American Community Survey (ACS) for 814 counties and Small Area Health Insurance Estimates (SAHIE) for 3,142 counties. SAEs of uninsured adults from the prediction model using census tract poverty were almost the same as SAEs using county poverty (correlation coefficients &gt;0.99). Both were strongly correlated with ACS and SAHIE SAEs (correlation coefficients &gt;0.86 and &gt; 0.80 respectively). Thus the cross-level inference in MRP results in very minor bias in SAEs. ","The basic theory for survey sampling is well established and presented in a number of textbooks, but most texts cover the material at an intermediate level. The proposed course is designed for researchers and practitioners interested in advanced techniques and the theory underlying those techniques. The topics include asymptotic theory in survey sampling, regression estimation, optimality in estimation and design, and use of models with survey samples. Participants should have a background in survey sampling and statistical theory. Recent graduates working in survey sampling and graduate students are encouraged to attend. The book Sampling Statistics (2009) by Wayne Fuller is the text for the course.","In survey sampling, some populations happen to be hard to reach. Two main reasons are their relative rareness and the absence of a suitable sampling frame. When no sampling frame is available for the desired target population, one can then choose a sampling frame that is indirectly related to this target population. We can then speak of two populations A and B that are related to one another. We wish to produce an estimate for B by selecting a sample from A and using the existing links between the two populations. This is referred to as indirect sampling. Using indirect sampling is one way for surveying hard-to-reach populations. Other approaches such as Network Sampling, Adaptive Cluster Sampling, Snowball Sampling, Respondent-Driven Sampling, and the use of multiple frames exist. Actually, most of these approaches can be put into the context of indirect sampling. One can then use the theory and developments surrounding indirect sampling to obtain a unified mathematical framework for the above approaches. After an overview of indirect sampling and its ready application to hard-to-reach populations, the course will describe indirect sampling in the context of network sampling, adaptive cluster sampling, snowball sampling, respondent-driven sampling, and multiple frames.","In the fall of 2015, the Costs Committee of AAPOR's Task Force on the Future of General Populations Telephone Surveys fielded a web-based survey of organizations that conduct U.S. dual frame RDD telephones surveys. Organizations were recruited by direct invitation, postings to survey research listserves, and via email invitations from survey sample vendors to their client organizations. The data gathered in the questionnaire included detailed information about the production rates associated with conducting dual frame RDD surveys broken out for the landline and cell phone sides of the survey. In total, information about nearly 50 dual frame RDD surveys conducted in the past three years by 30 research organization was gathered. The cost of cell phone interviewing has gone down relative to landline interviewing, compared to earlier studies. The change in the cell-to-landline cost ratio is a product of recent enhancements to cell phone RDD frames that allow active cell phones to be identified, as well as the declining productivity of landline RDD. The data analysis also identifies key drivers of the varying cost ratios of cell phone to landline calling. ","Survey costs are an important aspect of the total survey error perspective. Surveys either attempt to minimize error for a fixed cost, or obtain a specified level of error at any cost. Decisions about how to balance costs and errors are critical for the survey's success. There is a large survey literature on various sources of error - sampling, coverage, nonresponse, measurement. But the literature on costs is smaller and more fragmented, with costs reported differently across similar studies. This paper has two purposes: 1) to explore methods used for monitoring and estimating survey costs, and 2) to propose a set of standards for cost reporting and steps to help implement these standards. The goal is to improve the reporting of costs across studies so that survey methodologists will be able to make informed decisions about cost-error tradeoffs. ","For multiple and sound reasons, survey science is largely not an experimental science. Many surveys contain embedded small-scale experiments, for example, variations in mode, question wording, and incentives. Major questions, however, such as tradeoffs between cost and data quality, or between cost and decision quality, cannot be addressed experimentally. In this talk, I attempt to elucidate the extent to which a survey microsimulator--specifically, the World's Simplest Survey Microsimulator (WSSM)--can provide credible or actionable insight into the tradeoffs between survey costs and data quality. One advantage of the microsimulation approach is that the truth about the population is known, and multiple measures of data quality can be calculated. The most problematic aspect of microsimulations is, of course, validation. Results of several numerical experiments will be presented. ","The Medicare Current Beneficiary Survey (MCBS) is a continuous, multipurpose survey of a nationally representative sample of the Medicare population, conducted by the Centers for Medicare &amp; Medicaid Services (CMS) through a contract with NORC at the University of Chicago. The survey was designed to aid in administering, monitoring, and evaluating the Medicare program. In 2015, two new improvements were made to the sample design of MCBS. This paper reports on the expansion of the annual sampling frame to include all persons who will become eligible for Medicare by December 31 of the selection year, thus permitting the release of data products up to a year earlier than in the past. In addition we report on the results of an annual oversample of Hispanic beneficiaries taken for the first time in 2015, and on current plans for an oversample of Asian beneficiaries to be taken for the first time in 2016. We discuss the outcomes of these design innovations and their likely impacts on the survey data to be released for 2015 and future years. ","To assess the initial impact of the Affordable Care Act (ACA), the Medical Expenditure Panel Survey - Insurance Component (MEPS-IC) fielded a longitudinal arm comprised of 3,000 responding small business establishments to the 2013 MEPS-IC that were eligible to use the Small Business Health Options Program (SHOP) marketplaces to purchase insurance in 2014. This longitudinal design was implemented to capture detailed transition estimates during the ACA implementation period that would be missed by just comparing independent sample estimates for 2013 and 2014. The design was planned to balance precision considerations for two key outcome variables (offer health insurance and number of enrolled employees) by select firm characteristics. The initial survey design will be described and typical survey estimates will be provided as an example of the type of estimates available from these data.  In addition, changes to the sample design for subsequent iterations of this longitudinal survey will be presented. ","The Medicare Current Beneficiary Survey (MCBS) is a continuous, multipurpose survey of a nationally representative sample of the Medicare population, conducted by the Centers for Medicare &amp; Medicaid Services (CMS) through a contract with NORC at the University of Chicago. The survey collects extensive information on health care use, costs, access to health care, and satisfaction with care. CMS releases annual files for the MCBS, including the Access to Care (ATC) and Cost and Use (CAU) files. ATC, released first, contains survey reported data among \"always-enrolled\" beneficiaries. CAU, released later, contains a combination of survey reported data matched to Medicare claims and administrative data among \"ever-enrolled\" beneficiaries. In our presentation we will discuss and review the design of the MCBS and demonstrate how to combine the ATC and CAU files. In doing so, we make 1) ATC more generalizable to the \"ever-enrolled\" population, 2) allow for more robust analysis of MCBS data in the ATC file that are not in the CAU file (i.e. preventative services, satisfaction with care, and access to care) and 3) explore potential limitations/implications of combining ATC and CAU. ","Linking administrative and survey records is a cost-effective method to enhance survey data. The Medicare Current Beneficiary Survey (MCBS), a nationally representative survey of Medicare beneficiaries, is unique because its sampling frame is the Medicare enrollment data making the survey data inherently linked to administrative data. For Original Medicare Beneficiaries, the administrative data contain information on cost and utilization. The survey questionnaire also includes these measures, along with information that is not available in the administrative data. The overlap between the two sources provides a unique opportunity to examine their agreement. Beyond the valid instances where a claim is not linked to the event reported in the survey (e.g. due to Medicare Advantage enrollment) this examination can help highlight the potential for measurement error in the survey. In this paper we will compare beneficiary characteristics for those with survey only, claims only, and linked survey and claims data across certain health care event types. The results from this research will inform ways to use Medicare administrative data to improve the MCBS design and data collection. ","The Medical Expenditure Panel Survey Household Component (MEPS-HC) is a nationally representative survey of the U.S. civilian noninstitutionalized population, conducted every year since 1996. The MEPS-HC is based on an overlapping panel design that collects data on healthcare utilization and costs for all persons in sampled households. Each participating household is targeted for interview 5 times, and the collected data cumulatively cover a two year period. Due to the amount of detailed information gathered during the interview, participants may 'learn' to avoid reporting medical events to reduce the burden of answering additional questions for each event. In this study, we examine whether evidence exists of this 'respondent fatigue' by analyzing trends in healthcare reporting. The overlapping panel design provides two avenues for this analysis. We first compare differences in event reporting for a calendar year between the 'new' panel in its first year of data collection with the 'old' panel in its second year of data collection. Second, within panels we compare event reporting in year 2 to expected levels based on health status characteristics in year 1. ","Estimates of direct survey variances for areas with small sample sizes is problematic especially when combined with complex survey designs. However, reasonable estimates of these variances (or their associated design effects / effective sample sizes) are important when modeling data from these surveys. One important application is area-level small area models such as Fay and Herriot (1979). Other model-based uses include calibrating the likelihood to reflect the amount of uncertainty relative to a simple random sample. An ad hoc practice to estimate design effects or effective sample size for counts and rates in small areas and domains is to estimate the design effect for a larger aggregate and assume that the component areas or domains have the same design effect. This is valid only under restrictive conditions. Starting from the framework of a stratified sample, we will explore how the design effect at the aggregated level compares to the design effects at the lower level. We will study these design effects and propose a method to estimate them under several scenarios: unequal probability selection, unequal area means, and clustering within area. ","Nielsen regularly produces reliable survey design-based audience estimates of radio listening within the radio Metro markets in the US. This is accomplished in the larger markets using electronic measurement within radio panels, where sampled households report their listening for up to two years. However, audience estimates from these panels are sensitive to panel fluctuations and outlier households that may exhibit extreme listening. This may cause serious changes in the rankings of radio stations in a market between consecutive survey periods.In this study, we investigate the application of both design-based and Bayesian methods of ranking radio stations within each market. Stations are typically ranked using only the mean listening per time period, which ignores the reliability of each individual mean. We will assess the changes in ranking when the variances of the means are incorporated into the ranking and estimate the rankings for small demographic groups. We will utilize a model-based method, specifically small area estimation techniques, and compare the result to those of the typical design-based estimates.  ","It is nearly impossible to directly estimate diabetes incidence rates for most US counties due to the small number of individuals surveyed and the very low fraction of new cases observed in each county. Small area estimation (SAE) methods that use Bayesian hierarchical models help address this problem by borrowing strength from other counties and states. Still, the accuracy of the incidence estimates is limited by the lack of data; some counties have no survey data whatsoever. Using SAE models, county incidence is estimated by drawing from the posterior distribution, which is comprised of prior and likelihood factors. In counties with few or no observations, the posterior draws are heavily influenced by the selected prior distribution. To improve our estimates, we implemented a Bayesian hierarchical SAE model with an informative (power) prior that adds relevant historical information. We used survey and simulated data to examine our proposed model. Compared with the diffuse prior, in terms of relative root mean squared error (RRMSE), the power prior can improve estimation accuracy by 21% when 40% of counties have zero observations, and by 27% when 60% of counties have no data. ","Assuming sample survey framework of two domains, domain of interest Ui and complementary domain Uc in sample design strata, unequal unit error variances proportional to auxiliary variable values, an assumption appropriate in household surveys, ratio-synthetic estimator was proved to be more efficient than BLUP estimator. Two components of approximate efficiency of ratio-synthetic estimator were derived, assuming known domain population totals or auxiliary variable totals (Ghangurde,P.D.(2014)). In this paper unit error variances are assumed to be equal, an assumption appropriate in most other sample surveys. Approximate efficiency of ratio-synthetic estimator is derived by unconditional analysis. The results in the case of sample surveys under unified model are similar to those in earlier paper. Approximate efficiency under two models are compared assuming that auxiliary variable has lognormal distribution. Ratio-synthetic is more efficient than BLUP in sample surveys under standard regression model assuming unequal or equal unit error variances; it is simple to use as compared to BLUP. Some applications in survey practice and methods to obtain domain totals and means are reviewed. ","In population studies, it is standard to sample data via designs in which the population is divided into strata, with the different strata assigned different probabilities of inclusion. Although there have been some proposals for including sample survey weights into Bayesian analyses, existing methods require complex models or ignore the stratified design underlying the survey weights. We propose a simple approach based on modeling the distribution of the selected sample as a mixture, with the mixture weights appropriately adjusted, while accounting for uncertainty in the adjustment. We focus for simplicity on Dirichlet process mixtures but the proposed approach can be applied more broadly. We sketch a simple Markov chain Monte Carlo algorithm for computation, and assess the approach via simulations and an application. ","Hierarchical modeling has been used extensively for small area estimation. However, design weights that are required to reflect complex surveys are rarely considered in these models. We develop computationally efficient, Bayesian spatial smoothing models that acknowledge the design weights. Computation is carried out using the integrated nested Laplace approximation, which is fast. An extensive simulation study is presented that considers the effects of non-response and non-random selection of individuals, allowing examination of the impact of ignoring the design weights and the benefits of spatial smoothing. The results show that, when compared with standard approaches, mean squared error can be greatly reduced with the proposed methods. Bias reduction occurs through the inclusion of the design weights, with variance reduction being achieved through hierarchical smoothing. We analyze data from the Washington State 2006 Behavioral Risk Factor Surveillance System. The models are easily and quickly fitted within the R environment, using existing packages. ","Ideally, it is possible to make inferences on a population using a single survey. This requires a well-developed survey design, such that the collection of responses obtained are representative of the population of interest. Unfortunately, obtaining single-source data can be challenging. In these cases, data integration is necessary. Methods exist for combining information sources when each source is representative of the population, however in cases where some datasets are non-representative, data integration becomes notably more complicated. In this talk we discuss methods for data integration involving categorical variables when some data sources are representative and others are only conditionally representative, i.e. can be considered representative samples from some conditional distribution involving the survey variables. Our method leverages Dirichlet process mixtures of products of multinomials to parsimoniously and flexibly model the dependencies among variables. We present simulation studies illustrating problems that arise when all data sources are treated as representative and show how these can be alleviated. ","Bayesian models are increasingly employed by the U.S. Bureau of Labor Statistics (BLS) to render statistics, such as total employment, because these models readily account for structural dependencies in the data and estimate the full distribution from which variance estimates are computed. The estimation of posterior distributions is, however, computationally expensive. BLS collects data under informative sampling designs that assign probabilities of inclusion to be correlated with the response and induce a dependence among sampled observations. This article extends a computationally-scalable approach by composing the barycenter for a collection of pseudo posterior distributions estimated on disjoint subsets of the full data in the Wasserstein space. The extension generalizes the idea of stochastic approximation to calibrate uncertainty estimation on subset likelihoods by incorporating sampling weights. We construct conditions on known marginal and pairwise inclusion probabilities that define a class of sampling designs where consistency of the barycenter pseudo distribution is achieved. We demonstrate the result on an application to the Current Employment Statistics survey. ","Many analyses involve fitting regression models to sample survey data and these surveys use complex design features such as unequal probabilities of selection, geographical clustering and stratification based on key variables. Typically, these design features are related to the outcome or covariates in the regression model. Ignoring the design features may lead to biased estimates, especially if the population model is not perfectly specified. Since all models are approximations, it is important to tease out these design features before fitting the regression models. Furthermore, many data sets may be subject to both unit and item nonresponse. The basic idea is to \"uncomplex\" the complex survey design, handle missing data and fit the population level regression model, all simultaneously. A Bayesian framework is used to implement this idea to derive an iterative approach for estimating the parameters of regression models under a variety of scenarios. A simulation study investigates the repeated sampling properties of the estimates. The method is illustrated using data from National Health and Nutrition Examination Survey. ","The Bureau of Labor Statistics (BLS) is redesigning the Consumer Expenditure Survey (CE), which provides data on the buying habits of America's consumers. One of the new components, the in-person recall interview, requires respondents to report their household's expenditures for 38 expenditure categories for the preceding three-month period. Theoretically, asking a \"global\" question -- about expenses for an aggregate or high-level expenditure category -- requires less time and is less burdensome for both interviewer and respondent than asking a longer series of questions about individual items within a category. A study to test 116 global questions covering all 38 expenditure categories was completed in 2015. The main objective was to identify the appropriate level of aggregation and wording of questions needed to capture accurate expenditure data. Westat conducted a total of 85 cognitive interviews in nine iterative rounds to test and refine the questions. This presentation will cover the study methods and results. The focus will be the lessons learned in balancing specificity vs. generality in wording of the questions, with the goal of achieving accurate expenditure reporting. ","The Gemini redesign proposal for the Consumer Expenditure Survey includes an emphasis on records to aid in the reporting of difficult-to-recall expenditures. Research was needed to understand how this new interview protocol would affect the length, burden, and respondent use of records during the interview. For this study, two new protocols were tested: 27 participants were assigned to a \"respondent-track\" protocol and 25 participants to an \"interviewer-track\" protocol. In the respondent-track protocol, participants determined the order of questioning and maintained control of records. In the interviewer-track protocol, interviewers followed a fixed order of questioning and controlled the records during the interview. Analyses show that participants in the respondent-track group reported significantly more of their expenses using records as compared to the participants in the interviewer-track group. This difference may be explained by two causes: an interviewer effect, whereby one interviewer had significantly less records usage than the other five interviewers; and more use of electronic records in the respondent-track group. And, qualitative feedback from participants emphasized the importance of advance communication for setting participant expectations and understanding of the interview content. ","The U.S. Bureau of Labor Statistics (BLS) began the Gemini Project in 2009 with a goal of redesigning the Consumer Expenditure Survey (CE) as a response to increasing evidence in measurement error, declining response rates, the emergence of new data collection technologies, and a need for flexibility in addressing changes in the interviewing environment. In 2013, a redesign plan was approved. In order to test the basic underlying structure and components of the new design, a Proof of Concept (POC) test was fielded from July through October 2015. The main objective of the test was to assess the feasibility of completing one wave of the survey redesign, i.e., whether a single-sample expenditure survey design consisting of two personal interviews, individual diaries, incentives, record usage, and technology usage (electronic diaries) was feasible. The redesign \"concept\" would be proved if no major methodological, operational, or experience issues were evident. This presentation will discuss the POC test's design and results (e.g., response rates, interviewer and respondent feedback, expenditure data), and will be of interest to survey methodologists, data producers, and data users. ","Data collection field staff perform an essential role in the implementation of any non-self-administered survey instrument. Critical to the quality of collected data, field staff also have a unique perspective on the effectiveness of survey protocols, and potential improvements. The Bureau of Labor Statistics (BLS) Consumer Expenditure Survey (CE) program periodically asks field staff to provide feedback about data collection procedures, questionnaire design, and general topics of survey methodological interest. The 2015 Field Staff Survey sought feedback on (i) the protocol for a major upcoming survey redesign testing and implementation, (ii) using records as an aid to respondent recall, (iii) monetary incentive use, (iv) a proposal to return tabulated data of interest to respondents as a non-monetary incentive, and (v) the use and potential expansion of a Contact History Instrument (CHI) for tracking contact attempt-based observations. This presentation will focus on recommendations from field staff regarding the redesign of the CE, including broader methodological lessons learned, as well as implementation plans based on recommendations.  ","Deville and Sarndall (1992, Section 4) considered calibration on the known counts (cell counts or marginal counts) of a frequency table in any number of dimensions (generalized raking procedure). In this paper, we show that a similar procedure can be applied to the case of partly known overlapping counts. As an example we consider calibration of area-month-year unemployment estimates to month-year totals from a time series model of State estimates from the Current Population Survey and area-year totals from the American Community Survey. ","Auxiliary variables are extensively used in survey sampling to construct Generalized Regression (GR) estimators, or Optimal Regression (OR) estimators, of totals and means. These estimators are also calibration estimators, reproducing for the auxiliary variables the estimated parameters when the latter are available from external sources. This paper explores the possibility of improving the efficiency of such estimators when the utilized auxiliary variables are continuous, by augmenting the set of these variables with selected exponents of them. It is shown for the case of a single continuous auxiliary variable and simple random sampling or stratified simple random sampling, that the addition of a fractional exponent of the auxiliary variable improves the efficiency of the OR estimator to the degree of the implied increase of the coefficient of determination for the study variables. A simulation study shows that this additional variable improves the efficiency of the GR estimator greatly, even when the coefficient of determination is not increased.  ","This study examines how post-stratification raking adjustments to survey weights can reduce bias due to non-coverage in order to facilitate a cross-cohort comparison between the High School Longitudinal Study of 2009 (HSLS) and the NAEP HSTS survey. HSLS is a nationally representative study of 23,000 students from 944 schools who were 9th graders in 2009. HSLS survey weights are calculated for each student, representing the inverse probability of being sampled, with adjustments for nonresponse. The study begins with a brief overview of the rationale and methodology, including coverage error, measuring bias from non-coverage, and the use of post-stratification raking adjustments. The study then examines how post-stratification raking adjustments were applied to survey weights for three distinct subsets of the full HSLS sample: public school students, public and private school students, and those students who participated in the HSLS-NAEP overlap sample. Results indicate that raking adjustments substantially reduced bias due to non-coverage in all three subsets of the data. ","A well-known technique of estimation in the field of survey sampling is to adjust the design weights attached to the data and to calibrate those weights based on the optimization of a metric measure subject to certain constraints on the auxiliary information available at the estimation stage. Emphasizing on the weight system and deriving a distance procedure with the support of calibration equations we propose new improved calibration technique for two stage sampling. We also extend the proposed technique to higher order calibration in two-stage sampling.  ","In observational studies, samples drawn from one or more study populations are used to make inferences concerning a target population. Such inferences require adjustment for population differences. Minimum discriminant information adjustment (MDIA) can be employed to weight samples to conform to known population information. In the case of simple random sampling, Haberman (1984) has derived large-sample properties for MDIA-based weighted sample means. In this paper, these results are generalized to complex sampling designs, including stratified sampling and two-stage sampling. To illustrate use of these new results and to evaluate accuracy of large-sample approximations, applications are made to a statewide program in Florida middle schools to provide reading coaching. ","The 2014 Survey of Juveniles Charged in Adult Criminal Courts (SJCACC) will use a hybrid estimation strategy combining data or estimates obtained from a (partial) census, a probability sample, and in one or more instances, model-based or model-assisted estimation. A census of cases will be obtained from about 25 states which could provide statewide, electronic, case-level data files, representing roughly 50-60% of the population. The remaining states will be represented through a stratified, probability proportional to size sample of primary sampling units (PSUs) where case level data are obtained or collected locally. In one or more instances, the state's data availability circumstances allow for either a model-based or model-assisted estimation strategy as an alternative to the census or sample approach. Examples include calibrating statewide case-level data from a prior year to control totals available for the study reference year, and adjusting case-level data for a subset (probabilistic or otherwise) of counties to control totals for the entire state. The purpose of this paper is to present and discuss the overall approach for this hybrid estimation strategy. ","When the sample is obtained from a two-stage cluster sampling with unequal selection probabilities, the sample distribution can be different from that of the population and the sampling design can be informative. In this case, making valid inference under generalized linear mixed model can be quite challenging. In this paper, we propose a novel approach of parameter estimation using normal approximation of the sampling distribution of the profile pseudo maximum likelihood estimator of the random effects in the level one model. The computation for parameter estimation can be performed by either Monte Carlo EM algorithm or direct maximization of the pseudo marginal log-likelihood. Two limited simulation studies show that the proposed method using normal approximation performs well for modest cluster sizes. ","We develop a Bayesian framework for finite population inference under cluster sampling in a design-based survey context. The two-stage sampling design proceeds by first selecting clusters with probability proportional to cluster sizes and second randomly sampling units out of selected clusters. We incorporate the sampling design into multilevel modeling framework to account for the cluster structure and generalize the inference for nonsampled clusters. The cluster sizes will be treated as covariates. We consider both scenarios when the nonsampled cluster sizes are known and unknown, that is, when the sampling probabilities are known and unknown for the nonsampled units. We will estimate the unknown cluster sizes and simultaneously include them as predictors in a flexible hierarchical regression framework with weekly informative prior information. We use simulation studies to evaluate the performance of our procedure and compare it to the classical design-based estimator. We apply our method to the Fragile Family and Child Wellbeing Study. The model-based framework will fully account for the uncertainties and yield robust inference that is design-consistent.  ","Clustered survey data are commonly obtained from a multi-stage sampling design, so many methods propose the use of random effects to represent cluster-specific characteristics in the model. Because they often assume the  independence among random effects, they cannot represent inter-cluster correlation.  However, survey cluster with spatial information is important;  for example, spatial correlation in small area sampling.  Ignoring correlation structure could give inefficient analysis.  In this talk, we introduce the hierarchical (h-) likelihood method for the survey data analysis to accommodate correlated random effects. We also investigate the weighted h-likelihood method  to consider the informative sampling design.  Small simulation study is carried out to investigate the performances of the proposed methods. Real data examples illustrate the practical use in survey sampling. ","The Economic Census collects information on the revenue obtained from products from all sampled units. The collection is quite challenging as establishments can report values from a long list of potential products in a given industry. Moreover, product descriptions are quite detailed, many products are mutually exclusive, and reported products are subjected to strict additivity constraints. Consequently, legitimate missing values occur frequently and nonresponse is quite high. Auxiliary data are not available, and other predictors such as total receipts are often weakly related. In the 2017 Economic Census, missing product data will be imputed using hot deck imputation, and variance estimates for product data will be published for the first time. The variance estimator must account for sampling variance, calibration weighting, and imputation variance. Thompson, Thompson,and Kurec (2016) present results of a simulation study that examines the first two factors. This focusses on the estimation of the imputation variance component. Using a simulation study, we compare the statistical properties of this component estimated both the model-based and a design-based (model-assisted) frameworks. ","The Economic Census collects information on the revenue obtained from product sales. Product sales are only collected for a sample of establishments, and variance estimates are needed for these sample-based estimates. In any given industry, establishments can report values from a wide variety of potential products. Often, product descriptions are quite detailed, and many products are mutually exclusive. Consequently, legitimate missing values occur frequently. The reported product dollar values are expected to sum to the total receipts reported earlier in the questionnaire. Variance estimation of these product values will need to account for a number of factors including sampling variance, variance due to imputation, and the effects of calibration weighting. After reviewing available literature, three bootstrap methods adjusted for complex survey designs and a Bayesian model-based approach were identified as candidate variance estimation techniques. This paper will describe each of the candidate methodologies. A simulation study will be used to evaluate the application of each method in a complete response scenario for Horwitz-Thompson estimates and ratio estimates. Future research will evaluate each method in the presence of non-response and measure the impact of variance due to imputation. ","We study a scenario where we have a sample with two sets of categorical variables, and we have access to auxiliary marginal information on the distribution of one of these sets. The incorporation of this marginal information allows us to identify the nonignorable response mechanism under certain assumptions. We present different ways of accounting for the possible uncertainty in the marginal information. ","Empirical likelihood method is a powerful tool for incorporating moment conditions in statistical inference. We propose a novel application of the empirical likelihood for handling item nonresponse in survey sampling. The proposed method takes the form of fractional imputation (Kim, 2011) but it does not require parametric model assumptions. Instead, only the first moment condition based on regression model is used and the empirical likelihood method is applied to the observed residuals to get the fractional weights. The resulting semiparametric fractional imputation provides root n consistent estimates for various parameters. Variance estimation is implemented using a jackknife method. The proposed method is applied to impute for Systolic blood pressure variable in 2013-2014 National Health and Nutrition Examination Survey (NHANES) data. ","The triennial Survey of Consumer Finances (SCF) has maintained response rates over time while collecting highly sensitive financial information. However, gaining cooperation from SCF sample members has presented challenges that often necessitate extending the data collection period. A new strategy for the 2016 SCF involves developing an \"escalation need\" score using aggregated, real-time case management data - including indicators of gated communities, locked buildings, and the number of past contact attempts - along with a measure of low response propensity from the Census Planning Database. In this paper we first share our methodology for developing the escalation need score - that is, the degree to which the household should be considered for incentive escalation based on factors historically associated with tough-to-reach households. Next, we describe how we are using this score to inform strategies for interviewing and offering respondent incentives in the 2016 SCF. Finally, we present early results from the first eleven weeks of data collection. We close with a brief discussion for fellow survey researchers, practitioners, and other stakeholders challenged with the careful orchestration of field data collection in an era of declining response rates for U.S. household surveys. ","Every three years the Survey of Consumer Finances (SCF) is conducted to collect personal income and family finance data from a national area probability and list sample with a lengthy and complex survey instrument. The survey faces challenges in gaining cooperation from households due to the sensitive nature of the study. Since 2004 the field period had to be extended to reach both the targeted response rate and targeted number of completed cases. For the 2016 Survey of Consumer Finances (2016 SCF), and most surveys seeking high response rates, the pursuit of elusive respondents is necessary and both a lengthy and labor intensive process. To target special efforts designed to shorten start to finish time and reduce total labor associated with these respondents we have identified specific places from the 2013 Survey of Consumer Finances with both a higher than average incidence of number of contacts and longer than average time in-between the first and last contact. We will describe the places with a high percentage of hard-to-contact cases, and the characteristics of these cases, to identify ways in which we might reduce the level of effort and improve the outcomes of these more difficult cases. ","The Survey of Consumer Finances (SCF) collects expansive financial data from households. Gaining cooperation is challenging due to the complex and sensitive nature of the questionnaire. While the concerns and anxieties of many sample members are alleviated upon learning about NORC's long-standing reputation as a legitimate research organization and the extensive security measures in place to protect their data, others remain highly skeptical and guarded even after agreeing to participate. Are the data provided by skeptical respondents less complete than those who participate more willingly? This research will examine the 2013 SCF data to identify recalcitrant respondents, as defined by cases completing after a higher than average incidence of number of contacts, longer than average time in-between the first and last contact, and/or dispositions indicating a refusal or conversion complete. Based on findings from the 2013 data, we will develop a report that will be automatically generated throughout the 2016 round to alert FIs to cases for which extra care should be taken when data is collected. Our presentation will include our interviewer coaching protocol and early experiences. ","As an in-person data collection endeavor, the Survey of Consumer Finances (SCF) attempts to reach survey respondents living in all types of housing arrangements. Gated communities and locked buildings are two examples that present special challenges for SCF data collectors. Both of these residence types have very specific barriers, such as guardhouses, locked gates, and/or doormen, to keep interviewers from reaching potential respondents. This paper will explore the spatial distribution of such residences to examine potential clustering by geographic region, urbanicity, or other variables of interest. Additionally, we will explore specific characteristics unique to these housing types. Housing units with such barriers typically require additional time for the interviewer to gain access, so it is important to identify sampled housing units that are part of locked buildings or gated communities early on in data collection. This paper will also discuss strategies for early identification of these units, as well as specific strategies for gaining access, whether through negotiations with community authorities, repeated attempts, or other less traditional methods. ","The Survey of Consumer Finance (SCF) is a dual frame national survey conducted triennially for the Board of Governors of the Federal Reserve System, and provides unique and important insights into the assets, liabilities and net worth of American households. NORC at the University of Chicago collects survey observations for the 2016 SCF using face-to-face interviewing techniques designed to address more widespread challenges to participation. This paper outlines the contacting strategy and the utilization of paradata elements collected from previous national level data collection efforts to inform interviewer outreach when household level gatekeeper scenarios are encountered and prevent access to potential respondents. We will also discuss the application of operational strategies to address known systematic barriers to reaching respondents, including gatekeeper identification and engagement, as well as efforts to connect with respondents and to help ensure that an informed decision related to participation can be made. ","Rising nonresponse and increasing survey costs continue to threaten traditional methods of data collection. In recent years, alternative survey designs (with probability or non-probability sampling) and data collection methods have been explored by survey practitioners and statistical agencies. According to the Pew Research Center telephone surveys, the rise in Internet users has grown from 14% of the U.S. adult population in 1996 to 84% in 2015. Therefore, Web surveys and multimode data collection from households and establishments have become a common, alternative cost-saving approach. The National Health Interview Survey (NHIS) has been collecting information on Internet and email use among adults since 2012. This paper presents the estimated prevalence of adult Web users (defined as an NHIS respondent who uses the Internet or email) and their sociodemographic characteristics using data from the 2014-2015 NHIS. ","Mail surveys, as well as many web surveys, rely on mailings to the sample members inviting them to complete a paper or web questionnaire. Sample members are selected from a frame such as the address-based sampling (ABS) frame derived from the U.S. Postal Service's (USPS) Computerized Delivery Sequence file. A well-known problem with such surveys is determining the eligibility of sample members who mailings are returned as \"undeliverable.\" The undeliverable codes provided by the USPS are often inconsistent across repeated mailings to the same address, yet they typically are treated as accurate in determining case eligibility. This paper describes how sample member eligibility was estimated using a latent class analysis of four indicators of eligibility. In our application, three indicators were based on the USPS codes from 3 mailings sent within a 12-day period to all sampled households and the fourth was the vacancy indicator on the ABS frame. This approach was applied to data from the Residential Energy Consumption Survey National Pilot - a sample survey of 9,650 households - in the calculation of response rates and survey weights.  ","The Asthma Call-Back Survey (ACBS) is conducted with Behavioral Risk Factor Surveillance System (BRFSS) respondents who report an asthma diagnosis. To evaluate the effects of cell phone samples on prevalence estimates of selected asthma indictors, we reweighted and analyzed five state's 2011-2013 data from the BRFSS combined landline and cell phone survey, ACBS combined landline and cell phone survey, and the ACBS landline only survey respectively. Criteria to assess the effects of adding the cell phone sample included prevalence estimation (PE) for 12 asthma indicators by demographic characteristics, the mean square error (MSE) for bias and variance, and relative standard error (RSE) for reliability. Adding the cell phone samples did not change the PE significantly for most indicators. A lower prevalence of current asthma among children ages 0-4 years with lifetime asthma was seen when results were compared with BRFSS estimates (69% vs.78%). However, adding cell phone samples reduced the bias and variance estimation by reducing the MSE (range: 6% to 48%), and improved reliability by reducing the RSE (range: 7.7% to 22%), depending on the indicators used.  ","The validity of random digit dial (RDD) telephone surveys has been challenged due to declining coverage and response rates. Multi-mode data collection, e.g., RDD supplemented with mail survey, has been increasingly used in the past decade. It is unclear if the characteristics of mail respondents are different from those of RDD, especially in minority populations (e.g., blacks, Hispanics, Asians, and American Indians). We used data from a survey in 25 minority communities from 2009 to 2012, including 44,616 telephone and 57,888 mail respondents. Response rate was higher (31%) in mail survey than in RDD (13%). Of mail respondents, 40% were from cell-phone-only households. Item non-response rate was higher in mail survey than in RDD for only 8 out of 19 demographic or health indicators. Mail respondents were in general younger, had a lower household income and no health insurance than RDD respondents across all the minority groups. They were also less likely to exercise; have regular physical check-ups; or test for cholesterol or hepatitis, or have mammography. Mail survey is an important complement to RDD as it can reach population segments that may be missed by standard RDD survey. ","The U.S. Census Bureau collects the Census of Juveniles in Residential Placement (CJRP) and the Juvenile Residential Facility Census (JRFC) for the Office of Juvenile Justice and Delinquency Prevention. Paradata captured with online data submissions enable us to tell how often respondents change their answers. We can now tell which questions pose the greatest challenges to respondents. The first section of the CJRP asks for general information about the facility, while the second section of the CJRP is a roster of the juvenile offenders held in the facility. The first section of the JRFC is similar to the first section of the CJRP, while the rest of the sections ask detailed questions about mental health services, educational services, and substance abuse services provided to young persons in the facility. Does it take longer for a facility to respond to the CJRP even though the JRFC asks many more questions? Did respondents get frustrated when reporting online for the CJRP and then switch to paper for the JRFC the following year? The answers to these questions can help us improve the data collection process and enhance the quality of the respondent's survey experience in the future. ","Survey errors can severely affect survey accuracy and the validity of population statistics derived from them. For example, up to six out of ten cash welfare recipients are missing in the most important household surveys. However, the nature and causes of these errors remain a puzzle. This makes it difficult for data producers to reduce the extent of errors and for data users to assess the validity of survey data and address the resulting inaccuracy. This paper studies different causes of survey error using government transfers as a case study. We use high quality validation data from three major U.S. surveys. We find that survey design and post-processing as well as misreporting by respondents affect survey errors systematically. In terms of survey design, imputation for missing data can be particularly problematic as they induce substantial error at the individual level. Our results on respondent behavior confirm several theories of misreporting, e.g. that errors are related to salience of the issue, respondent's cooperation, forward and backward telescoping, event recall, and stigma of social programs. ","Survey misreporting is pervasive and biases common statistical analyses. I use validation data on SNAP to show that underreporting in survey data severely affects measures of the nature and impact of transfer programs. I develop a method to combine information from the validation data with public use data and show that it drastically improves estimates. Contrary to the validation data, the required information can be released to the public, making it possible to correct estimates without access to the validation data. The method is simple to implement and applicable to a wide range of econometric models. It performs better than survey based estimates and improves upon common corrections, particularly for bi- or multivariate relationships. Using the method to extrapolate across time and geography improves over survey data and corrections without validation data. Deviations from administrative aggregates are often reduced by a factor of 5 or more. The results suggest substantial differences in program effects, such as an additional reduction of the poverty rate by almost one percentage point, increasing the poverty reducing effect captured by the survey by 75 percent. ","Measures of U.S. earnings inequality rely on the Current Population Survey Annual Social and Economic Supplement (ASEC). A substantial and increasing share of individuals surveyed in the CPS either do not participate in the ASEC supplement, or participate but refuse to report earnings and other income. We use ASEC data for calendar years 1997-2010 matched to Social Security Detailed Earnings administrative tax records (DER) to examine the implications of nonresponse for inequality measures. We find evidence that nonresponse is U-shaped with high nonresponse in the left and far right tails of the distribution. Nonresponse bias causes inequality to be understated. Imputations for nonrespondents do not fully correct the bias. Measures of earnings inequality confirm that ASEC inequality measures are lower than measures for the same individuals based on administrative earnings, a gap that has widened somewhat over time. Earnings shares among the top 1% of earners are lower by at least 20 points in the ASEC compared to the DER, with about half accounted for nonresponse and half to topcoding in the ASEC. We examine a variety of hybrid approaches to addressing the issue. ","Over the past several decades, poverty among the population age 65 and over has declined precipitously while income has also steadily increased relative to the median (DeNavas-Walt et al. 2013; Meyer and Sullivan 2010). By necessity, previous studies of poverty and income are based on household surveys. Critics allege, however, that these survey data are subject to underreporting of various sources of income and that the problem may be particularly acute for the population age 65 and over who may rely on capital income including withdrawals from defined-contribution accounts. This project will provide a comprehensive evaluation of income data quality of the Current Population Survey March Supplement (CPS ASEC), with a focus on the population age 65 and over. While existing validation studies have typically focused on a single source of income, our project will use a wide array of administrative datasets linked to the CPS-ASEC to validate many components of total household income and provide a new look at income and poverty from both a static and multi-year perspective. ","This paper introduces the Vanguard Research Initiative (VRI), a new panel survey of wealthholders designed to yield high-quality measurements of a large sample of older Americans who arrive at retirement with significant financial assets. The VRI links survey data with a variety of administrative data from Vanguard. The survey features an account-by-account approach to asset measurement and a real-time feedback and correction mechanism that are shown to be highly successful in eliciting accurate measures of wealth. Specifically, the VRI data reflect unbiased and precise estimates of wealth when compared to administrative account data. The VRI sample has characteristics similar to populations meeting analogous wealth and Internet access eligibility conditions in the Health and Retirement Study (HRS) and Survey of Consumer Finances (SCF). To illustrate the value of the VRI, the paper shows that the relationship between wealth and expected retirement date is very different in the VRI than in the HRS and SCF-mainly because those surveys have so few observations where wealth levels are high enough to finance substantial consumption during retirement.  ","The U.S. Census Bureau conducted the 2015 Census Test as part of its research to develop methodology for using administrative records (ARs) to reduce the cost and improve the quality of the 2020 Census Nonresponse Followup (NRFU) data. The goal of the 2015 Census Test in Maricopa County, AZ was to test methodology and operations designed to reduce the NRFU workload. The 2015 Evaluation Followup (EFU) was part of the 2015 Census Test and collected additional data to allow a comparison of NRFU data with ARs available for the same addresses. The 2015 EFU analyses provide information about different uses of ARs that are topics of current research, such as determining occupancy status, enumerating a housing unit (HU), and providing data for imputation procedures. The 2015 EFU interviewed 5,000 HUs where there was a discrepancy between the NRFU results and the ARs. The evaluation of the quality of the NRFU responses and the AR file includes comparisons of the population count and the demographics of the persons listed in each source with 2015 EFU data. In addition, the study uses 2015 EFU results to assess the quality of the AR and NRFU data regarding HU occupancy status. ","The National Immunization Survey (NIS) is the primary tool for monitoring vaccination coverage for 19-35 month olds in the U.S. Immunization Information Systems (IIS) contain provider-verified, population-based immunization surveillance data, and can be also be used to monitor vaccination coverage. Differences in reported vaccination coverage estimates from the NIS and IIS are difficult to interpret. Differences might reflect bias in one or both of the data systems, or might reflect differences in methodologies for estimating vaccination coverage. We proposed simulating the NIS sample design and analytic methods within a convenience sample of five IIS sentinel sites (IIS-SS) that have high data quality and comparing vaccination coverage between the NIS and IIS-SS. The objective of this analysis is to develop an analytic approach that \"replicates\" or approximates the NIS methodology using IIS-SS data to more systematically distinguish between system biases and measurement differences for estimating vaccination coverage from IIS. We will simulate the NIS design one hundred times and produce point estimates and 95% interval estimates. ","Morris, Keller and Clark (2016) show how to identify reliable administrative records for enumerating the occupants of housing units in the context of the U.S. Decennial Census Non-Response Follow-up. We propose using Bayesian decision theory to extend the approach of these authors and account for costs and response propensities of field follow-ups. We elicit a loss function that emphasizes the importance of a correct enumeration for each unit. We exploit the properties of the loss function to make decisions between conducting new field follow-ups and utilizing administrative records to complete an enumeration. This leads to a general Bayesian decision theory problem. We attempt approximating the Bayes (optimal) solution of this problem through a version of \"backward induction\" (DeGroot 1970; Brockwell Kadane 2003). We give explicit formulas applicable to specific situations and derive possible strategies. ","The 2010 Census Coverage Measurement (CCM) program evaluated coverage of the 2010 Census and produced components of census coverage results that included estimates of correct enumerations, erroneous enumerations, imputations, and omissions of the national household population. A goal of the 2010 CCM program was to inform decisions for the 2020 Census. As part of the 2020 Census, the Census Bureau is researching the possible use of administrative records (AR) to provide a status and count for some nonresponding addresses. The goal is to understand the ramifications of using AR on coverage and quality in the decennial census. In general, this research demonstrates how the 2010 CCM can be another tool by which AR usage can be evaluated.  ","The CDC conducts the National Health Interview Survey (NHIS) and maintains a database of modeled air quality (AQ) concentrations, which are generated from a model developed by Environmental Protection Agency and its partners, linkable to NHIS data at the census tract level. A previous study by the authors used this linked dataset to model health status using complex survey design features, but under the assumption that the modeled AQ concentrations were deterministic. However, AQ concentrations were actually modeled using a Bayesian space-time framework, providing a prediction and its standard error, which in part is based on distances of census tract centroids to air monitors. As the geographical locations of households in the NHIS and closest air monitor vary greatly, associations of population health with predicted air qualities might be more accurately modeled if the standard errors of predictions are incorporated into the health analysis model. This study focuses on such methodologies in estimating air quality effects on population health. Results from these methods will be compared and discussed. ","This paper examines methods to correct for biases in auxiliary data (e.g. voter lists, consumer databases) using higher quality reference data (e.g. census data) in order to produce better ratio estimates (regression estimates).    Survey practitioners are increasingly eager to use auxiliary data (frame information) for statistical inference. This is particularly true in light of a rapidly deteriorating survey environment driven by budget cuts in government agencies and more stringent laws (ex. TCPA) driven by privacy concerns.   ","Integration of surveys with alternative data sources (sometimes described as \"big data\" or \"organic data\") requires evaluation of the bias and variance properties of each prospective source. In many such cases, directly computed variance estimators may be unstable, and use of estimated variance functions may be preferred. This paper extends previous literature on generalized variance functions (GVFs) for complex sample surveys to: (1) develop variance-functions estimators intended to reflect dominant error components in a given set of surveys and alternative data sources; (2) evaluate the convergence and stability properties of the estimators from (1); and (3) present some diagnostics based on (2). The primary ideas in this paper are illustrated with examples based on (a) an establishment survey that depends heavily on an administrative record source; and (b) a sensitivity analysis for prospective linkage of a household survey with commercial or administrative sources. ","In survey practice, the collected sample survey data often cannot represent the underlying population data due to unequal probability of selection, nonresponse and sample size constraints. It is crucial to balance the distribution between the sample and population data and generate valid survey inference. Survey weighting borrows the population information to adjust for the discrepancy, and facilitates the inference. When we need to adjust for multiple factors but only the marginal population distributions are available, classical weighting methods rely on raking, as an interactive proportional fitting approach, to construct weights that adjust for the marginal distributions. However, the increasing number of weighting variables and the limited sample size cause computation problems for the raking algorithm that cannot be directly applied. We propose a model-based alternative for raking and induce shrinkage structure via Bayesian hierarchical framework to stabilize the marginal adjustment. We will use simulation studies to compare the performances between model-based and classical raking methods and evaluate the partial pooling properties. The proposal will be applied in a public h ","Record linkage is used to extend the research capabilities of cross-sectional population health survey data. Linkage of medical records and other administrative data for survey respondents add detailed longitudinal information and, for some information, administrative data are considered more accurate than survey response. However, not all survey respondents agree to record linkage. At the National Center for Health Statistics, record linkage consent has been generally determined by provision of a Social Security Number (SSN). This provision has been shown to be associated with demographic and health characteristics and to differ among surveys. The purpose of this study is to examine correlates of recent SSN provision with factors associated with variables on the data collection process itself, known as paradata, in the 2015 National Health and Nutrition Examination Survey (NHANES). Examined paradata include information on the number of contact attempts needed for screener response, severity of refusal to participate, time of day and day of week of final screener. This analysis will show whether SSN provision is correlated with ease and willingness to participate in NHANES. ","The objective of the study was to develop methodologies and infrastructure in order to quickly survey blood donors in situations where there could be possible exposure to transfusion transmissible pathogens or other potential hazards to the blood supply.  The U.S. Food and Drug Administration (FDA) worked with five Blood Collecting Organizations (BCOs), NORC at the University of Chicago, and the American Association of Blood Banks (AABB) to develop sample frames, select samples, and administer web-surveys.  This paper describes the general protocol, the response rates and the rapidity of response for two surveys.  One survey addressed exposure to ticks and tick-borne infection and the second survey addressed exposure to the Hepatitis E virus. ","The National Crime Victimization Survey (NCVS) is a survey of the non-institutionalized U.S. population aged 12 and older and utilizes a 7-wave rotating panel design. In a survey such as the NCVS which is used to measure crime retrospectively, the panel design is useful in that it provides bounded reference periods for all but the first interview. This can improve survey accuracy by reducing recall bias among respondents, though it also introduces another source of potential bias not present in cross-sectional surveys: respondent fatigue. Since participants remain in the sample for a period of up to three years, and since each interview can be time-consuming, the potential for wave nonresponse, panel attrition, and underreporting is high. Each can have a negative impact on the quality of estimates. This paper addresses how the authors assessed the impacts of these error sources, with emphasis on the methods used to measure fatigue-induced rate deflation. After accounting for potentially confounding sources of bias, the impact of fatigue on estimates of personal and property crime is shown to be substantial, suggesting that respondent fatigue is a source of bias in the NCVS. ","Linking survey data to administrative data can increase the possibilities of analysis. Because of its high accuracy exact linkage is the preferred linkage method. However, the linkage process involves several problems and typically only a percentage of records can be exactly linked. Sometimes this percentage can be quite low if non-consent rates are high or if the unique identifiers used in the linkage process are of poor quality. The lack of complete linkage can reduce the quality of the combined data and inferences that are drawn from them.  ","Modifications of weights due to calibration, trimming, sometimes in multiple stages, are very common in survey analysis. It is typical to work with modified as opposed to design/inverse-inclusion-probability weights, especially in publicly released survey data. Various weight-smoothing methods (Pfeffermann-Sverchkov 1999, Zheng-Little 2003, Beaumont 2008) have been proposed to improve the efficiency of the Horvitz-Thompson (HT) and Generalized Regression (GREG) estimators of survey totals. These methods depend on correctness of model relationships between the survey attribute y, covariate x, and y and the given weights w. Little is known about the impact of treating modified weights as design weights, for example when the model assumptions connecting x, y, w might also be misspecified. It is, therefore, important to evaluate the performances of these three methods under different modified weights and misspecified models. In this simulation study, we generate finite frame populations from superpopulation models, simulate misspecified models, and quantified mis-calibrations of the weights to compare GREG and HT results with estimators based on the three weight-smoothing methods. ","We will compare the accuracy of the Monte Carlo Markov Chain (MCMC) and Adjusted Density Method (ADM) in approximating Hierarchical Bayesian (HB) solution in the context of small area estimation. For this comparison, we will treat the numerical integration method as the golden standard. We apply the hierarchical model to the poverty data from the 2005 American Community Survey. We use both MCMC and ADM to obtain approximated HB estimates of the poverty rate for the 0-17 year old children in each county. We also experiment with different priors. Both MCMC and ADM methods can be used to approximate the posterior distributions. MCMC uses the Gibbs Sampling Algorithm and ADM approximates a posterior density with a Pearson density. Plots of the MCMC and ADM approximations to the posterior distributions are shown respectively. Both MCMC and AMD methods provide strikingly similar results. The ADM method is very fast compared to the MCMC method and should be appealing when several posterior distributions need to be computed in a simulation environment. This work is completed under supervision of Professor Partha Lahiri. ","The Patient-Reported Outcomes Measurement Information System (PROMIS) v1.2 Upper Extremity (UE) item bank was recently developed to improve the precision and responsiveness of measuring upper extremity physical function but has not yet been fully evaluated in an orthopaedic population. The purpose of this study was to fill such gap. We administered the PROMIS UE instrument to patients who presented to an academic orthopaedic center and examined the items and instrument's measurement properties. Results indicated that the instrument had adequate reliabilities and item fit. However, the items were mostly targeting lower functioning patients, thus revealing large ceiling effect. More items need to be developed to target the higher functioning end. ","Large-scale databases from the social, behavioral, and economic sciences offer enormous potential benefits to society. However, as most stewards of social science data are acutely aware, wide-scale dissemination of such data can result in unintended disclosures of data subjects' identities and sensitive attributes, thereby violating promises--and in some instances laws--to protect data subjects' privacy and confidentiality. In this talk, I describe a vision for an integrated system for disseminating large-scale social science data. The system includes (i) capability to generate highly redacted, synthetic data intended for wide access, coupled with (ii) means for approved researchers to access the confidential data via secure remote access solutions, glued together by (iii) a verification server that allows users to assess the quality of their analyses with the redacted data so as to be more efficient with their use of remote data access. I describe methodologies to meet the challenges (disclosure risk and dataa quality) to releasing synthetic data and allowing automated verification. ","Administrative data, science, and privacy are on a collision course. New studies using confidential data are already framing discussions of income inequality and policy interventions. These studies were based on unfettered access to confidential microdata. Scientists working only within the agency's supervised, secure computing environment met statutory access rules. Restricting their published output met confidentiality requirements. Reproducible science requires that others be able to confirm the results using the same data. But sponsors only require the dissemination of public data. These public data are summaries of models estimated on the confidential data. They are never sufficient to verify the accuracy of the authors' specifications. Advances in data privacy methods now allow for provably accurate, provably safe public data sets. Every researcher granted access can verify previous research without additional privacy loss. The public data are iteratively updated to reflect the results of each new analysis. Curation of the public-use data meets the reproducible science requirement. Because of the privacy-preserving algorithms, it also meets confidentiality requirements. ","Motivated by the American Community Survey (ACS; US Census), we present Bayesian methodology to perform spatio-temporal change of support (COS) for survey data with Gaussian sampling errors. The ACS has published 1-year, 3-year, and 5-year period estimates, and margins of errors, for demographic and socio-economic variables recorded over predefined geographies. The spatio-temporal COS methodology considered here provides users a way to estimate ACS variables on customized geographies and time periods while accounting for sampling errors. Additionally, 3-year ACS period estimates will be discontinued, and this methodology can provide predictions of ACS variables for 3-year periods given the available period estimates. The methodology is based on a spatio-temporal mixed-effects model with a low-dimensional spatio-temporal basis function representation, which provides multi-resolution estimates through basis function aggregation in space and time. This methodology includes a novel parameterization that uses a target dynamical process. The effectiveness of our approach is demonstrated through two applications using public-use ACS estimates and is shown to produce good predictions. ","The increased popularity of multiple imputation to obtain valid inference on incomplete data has identified two potential problems. First, imputation techniques are developed to solve distinct problems. They are evaluated on their performance on these problems, but are potentially of great scientific use outside of their target application. Such innovative applications remain unknown. Second, the target audience for multiple imputation consists of applied researchers from all scientific domains. These researchers often lack the statistical knowledge to understand the methodology behind these imputation techniques. How can these researchers decide what imputation technique would be suitable for their problem?  ","Multiple imputation [Rubin, 1987] is difficult to conduct if the analysis model includes interactions, squares, or other transformations of variables with missing values for two reasons: First, the imputer must be aware of the analysis model to address the congeniality issue [Meng, 1994]. Second, the imputer must choose to produce either biased parameter estimates, even in the case of missing completely at random, by the passive-imputation algorithm (van Buuren and Groothuis-Oudshoorn [1999], a.k.a. impute, then transform) or inconsistent data relations by the just-another-variable algorithm (von Hippel [2009], a.k.a. transform, then impute). Although some research on imputing squares has been conducted [Vink and van Buuren, 2013], the conflict persists for all other nontrivial transformations. We propose a flexible local imputation model that builds upon the ideas of Cleveland [1979]. Implicitly, local imputation captures a broad range of transformations such as interactions, squares, cubes, roots, and logs. Hence, there is no need for the imputer to consider variable transformations. All they need to consider is the inclusion of all relevant variables as they are. In a simulation study, we compare our proposed local-imputation algorithm with, among others, random forest imputation [Doove et al., 2014], which also addresses nonlinearities. ","Advancements in survey administration methodology and multiple imputation software now make it possible for planned missing data designs to be implemented for improving data quality through the reduction in survey length. Many papers have discussed implementing a cross sectional study with planned missing data using a split-questionnaire design, but the research in applying these methods to a longitudinal study has been limited. Using simulations and data from the Health and Retirement Study, we compared the performance of several methods for administering a split-questionnaire design in the longitudinal setting. Our findings suggest that the optimal design depends on the data structure, specifically on both the within-wave and between-wave variable correlations, and there exists a trade-off between the complexity and robustness of the designs. These factors should be taken into account when designing a longitudinal study with planned missing data. ","Missing data due to nonresponse is a recurring problem in survey statistics, and can lead to biased inferences. Long questionnaires are part of the problem, as they can induce both, unit nonresponse if the respondent has information on the expected completion time, and item nonresponse due to interview break-off or question skipping. Split questionnaire survey (SQS) designs provide a mean to reduce this uncontrolled missingness. A crucial aspect of the selected designs is the avoidance of identification problems, i.e. everything to be analyzed jointly remains jointly observed for a subset of the original sample. The reduced questionnaire decreases response burden, but the data is now partially missing-by-design. However, the missing data can be assumed to be missing completely at random, and Multiple Imputation can provide the basis for unbiased complete-data analysis.  Based on the work of Raghunathan and Grizzle (1995), we introduce a very flexible approach to identify SQS designs which preserve the information of the data as good as possible. Our proposed method is based on genetic algorithms allowing for flexible constraints, such as the definition of inseparable items. ","Estimates of population characteristics such as domain means are often expected to follow qualitative assumptions. For example, average salary might be increasing in pay grade. Estimation and inference could be improved when these restrictions are incorporated along with sampling design of surveys, in comparison with the common methods that do not take account of these constraints. Particularly, domain estimation can be considerably improved when the correct shape restrictions are considered. Estimation of restricted domain parameters is accomplished through an adaptive domain estimation procedure. It consists on pooling the domains based on the assumed shape restrictions. However, even when shape restrictions can improve the precision of population estimates, it is important to use them appropriately. In fact, assuming incorrect restrictions could lead to biased estimators. We develop diagnostic methods that measure departures from the shape assumptions. ","Structural equation models (SEM) quantify causal relationships proposed by theory or from the study of counterfactuals. Small area estimation (SAE) is geared toward estimating survey parameters in small domains under the constraints of small sample sizes. This work explores the prospective role of an SEM in the context of SAE. This implies the estimation of a structural equation mixed model that performs well in moderate sized unbalanced datasets.   ","In 2011, USDA's National Agricultural Statistics Service started the complete implementation of the County Agricultural Production Survey (CAPS). CAPS is an annual survey to provide accurate county-level acreage and production estimates of approved federal and state crop commodities. The current top-down method of producing official county-level estimates that satisfy the county-district-state benchmarking constraint is an expert assessment incorporating multiple sources of information. We propose a model-based method that combines the CAPS survey acreage data with auxiliary data and improves county-level survey estimation, while providing measures of uncertainty for the county-level acreage estimates. Auxiliary sources of information include remote sensing, weather data, and planted acreage administrative data from other USDA agencies. A novel hierarchical Bayesian subarea-level model is proposed and implemented, with an additional hierarchical level for the sampling variances. County-level model-based acreage estimates have lower coefficients of variation than the corresponding county-level survey acreage estimates. Top-down benchmarking methods are investigated and the final acreage estimates satisfy the county-district-state benchmarking constraint. ","Non-parametric model using penalized spline regression in small area estimation context was proposed by Opsomer et al (2008). Wang et al (2015) applied this technique in granular area estimations. It was shown to be a useful tool to provide supplemental information where survey observations are few or non-existent. In this paper we further examine semiparametric generalized linear mixed model in producing consistent estimations for multiple area levels. We use the mosaic analogy to describe the process. We employ a design-based Jackknife method for variance calculation. ","We illustrate a small area estimation methodology to estimate employment by combining the U.S. Census Bureau's Annual Survey of Public Employment and Payroll (ASPEP) with the previous census records using an empirical best prediction (EBP) methodology. The employment data are usually subject to skewness and heteroscedasticity and thus the well-known EBP methodology based on unit level linear mixed normal model does not fit well. In order to get around the problem, we apply a unit level linear mixed normal model on the log-transformed employment. In this paper we discuss the performance of the parametric bootstrap to estimate mean squared error for different small areas and compare its performance between the unit-level and area-level. ","The National Immunization Survey (NIS) is a nationwide random digit dial (RDD) survey which monitors the vaccination coverage of children age 19-35 months. The NIS is designed to produce vaccination coverage rates for the nation, each state, and select local geographic areas. In this presentation, we describe small area estimation methods for generating county-level vaccination coverage rates for all counties in the United States for years 2005-2014. We describe area-level models such as the cross-sectional Lindley and Smith model (also known as the Fay-Herriot model), a time-series extension of the Lindley and Smith model, and unit-level models such as the logistic regression model with random effects. The use of county-level predictors of vaccination coverage for the models, methods for selecting county-level predictors for the models, limitations associated with the methods, and methods for evaluating the models will be discussed. County-level estimates for each of the models are generated using the James-Stein approach or an empirical best linear unbiased prediction approach. ","Unit-level multilevel models can generate small area estimates at low geographic levels, such as census blocks. Because detailed census cross-tabulation population counts are available only by age, gender and race/ethnicity, multilevel models can only use these three variables in prediction. By using bootstrapping, we fitted multilevel logistic regression models with individual age, gender, race/ethnicity, and education for two outcomes -- current smoking and obesity -- from the 2011 Behavioral Risk Factor Surveillance System. We introduced a parametric bootstrapping method to assign education status for Census 2010 block-level population by age, gender, race/ethnicity in model prediction using 2007-2011 American Community Survey 5-year estimates. We compared county-level estimates with Missouri County-level Study direct estimates: the inclusion of individual education in model fitting and then prediction increased the correlation coefficients from 0.40 to 0.45 for current smoking and from 0.27 to 0.34 for obesity. Thus, multilevel small area estimation could include additional individual variables via bootstrapping. ","When survey data are modified for disclosure limitation, the results of statistical analyses may be different from those that would be obtained based on the original data. One of the methods frequently applied to categorical variables is random swapping method and its variations. When categorical variables are swapped the relationships between different variables in the data can be considerably affected and the correlation structure damaged. It is especially problematic when these variables are design variables or the variables defining important subgroups in the population that are of a particular interest to the data analyst. In this paper we present a new disclosure limitation method which is similar to swapping; however, the important difference is that it takes into account other variables, continuous and categorical, thus reducing the impact caused by disclosure limitation on the overall data and within important subgroups. ","How do American homes consume energy? We tackle the question statistically with survey data and engineering estimates. Energy choices and home environments are measured for a national sample of homes by the Residential Energy Consumption Survey (RECS) conducted by the U. S. Energy Information Administration. From the survey response variables on building characteristics as well as energy end choices and uses, engineering models are formulated to estimate end-use energy consumptions. The sample homes' total energy consumptions are also captured in RECS. A statistical objective is to develop a robust probabilistic model to partition the total consumption into end-use consumptions, even when the engineering models may not be very accurate. In this paper, we try a Bayesian multilevel modeling approach for the partitioning, taking electricity as an example. By partially pooling sample homes by geography and end-use combination and by accounting for respondent selection through the weights, a multilevel model of the annual electricity consumption is built from engineering-based electricity end-use estimates. Our uncertainties about model parameters are incorporated as priors. ","During the past several years there has been a serious effort by the community of forensic scientists to move away from subjective conclusions based on the findings from a forensic examination of evidence, by trained and experienced forensic examiners, to a more logical, probabilistic framework for quantifying the weight of such evidence using likelihoods, likelihood ratios, or other appropriate \"objective\" measures. This talk will give a brief explanation of competing approaches, discuss their pros and cons, and illustrate them using specific application areas such as pattern evidence (fingerprints, shoe-prints, etc.) and trace evidence. ","The Canadian Census long form is a quinquennial large-scale sample survey for which millions of estimates on the Canadian population are published at various levels of geography. In 2016, to improve the analytical potential and the intelligibility of the published point estimates, Statistics Canada wants to be able to calculate a variance-based quality-indicator (QI) for each estimate. In addition, for the first time, analysts having access to microdata will be provided replicate weights enabling them to produce variance estimates on their own. This paper summarizes the development of a replication variance estimator that uses few replicates to be integrated into the existing dissemination systems. Emphasis will be put on the challenges of developing the variance estimator and the results of a Monte Carlo simulation supporting the choice of the method to be used. These challenges include the very large sample size along with the large sampling fraction, the need to calibrate the replicate weights and the numerous variance estimates being calculated for both smooth and non-smooth statistics in a limited timeframe while respecting confidentiality of the data provided. ","Surveys often provide numerous estimates of population parameters. Some of the population values may be known, with a high level of certainty, to lie within a small range of values. Calibration is used to adjust the weights associated with observations within a data-set. This process ensures that the \"sample\" estimates of known population totals (targets) lie within the known ranges of those population values. However, additional uncertainty due to the calibration process needs to be captured. In this paper, some methods to estimate the variance of the population totals are proposed for algorithmic calibration processes based on minimizing the L1-norm relative error. The estimates of the covariance matrix associated with the additional variation of the calibration totals are produced either by linear approximations or bootstrap techniques. Specific data structures are required to allow for the computation of massively large covariance matrices. In particular, the implementation of the proposed algorithms exploits sparse matrices to reduce the computational burden and memory usage. The computational efficiency is shown by a simulation study. ","The Behavioral Risk Factor Surveillance System (BRFSS) is a network of health-related telephone surveys-conducted by all 50 states, DC, and participating US territories. Data users often aggregate BRFSS state samples for national estimates without accounting for state-level sampling, a practice that could introduce bias because the weighted distributions of the state samples do not always adhere to national demographic distributions. We will be examining six methods of reweighting, which are then compared with key health indicator estimates from the National Health Interview Survey (NHIS) using the 2013 data. Although one of the six methods reduces the variance of weights and design effect at the national level, statistically significant differences between aggregated state weights and national weights did not occur in these analyses. Despite the outcomes in the analyses, the new method leads to weighted distributions that more accurately reproduce national demographic characteristics. To the extent that survey outcomes are associated with these demographic characteristics, matching the national distributions will reduce bias in estimates of these outcomes at the national level. ","Diabetes is a leading cause of death in the U.S. The National Health and Nutrition Examination Survey (NHANES) is ideal for estimating the prevalence of diabetes because it includes interview and laboratory data. However, the complex nature of NHANES can make selection of the appropriate sample weight challenging. To illustrate how to select the appropriate sample weight, the three prevalence measures in Health, United States (HUS)-total, diagnosed, and undiagnosed diabetes -were used. The appropriate sample weight depends on the analytic objective and data components used. The sample weighting options considered were: 1. Using three weights specific to the three prevalence measures: the interview weight for diagnosed diabetes, the fasting subsample laboratory weight for undiagnosed diabetes, and replicate weights for total diabetes; 2. Creating a new combined interview and fasting subsample laboratory weight for all three prevalence measures; 3. Using the fasting subsample laboratory weight for all three prevalence measures. This presentation will compare these three approaches' strengths and weakness.  ","The development of adaptive survey designs has been motivated by the desire to reduce nonresponse error and the cost of data collection. These goals are important, but they should not be considered the only aims of this approach to survey data collection. Considered from a total survey error perspective, adaptive survey design can be employed to achieve other goals such as response accuracy, reduced mode effects, and the ability to publish estimates for small subgroups. The choice of goals may entail tradeoffs with other possible objectives (e.g., reducing nonresponse error may affect measurement error). This roundtable will provide a forum for discussing alternative goals for implementation of adaptive survey designs and weighing the implications of the choice of goals on other sources of survey error. ","Four years of intensive research on innovations in completing a census culminated in the release of the baseline Operational Plan for the 2020 Census in the fall of 2015. This plan documents major changes in the design of the decennial census, focusing on four major pathways to innovation: Reengineering Address Canvassing, the source of the address frame for the Census; Optimizing Self-Response, in particular by providing internet questionnaires as a response option; Utilizing Administrative Records and Third-Party Data, taking advantage of data provided to other agencies for a variety of applications; and Reengineering Field Operations, using automation for most fieldwork functions and minimizing the footprint of field offices. This paper will provide the context for how the various efforts fit together to work toward the goal of controlling the cost of the census while maintaining the same level of quality.   ","The Master Address File (MAF) Coverage Study (MAFCS) is one component of the 2020 Census address canvassing operation. The intent of the program is to produce coverage estimates of the MAF, provide continuous updates to the MAF, evaluate in-office address canvassing updates, evaluate in-field address canvassing targeting strategies, and test new in-field address canvassing procedures. This paper discusses an overview of the survey including the objectives, sample design, and the analysis plan for the data collected. Approximately 20,000 blocks per year will be selected for canvass, beginning in 2016 through 2019. Field listers will canvass assigned blocks collecting address data for every place people might live or stay. The initial MAFCS uses a stratified sample based on geography and size of a block. Future sample designs will incorporate information from the in-office address canvassing operations as well. Key estimates include the numbers of added, deleted and verified addresses for the nation and for several domains of interest.  ","The decennial census seeks to maximize household response rates to reduce costs and minimize the number of addresses requiring in-person followup. However there are many challenging factors, including declining survey response rates over time, distrust in government, an increasingly diverse and  mobile population, and rapidly changing use of technology. After much research and testing, the 2020 Census will include several key methods to address these challenges and encourage households to self-respond. We will implement a customized communications and partnerships campaign to motivate people. We will also use a tailored contact strategy of mailed invitation materials, including sending a paper questionnaire in the first mailing to areas with relatively lower Internet usage. We will offer an online response option in multiple languages, that allows responses with or without a unique Census ID, to make it easy to respond anytime and anyplace. Finally, Census Questionnaire Assistance will provide telephone help, collect interviews over the phone, and will also include web chat. Mixing modes and tailoring methodology will be core principles for the 2020 Census self-response. ","The Census Bureau is researching how to use administrative record information from government and other sources in place of field visits during the Nonresponse Followup (NRFU) operation. This paper describes the latest research on the approach for identifying vacant and occupied housing units to be enumerated using administrative records, removing them from the NRFU workload. Methods defined here potentially could be used to reduce mail, telephone, and personal visits in survey methodology generally. ","In order to save money, yet preserve quality, in the 2020 census, the Census Bureau re-engineered its field operations, focusing first on the Nonresponse Followup Operation. A small team re-designed how we operationalize the Nonresponse Followup operation, from an enhanced control system with data-driven management practices, optimization and routing of enumerators each day, automated training, and a new management structure. In this paper, we will discuss the results of the 2015 Census Test where these new practices were introduced for the first time, efficiency gains, successes and lessons learned from that test. ","We proposed the test statistic for testing no effect in nonparametric regression with survey data. We also discussed choosing the tuning parameter.  ","The Willow Comes to WIC program was designed to encourage fruit and vegetable (F/V) consumption in low-income families participating in The Special Supplemental Nutrition Program for Women, Infants, and Children (WIC). Participants receive cash value vouchers (CVV) for purchasing F/V. A survey was given to measure attitudes and behaviors related to F/V consumption and family's usage of CVV. The goal of this study was to determine whether the two sites of 995 families could be merged to better represent the total attitudes and behaviors of the population. Propensity score (PS) methodology was applied to evaluate the balance of participant data on F/V consumption and demographics, without being influenced by the responses. Comparability was demonstrated by the PS in a subset of 15 covariates through binary and ordinal logistic regression. Location incomparability based on the standardized difference of the PS was mainly from the demographics. Once the sites were successfully merged, sensitivity of the covariates to CVV usage was analyzed with higher statistical power. Modeling the remaining covariates revealed that utilizing CVV positively affected fruit consumption and preparation. ","Generalized estimating equations (GEE) are often used for the marginal analysis of longitudinal data. Although much work has been done to improve the validity of GEE for the analysis of data arising from small-sample studies, little attention has been given to power in such settings. Therefore, we propose valid approaches with GEE in order to improve power in small-sample longitudinal study settings. Specifically, we use a modified empirical sandwich covariance matrix estimator within correlation structure selection criteria and test statistics. Use of this estimator can improve the accuracy of selection criteria and increase the degrees of freedom to be used for inference. Resulting power increases will be shown via a simulation study and application example. ","This paper explores the use of time-to-event models under right censoring in the Generalized Difference estimator framework. Model-assisted estimation has been widely used for sample surveys. One class of model-assisted estimators is Generalized Difference estimators. These estimators have been explored for Generalized Linear Models but not time-to-event models under right censoring. Adaptations to the current theory need to be made through the use of a counting process approach to time-to-event analysis. Here, both parametric and semi-parametric time-to-event models will be explored. ","Direct estimates of sampling variance in complex surveys are typically calculated with some form of replication, such as Balanced Repeated Replication, but these estimates can be volatile. Generalized Variance Functions (GVFs) are regression models fit to existing direct estimates of sampling variance to improve estimates of those variances (Wolter 2007).If multiple items exist then similar items may be grouped together to improve the model's strength, but proper item groupings may not be clear.  ","The data used in social, behavioural, health or biological sciences may have a hierarchical structure due to the natural structure in the population of interest or due to the sampling design. Multilevel or marginal models are often used to analyse such hierarchical data. The data may include sample units selected with unequal probabilities from a clustered and stratified population. Inferences for the regression coefficients may be invalid when the sampling design is informative. We apply a profile empirical likelihood approach to the regression parameters, which are defined as the solutions of a generalised estimating equation. The effect of the sampling design is taken into account. This approach can be used for point estimation, hypothesis testing and confidence intervals for the subvector of parameters. It asymptotically provides valid inference for the finite population parameters under a set of regularity conditions. We consider a two-stage sampling design, where the first stage units may be selected with unequal probabilities. We assume that the model and sampling hierarchies are the same. We treat the first stage sampling units as the unit of interest, by using an ultimate cluster approach. The estimating functions are defined at the ultimate cluster level of the hierarchy. ","In recent years many government agencies and statistical institutions have endeavored to release statistical data through online real-time system - a high-technology product which is designed to convey information in a timely and flexible manner. In an online real-time system, the data users submit queries in required format and expect to receive tailored statistical results (e.g., tables with aggregated statistics) quickly. While a real-time system provides the convenience and flexibility of conducting statistical analysis, it faces the same challenge of protecting data confidentiality as traditional data dissemination approaches - the data products have to be screened and/or treated to ensure low risk of statistical disclosure. There are two types of confidentiality protection approaches for real-time systems: static and dynamic. The former implements the statistical disclosure control (SDC) treatments outside the system while the latter implements the treatments on the fly. In this paper we will review a few confidentiality approaches which are feasible for real-time systems, and discuss the use of risk and utility measures to justify the dissemination of data products.  ","Nonresponse and measurement error are common problems in survey data collection, especially in longitudinal surveys where each sample unit is asked to give multiple responses over time. With multiple response periods, there are more opportunities for nonresponse as well as measurement error. Using regression tree models and the longitudinal Job Openings &amp; Labor Turnover Survey conducted by the US Bureau of Labor Statistics, we analyze characteristics associated with establishment nonresponse and measurement error during data collection. The characteristics include length of time in survey, establishment characteristics (size, industry, ownership, multi-establishment, etc.), and population size of the establishment's geographic location. ","In this paper, we introduce the R package, RPMS, (Recursive Partitioning for Modeling Survey Data), which fits a linear model to survey data conditionally on variables selected through recursively partitioning. This application of recursive partitioning produces design consistent regression tree models of the data. We demonstrate the software through the modeling of establishment level employment data collected by the U.S. Bureau of Labor Statistics. These applications demonstrate the easily interpretable structure and the ability of regression trees to handle large data sets with complex associations among the variables. ","Increasing reluctance to participate in surveys leads to low response rates-as low as 9% in telephone surveys-increasing the potential for nonresponse bias. When auxiliary information is available on both respondents and nonrespondents, one common method for correcting nonresponse bias is to model response propensity, typically using a logit or probit with a linear link. Recent developments in machine learning, allow for flexible functional form estimation and variable selection. We examine whether current machine learning techniques can help reduce nonresponse bias in surveys, especially when response is a more complicated function of covariates than typically implemented. We apply these techniques to the German panel study Labour Market and Social Security. We compare traditional techniques (e.g., raking, post-stratification, logistic regression) with machine learning techniques (e.g., classification trees, random forests, neural nets, adaptive LASSO with a polynomial expansion of regressors). ","Research continues to emerge exploring methods for reducing the impact of self-selection bias on the estimates derived from non-probability samples. Sample matching is one such common method that identifies a subset of the non-probability sample that is linked to units within a relevant probability sample using a distance function measured on key indicators, or more generally bases the match on a propensity score, typically derived from a logistic regression model. Analyses and estimates are then produced using the matched sample subset of the non-probability sample. To date, there has been relatively little research comparing various machine learning algorithms for generating matches that are better suited for larger number of predictors. In this study we explore the use of RFs for generating matched samples using the proximity measure generated from an unsupervised version of random forests grown using the probability data set and then applied to the non-probability sample to obtain an overall distance matrix. We compare the resulting matched sample to one obtained using a more common proximity measure based on a simple matching coefficient. ","One approach to inference from sample surveys subject to unit nonresponse is to frame the problem in terms of the propensity to respond. But because the propensities are rarely known in practice, the nonresponse problem is transformed into one of estimation or prediction. Recently, a number of researchers have explored the possible application of techniques from the machine learning literature to the prediction of response propensities. For example, Rand researchers use generalized boosted models (GBM) to model response propensity for their 2014 RAND Military Workplace Study. We describe nonresponse adjustments that we provided for a parallel 2015 study of reservists, again using GBM. We then examine through simulation the relative merits of this approach compared to some other competitors, such as random forests, that have been recently studied. ","The increasing use of internet panel surveys has led to a growing need for inferences from these samples that are generally selected with non-probability sampling methods from a very large panel population. Also lacking is evidence of the validity of these samples at least in the sense of representation, coverage, and comparability with probability samples of similar populations. This paper first describes the weighting methods used to generate valid estimates. It then explores the development of variances and confidence intervals for these weighted samples, and uses these to compare the survey estimates with probability sample estimates. This research uses data from a series of internet panel studies; specifically, we used the data from the study conducted in Los Angeles County. We compare the results with local data from the Behavioral Risk Factor Surveillance System (BRFSS) and from the California Health Interview Survey (HIS). ","Non-probability surveys (NPS) are already widely in use in market research. However, their adoption for official statistics is much more problematic. AAPOR (2013) identifies a \"Fit for Purpose\" approach where the most difficult issue to address is making point estimates that are statistically valid, that is, can be used for statistical inference. This paper describes a methodology to empirically evaluate NPS surveys selected from a panel for statistical inference. The method compares estimates from an online panel with data from a gold standard probability survey. The key aspect of the methodology include transparency through an a priori decision rule motivated by the ASPIRE system developed by Bergdahl et al. (2014). We propose a distance metric and a predetermined cutoff value for deciding whether to accept or reject NPS estimates. Our decision rule is based on comparisons of 1) overall survey and subgroup estimates 2) the cv of the variability of the post-stratification weights and 3)the ratio of response rates. We illustrate our proposed empirical method by comparing data from a NPS quota sample for the Los Angeles area with a probability health survey of the same area. ","The use of non-probability online opt-in samples is very common in commercial marketing and survey research practice.  The use of non-probability online samples in experimental studies is well understood and without concern.  When their use is for population estimation, it is here that classical survey statistical theory has concerns in their use.  Given the cost, speed and flexibility of online surveys, non-probability samples are being used more and more for the measurement and tracking of attitudes and behaviors.  In this study, we examine two non-probability online sample surveys for the purpose of exploring strategies to adjust and calibrate non-probability samples.  Post-survey weighting is examined including standard raking ratio, propensity, and Generalized Regression weighting.  The different auxiliary variables used in the adjustments include standard demographics, technology adoption, and components of an attitudinal-behavioral consistency strategic model.  The attitudinal-behavioral consistency model motivates the discussion and provides guidance for measures to include in adjustments.  It is under this framework that we examine the efficacy of non-probability online samples. ","Future censuses and surveys are almost certain, for reasons of cost-savings, to make heavy use of observational administrative-records databases and maybe of databases generated by web searches or optional (self-selected) web-respondents. To make Census Bureau data products based on such observational data fully representative of the general population (in mathematically demonstrable ways, as our current surveys are), new sampling designs and analysis methods will have to be developed. This talk describes new research concerning probability sample survey designs for data collection which will yield consistent population estimates for survey attributes under various model assumptions (i) on the propensities for population members to be in an observational database; (ii) on the coverage by a population frame on the population members not captured by the database; and (iii) on nonresponse in the designed probability sample.   ","The main objective of sampling is to obtain a representative sample for an unbiased and efficient estimate within a budget constraint. The current paper is to develop a new measure of representativeness of a sample. A population characteristics or measures of N population elements could be interpreted as a vector on N-dimensional space. Similarly, a sample characteristics or measures of n sample elements could be interpreted as a vector on n-dimensional subspace, imbedded in N-dimensional space. The length of a population vector is defined as the square root of the sum of squares of N components. The length of a sample vector is the square root of the sum of squares of n components. A weighted length of a sample vector could be obtained by weighting the sum of squares of n components with sampling weights. We can measure the sample representativeness by comparing the weighted length of a sample vector with the length of the population vector. Simulation study will be conducted to demonstrate the effectiveness of the measure for sample representativeness.  ","With increasing survey costs, reduced participation rates, and a greater need for timely and domain specific estimates to inform treatment and policy decisions, interest has grown in the use of data obtained from unconventional sampling to supplement or replace traditional survey programs. Given these unconventional data sources may result in unknown levels of bias in their estimates, we explore the use of composite model-based estimation methods to blend data from an unconventional sample with parallel data from a traditional probability-based sampling strategy. We present a number of approaches for blending data from two sources. We then study their utility in a large simulation study based on public use data from the National Health Interview Survey (NHIS). We show that use of the proposed methods enables one to determine the fitness of use of an unconventional data source and to leverage such data appropriately to fulfill program objectives. ","In today's survey environment, we use various types and numbers of variables for survey sample design, as well as survey data collection and adjustment. Adaptive survey designs seek to use paradata to customize survey protocols based on various scenarios that can, in part, be driven by estimated behavior modeled using available frame and paradata. Nonresponse adjustments are often restricted to including a limited number of frame variables, but in some sampling scenarios such as ABS sampling, more geographic and demographic variables can be appended to samples, thus enlarging the number of possible predictors for nonresponse adjustment models. At the design stage, several of these geographic variables also may be appended to sampling frames, allowing for a more diverse set of variables to be included in the construction of sampling strata, PSUs, and overall sampling strategy. In this roundtable we will discuss various ways machine learning methods have and can be used throughout the survey data-collection process and survey weighting adjustment phases, including random forests, cluster analysis, and k-nearest neighbors approaches. ","Model-based small area estimation methods can lead to reliable estimators. provided the underlying model assumptions hold at least approximately. In this talk I will address practical issues related to assumed models, focusing on basic area level and unit level models. In particular, I will address measurement errors in covariates, misspecification of linking models, benchmarking to ensure agreement with a reliable direct estimator at an aggregate level, informative sampling and robust estimation. Methods for measuring variability of area estimators will also be examined. ","Throughout his career, Graham Kalton has contributed in a major way toward how we, as survey statisticians, think about and apply probability sampling and estimation methods to studies of rare and hard to survey populations. This presentation will seek to honor those contributions through a brief review of the general approaches and a more in-depth look at recent developments in several approaches to probability sampling of populations with rare or hard to detect attributes.   ","Kalton and Kish (1984)suggested fractional imputation as an efficient hot deck imputation method. Fully efficient fractional imputation is the limit case in which the estimated conditional distribution is imputed for each missing item. We discuss developments in replication variance estimation, extension to parametric imputation, approximation to fully efficient fractional imputation, and extension to multivariate imputation. ","It is difficult to design a survey because prior information on response rates and the like is likely generated from a different random process than the target one governing the survey to be designed, and the impact on the properties of the estimator can be significant. Nowadays, computer-assisted data collection methods provide an instant variety of observations on the target random process governing the survey under consideration. These data and paradata enable the survey producer to make decisions regarding the need for methodology-process revision, which involves consideration of both a model that represents how the target information relates to the prior information and the design that describes how the observations are obtained. We think of the prior and target information as a random process that has a joint distribution with some probability function. Then at each phase of data collection, after receiving the information that the target random process has taken specific values, we update the joint probability distribution, to revise the design specification in the course of the data collection period. A coefficient of reliability is also discussed. ","More and more large-scale surveys have implemented responsive design strategies in response to falling sample yields and response rates and the availability of paradata. These methods have helped to build a framework for visualizing and reacting to changing circumstances during the data collection process. In the 2003 National Assessment of Adult Literacy, new strategies were formulated for focusing follow-up attempts based on observed sample monitoring indicators, which were refined during the 2012 Programme for the International Assessment of Adult Competencies (PIAAC) survey. The need to sample specific domains in the 2014 PIAAC National Supplement has led to further efforts to find methods that meet sample yield targets with minimal impact on response rates, nonresponse bias, and cost. We focus on three components: projecting whether a sample yield target will be achieved and determining if extra cases need to be released; prioritizing cases; and providing guidelines for the number of contact attempts. The effectiveness and impact of these components will be tested on the PIAAC sample data. Strategies that need to be customized for the literacy study will also be presented. ","The SDR follows a sample of U.S.doctorates in science, engineering, and health (SEH) fields to monitor the nation's education, supply, and employment of doctorate recipients in SEH fields. The SDR underwent a major sample redesign during the 2015 cycle. As a result, the vast majority of the 120,000 sample members are new to the SDR, making it a tremendous challenge to locate the new sample members. To meet this challenge, the SDR team created a comprehensive adaptive design to periodically identify low performing cases and adjust data collection strategies to ensure data quality. The following measures are computed on a weekly basis: contact and completion rate by key domains, partial R indicators associated with key variables, Mahalanobis distance between respondents and nonrespondents, and cumulative yield by key domains. Based on these statistics, data collection resources are redistributed in order to achieve a more balanced sample. Early evidence shows that the adaptive design system has been moderately effective in boosting response rates for traditionally low performing groups. This paper will report the key outcomes associated with the 2015 SDR adaptive design. ","A number of challenges are associated with conducting longitudinal survey research with at-risk young people who are often highly mobile and difficult to engage. This paper describes our use of statistical tools for dynamic monitoring of data quality and our assessment of innovative strategies for increasing sample retention and survey completion among a sample of at-risk young adults. Drawing on multiple data sources-including baseline information and paradata-we calculate quality indicators and construct prediction models to (1) assess the representativeness of our data, (2) identify any over- or under-represented groups, (3) investigate the efficacy of our engagement and retention strategies overall and specifically for those under-represented groups, and (4) adapt our data collection efforts to maximize the representativeness of our data. The findings from this research add to the knowledge base regarding the use of alternative measures of quality in survey practice and the efficacy of using texting and social media, as tools for retaining and engaging sample members in longitudinal research. ","Probability samples are usually selected from a fixed sample frame, which is a close facsimile of the target population, and the probability of selection can be easily quantified. The sample design is necessarily more complicated when the sample frame is not fixed, but changes over time. In this paper, we describe an adaptive sample design for a target population that are members of a disabled population who had successful work experience. The highest priority in this design is to minimize the length of time between the interview date and the period of successful work. We describe a design that was intended to do this, while at the same time accommodating the changing frame. ","A new sampling design is derived for sampling a rare and clustered population under both cost and logistic constraints. It is motivated based on the example of national TB prevalence surveys, sponsored by WHO for high TB-burden countries and usually located in the poorest parts of the world. A Poisson-type sampling design named Poisson Sequential Adaptive (PoSA) is proposed with a twofold purpose: (i) to increase the detection rate of positive cases; and (ii) to reduce survey costs by accounting for logistic constraints at the design level of the survey. PoSA is derived by integrating both an adaptive component able to enhace detectability and a sequential component for dealing with costs and logistic constraints. An unbiased HT-type estimator for the population prevalence (mean) is derived by adjusting for both the over-selection bias and for the conditional structure induced by the sequential selection. Unbiased variance estimation in a closed form is also provided. Simulation results are presented and show a significant pontential of PoSA in improving the sampling methodology currently suggested by WHO guidelines. ","Surveys concerned with substance use or the etiology of medical conditions often want to have larger samples of persons who are in specific subpopulations, while maintaining the ability to monitor transitions from one subpopulation to another. We consider optimal survey design using a two-phase sample, in which persons are selected for the first phase using a screening instrument that misclassifies some individuals, and the second-phase sample is selected using a more accurate classification instrument. We discuss software that can be used to solve the optimization problem with constraints, and illustrate with an example. ","The Occupational Requirements Survey (ORS) is an establishment survey conducted by the Bureau of Labor Statistics (BLS) for the Social Security Administration (SSA). The survey collects information on the vocational preparation and the cognitive and physical requirements of occupations in the U.S. economy, as well as the environmental conditions in which those occupations are performed. Imputation is a multi-step process that involves determining recipients and donors, matchmaking based on a collapse pattern and nearest neighbor, and flexible imputation based on level of and retaining collected data. This paper will describe the testing that was conducted to identify the imputation method most appropriate for generating estimates for this survey. It will also describe the imputation method that is being implemented with the estimates scheduled to be released in late 2016. ","The Annual Survey of Public Employment &amp; Payroll (ASPEP) provides state and local government data on full-time and part-time employment, part-time hours worked, full-time equivalent employment, and payroll statistics by governmental function. For nonresponding general purpose governments, dependent and independent school districts, and special district governments with no historical data available (i.e. births), missing data are imputed using the hot-deck imputation method. Since the hot-deck imputation method entails the use of a random donor for imputation, it is possible that the imputed data do not accurately reflect the data of the unit that was imputed. In an effort to increase data quality, research was conducted to determine if a more accurate imputed value could later be obtained through backwards imputation. This paper describes the results of this research. It discusses the backwards imputation methodology as implemented for the survey and compares the current hot-deck methodology with the backwards imputation methodology. ","Discrete outcomes are often observed among survey responses, i.e., counts distributed as Poisson, Bernoulli, multinomial, etc. When some responses are missing, the observed data provide a foundation for predicting the unobserved values or estimating some statistic for the full sample.  In this paper, data are assumed to be a random sample from a discrete distribution in the exponential family with missing at random (MAR) responses, i.e., the probability of a response missing is unrelated to the value of that response, but could be related to other variables. A distribution-based technique to compute lower and upper bounds for a missing response is developed. The algorithm makes no use of the parameter estimate. Simulations are used to illustrate the technique and to assess its efficiency. Bayesian prediction bounds constructed based on gamma, Jeffreys, and uniform priors are also discussed for comparison purposes. We tested the algorithm using actual data from USDA's National Agricultural Statistics Service's Quarterly Hog Survey. ","Data from the National Health and Nutrition Examination Survey (NHANES) have been linked to the Center for Medicare and Medicaid Services' Medicaid Enrollment and Claims Files for the survey years 1999-2004. This project examines the gains in estimate precision and quality when multiple imputation is used in conjunction with data-linkage. As not all survey participants provide sufficient information to be eligible for record linkage, multiple imputation methods estimate the Medicaid status of those who are not eligible to be linked to the administrative data. Medicaid status among children who are linkage eligible and linked to Medicaid enrollment during the month of interview determine the methods of missingness and imputation is executed using IVEware. Comparisons are drawn across the three different estimates: those made using Medicaid status from the survey interview, those made from linked data among those who are linkage eligible, and those made from linked data after imputation accounts for children who are not linkage eligible. In order to demonstrate how the method applies to the analysis of health outcomes, cotinine levels in children ages 3-17 are explored. ","The fraction of missing information (FMI) is the key factor defining the relative efficiency (RE) of multiple imputation. Rubin used this RE to determine m, the number of imputations, for MI and concluded in 1987 that a small m (?5) would be sufficient. In recent years, however, increasing evidences show that much greater m values are needed. A better understanding of FMI may help explain why Rubin's RE may give an m that is apparently sufficient but may actually be too small. RE assumes that m is independent of FMI, or there would be a logic issue in using FMI to determine m. The correctness of this assumption has not been verified in published literature using real survey data. This paper discusses the relationship between FMI and m using the 2012 National Ambulatory Medical Care Survey (NAMCS) Physician Workflow Mail Survey. ","Recent evidence suggests that labor earnings reported in household surveys compare favorably with labor earnings in administrative records. On the other hand, imputed labor earnings in household surveys seem to match labor earnings in administrative records less closely. This finding has led many researchers to question the reliability of imputed earnings and to exclude these observations from wage analyses. However, this strategy might result in sample selection bias if earnings are not missing at random. In this paper, we compare reported and imputed earnings from the 2008 panel of the Survey of Income and Program Participation to earnings from the Social Security Administration's Detailed Earnings Record. Individuals who are earnings nonrespondents, male, more educated, self-employed, and nonrecipients of social programs have larger average deviations between survey and administrative earnings. Males, self-employed individuals, and nonrecipients of social programs are more likely to exhibit imputed earnings. We establish a novel pattern of nonresponse over the administrative earnings distribution. The implications for estimates of the earnings structure are ambiguous. ","The USDA's National Agricultural Statistics Service (NASS) conducts a census of agriculture every 5 years, in years ending in 2 and 7. The census describes the characteristics of U.S. farms and the people who operate them. It is the only source of uniform, comprehensive data for every state and county. To adjust for undercoverage, nonresponse and misclassification, NASS adjusts the weights on the responding records using a capture-recapture methodology. However, the weights need to be further refined through a calibration process so that the census estimates agree with known population values. Some of the census estimates, such as demographic estimates, are included among the targets so that these estimates are not distorted during calibration. Current NASS calibration methodology is robust but often fails to match all target simultaneously. In this article, we describe a new calibration procedure based on L1-norm relative error. We present the results of a simulation study designed to investigate the consistency and efficiency of the estimators. The calibration estimator can match more targets simultaneously thus providing a foundation for a better methodology. ","Interviewer effects result from responses collected by the same interviewer being correlated, reducing the efficiency of survey estimates and decreasing effective sample sizes. While having received attention in early survey methods research, this well-known problem has been much less studied recently. This may be due to the fact that standard approaches for assessing interviewer effects requiring random assignment of cases to interviewers (interpenetrated sampling), which is often infeasible in many sample designs. In the absence of interpenetration, standard clustering methods may overestimate interviewer effects by confounding non-random assignments of respondents with measurement error introduced by interviewers. Here we propose two methods to assess interviewer effects in the absence of interpenetration: \"anchoring,\" which uses variables assumed to have little or no interviewer measurement error to remove intra-interviewer variability due to assignment via joint distribution modeling assumptions; and \"propensity adjustment,\" which uses weights developed from interviewer assignment propensity models to provide pseudo-interpenerated-design estimates. ","After the Affordable Care Act was passed, many states in US expanded their Medicaid program and created marketplace to offer affordable health insurance options to low/middle income families in 2014. Stakeholders are interested in the effectiveness of such policy, particularly, the potential impact on individuals' health and healthcare utilization. To gauge the policy impact using available data, we propose a statistical framework for estimating the average population causal effect using cross-sectional survey data. Large population health surveys are major sources of data for policy planning and program evaluation. However, literature on how to appropriately conduct causal inference using complex survey data is relatively scarce. Propensity score based adjustments are popular in analyzing observational data. Ad-hoc propensity score adjustment incorporating survey weights have been used with complex survey data, without rigorous justification. We propose a new super population framework, which includes a pair of potential outcomes for every unit in the population, to streamline the propensity score analysis for complex survey data. Based on the proposed framework, we explore diffe ","When obtaining information on a population of interest, state and local entities are often interested in producing estimates for small geographic areas. These areas may be a single county or a collection of counties. One of the more inexpensive modes to collect these data are through a telephone survey. Given population shifts in the types of telephones used, in order to obtain a representative sample, a dual-frame incorporating landline and cellphone phone numbers is used. To best represent the population, at least half the sample should come from the cellphone frame. However, for cellphone numbers there is no perfect identifier for county or county group. In this paper, we present and compare different methods for targeting small areas in a dual-frame telephone survey. We discuss how the classification errors can be derived and incorporated into the allocation of the desired sample size. Data from the 2015 Ohio Medicaid Assessment Survey (OMAS) is used as illustration. ","The utility of a data set that has been altered to preserve confidentiality can be assessed by general or specific measures. The former summarize differences between the distributions of the real and altered data while the latter compare differences between results from particular analyses using the two data sets. We extend previous work on utility for the specific case of synthetic data and exhibit our measures for two real data examples with synthesis. Methods are tailored specifically to improve usability for researchers seeking to generate analytically useful synthetic data. All methods in this paper are implemented in the synthpop package in R. Our extension includes a new statistic, the adjusted propensity mean squared error, that involves: (i) derivation and standardization of the statistic by a null expected value, (ii) the use of non-parametric CART models to estimate propensity scores values, and (iii) the use of the entire data set rather than only the changed variables in computing the utility measures. For specific utility measures, we use confidence interval overlap percentage, and introduce standardized measures for improved utility estimation under certain analyses. ","When a multi-site randomized trial reveals between-site variation in program impact, methods are needed for further investigating heterogeneous mediation mechanisms across the sites. We conceptualize and identify a joint distribution of site-specific direct and indirect effects under the potential outcomes framework. A method-of-moments procedure incorporating ratio-of-mediator-probability weighting (RMPW) consistently estimates the causal parameters. This strategy conveniently relaxes the assumption of no treatment-by-mediator interaction while greatly simplifying the outcome model specification without invoking strong distributional assumptions. We derive asymptotic standard errors that reflect the sampling variability of the estimated weight. ","The ready availability of public-use data from various large national complex surveys has immense potential for the assessment of population characteristics using regression models. Complex surveys can be used to identify risk factors for important diseases such as cancer. Existing statistical methods based on estimating equations and/or utilizing resampling methods are often not valid with survey data due to complex survey design features. That is, stratification, multistage sampling and weighting. In this paper, we accommodate these design features in the analysis of highly skewed response variables arising from large complex surveys. Specifically, we propose a double-transform-both-sides (DTBS) based estimating equations approach to estimate the median regression parameters of the highly skewed response; the DTBS approach applies the same Box-Cox type transformation twice to both the outcome and regression function. The usual sandwich variance estimate can be used in our approach, whereas a resampling approach would be needed for a pseudo-likelihood based on minimizing absolute deviations (MAD). Furthermore, the approach is relatively robust to the true underlying distribution, ","Does the effort of bringing in hard-to-contact or reluctant respondents decrease total survey error? Or will these high response rates end up in causing high measurement error because of bad responses of the additional respondents? The evidence for a relationship between measurement and nonresponse error is still inconsistent. While some authors found evidence for a link between nonresponse and measurement error (Fricker 2007) others have not (Olson 2006). For better understanding this relationship, a further examination of the various reasons for measurement error has to be done. This study examines motivated underreporting as a reason for measurement error in filter and eligibility questions. Using three datasets from a variety of countries and modes, this paper explores whether motivated underreporting is worse among those respondents who were the least likely to respond to the survey. To identify the reluctant respondents, I build response propensity models using statistical boosting. This study found evidence that there does not appear to be more motivated underreporting among reluctant respondents in filter questions but in eligibility questions.  ","Respondent-driven sampling (RDS) is a network sampling methodology used worldwide to sample key populations at high risk for HIV/AIDS who are not typically reachable by conventional sampling techniques. In RDS, study participants recruit members of their social network to enroll, resulting in an unknown sampling mechanism. Current RDS estimators require many assumptions about the sampling process, including that people recruit uniformly at random from their network. This is likely not true in practice. In this talk, I present a two-sided rational-choice framework to model preferential recruitment. Each person's recruitment and participation choices depend on pairwise utilities, which are functions of observable covariates plus unobserved heterogeneities. I develop inference for this model within a Bayesian framework by approximating the posterior distribution of the covariate preference coefficients via Markov Chain Monte Carlo (MCMC). The algorithm is a form of constrained Metropolis-Hastings. My framework results in a tractable generative model for the RDS sampling mechanism. This greatly enhances both design-based and model-based inference. ","In response to recommendations from a recent review of the Hawaii Marine Recreational Fishing Survey (HMRFS), we designed a roving effort and catch survey for shore fishing. The on-the-ground roving effort survey was complemented by an aerial survey and a mail-in survey to cover private and remote shoreline areas and night fishing activities. The roving survey was stratified by region (rural or urban), shift (three 4-hour periods during the day), and day type (weekday or weekend). Each region included three non-overlapping coastal segments (sample units). All of these surveys were field tested in January-April 2015. Results from the pilot mail survey indicate that night fishing accounted for more than one third of the total trips for rod and reel (the major gear type). Over 20% of night fishing was reported in private and restricted areas. Results from the aerial survey indicate that up to 20% of all anglers counted in daylight hours were from remote areas.Thus the effort and catch estimates from the ground-based roving survey need to be appropriately adjusted for under coverage in time (night fishing) and space (remote and private/restricted areas).    ","The Consumer Price Index (CPI) produced by Statistics Canada is an indicator of the change in consumer prices, which is commonly used as a proxy for inflation in Canada. The CPI collects the price of the same set of products each month to produce an estimate of the pure price movement. However, the products available to consumers are constantly changing so the price collected must be adapted to accommodate for these changes. The clothing and footwear component of the CPI can be very challenging in this regard due to the rapid change of clothing products as a result of fashion trends and the change in consumer shopping patterns over time. This paper will give an overview of two initiatives that were recently conducted at Statistics Canada to address these issues. The first part looks at the coverage of the sample, in terms of the types of stores where products are priced, before and after a recent sample enhancement. The second part of this paper examines the use of quality adjustment measures that are used to compensate for changes in product quality over time.  ","Effect sizes are widely used quantitative measures of the strength of a phenomenon, with many potential uses in psychology and related disciplines. In this article, we propose a general theory for a sequential procedure for constructing sufficiently narrow confidence intervals for effect sizes (eg. correlation coefficient, coefficient of variation, etc) using smallest possible sample sizes, importantly without specific distributional assumptions. Fixed sample size planning methods cannot always give a sufficiently narrow width with high coverage probability. The sequential procedure we develop is the first sampling procedure developed for constructing confidence intervals for effect sizes with a prespecified width and coverage probability. We first present a method of planning a pilot sample size after the research goals are specified. Then, after collecting a sample size as large as the estimated pilot sample size, a check is performed to assess if the conditions to stop the data collection have been satisfied. If not, an additional observation is collected and the check is performed again. This process continues, sequentially, until the specified conditions are satisfied.  ","We consider the problem of optimal stratified single stage sampling design where minimal sample size requirements are specified for all strata. We show that the problem reduces to unidimensional optimization, and present an algorithm that solves it. We discuss the substantive interpretation of the algorithm and Lagrange multipliers in terms of the sampling problem at hand. An illustrative numerical example is provided. ","Due to the shift in survey data collection from mail to web surveys, breaking off prior to the end of a survey becomes a more prevalent problem on web. Given the lower response rate in web surveys, it is crucial to keep as many and as diverse respondents as we possibly can to maintain a high data quality standard and thus accurate survey estimates. As a first step of preventing and reducing break-offs, this study aims to predict the exact break-off timing in an online German labor force survey, which was conducted by the Institute for Employment Research (IAB), Research Institute of the Federal Employment Agency, Nuernberg, Germany. This study will make use of the survey data, along with the rich paradata and accessible administrative information from the sampling frame to investigate the factors that associated with breakoffs using logit modeling and survival analyses approach. ","Although not widely used, advocates of conversational interviewing argue that it reduces respondent burden and results in higher quality data in situations where the survey information being requested is complex, highly sensitive, or where the topic elicits stress. However, because it does not rely on a scripted interview, conversational interviewing also raises concerns and challenges concerning its consistency and reliability. This study explored an approach for assessing the reliability of response coding in a conversational interview by asking 86 interviewers to observe and code a video of an interview conducted conversationally. Responses coded by interviewers were then compared to gold-standard answers. To assess the impact of experience, the video observations were conducted on two occasions: once immediately after initial training, and, again, four months later in the data collection period. Results after initial training showed high levels of interviewer consistency using percent agreement with the gold-standard answers, and after four months of data-collection experience, performance significantly improved. Other measures of reliability that correct for chance agreement such as Intraclass Correlation Coefficients (ICC) and Cronbach's alpha were also explored and their use as possible tools for assessing interviewer consistency discussed.  ","Recent trends in U.S. official statistics are characterized by the rising cost of data collection and in- creased nonresponse. At the same time, inexpensive data from commercial nonrandom web panels have become readily available. This paper discusses the possibility of capitalizing on both sources of information by calibrating data from web panels on estimates from conventional randomized surveys. We treat the probability of being included in a web panel as similar to a response probabil- ity and use propensity score adjustment (PSA) of sampling weights and generalized calibration to produce g-weights, thus potentially making nonrandom samples representative of the general pop- ulation. The simulation study discussed in this paper demonstrates that propensity score model or calibration using covariates correlated with both web sample indicator and target variable can elim- inate bias of estimates from web sample. Variances estimated using a two-phase sampling approach match Monte Carlo variances of point estimators. If the web panel inclusion probability depends on the target variable Y , bias can be removed by using Y as an instrumental variable and calibrat- ing on a closely correlated covariate. However, this approach may lead to a significant increase in variances. Conclusions of the simulation study were validated by applying the methods used in that study to a real web sample representing a subset of the National Health Interview Survey (NHIS) questions. The NHIS public-use file was used as an auxiliary for calculating estimates from the web sample data and for their subsequent validation. The g-weight adjusted estimates from the web and random NHIS samples matched within the limits of statistical significance. ","In this note, a new method of calibration of design weights has been suggested by following the two-step calibration approach of Singh and Sedory (2013, 2015). The traditional empirical log- likelihood estimator due to Owen (1998) is shown to be a special case of it. In addition the resultant new estimator is shown to have properties similar to the well known shrinkage estimator due to Searls (1964). The minimum mean squared error of the new proposed estimator is shown to be always less than that of the traditional empirical log likelihood estimator. ","When adjusting for unit nonresponse in a survey, it is common to assume that the response/nonresponse mechanism is a function of variables known either for the entire sample before unit response or at the aggregate level for the frame or population.  Often, however, some of the variables governing the response/nonresponse mechanism can only be proxied by variables on the frame while they are measured (more) accurately on the survey itself.  For example, an address-based sampling frame may contain area-level estimates for the median annual income and the fraction home ownership in a Census block group, while a household's income category and ownership status are reported on the survey itself for the housing units responding to the survey.  A relatively new calibration-weighting technique employed by the WTADJX procedure in SUDAAN\u00ae 11 allows a statistician to calibrate the sample using proxy variables while assuming the response/ nonresponse mechanism is a function of the analogous survey variables. We will demon-strate how this works with data from the Residential Energy Consumption Survey National Pilot, a nationally representative web-and-mail survey of U.S. households sponsored by the U.S. Energy Information Administration.   ","In analyses of complex sample surveys, using balanced repeated replication (BRR) is a commonly used method for variance estimation. To create BRR weights, two primary sample units (PSU) are required per stratum. Creating pseudo-stratum and/or pseudo-PSU is needed when there are not exactly two PSUs per stratum. In NHANES, it is not straightforward to create BRR weights for single year samples as variance strata are formed for two-year data releases with one PSU per year. The purpose of this paper is to compare alternative methods for creating BRR weights using a one-year NHANES sample. Three approaches are considered: 1) randomly assigning two PSU to form a pseudo-stratum, 2) using the upper sampling stage as pseudo-stratum, and 3) treating the sampling PSUs as pseudo-strata and the lower sampling stage as pseudo-PSUs. Simulation results of relative bias and stabilities of the variances are estimated for the proposed methods. Simulation samples are generated from 16 years of NHANES samples. The initial results suggest the instability of single year data is more important than choice of BRR method. ","Radio panels, such as the Nielsen Audio panel, are a reliable source of information on the population's radio listening behavior. Nielsen panels consist of a representative probability sample of a target population, and panel members constantly carry a wearable device that extracts radio signals 24 hours a day. This device generates a binary variable (0/1) to indicate if a respondent listened to a particular radio station for a given 15-minute time window. This raises the question of can limited capture of exposure data (e.g., one week) be used to project radio listening behavior for longer periods of time? This study evaluates a proposed beta-binomial modeling approach for generating correlated synthetic indicators of radio listening behavior for an entire month based on one week of panel data. The presentation compares synthetic estimates of \"cumulative reach\" (i.e., number of people were exposed to a radio station) for one month to direct estimates of reach based on a full month of panel data, and finds that the proposed methodology works quite well. ","The National Center for Education Statistics (NCES) recently added several new modules to its extensive system of online data training modules to introduce new and expert data users to NCES and to facilitate easy access and use of its data. In addition, these modules can be a useful supplement to class materials when teaching practical complex survey design and analysis. This session provides guidance and advice on using these readily available online modules to get an overview of NCES' data resources and learn and navigate a myriad of NCES survey and administrative data sets. The session will cover new additions to the Distance Learning Dataset Training System (DLDT), which provides information about NCES data and the tools that users need in order to find published reports, explore and acquire data, create custom tables and analyses using online tools, create custom data files, understand NCES' complex survey designs and content, and conduct analyses in selected statistical software packages. ","In population-based household surveys, households are often sampled by stratified multistage cluster sampling, and multiple individuals related by blood are often sampled within households. Therefore, genetic data collected from these population-based household surveys can be correlated due to two levels of correlation: one level caused by the multistage geographical cluster sampling and the other caused by biological inheritance among participants within the same sampled family. In this paper, we develop an efficient Hardy Weinberg Equilibrium (HWE) test utilizing pairwise composite likelihood methods that incorporate the sample weighting effect induced by the differential selection probabilities in complex sample designs, as well as the two-level correlation effects described above. Monte Carlo simulation studies show that the proposed HWE test maintains the nominal levels, and is more powerful than existing methods (Li et al. 2011) under various (non)informative sample designs that depend on genotypes (explicitly or implicitly), family relationships, or both. The developed tests are further evaluated using simulated genetic data based on the Hispanic Health and Nutrition Survey. ","Determining the extent of a disparity, if any, between groups of people, is of interest in many fields, including public health for medical treatment and prevention of disease. An observed difference in the mean outcome between an advantaged group (AG) and disadvantaged group (DG) can be due to differences in the distribution of relevant covariates. The Peters-Belson (PB) method fits a regression model with covariates to the AG t0 predict, for each DG member, their outcome measure as if they had been from the AG. The difference between the mean predicted and observed outcomes of DG members is the (unexplained) disparity of interest. We focus on applying the PB method to estimate the disparity based on logistic regression models using data collected from complex surveys with multiple DGs. Estimators of the unexplained disparity, an analytic variance estimator that is based on the Taylor linearization variance method, as well as a Wald test for testing a joint null hypothesis of zero for unexplained disparities between multiple minority groups and a majority group, are provided. Simulation studies and analyses of disparity in BMI in NHANES 1999-2004 are conducted. ","Respondent-driven Sampling (RDS) is a recruitment procedure for surveys of hidden or hard to reach populations. In RDS, enrolled subjects get extra rewards for successful recruitment of their friends who are also in the target network. The quality of statistical estimates from RDS depends on assumptions about the nature of the recruitment process and the social network underlying the target population. Selection bias may affect estimates from RDS surveys since selection of subjects is not controlled by researchers, but by the subjects themselves. In this paper, we characterize RDS procedure under a survival framework, establish a rigorous definition of uniform recruitment and develop statistical estimators and formal tests for recruitment bias when the network of respondents and their alters is fully observed. We apply these tests to a unique RDS study of people who inject drugs in Hartford, CT, USA and find strong evidence to reject the hypothesis of uniform RDS recruitment in this population. ","Complex surveys based on multistage design are commonly used to collect large population data. Stratification, clustering and unequal probability of the selection of individuals are the complexities of complex survey design. Statistical techniques such as the multilevel modeling- scaled weights technique and the standard regression - robust variance estimation technique are used to analyze the complex survey data. Both statistical techniques take into account the complexities of complex survey data but the ways are different. Performance of these two techniques will be examined by Monte Carlo simulation based on Canadian Heart Health Survey (CHHS) or Statistics Canada 2012-Canadian Community Health Survey. ","In medicine, surveys/questionnaires are common research tools to identify the most important barriers that physicians face to improve patient care. Typically, potential barriers are preselected by investigators and described in a question for participants to rank in order from most to least important. Problems may arise if not every barrier applies or if multiple barriers are equally ranked. The nature of this type of ranked data can complicate data analysis and the number of primary barriers ultimately selected is generally performed ad hoc. To overcome this issue, the question can be broken down into barrier-specific questions in Likert or visual analogue scale (VAS). These two scales are common in psychometric research, but the choice of appropriate statistical methods remains controversial based on the nature and scale of the data. In this paper, we compare the three question designs via simulations in order to identify primary factors. We focus our investigation on the differences in designs by the scale of data and truncation. We introduce a simulation-based method that accounts for correlation between ranks as well as ratings within subjects. ","Households receiving rental subsidies through the U.S. Department of Housing and Urban Development's (HUD) assisted housing programs receive a reduction in their monthly rent, or a utility allowance (UA), for out-of-pocket utility costs. UA amounts are estimated by housing sites and are not necessarily equal to a household's actual monthly utility expenses. They are derived using a variety of methods, including the HUD Utility Schedule Model (HUSM). The Utility Allowance Comparison study, sponsored by HUD, seeks to ascertain whether the HUSM UA method is better or worse at predicting actual out-of-pocket utility expenses than a comparison UA calculated via other methods. We use data collected from a nationally representative sample and mean square errors (MSEs) to assess the accuracy of the two UA methods in predicting the actual utility expenditure. The analysis uses jackknife replication to produce standard errors and confidence intervals of the two MSEs, and a determination of prediction performance between the methods is made. Results may inform policies and methods by which UAs are calculated to mitigate both under- and over-subsidizing for out-of-pocket utility costs. ","This paper draws statistical inference for finite population mean based on judgment post stratified (JPS) samples. The JPS sample first selects a simple random sample and then stratifies the selected units into H judgment classes based on their relative positions (ranks) in a small set of size H. This leads to a sample with random sample sizes in judgment classes. Ranking process can be performed either using auxiliary variables or visual inspection to identify the ranks of the measured observations. The paper develops unbiased estimator and constructs confidence interval for population mean. Since judgment ranks are random variables, by conditioning on the measured observations we construct Rao-Blackwallized estimators for the population mean. The paper shows that Rao-Blackwallized estimators perform better than usual JPS estimators. The proposed estimators are applied to 2012 United States Department of Agriculture Census Data. ","USDA's National Agricultural Statistics Service (NASS) conducts multiple surveys over the course of a growing season. Each of these surveys reflects current growing conditions and provides a prediction of end-of-season crop yield. In particular, NASS conducts two interview-based surveys and one field measurement survey from which indications of crop yield may be obtained. It is also known that a number of weather conditions during the growing season may contribute to changes in crop yield. This talk describes a Bayesian hierarchical model that improves end-of-season crop yield predictions by combining these several disparate sources of information. The model incorporates benchmarking of state-level forecasts with regional forecasts of crop yield and gives rise to rigorous measures of uncertainty. It also permits a useful decomposition with respect to the emphasis placed on each information source. Several aspects of covariates selection and model performance are discussed. ","In order to fill gaps in labour statistics data on job vacancy, the Canadian Job Vacancy and Wage Survey was launched in July 2014. The goal of the job vacancy component is to produce quarterly estimates of the number of job vacancies by economic region and occupation, while the goal of the annual wage component is to produce average hourly wage and employment estimates by economic region and occupation. Together, the two components could be used to produce vacancy rates by occupation. The components are integrated and use quarterly samples of 100,000 business locations in Canada on a rotating basis, making the survey one of the largest business surveys at Statistics Canada. The paper highlights methodological issues and solutions related to producing statistics at such detailed levels, while managing respondent burden. One challenge, common to both components, was defining a probabilistic non-response follow-up strategy prioritizing large locations. For the wage component, the main challenge was generating a sample of relevant occupations used to collect detailed wage information from each location. The solutions used sequential Poisson sampling and Pareto sampling. ","Experiments embedded in surveys are a valuable way to test potential survey improvements; however, inference from these experiments can be difficult because one must account for the survey design aspects. Operational issues can restrict possible experimental design choices. In this paper, we discuss a design-based and a randomization-based approach to inference for experiments in an embedded survey. Additionally, some surveys are ongoing and present the option of utilizing data from previous time periods as well as the data from the test period. We discuss and compare an estimator that utilizes data only from the test period with one that compares outcome differences between the study period and a prior period across study treatment groups. Using an example from the American Community Survey (ACS) as well as a simulation, we compare the two inferential approaches and estimators. We find that the permutation-based inferential approach is a flexible and robust method for analyzing embedded experiments and that when the correlation is large between time periods, the difference in time-period differences may provide inferences with greater precision. ","Underage drinking is a persistent threat to the health and well-being of young people with substantial societal costs. The paper evaluates the impact of grantees' environmental impact strategies to reduce underage drinking and associated misconduct. We built databases that included performance measures submitted by grantees and outcome measures, such as campus liquor law violations and traffic accidents. We then geographically mapped these data to the grantee's intervention catchment area using the first three digits of the zip codes or the section center facility code (SCF). We tested specific hypotheses about the relationship between the intervention activities and youth outcomes. We then included state-level alcohol policies to the dataset to created multilevel models examining the effects of state-level alcohol policies as a measure of alcohol control. ","One of the design decisions for complex multi-stage sampling is how to allocate samples to each stage (e.g., number of sample PSU vs. number of sample households within PSU). In order to maximize efficiency of estimates given fixed costs, the decision can be informed by variance component analysis that decomposes the overall sampling variance into individual components associated with each selection stage. When increasing sample sizes for stages with larger variance than their counterparts, the overall variance can be reduced. However, design-based estimators for this analysis in the literature are limited in that they may result in negative variance components particularly when the between-PSU variance is small compared to within-PSU or between-SSU variance. This paper uses an anticipated variance based on a model and a three-stage sample to reduce or eliminate the problem of negative component estimates. We illustrate its application to three-stage sample allocation for the Health and Retirement Study. ","Cluster analysis is a statistical method in which observations are grouped into different classes based on some measure of similarity and is a natural exploratory statistical method when working with multiple, isolated networks such as those seen in the social sciences. There are several ways to cluster social networks depending on how the network is summarized; in particular, we explore using feature vectors in which the paths among different types of nodes are enumerated as network summary statistics. This method is particularly useful in grouping networks based on the ways in which leaders and non-leaders interact, and we will use teacher advice-seeking networks to illustrate these methods. ","Subgroup structure is an inherent characteristic of most types of social networks in which ties are more common among actors within a subgroup than between actors in any two different subgroups. Subgroup integration describes the degree of separation across subgroups; more ties for actors in different subgroups indicates higher subgroup integration. In this paper, we incorporate network level covariates in the hierarchical mixed membership stochastic block model (HMMSBM) to estimate the effects of network attributes on subgroup integration. In addition, we present several simulation studies to evaluate the performance of HMMSBM with network-level covariates as well as a real-world data example to relate teacher practices on students' friendship network integration and provide insight of how this model can be used in practice. ","In this talk, I will focus on the longitudinal extension of the static latent space network model with assumption that the nodes in the networks have underlying positions in a low dimensional Euclidean space and that the probabilities of ties are inversely related to the inter-nodal latent distances. It is further assumed that the network dynamics in time are direct functions of the temporal dependence in the latent nodal positions which show stationary vector autoregressive (VAR) evolution of order 1. I will also present an MCMC algorithm to draw samples from the posterior distribution of the parameters, and explain some of the challenges with unidentifiability associated in this approach. Finally, I use this model to analyze advice seeking networks among teachers observed at different time points. This analysis gives us insights into the important structural changes in the network and the implications of those changes.  ","A number of interventions in social settings such as education seek to improve the structure of social networks as a means to improving more distal outcomes, such as test scores. The rapidly expanding field of network modeling offers a wide range of models to summarize networks, as well as model the effects of interventions on network structure or the effects of network structure on other outcomes. In this talk we will introduce a framework for modeling networks in their intended role: as causal mediators, mediating the relationship between an intervention and an outcome. Specifically, we will unify the hierarchical latent space model for networks with the Neyman/Rubin causal model, by modeling subjects' potential positions in the latent space in treatment and control conditions, respectively. This leads to potential values for network-based mediators. We will discuss options and issues in estimating network mediation models ","We discuss how statistical methods from the field of disease mapping can be used in the area of data privacy with an application to county-level stroke death counts from 1973-2013 across multiple age groups, data which are plagued with low counts. Our primary goal is to identify and summarize spatiotemporal trends in stroke mortality across these age groups. This will require flexible models which account for not only spatiotemporal associations, but also the correlation between age groups to achieve reliable rate estimates. Our second objective pertains to the release of public-use data and data privacy, where NCHS guidelines suggest that death counts less than 10 should be suppressed to protect the confidentiality of data-subjects. For these data, however, this criterion results in nearly 70% of the over 380,000 data points being suppressed. As high suppression rates can significantly reduce the utility of the publicly available data, the secondary goal of this work is to generate so-called \"synthetic data\" that preserve the complex dependence structure of the original data while avoiding the disclosure risks associated with releasing unsuppressed confidential data. ","This paper explores methods of spatial modeling to identify opportunities for reduced fieldwork in census Address Canvassing operations. The purpose of Address Canvassing is to improve the coverage and quality of the Census Bureau's address list, the Master Address File (MAF), prior to census enumeration. Various modeling techniques such as zero-inflated negative binomial regression have been explored in the past to predict areas with many coverage errors on the MAF and identify blocks which would likely contain change (and those which would not). Such information could inform a reduction to the in-field canvassing workload and reduced field costs. We use a recently developed spatial mixed effects model with dimension reduction, and take New York County as an example. It is seen that accounting for spatial dependence has a large effect on the estimated coefficients, including which predictors are significant. The impact to predicted values is more subtle, with the spatial model producing slightly more accurate predictions.  ","Fitting high-dimensional dependent data models is a challenging endeavor and typically requires some form of dimension reduction or stylized estimation algorithm. Particle swarm optimization (PSO) refers to a class of heuristic optimization algorithms that exploit analogies with animal flocking behavior in order obtain optima without strong conditions on the objective function. Bare bones PSO (BBPSO) is a particularly simple PSO algorithm that depends only the particle's personal best location, the particle's group best location, and Gaussian noise where \"best'\" is defined in terms of the objective function. We introduce a class of PSO algorithms based on BBPSO, termed adaptively tuned BBPSO (AT-BBPSO), by adding a scale parameter to the algorithm and which we dynamically tune using an analogy with random walk Metropolis. Our proposed algorithm is embedded within a Metropolis Hastings algorithm to provide an efficient proposal distribution and thus improves mixing and convergence. In order to illustrate this optimization method and compare it to alternatives, we provide a simulation study and apply it to a high-dimensional spatial problem arising from official statistics. ","We introduce a Bayesian approach for analyzing high-dimensional dependent data that are distributed according to a member from the exponential family of distributions. This problem requires extensive methodological advancements, as jointly modeling high-dimensional dependent data leads to the so-called \"big n problem.\" The computational complexity of this problem is further exacerbated by allowing for non-Gaussian data models. Thus, we develop new computationally efficient distribution theory for this setting. In particular, we introduce a class of conjugate multivariate distributions for the exponential family. We discuss several theoretical results regarding conditional distributions, an asymptotic relationship with the multivariate normal distribution, parameter models, and full-conditional distributions for a Gibbs sampler. We demonstrate the modeling framework through several examples, including an analysis of a large dataset consisting of American Community Survey (ACS) period estimates. ","Human migration patterns present a rich set of modeling challenges, including very large data sets and many covariates that may directly or indirectly influence migration. With the advent of the American Community Survey, we are provided with finer spatial detail and shorter time intervals than were previously available in U.S. population migration data as well as social, economic, and geographical variables that may inform migration. Furthermore, migration can be stratified by gender, race, or socioeconomic status. We develop a flexible multivariate spatio-temporal model for migration that incorporates covariates and their uncertainty. The model is designed for multivariate human migration, but is capable of modeling ecological migration, as well as non-standard \"migration\" problems such as traffic flow, and trade flow. ","Statistical agencies aim to release informative data to the public, but they need to avoid disclosure of respondents' information, which requires more than removing direct identifiers. Usually a perturbed data set is generated from original data using statistical disclosure avoidance techniques and released. However measuring disclosure risk is difficult as it can occur in many different forms and depends on behavior of intruders. As a result the tasks of designing the perturbation mechanism and assessing data utility of the perturbed data are quite challenging. In this paper we propose a novel and rigorous measure of identification disclosure risk and use it to articulate clear and realistic disclosure control goals. Then we present unbiased post-randomization methods for achieving those goals. Specifically, the probability of correct identification of any sample units will not be larger than a pre-chosen value. We also assess the utility of perturbed data and show that the added variance due to our perturbation procedure is trivial comparing to sampling variance. Finally, as an illustrative example we apply our procedure to a public use micro sample released by US Census Bureau. ","Data users in government, private industry, non-profit organizations, and academia have substantial demand for data from the Census Bureau's surveys and censuses. Hence, the Census Bureau aims to disseminate data widely and with as much detail as possible while keeping the pledge of confidentiality given to all respondents. The Census Bureau is working on initiatives to improve our disclosure avoidance techniques so that we fulfill both of these aims. In this paper, we briefly discuss previous research involving a remote analysis system. Unfavorable results of this research have led us to pursue other options, including the increased use of synthesis and perturbation to protect underlying microdata. We discuss our initial research involving using Classification and Regression Tree (CART) models and noise infusion for the American Community Survey.  ","Publically-released data users often experience the challenge of computing regression estimates that are similar to those they would have attained were they able to access the internal data. When a variable is binned for being too sensitive for release, the task becomes more critical. This study investigates how releasing frequency data for a sensitive variable and unbinned data for a non-sensitive covariate affect the likelihood of estimating regression parameters that do not substantially differ from those obtained from the internal data. Simulations will determine whether the likelihood is impacted by the number of bins, the underlying distribution of the sensitive variable, and whether noise multiplication is introduced. An application to economic survey data follows. ","Under Title 13 of the U.S. Code, the U.S. Census Bureau is required to release information from its data collection to the public, while also promising to protect the confidentiality of individual respondents. Prior to releasing any data product to the public, we must apply various disclosure limitation procedures and review these to ensure that the data are adequately protected. In most cases, products for public release will also need review and approval from the Census Bureau's Disclosure Review Board (DRB). Many individuals know what the DRB is, and how they operate, but they are not aware of the challenges the DRB faces when approving certain data products. This paper looks to fill that void. We provide detailed examples and discuss many of these challenges. In doing so, we give an inside look at how the DRB overcomes these issues to ensure that the products we approve are safe for public release. ","Cancer surveillance research often begins with a 5-year tabulated Lexis diagram of cancer incidence derived from cancer registry and census data. This smoothing approach suffers from a significant limitation; important details useful in studying time trends may be lost in generating the summary rate. This study utilizes a Poisson model to describe the relationship between the number of new cases, the number of people at risk, and a smoothly varying incidence rate for the study of the incidence rate function. We propose a Bayesian approach to construct the posterior distribution of smoothed Lexis diagrams for the study of the effects of age, period, and cohort on incidence rates in terms of straight-forward graphical displays. These include the age-specific rates by year of birth, age-specific rates by year of diagnosis, year-specific rates by age of diagnosis, cohort-specific rates by age of diagnosis and annual percent change of incidence rate. Simulation studies indicate that this Bayesian approach outperforms standard Lexis diagram in terms of incidence rate estimation. We apply this method to compare the time trends in breast cancer incidence in Taiwan and those in the US.  ","We consider age-period-cohort (APC) model in social studies and chronic disease epidemiology on an a\u00d7p table with single observation in each cell, where rows, columns and diagonals represent age, period and birth cohort. The APC classification regression model suffers from an identifiability problem with multiple estimators having the same fitted values. Here we develop a two-stage smoothing-cohort model to address the identifiability problem. In stage 1, a smoothing cohort model yields a unique estimator with consistent estimation for age and period effects but not cohort effect. In stage 2, a non-contrast constraint is applied to age or period effect with estimates from stage 1. Three constraint selection methods are examined, including the largest ratio of estimates, the smallest variance of the estimate ratio and the smallest variance of linear combination of estimates. Our simulation results based on an a&gt;p data set show that the constraints on period effects outperform those on age effects. The constraint by the smallest variance of the period effect ratio yields the best estimation. We demonstrate our method with SEER cancer mortality data and sociology data. ","Age-period-cohort (APC) models have been studied extensively in social science and public health. It models data on an axp table of a rows of age groups and p columns of periods. It is a special case of two-way ANOVA with diagonal effects as special age by period interaction. In general, the parameters in one-way or two-way ANOVA models need constraint, either by parameter centralization or by setting a reference level. In this paper, we examine the impact of such constraints. We first exam the variance of parameter estimates and how it varies with the constraint. We then further introduce a sensitivity analysis to study the estimator's robustness in data analysis. We conduct simulation studies and demonstrate the results with real data in cancer research. We conclude that constraint on the parameters plays a crucial role in data analysis and inference, except for balanced data. For the APC model with the number of observations varies with the diagonal, a robust parameter estimation is desirable and caution should be used in selecting the parameter constraint.  ","Statistical age-period-cohort analysis has been studied extensively in the literature and has many applications to demography, public health, and sociology. However, due to the linear dependence of age, period and cohort, the regression model suffers from the identifiability problem, where multiple estimators fit the model equally well, making it difficult to determine which estimator yields the correct parameter estimation. In this work, I apply the Lasso shrinkage method, which not only helps to determine a unique estimate, but also yields consistent feature selection since the Lasso estimator yields consistent variable selection if the covariates of the model satisfy the irrepresentable condition. We apply the Lasso to the eigenvectors of the design matrix and thus the irrepresentable condition is satisfied as it sets the coefficient for the null eigenvector to 0, leading to consistent estimation. We will compare the Lasso estimator with the intrinsic estimator in real data examples and illustrate that the Lasso method works well and yields sensible trend estimation in age, period and cohort. ","The age-period-cohort (APC) models have been applied to demography, economics, public health and sociology. Due to the linear dependence of the age, period and birth cohort, the APC classification model yields multiple estimators, leading to indetermination of parameter estimation and temporal trend. To achieve a unique estimator, an equality constraint has often been used to estimate the parameter and the temporal trend. However, such constraints lead to arbitrary parameter estimation and yield biased estimates almost surely. Given that many APC studies have been analyzed using the constraint method and published without the original data, bias correction is desirable but challenging, particularly after the original data become unavailable. The challenge is how to achieve consistent estimation by the intrinsic estimator with only biased estimates. In this paper, we present a bias correction method to correct the bias based on only the biases estimates without requiring the original data. We demonstrate our method with two data sets, one with original data and one without, and show that both yield the intrinsic estimator and achieve sensible trend estimation. ","Various forms of auxiliary information are being sought to augment survey samples and adjust for possible nonresponse bias in key survey estimates. Auxiliary data options are typically limited in most general population surveys and there are questions concerning their utility for nonresponse adjustment. Federal administrative databases provide a potentially rich source of auxiliary information for nonresponse adjustment, but linking them to general population samples is usually restricted to surveys which draw their samples from population registers containing unique personal identity numbers which can be directly linked to federal databases. In this article, we examine the feasibility of linking a federal administrative database to a general population survey sample in the absence of unique identifiers. We employ a series of indirect linkage procedures that rely instead on non-unique and error-prone identifiers collected from the sampling frame to link a federal employment database to a general population survey in Germany. The quality and selectivity of the established links are evaluated using a mix of household- and person-level interview data. ","If there is x% error in individual source files and x% error in the linkage across files, then there can be as much as 3x% error in the analysis of the data from two files that are linked with non-unique common identifiers such as name, address, and date-of-birth. This paper describes methods of cleaning (improving quality) in individuals files, methods for increasing the accuracy of the linkages, and a model for adjusting statistical analyses for the linkage error. ","In the task of record linkage, one compares information on records in two files on a single population and decides which of the pairs of records, one from each file, pertain to single individuals. The result of record linkage is a file with one record per individual that contains the sum of all information on the individuals from the two files. One also learns the number of records on the two files that appear on both and on one but not the other. Probabilistic record linkage uses statistical models to estimate the probability that a pair of records arises from a single person. Several fields of information, including names, dates, locations, and relations to other records, on the two files are compared. One set of models used to estimate probabilities based on these data are latent class and mixture models. After linkage one often desires to estimates relationships among variables that were included on the two files. Linkage is necessary when analysis variables are separated across the two files. Enhanced mixture models are proposed and illustrated for the case in which some nonmatches are similar to one another due to constraints in blocking criterion used to screen pairs. ","The Medical Expenditure Panel Survey (MEPS) is an ongoing household survey that yields national estimates of various health care metrics; including health care use, expenditures, and insurance coverage. The MEPS also collects information from a sample of health care service providers reported by the household. The medical provider data are an invaluable complement to the household reported data. Often more detailed and accurate, the provider data serve as the gold standard for MEPS expenditure estimates and are the source for MEPS expenditure imputations. Because of increased demand for data on organizational characteristics of providers and/or health care practices, the Robert Wood Johnson Foundation has sponsored a Medical Organization Survey (MOS) which will collect this type of data from a subset of the MEPS medical provider sample. This paper presents the underlying design considerations of the MOS (e.g., identification of the sampling frame, sample selection, construction of analytic weights, linking of person-level and organization level characteristics) and discusses the analytic potential of the data. ","Combining information from two independent surveys is considered. We consider the calibrated Bayesian method that has nice features of Bayesian method and also provides design-consistent estimation. This method is particularly useful in combining two survey samples; we consider not only nested two-phase samples where the second-phase sample is a subsample of the first phase but also non-nested two-phase samples where two independent samples are obtained from the same population. The proposed method can be extended to the case when the two samples are obtained under complex sampling design. We provide a simulation study to demonstrate the performance of the proposed method. ","Reducing duplication and matching records lacking unique identifiers are common practices associated with the construction and maintenance of a list sampling frame. The U.S. Department of Agriculture's National Agricultural Statistics Service (NASS) employs a record linkage system built using AutoStan and AutoMatch (originally developed by MatchWare Technologies) to maintain its list frame of farm operators and agribusinesses. The overall process consists of four steps: 1) reformatting, 2) standardizing (AutoStan), 3) matching (AutoMatch) and 4) review. Because the current matching engine is no longer supported and becoming increasingly obsolete, NASS has recently begun to explore alternative software options, such as Statistics Canada's G-Link package. In this paper, we describe the results of a preliminary study comparing G-Link with AutoMatch using list frame data from a national survey of organic farmers and discuss issues associated with upgrading the agency's record linkage system.   ","The Consumer Price Index (CPI) produced by Statistics Canada is an indicator of the change in price in price for commercial goods and services. It is commonly used as a proxy for inflation in Canada. The CPI collects the price of the same set of products each month to produce an estimate of the pure price movement. However, the products available to consumers are constantly changing so the price collected must be adapted to accommodate for these changes. The clothing and footwear component of the CPI can be very challenging in this regard due to the rapid change of clothing products as a result of fashion trends and the evolution of consumer shopping patterns over time. This paper will give an overview of two initiatives that were recently conducted at Statistics Canada to address these issues. The first part looks at the coverage of the sample, in terms of the types of stores where products are priced, before and after a recent sample enhancement. The second part of this paper examines the use of quality adjustment measures that are used to compensate for changes in product quality over time.  ","Balance repeated replicate (BRR) weights are commonly used for variance estimation for multi-stage complex sample surveys. BRR requires two primary sample units (PSU) per stratum. Creating pseudo design variables, BRR stratum and/or BRR PSU, is needed when there is only one PSU per stratum. The National Health and Nutrition Examination Survey (NHANES) is a multi-stage sample survey with a sample design that has changed over time. For NHANES 2011-2014 stratification, five state groups were formed using external health information (e.g., infant mortality rates). Using these five state groups, major strata were created by geographical and urbanization characteristics. This design is used to create variance units on released data files for variance estimation; major strata and certainty PSU are both used to form variance strata. NHANES has two (sometimes three) variance PSUs per stratum for each two-year data file. For single year data files, there is generally one variance stratum and 15 variance PSU. Therefore, it is not straightforward to create BRR sample weights from single year data files. Although most health measures are available on two-year data files, some components are only available for one year. The purpose of this paper is to compare three methods for creating BRR sample weights with a one-year NHANES data file. First, two PSUs are randomly paired into one BRR stratum (Random). Second, the five state groups are used as BRR strata and PSUs within each state group are paired (State Group). Third, each PSU is defined as a BRR stratum and segments within the PSU are divided into two BRR PSU (Dividing PSU). Simulations using 16 years of NHANES data are used to examine variance estimates using these three methods. Relative bias and coefficient of variations (CV) are compared among the three proposed methods. ","For survey statisticians and methodologists, field observation of production-level data collection is a crucial part of our process for research, development, and improvement of operations. In this roundtable, participants will be invited to address the following questions: Does your organization use field observation of data-collection processes? What are the primary goals for your field-observation work? What do you watch and listen for during field-observation work? What are important \"lessons learned\" your organization has gleaned from field observations? ","We investigate the Current Population Survey (CPS) sample size derived from survey reliability requirements, which are expressed in terms of the coefficient of variation (CV) and significance level. We study the CHIP sample expansion, which is designed to improve estimates that support the Children's Health Insurance Program. What is the minimum sample needed to meet design requirements for the CPS? How does the CHIP sample expansion affect our estimates? At last, we investigate minimizing variance subject to 1) the total sample size being fixed; 2) the implementation of sample reductions due to budget cutting and/or field operation cost increases. ","In this research, we propose an iterative AK composite estimator for Current Population Survey. Using iterative procedures, we construct a general family of composite estimators, which include the current AK composite estimator as a special case. Our proposed method will reduce the mean squared error of the AK composite estimator when we choose the optimal estimator in this general family. Based on the analytic formula of the mean squared error, we discuss how to choose the optimal tuning coefficients involved in the iterative AK composite estimator. Finally, we will examine the proposed method via comprehensive numerical studies. ","The Current Population Survey (CPS) has measured U.S. labor force characteristics since 1940. Its variance methodology reflects a complex sample design, statistical innovations, and computational improvements. This methodology also combines best theoretical methods with techniques specific to the particular application. In this talk, we give a detailed overview of the components of CPS variance (reflecting different stages of the sample) and other related characteristics (such as design effects and generalized variance functions). We discuss the details of CPS variance methodology: how it impacts each of these components, and how they relate to the variance estimate as a whole. We also address some challenges, and identify areas of potential improvement. Finally, we use data from the CPS to show how the methods are working.  ","In the Current Population Survey (CPS), replication methods are used to calculate variances of survey estimates. Since these are often noisy, generalized variance functions (GVFs) are used to produce published estimates of variance that are more stable over time. Recently, the calculation of GVF model parameters has been reconfigured in the CPS. Rather than cluster series and create interdependencies among variance estimates, the parameters for each series are calculated individually, based only on their own histories. Instead of an iterative refitting, a single model is constructed for each historical series, smoothing out the noisiness of replicate variances while retaining seasonality. This paper details these changes to the GVF model framework and presents the resultant improvements in CPS published variance estimates. ","The Current Population Survey (CPS) of households is a source of national and subnational labor force data, including the official unemployment rate which is a Primary Federal Economic Indicator. The sponsoring agencies (Bureau of Labor Statistics and the Census Bureau) only implement changes that are supported by sufficient analysis. Several ongoing research projects are discussed including a study of subnational sample allocation, variance estimation including a new approach to Generalized Variance Estimation, and modifications to the composite estimator. Proposals for future research are outlined. ","Federal agencies have conveyed the desire to use Federal statistics on sexual orientation and gender identity (SOGI) to inform policy, noting, in particular, the unique health and discrimination concerns faced by the lesbian, gay, bisexual and transgender (LGBT) populations in the United States. Although some Federal surveys have collected information describing LGBT populations for well over a decade, some aspects of SOGI have been more routinely collected and extensively studied than others. Further, some Federal agencies that are not currently collecting SOGI information have expressed interest in doing so. Because it is the responsibility of Federal statistical agencies to produce relevant, accurate, and objective statistics in a manner that protects participant confidentiality, OMB has established an Interagency Working Group on Measuring SOGI (SOGI IWG). This presentation describes recent progress made by the Federal Statistical System (FSS) in the collection of SOGI as well as the progress made by the SOGI IWG in describing current SOGI measures, scientific evaluations of SOGI measures, and the establishment of a research agenda on SOGI measurement within the FSS. ","The Census Bureau has been working to improve measurement of same-sex couple households, especially for same-sex married couples. In 2015, several tests of the revised relationship question were conducted. The new question lists explicit categories for \"opposite-sex husband/wife/spouse,\" \"same-sex husband/wife/spouse,\" \"opposite-sex unmarried partner,\" and \"same-sex unmarried partner.\"    A relatively small population such as same-sex married couple households are difficult to count since it only requires a very small percentage of opposite-sex couples to mismark relationship or sex to affect the estimates. In anticipation of respondent mismarks, an automated check was also added to the self-response internet and mobile device nonresponse follow-up instruments.    Tests in the spring of 2015 included the new relationship categories and edit across three modes (mail, web, and personal visit). Using response data and paradata, the paper will report on item nonresponse rates, consistency of reports between sex and relationship, and degree of misreporting and distribution of categories between the test and control versions. ","Items to accurately measure sexual orientation and gender identity in surveys are needed to assess disparities. Despite progress in the development and use of a measure of sexual orientation, our knowledge about response and comprehension issues is limited, especially among older adults in general and Spanish speaking older adults in particular. Currently no Federal survey includes questions to measure gender identity, in spite of developing consensus on a two-question approach: one to assess sex assigned at birth and a second to assess current gender. In order to address this research gap, NORC, in collaboration with the Centers for Medicare &amp; Medicaid Services (CMS), conducted cognitive interviews with Medicare beneficiaries of items to include in the Medicare Current Beneficiary Survey (MCBS). The cognitive research assessed comprehension and reactions to the sexual orientation measure and a two-step approach to measure gender identity. The presentation will present findings from over 50 cognitive interviews that were conducted with a diverse group of both LGBT and non-LGBT Medicare beneficiaries (most over 65 years old) with special attention paid to Spanish speakers. ","The NCVS collects information on nonfatal personal and property crimes both reported and not reported to police. As part of the ongoing redesign efforts for the NCVS, the Bureau of Justice Statistics plans to include sexual orientation and gender identity (SOGI) questions in the survey's demographic section. Sexual orientation and gender identity have been identified in other research as correlates of victimization, and the Violence Against Women Reauthorization Act of 2013 explicitly bars discrimination based on actual or perceived SOGI status ensuring access to key services. The inclusion of these measures will provide important national-level estimates on victimization of LGBT people and allow researchers to understand victimization risk and access to victim services. In the fall of 2015, the BJS and Census Bureau, conducted cognitive testing of the proposed SOGI questions. This presentation will discuss the results of the research to test the wording of these questions. Other considerations such as item placement, frequency of administration, minimum age for administration, and translation of the questions into languages other than English are also discussed. ","The Institute of Medicine has identified a need for more and better research focused on sexual and gender minorities. Our ability to conduct insightful and meaningful analyses within this group is significantly hampered by our inability to identify and systematically study individuals who are sexual and gender minorities. In this paper we explore our ability to use Medicare provider billing data to identify and describe individuals who may be transgender. Using 2013 Medicare claims files, we employ a combination of diagnosis and condition codes to identify transgender individuals enrolled in the Medicare program. We will discuss our methodology and describe the demographic characteristics of those identified in the study. There is a growing body of evidence suggesting that older adults who are both gender minorities and aged may be vulnerable to significantly poorer health access and outcomes. To our knowledge this study is the first attempt to use Medicare data to identify and describe the demographic characteristics and disease burden of Medicare beneficiaries who are transgender. ","Throughout history there have been many perspectives on the approach to planning cities, with a notable clash between dense, organically- formed urban spaces and suburban space planned around the automobile. When suburban design principles (wide streets, parking) have been imposed within the city, there have been unique effects (mostly believed to be negative) on urban life. Urban data analysis has been recently improved through publicly available high resolution data. The goal of our research is to empirically support and to question many urban design principles of the past half-century. Our methodology involves the analysis of local neighborhood features, including crime, land use, zoning, business revenue, MLS comparative sales, walkability, and population demographics. Philadelphia is an interesting case study for this work, with its rapid urban development in the last decade, after a long period of middle class flight and suburbanization. Our secondary goal is to inform public and private decisions in development, and create awareness for smart development by city residents and investors.  ","The U.S. population is becoming more racially and ethnically diverse yet disparities in health care continue to exist. One specific area of medical care in which racial disparities have been identified is total knee arthroplasty (TKA)-an efficacious and cost-effective treatment option for individuals with advanced arthritis of the knee. Studies have documented that racial and ethnic minorities tend to have higher rates of adverse health outcomes and face more barriers in utilizing the procedure. As TKA can potentially improve the health of many Americans, there is a great need to understand, track and resolve racial/ethnic disparities in its utilization. In this study of nationally administrative data collected for the State Inpatient Databases (SID) from eight states between 2001 and 2008, we found that minorities had higher rates of adverse outcomes (mortality and complications) and lower rates of TKA utilization. Furthermore, minorities were less likely to undergo TKA in high-volume hospitals compared to whites.  ","The purpose of this presentation is to discuss statistical methods for model-building using complex survey data. The balanced repeated replications method with user-supplied weights for variance estimation will be addressed and illustrated using the Tobacco Use Supplement to the Current Population Survey data with 160 replicate weights. The methods will be demonstrated via a recent study conducted to estimate the prevalence of unassisted smoking cessation (i.e., quitting without pharmacological aids or behavioral interventions), and identify the most common smoking cessation methods used by US adult smokers while trying to quit smoking between 2007 and 2011. Among successful quitters, factors associated with using pharmacological methods and quitting unassisted were examined via a survey logistic regression. Considered socio-demographic factors included age, gender, race/ethnicity, highest level of education, marital status and employment status. Additional factors included visiting a doctor twelve months prior to quitting and survey mode. Analyses were performed using SAS\u00ae9.4. ","A review of the randomized response model introduced by Warner (1965) is given, then a new randomized response model applicable to continuous data that considers a mixture of two normal distributions is introduced and analyzed in a parallel assessment to the work provided by McDonald (1994). This article provides a study of the efficiency, an estimation of some unknown parameters and a discussion of contaminated data issue. Also, this article includes inference for two or more populations of the same structure as the randomized response model introduced. ","In this paper, we revisit the two deck randomized response model by Odumade and Singh (2009). We adjust Odumade and Singh model following Su, Sedory and Singh (2016) for protection and efficiency. The adjustments makes use of known proportions of unrelated characteristics. ","In this paper, we introduce a new problem of simultaneous estimation of means of two quantitative sensitive variables by using only one randomized response and another pseudo response from a respondent in a sample. The proposed estimators are extended to stratified random sampling, and the relative efficiency values are computed for equal, proportional and optimum allocation with respect to the newly introduced naive estimators. ","Nonresponse bias is an ongoing concern to all who collect data via surveys. It occurs when nonresponding sampled units differ from those who respond, in ways related to the survey outcome variables of interest. Although every effort should be made to minimize nonresponse at the point of data collection to minimize the risk of bias, these efforts are rarely adequate in preventing a high degree of nonresponse. Thus a post-hoc analysis of nonresponse is often necessary to determine the effects of nonresponse on survey outcomes. In this study, we will analyze nonresponse in a sample of 2,067 urban households selected for a survey of community health needs in Chicago. As the study was conducted via face to face interviews, we gathered neighborhood (block) level data and observable unit level data. In addition, we appended block group level Census Planning Data to the block groups to which the sampled blocks belong. We have used these data to model total response, contact and cooperation propensity among sampled households. Those variables predictive of these various response propensities were examined for their association with survey outcome variables of interest, including chronic and ","High survey nonresponse provides substantial potential for nonresponse bias in population estimates. As a result, surveys increasingly rely on auxiliary information to (1) estimate nonresponse bias, (2) attempt to reduce nonresponse bias during data collection, and (3) use statistical models in weighting and estimation. All three rely on auxiliary data that are strongly correlated with key survey variables. Such data are rare in household surveys. We propose and evaluate the collection of proxy survey variables from some nonrespondents. We leverage the two-phase interview design in household interview surveys by embedding proxy measures on the household and the selected person in the screening instrument, collected from the household informant. We include two key health questions (health conditions and public health insurance) in the California Health Interview Survey screener, to potentially use in the estimation, reduction, and adjustment for nonresponse bias. We evaluate the measurement properties and causes of measurement error in these questions and the impact on each goal. ","Selection bias can result in biased estimates of epidemiologic parameters (i.e. incidence, prevalence) in a survey study due to non-respondents. This paper focuses on the importance of collecting secondary data and appropriate method selection to reduce the bias. We use a population survey, along with secondary patients' information from Electronic Medical Record (EMR) as secondary data, from a health care system to estimate age-specific prevalence of urinary incontinence (UI) from six models:  M1: direct estimate using early respondents M2: direct estimate using all respondents M3: application of an inverse probability weighting (IPW) scheme method 1 and secondary data M4: application of an IPW scheme to method 2 and secondary data M5: application of a multiple imputation (MI) scheme to method 1 and secondary data Method 6: application of a MI scheme to method 1 and secondary data M2, 3 and 4 results in similar age-specific prevalence estimate, while prevalence estimate from M1 is higher for older age groups. M5, 6 tend to overestimate.  Inverse probability weighting method yields the most consistent estimate by incorporating the probability estimated from secondary data. ","Deteriorating response rates and potential substantial nonresponse bias have been the concern for survey researchers. Nonresponse follow-up is not just a way to boost the response rate but also a means to learn our converted respondents, who otherwise would be the nonrespondents. This paper will study the nonresponse follow-up cases within different demographic sub-groups as to how they help reduce nonresponse bias for surveys with different response rates, complexities and topics. This study will utilize survey data collected from respondents from NORC's AmeriSpeak\u00ae Panel, a probability-based household panel with extensive effort on nonresponse follow-up. In the initial recruitment, sample units are invited to join AmeriSpeak via contacts by mail and telephone. The second-stage non-response campaign targets a random sub-sample of the nonrespondens from the initial recruitment. Units sampled for the non-response follow-up campaign are sent by express mail a new recruitment package with an enhanced incentive offer.NORC interviewers then make personal visits to the respondents' homes to encourage participation. ","Low response may render a probability sample behave like a nonprobability sample. High WRR using an NRFUS may be misleading due to instability in the resulting estimator. Release of many reserve replicate samples helps in reaching the target sample size but puts a lot of burden on the nonresponse model-based adjustment. Use of ad hoc substitution by similar units to offset nonresponse is subject to selection bias. As an alternative, a random replacement strategy for unbiased estimation is proposed based on the idea of reserve samples of size one which can be viewed as follow-ups for nonresponding units. It is a take-off from the random group method of Rao-Hartley-Cochran where each stratum is randomly split into groups, and then a single unit is drawn within each group. In the proposed method, each stratum is sorted via implicit stratification before forming zones. The number of zones is about half the allocated sample size. Each zone is randomly split into groups within which replicate samples of size one are selected. Weighted estimates from all responding groups are combined after adjustments for nonresponding groups. Here group RR is meaningful and, in fact, can be made high. "," The Current Employment Survey provides the information on the increase or decrease in the number of jobs in the U.S. economy on a monthly basis. This measure is considered to be one of the leading economic indicators. The initial numbers may be revised due to firms that do not report in a timely manner leading to revisions in the initial numbers for several months afterwards. These revisions, up or down, can be significant at times, and may change the inferences made about the health of the economy.  ","Decreasing response rates in sample surveys is a serious problem and nonresponse weight adjustment using propensity score method is commonly adopted to compensate for unit nonresponse. However, the inference after nonresponse weight adjustment is complicated because the effect of estimating propensity score method needs to be incorporated. In this paper, we develop a new Bayesian approach to handling unit nonresponse under a parametric model for the response probabilities. The proposed Bayesian method is calibrated to frequentist inference. The computation does not involve Taylor linearization. Our proposed method is applicable to both ignorable and nonignorable response mechanism. Some asymptotic properties are established and the results from two simulation studies are presented. ","Adaptive survey designs are emerging more frequently in practice as a means of achieving higher quality data per unit cost. These designs tend to apply differential data collection protocols to the sample at various stages of collection. Protocols may be determined a priori or identified during data collection (as is the case for a responsive design) and are allocated to the sample based on paradata and other auxiliary information. The manner in how protocols are identified and allocated have implications for weighting and variance estimation. Since information at early stages of collection may be used to determine later stage protocols, variance computations for final combined-stages estimators must reflect the variation in the realized information at earlier stages. Thus, traditional variance estimators must be extended so as to fully reflect the variance contributions from each stage. Following a review of concepts, we outline a mathematical framework for weighting and variance estimation that incorporates the aforementioned variance contributions. We also present a simulation study to illustrate this framework and identify practical considerations for survey practitioners. ","Attempting to obtain responses through repeated follow-ups of reluctant respondents both complicates the data collection process and incurs considerable extra costs to the Medical Expenditure Panel Survey (MEPS) Household Component. Due to more extensive follow-up and lower response rates, the costs per completed interview for these households are significantly higher compared to households that respond to initial contacts. A responsive design that subsamples non-respondents after a reasonable number of follow-ups is being considered as an option to reduce data collection efforts and costs in the MEPS. Using survey paradata, this talk presents a cost-benefit analysis of subsampling interim nonrespondents. It will discuss potential benefits in terms of costs savings and increased response rates versus loss in precision of estimates due to increased design effects. ","The U.S. Census Bureau conducted a series of experiments to evaluate alternative contact strategies for use in the upcoming 2017 Economic Census. We hoped to identify effective mail strategies that increase timeliness of response and reduce the number of cases receiving more-costly follow-ups, especially those involving telephone contacts. These experiments were incorporated into the collection of several inter-census (annual and quarterly) business surveys. This paper summarizes results from five experiments and discusses their potential for application in adaptive design approaches for business surveys. ","Like other longitudinal household surveys, the Survey of Income and Program Participation (SIPP) experiences challenges maintaining sample balance across waves. To combat this, SIPP implemented an adaptive survey design experiment in Wave III aimed at facilitating field interviewers' work and increasing sample balance by prioritizing under-represented cases and de-prioritizing over-represented cases. Because SIPP interviewers attempt to interview all original sample members, the experiment also prioritized movers, historically hard-to-interview cases. The results of R-indicators and business rules determined the prioritization of sample cases, and case management systems supplied field interviewers with cases labeled \"low,\" \"medium,\" or \"high\" priority on their laptops. All SIPP interviewers were randomly assigned to treatment or control. Control interviewers saw only medium priority cases, while treatment interviewers were eligible to see high, medium, and low priority cases. Results compare average number of contact attempts, contact rates, and completion rates daily across low, medium, and high cases as well as between treatment and control FRs. ","The Current Population Survey (CPS) is a household survey sponsored by the Bureau of Labor Statistics and collected by the United States Census Bureau. CPS has a short panel/longitudinal component that acquires data through both CAPI and CATI modes that samples 72,000 households per month. Households are in the survey a total of eight months, with a rotating panel in which each household is in sample for four months, then out for eight months, then again for another four months. This research explores if the survey can reduce effort and maintain the same level of data quality through case prioritization of CAPI field cases and automated selection for CATI phone cases. Effectively assigning which cases should be prioritized and deprioritized for CAPI field cases and which cases should be selected for CATI for phone cases. For CAPI, we introduce the idea of using R-Indicators in order to deprioritize over-represented low impact cases. For CATI, we introduce a mechanism for automatic case selection that explores if the survey can reduce the number of CATI that are reassigned to the field. ","The goal of systemic roadway safety analyses is to identify roadway characteristics (called \"risk factors\") associated with an increased occurrence of car crashes. However, the statistical methods generally used in these analyses often fail to take into account the spatial correlation between road segments and are thus not appropriate for the data. In addition, traditional spatial methods are unsuitable for the data used in roadway analyses, since these methods estimate the correlation between two points based on straight-line distance rather than a measure such as driving distance, which is more appropriate for roadway data. Data for this project were obtained from the Highway Safety Information System (HSIS) database, which contains information such as traffic volume, roadway characteristics, and number and type of crashes for each segment of state-owned road in several states. In this project, we implement a point pattern Poisson process to model the locations of crashes along an interstate network -specifically five interstate highways in Washington-and make inference for the effect of risk factors on crash location.  ","The Federal Highway Administration's National Household Travel Survey (NHTS) is an excellent source of travel information for large geographic areas in the United States (US) but not for small areas due to sample size limitations. This study, uses a NHTS break-out into six geographical areas and urban/suburban/rural classifications to estimate average weekday household: person trips; vehicle trips; person miles traveled; and vehicle miles traveled, using household and demographic characteristics. These estimates are then transferred to individual census tracts using the household and demographic data for each census tract. While individual census tract estimates may have limited accuracy, they can be beneficial indicators to local governments, and other interested customers, who may not have the budget and/or time for conducting their own surveys. ","Geographic Information Systems (GIS) and geostatistics have been shown to have utility in assessing the predictors of response rates in survey research. When controlling for demographic variables, geography can impact response rates when the same study is conducted across regions. As the burden increases for survey respondents in terms of the tasks they must perform, there is an expected drop-off rate in full study compliance. In GPS-based travel surveys where GPS data loggers are deployed to recruited households, the households are expected to both record their locational data and return the units for full compliance.   ","Using the 2011 linked crash and hospital discharge data, this report intends to provide data and information on cost and medical outcomes of occupants of passenger cars and light trucks who were injured in motor vehicle crashes and hospitalized in Illinois hospitals. Specifically the report contains data and information on number of discharges, average charges, primary injuries, types of crash controlling for demographics (age and gender), discharge status, belt status, collision type, alcohol involvement, vehicle type, road type and several other characteristics. Based on the linked data, a total of 45,963 occupants of passenger cars and pickup trucks involved crashes were identified to be admitted and discharged from Illinois hospitals. Overall the mean charge for traffic injury for inpatient and emergency department (ED) discharges was $7,421. To determine the effects of safety belt use on head injuries and hospital charges, path analysis technique, a form of multiple regression models was used. Specifically, this paper intends to decompose the direct and indirect relationship belt use on head injuries and hospital charges, controlling for demographics (age and gender), occu ","In this paper we ask whether interviewer characteristics influence the answers to a standard set of financial literacy questions. Interviewers are particularly relevant as a potential source of response bias in the case of financial literacy because, unusually, they know the answers to the questions. We use data from Germany's wealth survey \"The Panel on Household Finance\" (\"PHF\") to investigate this issue. We find interviewer fixed effects explain a substantial fraction of the variance of the financial literacy score. Interviewer effects are related to interviewer's age, gender and education. We show how either interviewer identifiers or an estimate of the intra-interviewer correlation in financial literacy can be used to improve estimates of the effects of financial literacy.  ","Currently, most surveys ask for occupation with open-ended questions. The verbatim responses are coded afterwards, which is error-prone and expensive. When textual answers have a low level of detail, exact coding may be impossible. We describe an alternative approach that was tested in a telephone survey: A supervised learning algorithm searches for candidate job categories at the time of the interview. Those job categories that have the highest probability to be correct are then presented to the interviewer who asks in turn the respondent to select the most adequate job title. Respondents can also choose that no suggested job category is adequate. 72.4% of the respondents did select an occupation during the interview making additional manual coding superfluous. The quality of interview-coded occupations is compared with two independent codings from professional coders. It is comparable to one of them but slightly worse than the second. While these results are already highly promising, a number of factors have been identified how the process can even be improved. "," This paper analyzes medical event estimates in the Medical Expenditure Panel Survey (MEPS) in an effort to understand event reporting behaviors. MEPS is a nationally representative panel survey studying health care use, access, expenditures, source of payment, insurance coverage, and quality of care. Each year a new panel begins and each panel has 5 rounds of data collection over 2 \u00bd years that cover a two-year reference period.   To understand medical event reporting behaviors of self vs. proxy reporting, we analyzed trends from 2005 to 2013 in reported utilization for the respondent and for other members of each responding unit (RU) in terms of the other members' relationships to the RU respondent. We also looked at these trends in reported utilization by the number of persons in the RU. The type of events analyzed were ambulatory visits and purchased prescriptions. During the period 2005 to 2013, our data suggests declines in the percent of persons with, or, average number of events per person for certain relationships to the respondent in certain sized RUs for ambulatory visits and/or purchased prescriptions. ","Previous studies of misreporting to filter questions have shown that asking filter questions in different formats results in different levels of misreporting. While respondents in a grouped format are asked follow-up questions only after multiple filters have been administered, respondents in an interleafed format are asked follow-up questions immediately after each filter. The common interpretation of this finding is that respondents surveyed in the interleafed format learn to say \"no\" to the filter questions in order to shorten the interview. However, so far, no study has examined whether misreporting to filter questions worsens over time as respondents may remember the structure of the interview when being surveyed for the second time. We conducted an experiment using filter questions in two consecutive waves of a monthly online panel, which allows us to study how misreporting to filter questions changes over time. While we replicate previous findings on the format effect, we find no support for the hypothesis that respondents reporting behavior gets worse over time. Our findings add to the literature on web panels, panel conditioning and motivated misreporting. ","Administrative register data are increasingly important in statistics, but, like other types of data, may contain measurement errors. To prevent such errors from invalidating analyses of scientific interest, it is therefore essential to estimate the extent of measurement errors in administrative data. Currently, however, most approaches to evaluate such errors involve either prohibitively expensive audits or comparison with a survey that is assumed perfect.  ","Stratified sampling study designs are commonly implemented for inferences regarding rare subpopulations that might be missed under simple random sampling designs. However, for analyses, the sampling scheme must be acknowledged appropriately to ensure valid and efficient inferences of population parameters. In many circumstances, stratum definitions are effectively measured without error and so generalization from the sample to the population using survey-weighting techniques is common. However, when the sampling frame and stratum definitions are based on electronic health records (EHRs) that are measured with error, stratum definitions and therefore the target population itself is unclear. The eMERGE CERC Survey Study seeks to understand patient characteristics associated with agreeing to participate in biobank research studies; however, sampling from this population was based on strata that were defined, in part, on EHR data. We propose analytical approaches to acknowledge errors in stratum definitions, and because the population itself is unknown, we propose sensitivity analyses to examine the extent to which the stratum definition errors impact inferences of population targets. ","Total survey error (TSE) is the difference between the estimate of an outcome derived from survey data and the outcome's true population value and reflects the total effect of sampling and nonsampling error. Using data from the National Immunization Survey (NIS) on influenza vaccination among children 6 months -17 years, this analysis considers three components of nonsampling error, or bias, due to: undercoverage of households by the sampling frame; household nonresponse; and recall error in parental reports of children's vaccination status. We estimate the distribution parameters of each source of bias and employ simulation techniques to combine nonsampling with sampling error and estimate TSE, which we define as a random variable with a statistical distribution. We approximate the distribution of total bias in estimates of influenza vaccination and quantify the extent to which each source of survey error contributes to total bias. Findings can be used to guide programmatic interpretation of NIS survey estimates and assess the sensitivity of models of the number of influenza illnesses in children prevented by vaccination due to errors in estimates of vaccination coverage. ","Complex statistical models that have multiple sources of dependencies and variability in the observations are of primary importance in studying data from multiple disciplines. These include spatial, temporal, spatio-temporal, various mixed effects and other statistical models. Of special importance among such models are those that are useful for studying problems where there is limited directly observed data, for example, as in small area models.  ","We consider estimation of mean squared prediction error (MSPE) in small area estimation (SAE) when a procedure of model selection is involved prior to the estimation. We discuss the difficulty of achieving both second-order unbiasedness and positivity at the same time, the so-called double-goal, in MSPE estimation, and propose a simple alternative by estimating the logarithm of the MSPE. A unified Monte-Carlo jackknife method, called McJack, is proposed for estimating the log-MSPE. We prove the second-order unbiasedness of McJack, and demonstrate the performance of McJack in assessing uncertainty in SAE after model selection through empirical investigations that include simulation studies and real-data analyses.  ","In this presentation I develop a method of bias correction, which models the error of the target estimator as a function of the corresponding bootstrap estimator, and the original estimators and bootstrap estimators of the parameters governing the model fitted to the sample data. This is achieved by considering a large number of plausible parameter values, generating a pseudo original sample for each parameter and bootstrap samples for each pseudo sample, and then searching for an appropriate functional relationship. The bias corrections are shown to be of the right order. Under certain conditions, the same procedure also permits estimation of the mean square error of the bias corrected estimator. The method is applied for estimating the prediction mean square error in small area estimation of proportions under a generalized mixed model. Empirical comparisons with the jackknife and the double bootstrap methods are presented. ","In this paper, we propose a new weighted, squared distance while minimizing the distance between the true proportions and the observed proportions of (Yes, Yes), (Yes, No), (No, Yes), and (No, No) answers in the setup of Odumade and Singh (2009) model.   ","Randomized response models have been considered the best method for sensitive surveys which provide the maximum privacy protection to the respondents and procure honest responses. In this paper, we concentrate on the issue of estimation of proportions of the population possessing sensitive characteristics. However, there is amazing deficiency of articles that have addressed the simultaneous estimation of three dependent sensitive attributes. In filling this hole, we present three new models to estimate the proportions of three dependent sensitive characteristics simultaneously. Following Lee, Sedory and Singh (2013), one simple model can be used to develop proportion estimators. This simple model could consist of three decks of cards used in series, and in using these procedures, we can derive the expressions for estimators of some of the marginal and conditional proportions and their variances. These estimators will allow us estimate all the conditional and marginal proportions, relative risks, correlation coefficients, multiple correlation coefficient and partial correlation coefficients. The estimators are developed and investigated from the efficiency perspective.  ","In this paper, we propose a new unrelated question model for estimating the proportion of a sensitive attribute in a population on the same lines of Odumade and Singh (2009) model of efficient use of two decks while collecting in information from respondents in a sample. The proposed estimator is shown to be more efficient and protective than the Warner (1965), Odumade and Singh (2009), Singh and Sedory (2011, 2012) and Greenberg et al. (1969) unrelated question models. ","In this paper, we first define odds ratio and attributable risk while considering investigating two sensitive attributes in real practice. Then we define two estimators of odds ratio and two estimators of attributable risk based data collected either using the simple model or crossed model proposed by Lee, Sedory and Singh (2013). We derive expressions for biases and variances of the resultant estimators. We investigate the performance of crossed model over the simple model under the same choice of parameters as discussed in Lee et al. (2013). Also the values of the odds ratio and attributable risk are reported based on a real data set. ","The Family Life, Activity, Sun, Health and Eating (FLASHE) study is a National Cancer Institute sponsored survey that examines psychosocial, generational (parent-teen), and environmental correlates of cancer-preventive behaviors. Data was collected in two web surveys (diet focused and physical activity focused) in 2014 from dyads of caregivers and their adolescent children aged 12-17. After data collection was completed, eight variables from the parent physical activity survey were found missing for approximately half of the sample because of a system programming error in the web-based data collection tool. To avoid potential bias and loss in efficiency in estimation and inference involving those variables, we considered and compared multiple imputation methods including the Sequential Regression Multivariate Imputation Algorithm and the Weighted Sequential Hotdeck to accurately impute the missing values for those variables. This paper describes the approaches used in this imputation and evaluates the methods by comparing the distributions of the original and the imputed data. A simulation study using the observed data is also conducted as part of the model diagnostics.  ","Most imputation techniques are designed for ignorable missing data mechanisms since nonignorability is an assumption more challenging to handle. Under nonignorable missingness (NM), one assumes the nonresponse mechanism depends on unobserved values, and the outcome model for the variables with missing values and the nonresponse model must be modeled jointly. Consequently, joint modeling can produce results that are sensitive to misspecification of the outcome and nonresponse models. We propose a nonparametric method for handling NM via bootstrap imputation and multiple imputation (MI). The key idea underlying our proposed approach is to formulate two working models for the outcome and for nonresponse. Using the two working models, we derive predictive scores which achieves dimension reduction and use the resulting scores coupled with a nearest neighbor hot deck to multiply impute missing values. Our approach allows users to incorporate prior knowledge on the working models through the use of weights. Compared with the existing MI methods, our approach is more robust to misspecification of the two models and allows for a natural sensitivity analysis. ","The statistical analysis of social networks is increasingly used to understand social processes and patterns. The association between social relationships and individual behaviors is of particular interest to sociologists, psychologists, and public health researchers. Several recent network studies make use of the fixed choice design (FCD), which induces missing edges in the network data. Due to the complex dependence structure inherent in networks, missing data can pose very difficult problems for valid statistical inference. In this paper we introduce novel methods for accounting for the FCD censoring and introduce a new survey design, which we call the Augmented Fixed Choice Design (AFCD). The AFCD adds considerable information to analyses without unduly burdening the survey respondent, resulting in improvements over the FCD. We demonstrate this new method through simulation studies and an analysis of a network of undergraduates living in a residence hall. ","As nonresponse continues to increase, weighting adjustments increasingly rely on implicit (or explicit) mathematical models for explaining nonresponse. Although it is crucial to understand this relationship and its impact on survey estimates, the literature that describes the different weighting adjustments is dispersed and sometimes contradictory. We propose a more unified way for understanding these relationships. We present an expression for nonresponse bias for estimates computed using nonresponse adjusted weights. We explain why this expression may be preferred for examining survey statistics under the Total Survey Error framework. We argue that weighting for nonresponse should be seen as an estimation task and once the statistical models have been identified, classical statistical tools such as goodness of fit and model diagnostics can be used to evaluate the quality of nonresponse adjusted weights. This approach enables us to evaluate the effect of model misspecification from incorrect functional forms or from omitted variables in the nonresponse models. ","Unit-nonresponse is usually compensated for by weighting adjustment, and calibration techniques are frequently used for benchmarking purposes and for improving survey estimates. Traditionally, nonresponse adjustment and calibration are performed in two separate steps, and this weighting procedure is called the two-step procedure. Lately, there has been considerable discussion on whether the calibration step can take care of the nonresponse adjustment as well in a single step (called the one-step procedure). The answer to this question is not clear cut, and in this paper we want to address this issue for various situations arising in practice in terms of relationship between auxiliary information and survey variables and response mechanisms. This is an empirical investigation built upon the work by Haziza and Lesage (2015). ","Survey nonresponse can be accounted for during weighting by using methods such as response propensity adjustments and calibration adjustments. These methods typically rely on the availability of adjustment variables related to an individual's response propensity and to key survey variables. However, complicated patterns of nonresponse can prevent the ability to control for all levels of relevant variables simultaneously, forcing compromises, such as using a simpler response propensity model or coarsening the level of data used in forming adjustment cells. These compromises may lessen the effectiveness of adjustments at reducing nonresponse bias. Further, sample size limitations could lead to increased design effects, given that small adjustment cells could lead to increased weight variation.  ","The Bureau of Labor Statistics' Quarterly Census of Employment and Wages (QCEW) is a federal/state cooperative program that publishes a count of monthly employment and total quarterly wages. The survey is reported quarterly by employers covering 98 percent of U.S. jobs. Estimates are available at the county, MSA, state and national levels by industry. The Multiple Worksite Report (MWR) asks most multi-location employers to provide employment and wage data for all of their establishments covered under one Unemployment Insurance (UI) account in a State. They are encouraged to report MWR data online through the MWR Web system, however those that do not report online are sent a paper form twice per quarter (an initial request, and a follow-up). There are two subsets of employers being targeted in the new approach: long-term non-respondents, and those that do not fill out the form but instead send in a print-out listing. In lieu of an initial mailing form, a one-page letter was sent out with web credentials to encourage these respondents to report on the web. This paper summarizes the procedures and response results associated with the Web Only Letter. ","Accurate state-level surveillance of diabetes and prediabetes is paramount, but most states do not have one definitive data source for accurate prevalence estimation, especially for undiagnosed cases. We present a two-stage approach for combining estimates from various sources, including nationally representative surveys, state representative surveys, non-representative surveys, and administrative and clinical archives. Challenges posed by these data sources include non-representativeness, non-overlapping frames, and missingness not at random. First we use techniques including raking and propensity score weighting to reduce the bias of each data source. Then we create a composite estimate, where source estimates are weighted inversely proportional to their mean squared error. The variance of our final estimate includes sampling errors and the estimated unknown biases, ensuring our combined estimate is not overwhelmed by large, unrepresentative data sources. Using California as a case study, our estimate of self-reported diabetes prevalence is 7.6%, compared to the BRFSS California estimate of 10.2%. We find 3.6% undiagnosed diabetes, resulting in a total estimate of 11.2%. ","The Occupational Requirements Survey (ORS) is an establishment survey conducted by the Bureau of Labor Statistics (BLS) for the Social Security Administration (SSA). The survey collects information on the vocational preparation and the cognitive and physical requirements of occupations in the U.S. economy, as well as the environmental conditions in which those occupations are performed. In fiscal year (FY) 2015, the BLS completed data collection for the ORS pre-production test. This paper focuses on the process for computing and analyzing the response rate from the ORS pre-production test, utilizing Office of Management and Budget (OMB) approved methods and formulas to produce detail statistics -weighted and unweighted at the establishment, occupation, and item levels. The results from this process will be used to help identify important auxiliary variables for use in estimation process to reduce potential bias due to nonresponse. ","Regulations state that any U.S. government survey with an overall unit response rate of less than 80 percent shall conduct a nonresponse bias analysis. The Quarterly Financial Report (QFR) falls below 80 percent unit response; therefore, QFR staff performed the first nonresponse bias analysis for this survey since 2006. First, regression analyses are performed upon response rate measures to show which industries have abnormally low response and should be targeted for follow-up. Then, sampling frame data is used to perform equivalence of means tests to determine whether nonrespondent and respondent data differ across various crosstabs, and to calculate relative bias. This analysis is generalizable to many surveys across the U.S. Census Bureau and other agencies, and is comprehensive in that it addresses both sources of nonresponse bias and bias magnitude.  ","Due to cost savings, web-based surveys are sometimes considered as alternatives to the traditional surveys based on probability sampling methods. Web surveys often have multiple stages of selection, with some stages structured by design-based sampling, but with other stages (possibly all) sampled non-randomly. A task for survey planners is to determine a set of web-survey weights for the respondents that lead to unbiased population estimation. One such method assigns web-survey weights based on propensity score estimation computed by combining a traditional \"representative\" survey with a web-survey with the latter taking the role of \"treatment\". For this study, a web-survey having questions in common with those on the National Health Interview Survey (NHIS) was fielded in 2015. Several variations of propensity score methodologies appearing in the literature are implemented. The results of this study are discussed. ","The Current Population Survey (CPS), a government survey that is the nation's source for the unemployment rate, recently conducted a study to determine the effects of sending a pre-notice postcard to sampled cases. The goals of this postcard study were to decrease the cost of the survey by reducing the number of contact attempts needed to complete an interview and increase the overall response rate. Decreasing the contact attempts for the cases that are more likely to respond could allow for more resources to be focused on the harder-to-reach cases. Field interviewers conduct the majority of the interviews, either in person or by phone. The postcards included the name and contact number of the interviewer assigned to that particular case, which gives the respondent an opportunity to call their interviewer and respond or set up an appointment. This study analyzed data from 500 interviewers and 25,000 cases over three interview months. Count regression was used to determine what effect the postcard had on the number of contact attempts per case. Mixed effects logistic models were fit to determine what characteristics, including the postcard, predict a cases' propensity to respond. ","Nonresponse bias can be tested by comparing characteristics of respondents who returned completed surveys and non-respondents who failed to return a completed survey. For years, statisticians have proposed theories and findings in regard to nonresponse as well as the statistical models that link non response to bias in estimates. Drawing inference about the population from a sample with non-response may generate biased results. Nonresponse can lead to bias in the estimates even if nonrespondents are similar to respondents in observable characteristics. Thus, the focus of this study was to empirically measure nonresponse and for sole-proprietor non-employers in the current Survey of Business Owners (SBO). Response rates were first examined at subgroup levels to determine areas for potential nonresponse. In this paper, we use data on nonrespondents obtained by using administrative data from the IRS and the American Community Survey (ACS). Using this data, we test for nonresponse bias and illustrate the effect of bias on aggregate benefit estimation.   ","This continuing education course will provide a detailed overview of the topic, covering all important aspects relevant for the synthetic data approach. Starting with a short introduction to data confidentiality in general and synthetic data in particular, the workshop will discuss the different approaches to generating synthetic datasets in detail. Possible modeling strategies and analytical validity evaluations will be assessed and potential measures to quantify the remaining risk of disclosure will be presented. To provide the participants with hands on experience, all steps will be illustrated using simulated and real data examples in R.The course intends to summarize the state of the art in synthetic data. The main focus will be on practical implementation and not so much on the motivation of the underlying statistical theory. Some background regarding general linear modelling is expected. Familiarity with the concept of Bayesian statistics is helpful but not required. The statistical software R will be used to illustrate the implementation of the approach. Familiarity with basics in R would be useful but is not required.","Most surveys are designed to provide statistics for a possibly (very) large number of characteristics of interest. Typically, the data collected are stored in a rectangular data file, each row corresponding to a sample unit and each column corresponding to a characteristic of interest. Made available on the data file is a weighting system. The idea is to construct a single weighting system applicable to all the characteristics of interest. The typical weighting process involves three major stages. At the first stage, each unit is assigned a base weight, defined as the inverse of its inclusion probability. The base weights are then modified to account for unit nonresponse. At the last stage, the weights adjusted for nonresponse are further modified to ensure consistency between survey estimates and known population totals.  When needed, the weights undergo a last modification through weight trimming or weight smoothing methods in order to improve the efficiency of survey estimates.  The goal of the course is to provide a detailed description of each stage. Participants should have a background in survey sampling and regression analysis. The course is intended to survey statisticians working in survey organizations, graduate students and users of survey data.","This full-day course assumes you want fewer data collection and analysis mistakes in your work, more efficient and productive meetings, and time and sanity back in your life. This course presents the most useful and generally-applicable tools and techniques culled from the instructors' 30+ combined years of experience in statistics, data science, collaborative biomedical research, consulting, public health, social science, and survey research. It emphasizes simple tools and basic habits that will streamline your research process, whether you are involved in data collection, statistical analysis, or other aspects of the research lifecycle. It focuses on tools that cost little beyond the time and effort it takes to learn and practice them. The first half of the course will include general project and time management techniques. The second half of the course will focus on best practices for your data science pipeline to minimize errors, maximize time to think, and maintain reproducibility. All techniques taught have been tested and adapted by the instructors in their project management work. Students will benefit from the instructors' extensive research and statistical experience. Students will leave with a collection of concrete tools and tips that they can implement immediately. ","This presentation will provide an overview of the different methodological challenges of environmental surveys as opposed to surveys of human or establishment populations, for which most of survey expertise has been accumulated. All stages of survey and design and analysis for environmental surveys, from frame construction to measurement error, to nonresponse compensation, will be carefully reviewed. We will discuss the complexities in surveying lead in drinking water, dust, and soils; fish consumption rates used to establish water-quality standards; and hydraulically fractured wells (fracking). ","Researchers and government officials have long been interested in monitoring the environmental conditions and estimating the temporal changes at the national and regional scale. Spatio-temporal sampling design is an important component of large-scale natural resource monitoring surveys, and a good sampling design can reduce the cost while allowing us to estimate the condition and temporal changes with efficiency and minimum bias. The US National Resources Inventory (NRI) is an annual longitudinal survey of soil, water, and related environmental resources designed to assess conditions and trends on non-federal US lands. In this talk we will present some of our recent research results on balanced spatio-temporal sampling, and discuss how they can helped improve some of our current and future sampling designs for NRI-related surveys.     ","Recreational fishing characteristics and catch along the US coasts have traditionally been monitored using combinations of intercept and telephone/mail recall surveys conducted by state and federal agencies.  Because of increasing costs and declining response rates, efforts have been initiated to investigate ways to collect data via alternative channels, including logbooks.  Even when logbook reporting is mandatory, however, there are concerns about underreporting of trips and/or misreporting of catch, so that it is of interest to combine the logbook data with validation surveys.  In this presentation, we discuss a number of approaches to create combined estimators for the charter fishery in South Carolina.  These approaches rely on matching of individual logbook and survey trips, followed by different ways to account for discrepancies between the logbook and survey data in estimation and inference.  We describe and compare the methods developed for this application, and discuss their usefulness in other contexts that require combination of data from survey and non-survey sources. ","Non-probability (or design-free) surveys are becoming more prevalent because they offer both increased speed in obtaining data on emerging issues (e.g., online panel surveys) and decreased costs compared to many probability-based surveys. Evaluation studies to date have shown that many population estimates from non-probability surveys miss the mark (i.e., lack external validity) and suffer from errors associated with coverage, selection, or model misspecification. Few of these studies investigate if these estimates better serve subdomains within the population instead the population as a whole.  ","The use of non-probability samples in survey research has increased in recent years, but the validity of estimates varies, and it is not clear under which circumstances weighted non-probability samples yield valid inferences of population-level estimates. This study assesses the validity of data from non-probability panels in the context of health-related outcomes with items from the Heath Information National Trends Survey (HINTS), a national probability-based mail survey. We review differences in population-level estimates between HINTS and online surveys administered by four vendors, using a range of sampling methods including probability, quota-based, and convenience. First, we compare weighted estimates from the different methodologies to eight national benchmark estimates, and examine whether differences from benchmarks are associated with demographic characteristics. Second, we reproduce associations from the health literature and determine whether data from online and non-probability samples yield similar inferences. These analyses will help clarify conditions under which online panel data can be used to make valid inferences about health-related knowledge and behaviors.  ","This paper evaluates the external validity of estimates from a new non-probability sampling method with calibrated survey weights. Individual callers that misdial into a bank otherwise non-active toll-free numbers are given a survey using an Interactive Voice Response system. These callers comprise a nonprobability sample of the United States population. The questionnaire, designed to evaluate bias, mimics questions from the American Community Survey (ACS) and the National Household Interview Survey (NHIS). Sample weights are created by calibrating to the demographic distributions estimated from the ACS. The distributions of the respondents' unweighted demographic characteristics are surprisingly close to the population, a result that is consistent with other non-probability samples fielded using this methodology. Estimates of public health outcomes from the non-probability survey are compared to the ACS and the NHIS. We calculate bias in the survey estimates assuming the ACS and the NHIS estimates reflect the true population values. We conclude that the low cost and speed coupled with the minimal bias of this data collection technique may make this technique a viable option in som ","Increasingly, surveys have to rely on more than one sample source to improve coverage and/or secure the needed sample size in a defensible and cost-effective manner.  As such, two or more independent samples are selected from separate sampling frames with varying representations of the target population of interest.  In many instances this involves combining probability and nonprobability samples, particularly when nonprobability samples from online panels are used to augment a main probability-based sample.  This paper provides an overview of weighting refinements that can improve the external validity of data from such surveys, including a method for optimal integration of surveys. ","In this panel, we plan to discuss the following:  ","Both the 2000 and 2010 U.S. Censuses included a social marketing communications campaign that aided in maintaining the mail response rate in an environment when response to surveys was declining. As the 2020 U.S. Census approaches, the preparations include tests of new methodologies for enumeration that have the potential to reduce cost and improve quality. In parallel, the research includes formulating methods for the 2020 Census communications campaign that will aid the effectiveness of the enumeration operations. The 2015 Census Test in Savannah, GA included tests of Internet and mail response modes and of online delivery of social marketing communications focused on persuading the public to respond by Internet and mail. Merging data from the 2015 Census Test with the third-party lifestyle segments and the Low Response Score developed at the U.S. Census Bureau produces a dataset suitable for studying correlations between census response and lifestyle segments. This paper uses the merged dataset to examine whether lifestyle segments can provide insight to hard-to-survey populations, their response behavior, and interactions with social marketing communications. ","Record linkage has become an important tool for increasing research opportunities in the social sciences and is likely to become even more important in the \"big data\" era. Surveys that perform record linkage are often required to obtain informed consent from respondents prior to linkage. A major concern is that record non-consent can introduce biases. One strategy to solve this missing data problem is statistical matching. The missing administrative data of a non-consenter is estimated using the data of a statistically similar individual from the administrative dataset. To evaluate statistical matching in that regard, we use data from two major German panel surveys - the National Educational Panel Study and the Panel 'Labour Market and Social Security'. Both were linked to process data of the German Federal Employment Agency on almost the complete German population of employable age. Our results show that non-consent biases in marginal distributions can be reduced. Biases in correlations and coefficients of regression models, using variables from both survey and administrative data can sometimes even be worse after supplementing biased record linked data with statistical matches. ","Probabilistic Record Linkage has made it possible to combine multiple data sets from different when a unique data set with all necessary information is not available or creating a new set is time consuming and extremely costly. Linkage errors are inevitable in the linked data sets because of the unavailability of error-free unique identifiers and because of possible errors in measuring or recording. Small linkage errors can lead to substantial bias and increased variability when estimating the relationship among the variables. Recent research has focused more on bias correction for linear regression analysis and assumes the linkage process is complete, i.e. all records on the two data sets are linked.  In this paper, we extend these ideas to allow generalized linear regression analysis in the presence of linkage errors and incomplete linkage. We consider a special case where survey data and administrative data are linked. A generalized linear model of linked data with linkage errors is proposed. Simulation results will be presented to evaluate the performance of the proposed estimators, which account for linkage errors.  ","Abstract  ","The literature has no standard method for estimating the coverage of address-based frames, especially for subnational areas such as segments in area probability designs. Versatility is desirable for different study needs, but standardization improves comparability and informs frame supplementation decisions. Many estimates are simple ratios of counts of addresses in the frame to control totals; others are model-based predictions. Limitations to these methods include geocoding error, outdated control totals, errors in the address frame, model lack of fit, and systematic exclusion of types of addresses. We tested various estimates on segments selected for the 2015 Residential Energy Consumption Survey. \"Truth\" was based on field enumeration or a field-based frame supplementation procedure. We matched field and frame addresses to determine which housing units were covered. We investigated relative costs and presumed accuracy of control totals. Here we share the results of our investigation and provide a public resource with net coverage estimates for counties. ","The AmeriSpeak\u00ae Panel is a multi-mode address-based (ABS) panel designed to support NORC's mission to deliver reliable data to guide critical programmatic, business, and policy decisions. AmeriSpeak uses the continuously-updated NORC National Sampling Frame to create a nationally representative sample of all Americans, with specific age and race/ethnic oversamples.  The ABS design allows for the enhancement of addresses from an extract of the United States Postal Service Computerized Delivery Sequence file (USPS CDSF) with lists designed to flag households as being members of specific age, race/ethnicity, or other targeted groups.   AmeriSpeak has employed such \"vendor-provided\" lists to increase the sample size for specific demographics while not undermining the probability basis of the design. Our paper examines the utility of such lists in this context as well as any tradeoffs between efficiency, expressed as a \"hit rate\", and the coverage of the target population.  This research is relevant to survey practitioners interested in improving design efficiency for particular domains.   ","One factor that is often overlooked in dynamic adaptive designs is whether a certain data collection strategy is \"worth it\", with respect to reaching data collection goals. Ideally, we would not send that expensive field interviewer to attempt a case that will not respond to either a telephone attempt or a personal visit, as resources could be better spent on other similar cases.  Alternatively, perhaps a small incentive would convince a sample individual to respond. To use this type of information, we need to look at the response propensity of a case (and how it is affected by different data collection features) in addition to its value to the data collection goal. This is difficult, as we do not know in advance the effect of different combinations of features on survey response - we cannot tell the future.  As a result we rely on models, but early in data collection, when a decision could have a large impact at reducing cost and burden, response propensities are often unreliable, shown by Wagner. This talk will illustrate how poor estimates of response propensity can affect data collection decisions, the potential benefits of external information, and how to integrate that data. ","Longitudinal studies benefit from prior information to inform data collection strategies. The presentation describes two models used together during data collection of a US/ED National Center for Education Statistics study to identify cases for interventions. The presentation describes a response likelihood model used to identify, in advance of data collection, likelihood of cases to participate. Using prior data/paradata, we fit a model predicting prior-round response. We used coefficients associated with predictors to estimate response likelihood. The response likelihood model informed decisions about inclusion/exclusion of cases for interventions to control costs. The presentation describes a bias likelihood model used to select cases for interventions. The bias likelihood model was used to identify cases most unlike cases that had already responded at the time the model was run. The model used key survey and frame variables as predictors to identify nonrespondents most likely to cause bias in key survey variables if they did not respond. The model was run multiple times during data collection to identify cases for various interventions (e.g., incentives; field data collection). ","Response propensity (RP) models are widely used in survey research to analyse response processes. One application is to predict sample members who are likely to be nonrespondents. The potential nonrespondents can then be targeted using responsive and adaptive strategies with aim of increasing response rates and reducing survey costs. Generally, however, RP models exhibit low predictive power, which limit their effective application in survey research to improve data collection processes. This paper explores whether the use of a Bayesian framework can improve the predictions of response propensity models. In a Bayesian approach, existing knowledge regarding model parameters is used to specify prior distributions. In this paper we apply the method in a longitudinal context and analyse data from the UK household longitudinal study (Understanding Society). We use estimates from RP models fitted to response outcomes from earlier waves as our source for specifying prior distributions. Our findings indicate that conditioning on previous wave data can lead to small improvements in the response propensity models' predictive power and discriminative ability. ","A Bayesian analysis of survey data collection may be profitable when expert knowledge and/or historic survey data from the same or similar surveys are available. This knowledge and data may then be employed to set informative prior distributions to coefficients in regression models for survey design parameters, e.g. contact propensities, eligibility propensities, participation propensities, costs per sample unit and survey variable outcomes. During or after data collection posterior distributions may be derived for the same parameters, but also for overall quality and cost measures. Even when survey design parameters change gradually in time or change from one survey to the other, the posterior distributions during or after data collection may be more informative than without the prior knowledge.  In earlier papers, we demonstrated how a Bayesian analysis may be implemented and analyzed in monitoring survey data collection. In the current paper, we discuss the optimization and adaptation of survey design using the posterior distributions for survey design parameters, and quality and cost measures. We do so using two case studies.  ","Rankings of cases based on estimated response propensities have been used to create inputs to adaptive survey designs. These inputs may be needed during data collection as triggers for design decisions. Cases above or below a certain threshold may receive a special recruitment protocol. However, Wagner and Hubbard (2014) showed that estimates of response propensity models can be biased when fit on a daily basis during data collection using the incoming data. These biases may lead to inaccurate ranking of cases, which, in turn, leads to inefficient or even counterproductive interventions. The use of informative priors in Bayesian logistic regression is explored. The goal is to identify a method for developing priors from other surveys and expert opinion that reduces or eliminates any potential biases in the rankings of cases. ","It has been demonstrated that it is possible to reduce unproductive calls in CATI-surveys by using information from previous waves in longitudinal surveys. In the Swedish Labour Force Survey have data from both registers and paradata been used to build nonresponse models. The models are used to formulate strategies to support the survey agency in the data collection work with the ambition to reduce costs but at the same time maintain the survey quality. In this paper we will investigate if a Bayesian analysis could be an option in monitoring the data collection. Different strategies based on the previous nonresponse models are tested. ","Auxiliary information can increase the efficiency of survey estimators when the estimator accurately captures the relationship between the variable of interest and the auxiliary variables. Under a model-assisted framework, we present a regression tree estimator for a finite population mean.  Regression trees can capture important interactive effects missed by linear regression and do not suffer from multicollinearity issues when the auxiliary variables are highly collinear. We establish consistency of the model-assisted regression tree estimator and compare its performance to other survey estimators using the US Bureau of Labor Statistics Job Openings and Labor Turnover Survey.  ","The Internal Revenue Service uses the Current Population Survey (CPS) to estimate the number of individual income tax returns that are required to be filed. The filing requirement depends on filing status, age, and income levels; in 2015, for example, a single person under age 65 was required to file a tax return if his or her gross income was above $10,300. Many CPS respondents, however, report values for wage and other types of income that are rounded. A respondent reporting $10,000 in wage income may have rounded his or her data and may in reality have had wage income above the filing threshold. We compare three methods for addressing respondent rounding: parametric density estimation with imputation, kernel smoothing, and weight redistribution. We perform a simulation study to investigate the methods under a variety of \"true\" distributions and also apply the methods to public use data from the CPS. ","Recursive partition algorithms used to build regression tree models require a method for selecting splits based on the values of a set of potential variables. Tests for equality of model parameters between two groups can be used for estimating p-values for potential splits. Permutation tests are distribution free way to conduct these hypothesis tests.  ","Single imputation is widely used in surveys for treating item nonresponse. Commonly encountered imputation procedures used in practice include parametric procedures such as deterministic and random regression imputation, ratio imputation and random hot-deck imputation. In this presentation we study the properties of imputed estimators obtained under B-spline imputation procedures and derive consistent variance estimators. The results of a simulation study comparing the performance of several imputation procedures in terms of bias and efficiency will be presented. ","In the context of semi-parametric regression with multiple covariates, it is known that the solution to the penalized least squares minimization problem can be interpreted as the mean of a Gaussian process arising from the posterior distribution of an empirical Bayesian approach. Using the Representer Theorem, we propose a Bayesian regression model with normal distributed errors at the response level and prove that conditionally to the variance, it defines the same Gaussian Process. A Gaussian process which approximates the solution to the penalized least squares minimization problem using its mean function is described. We study using simulations, the performance of the means of the posterior predictive as point estimates for the regression function and the empirical coverage of the pointwise credible intervals from the the posterior predictive distributions of the approximated Gaussian Process. ","The Internet Corporation for Assigned Names and Numbers (ICANN) coordinates web address identifiers worldwide. Any individual, business, or organization who registers a domain name must provide names, addresses, emails, and phone numbers for the registration service called WHOIS. This data is managed by \"registries\", which are under contract with ICANN to operate top level domains like .COM, .ORG, or new ones now in operation (e.g., .CONSULT), as well as \"registrars\" (e.g., GoDaddy).  Anyone can use WHOIS to search and identify the registered name holder of a domain name. The WHOIS Accuracy Reporting System (ARS) is a formal examination of the accuracy of contact information provided to registries and registrars. This project examines syntactical (e.g., does the email contain an \"@\" symbol; does the phone number have the correct number of digits; does the mailing address have the required fields?) and operational (e.g., does the email bounce; does the phone number connect; is the postal address deliverable?) accuracy.  This presentation will provide statistics on Internet domains worldwide as well as the accuracy of domain contact information based on four studies over two years.  ","The National Agricultural Statistics Service (NASS) of the United States Department of Agriculture (USDA) produces hundreds of publications annually. The research conducted at NASS is based on survey data, which is compiled in the NASS list frame. Therefore, it is imperative that the NASS list frame is complete and up-to-date in order to produce valid and accurate estimates for agriculture. For this reason, NASS is constantly updating the list frame by adding new farms. Conversely, farms also go out-of-business, and these farms need to be removed from the list frame for it to stay current. In this paper, we examine the efficacy of boosted trees to identify out-of-business records prior to data collection. We found that boosted regression trees outperformed logistic regression and random forests. Boosted regression trees were shown to have the lowest misclassification rate and highest R2 . ","The Medicare Current Beneficiary Survey (MCBS) is a continuous, multipurpose survey of a nationally representative sample of the Medicare population, conducted by the Centers for Medicare &amp; Medicaid Services through a contract with NORC at the University of Chicago. The MCBS evaluated the potential benefit of oversampling low-income beneficiaries to allow for improved estimation of parameters of health disparities experienced by this population. This paper demonstrates the methods used to conduct the evaluation, which included examining alternative approaches for oversampling low-income beneficiaries, such as using U.S. census data on income at the tract level, data from consumer databases on income at the household level, and geographic variation indicators of low-income status at the beneficiary level, and examining the effects of oversampling low-income beneficiaries on the standard errors of key statistics. Findings from the evaluation revealed that there were already sufficient numbers of low income beneficiaries in the MCBS for analytic purposes. ","By representing a data table and the additive relationships between the table cells as a network graph, Causey, Cox, and Ernst (1985) discuss how the solution to a number of sample design problems are equivalent to solving a problem that is referred to in the operations research literature as a transshipment problem. This paper briefly reviews the Causey et al. paper, discusses the usefulness of the network-graph representation, and illustrates the solution of associated transshipment problems for two examples, focusing on advantages and disadvantages of different software solutions. The first example is a randomization problem in which elements in multiple sub-populations are randomly assigned to equal-size treatment and control groups, subject to controlled-selection constraints with respect to a two-way stratification of each sub-population. The second example allocates stratum-level fielded sample sizes for a second wave of data collection such that in overlapping estimation domains the expected numbers of completed cases are equal to specified targets. The software solutions discussed include Excel Solver, OpenSolver, SAS's PROC NETFLOW, and SAS's PROC OPTMODEL. ","In India, bonded labour is the most widely used method of enslaving people. This is the first survey-based study that describes the extent and characteristics of bonded labour in a whole state, Tamil Nadu. A stratified sample of 1,936 labourers in panchayat villages and 189 in town panchayats was drawn form 31 districts (excluding Chennai). Between September and November, 2014, 66 local enumerators interviewed over 9,000 labourers at or near their worksites in 11 different types of industries. Enumerators recorded the industry type and physical location of the site. Respondents estimated worksite size, demographic information, and information on working conditions (e. g. freedom of movement, freedom of employment, wage amount, and any receipt of an advance). Population estimates were obtained using SAS procedures for survey analysis. Results showed that 29.9% (N=463,000) of manual labourers working in the 11 industries surveyed are bonded. The level of bondage varied by type of industry, district, labourer demographics, and other factors.  ","To collect information on Zika virus awareness and prevention methods in Puerto Rico, a sample of recent births was designed to provide island-wide and regionally representative estimates.  Within each region, a stratified, cluster sample of women recently delivering a live birth was drawn from birth logs in hospitals with 100 or more births in 2015.  Within each region-hospital stratum, days were selected using random sampling, and all women who gave birth on the selected days were included in the sample.  Sampled women were approached 24-36 hours following their delivery during their hospital stay and offered the option to complete a self-administered survey on tablet or paper.  Data were weighted to account for sampling design, non-response, and non-coverage.  Of the 3237 eligible births during the 13-week study period, 2926 were identified from hospital birth logs.  Of those identified, 2361 responded (73% response rate).  Among women who responded, 74% used the tablet and 26% used the paper form.  The study design and data collection methodology allowed for rapid deployment in the field and rapid analysis of survey data, which are essential in emergency response situations. ","The USDA's National Agricultural Statistics Service (NASS) is exploring sampling approaches that allow for coordination of multiple samples drawn within a year across the population in an effort to control the respondent burden.  Most of these sampling techniques, which are both design-based and model-based approaches, utilize permanent random numbers (PRN) for the purpose of achieving the amount of desired overlap within a survey or between different surveys and, therefore, help reduce the respondent burden.  However, little discussion comparing design-based and model-based approaches or examining a combination of these two approaches appears in the literature. Using a simulation study, we investigate different sampling strategies (a combination of sampling design and estimator) that utilize both design-based and model-based inferences.  Our simulations are based on data from several USDA surveys.  Results are presented. ","At the opening workshop of the SAMSI ASTRO program in August 2016 statisticians and astronomers formed five working groups: (i) Uncertainty Quantification and Astrophysical Emulation, (ii) Synoptic Time Domain Surveys, (iii) Multivariate and Irregularly Sampled Time Series, (iv) Astrophysical Populations, (v) Statistics, computation, and modeling in cosmology. An overview of the directions and progress of each working group will be given.  ","The objective of this work is to use the quasar Lyman-alpha forest data of twelfth data release of the Baryon Oscillation Spectroscopic Survey to produce a three-dimensional map of the H I density fluctuations in the intergalactic medium (IGM) in the redshift range 1.94 &lt;   z &lt;   3 with a total sky coverage of 12,300 deg^2. Specifically, we utilize a multi-resolution Gaussian random field model. With this methodology we are able to reproduce the complex multi-scale structure of the IGM while still maintaining the computational parsimony necessary for a problem of this magnitude. Furthermore, given the irregular spacing of the observations dictated by line of sight (LOS) sampling and the locations of quasars, a key feature of this methodology is that no assumptions are made about the sampling scheme. Moreover, the resolution of the predicted map inherently adapts to the number of locally available LOSs and the measurement variance of the observations so as to extract as much signal from the data as possible. The map can be viewed on the WorldWide Telescope and downloaded from my personal webpage. ","In a series of three papers in the Astrophysical Journal, Eadie et al (2015), Eadie &amp; Harris (2016) and Eadie et al (2017) developed a hierarchical Bayesian statistical framework for estimating the mass of the Milky Way Galaxy. The method confronts a physical model for the Galaxy with position and velocity data of objects such as globular clusters which orbit the Milky Way. The hierarchical Bayesian analysis of such data, which suffers from incompleteness and varying degrees of uncertainty, has led to a more constrained mass estimate for the Milky Way. However, the physical model used in this method is a simplification of the true structural complexity of the Milky Way (which includes a disk, bulge, and dark matter halo). To investigate the performance of the hierarchical Bayesian method with this underlying simple physical model, we apply the framework to mock observations of simulated Milky Way-type galaxies that were created using hydrodynamical and cosmological simulations. These simulated galaxies are state-of-the-art and represent our best current understanding of the Milky Way's structure and dynamical interactions. ","The question of how best to select among models has bedeviled statisticians, particularly Bayesian statisticians, for decades. The difficulties with interpreting p-values are well known among Bayesians and non-Bayesians alike. Unfortunately, the strong dependence on the choice of prior distribution of the most prominent fully Bayesian alternative, the Bayes Factor, has limited its popularity in practice. In this poster, we explore a class of non-standard model comparison problems that are important in astrophysics and high-energy physics. The search for the Higgs boson, for example, involved quantifying evidence for a narrow component added to a diffuse background distribution. The added component corresponds to the Higgs mass distribution, accounting for instrumental effects, and cannot be negative. Thus, not only is the null distribution on the boundary of the parameter space, but the location of the added component is unidentifiable under the null. We discuss how this can be formulated as a multiple testing problem and compare the resulting p-value with Bayes Factors. In this case, the prior dependence of the Bayes Factor results in a natural correction for the multiple testing. ","We have developed a novel statistical method to address a fundamental scientific goal: disaggregation, or estimation of the composition of an unknown aggregate target. By combining forward (computer) models of the target of interest with measured data, our approach enables computer-model calibration techniques to directly solve the disaggregation problem. We develop our method in the context of chemical spectra generated by laser-induced breakdown spectroscopy (LIBS), used by instruments such as ChemCam on the Mars Rover Curiosity. Because a single run of the LIBS computer model may take hours on parallel computing platforms, we build fast emulators for single-compound targets. We then construct multi-compound emulators by combining the single-compound emulators in a hierarchical model. Our approach yields the first statistical characterization of matrix effects, i.e. spectral peaks that are amplified or suppressed when compounds are combined in a target versus measured in isolation, and the first capability in uncertainty quantification (UQ) that addresses the unique challenges of chemical spectra. ","We systematically studied the association between somatic copy number aberration (SCNA), DNA methylation, and gene expression using -omic data from The Cancer Genome Atlas (TCGA) on six cancer types: breast cancer, colon cancer, glioblastoma, leukemia, lower-grade glioma, and prostate cancer. A major challenge for such integrated study is that the association between DNA methylation and gene expression is severely confounded by tumor purity and cell type composition, which are often unobserved and difficult to estimate. To overcome this challenge, we developed a method to remove confounding effects by calculating the principal components that span the space of the latent factors. After decades of study of DNA methylation, there is still debate on a fundamental question: whether alteration of DNA methylation causes gene expression variation or it reacts to gene expression variation. Our new method allows us to reveal an intriguing observation that the association between SCNA and DNA methylation is mediated at least in part by gene expression. This finding suggests that at least part of the variation of DNA methylation is because DNA methylation is a passive mark rather than an acti ","Randomization balances covariates between treatment groups on average, but in any particular randomization, covariate imbalance between groups is possible, and even likely.  Rather than proceeding with observed imbalance, if the experiment has not been conducted yet, units can and should be rerandomized to treatment groups.  Criteria for adequate covariate balance should be specified in advance, but these criteria, both in how covariate balance is measured and in how stringently balance is enforced, are flexible and can be specified in the context of a particular experiment.  The general idea, rerandomizing units when covariates are imbalanced, is applicable to a wide variety of experimental settings.    ","Instrumental variables is an approach used to estimate regression parameters when covariates are correlated with errors.  Applications of instrumental variables are common in econometrics among other fields where the method is regarded as helping to uncover causal relationships between covariates and a response.  Statistically, the method succeeds in producing a consistent point estimator of the regression coefficients.  However, standard interval estimates for the regression coefficients are unreliable.  We present a new technique, using inferential models, to provide valid inference in instrumental variables regression. ","Recent technological breakthroughs have made it possible to measure gene expression at the single-cell level. These advances coupled with new computational algorithms allow us to better describe the types of single cells, characterize the stochasticity of gene expression across cells, and improve our understanding of cellular function in health and disease. A fundamental theme in RNA-seq is to detect genes with differential expression. However, current single-cell RNA-seq protocols are complex and introduce technical biases that vary across cells. Differential expression analysis in the context of single cells must not only account for technical noise, but also characterize changes in gene expression beyond a shift in mean. There is now ample evidence for gene transcription being a bursty process, with the two-state on/off bursting model having both empirical support and mathematical tractability. Here we present a method for differential expression analysis that can account for cell-specific technical bias, and detect and characterize changes specific to the transcriptional bursting process. We will show results from both simulation as well as real data analysis. ","The US Environmental Protection Agency is tasked with protecting human health and the environment.  One venue for this is through the implementation of the Clean Air Act.  A wide array of statistical problems are considered in assessing risk, determining compliance of the National Ambient Air Quality Standards, and monitoring air quality.  These include evaluating temporal trends, spatial interpolation, data fusion, satellite product incorporation, quality standards implementation, air quality modeling, assessing human health effects and environmental impacts, and more.  This talk will consider a brief overview of statistics within the Office of Air Quality Planning and Standards as well as across EPA departments and federal agencies, with an in-depth look at a couple of time-relevant projects.  A few threads that will be also discussed include: What are the different EPA branches and offices and how do they work together on emerging scientific issues?  What is the interplay of statistical science with decision and policy makers? What types of opportunities exist for collaboration and employment?   ","This poster studies the performance of Approximate Message Passing (AMP), in the regime where the problem dimension is large but finite. We consider the setting of high-dimensional regression, where the goal is to estimate a highdimensional vector x from an observation y = A x + w. AMP is a low-complexity, scalable algorithm for this problem. It has the attractive feature that its performance can be accurately characterized in the asymptotic large system limit by a simple scalar iteration called state evolution. Previous proofs of the validity of state evolution have all been asymptotic convergence results. In this work, we derive a concentration result for AMP with i.i.d. Gaussian measurement matrices with finite dimension n times N. The result shows that the probability of deviation from the state evolution prediction falls exponentially in n. Our result provides theoretical support for empirical findings that have demonstrated excellent agreement of AMP performance with state evolution predictions for moderately large dimensions. ","Standard penalized methods of variable selection and parameter estimation rely on the magnitude of coefficient estimates to decide which variables to include in the final model.  However, coefficient estimates are unreliable when the design matrix is collinear.  To overcome this challenge an entirely new method of variable selection is presented within a generalized fiducial inference framework.  This new procedure is able to effectively account for linear dependencies among subsets of covariates in a high-dimensional setting where $p$ can grow almost exponentially in $n$.    ","The Dempster-Shafer (DS) theory of belief functions is a generalization of Bayesian  inference. It mobilizes the mathematical construct of random sets to describe uncertainty. Random sets make up a language far richer than random variables or vectors. They can truthfully encode partial, low-resolution, or even the lack of knowledge, which are otherwise inexpressible ideas in statistical models. We examine random sets in most plain probabilistic terms, and discuss implications on the aggregation of statistical evidence, in particular confirmatory vs contradictory inputs in this context. ","Approximate Bayesian computing (ABC) is a likelihood-free method that has grown increasingly popular since early applications in population genetics. However, the theoretical justification for inference based on this method has yet to be fully developed especially pertaining to the use of non-sufficient summary statistics. We introduce a more general computational technique, approximate confidence distribution computing (ACC), to overcome a few issues associated with the ABC method, namely, the lack of theory supporting the use of non-sufficient summary statistics, the lack of guardian for the selection of prior, and the long computing time. We establish frequentist coverage properties for the outcome of ACC by using the theory of confidence distributions; thus inference based on ACC is justified even if one uses a non-sufficient summary statistic. Furthermore, the ACC method is very broadly applicable; in fact, the ABC algorithm can be viewed as a special case of ACC without damaging the integrity of ACC-based inference. We supplement the theory with simulation studies and an epidemiological application. We demonstrate the computational savings of a well-tended ACC algorithm. ","This poster gives an introduction to the R package TDA and how to do a statistical inference on topological data analysis with the package TDA. The salient topological features of data can be quantified with persistent homology. Between persistent homologies, the bottleneck distance is defined. The bottleneck distance between persistent homologies is bounded by the distance between corresponding functions, which is the stability theorem. Based on the stability theorem, the confidence band can be computed to distinguish significant topological features from noisy features in the persistent homology. This poster illustrates how R package TDA compute preliminary functions, persistent homologies, and confidence bands with examples. ","Statistical consulting classes are a great way for graduate students learn soft skills and data sense by working with clients on real data problems in a supervised setting. Programs often teach these skills in small, discussion-based courses. As many universities are currently experiencing increases in enrollment and decreases in budget, keeping these classes small may become impractical. I will discuss how Statistical Practice, a graduate-level consulting course at North Carolina State University, has been designed to rise to the challenge of larger class sizes. ","The demand for statisticians has grown dramatically in recent years due to the increasing availability of data in many fields. While the number of graduates in statistics is growing, the McKinsey Global Institute predicts there will soon be a shortage of professionals with the quantitative skills necessary for the new data-rich environment. In order to meet this demand, the approach to statistics education must change to better recruit, retain, and prepare capable students. Experiential learning has the potential to improve statistics education. SCHOLAR (Statistical Consulting Help for Organizational Leaders and Academic Researchers) is an NSF funded program at the University of Central Oklahoma in which undergraduate students participate in experiential learning through real statistical consulting projects submitted by researchers from both on- and off-campus. Since 2009, more than 50 students have participated in SCHOLAR, and have completed more than 30 projects. In this presentation, we will outline the history of SCHOLAR and present results of a survey on the attitudes toward statistics, undergraduate research, and graduate school among both SCHOLAR and non-SCHOLAR students. ","We develop a geometric framework, based on the classical theory of fibre bundles, to characterize the cohomological nature of a large class of synchronization-type problems in the context of graph inference and combinatorial optimization. We identify each synchronization problem in topological group $G$ on connected graph $\\Gamma$ with a flat principal $G$-bundle over $\\Gamma$, thus establishing a classification result for synchronization problems using the representation variety of the fundamental group of $\\Gamma$ into $G$. We then develop a twisted Hodge theory on flat vector bundles associated with hese flat principal $G$-bundles, and provide a geometric realization of the graph connection Laplacian as the lowest-degree Hodge Laplacian in the twisted de Rham-Hodge cochain complex. Motivated by these geometric intuitions, we propose to study the problem of learning group actions - partitioning a collection of objects based on the local synchronizability of pairwise correspondence relations - and provide a heuristic synchronization-based algorithm for solving this type of problems. We demonstrate the efficacy of this algorithm on simulated and real datasets. ","Advances in mobile computing technologies have made it possible to monitor and apply data-driven interventions across complex systems in real time. Markov decision processes (MDPs) are the primary model for sequential decision problems with a large or indefinite time horizon. Choosing a representation of the underlying decision process that is both Markov and low-dimensional is non-trivial. We propose a method for constructing a low-dimensional representation of the original decision process for which: 1. the MDP model holds; 2. a decision strategy that maximizes mean utility when applied to the low-dimensional representation also maximizes mean utility when applied to the original process. We use a deep neural network to define a class of potential process representations and estimate the process of lowest dimension within this class. The method is illustrated using data from a mobile study on heavy drinking and smoking among college students. ","A dynamic treatment regime (DTR) formalizes precision medicine as a series of functions over decision points. At each decision point, it takes the available information of a patient as input and outputs a recommended treatment for that patient. A high-quality DTR tailors treatment decisions to individual patient as illness evolves, and thus improves patient outcomes while reducing cost and treatment burden. To facilitate meaningful information exchange during the development of DTRs, it is important that the estimated DTR be interpretable in a subject-matter context. We propose a simple, yet flexible class of DTRs whose members are representable as a short list of if-then statements. DTRs in this class are immediately interpretable and are therefore appealing choices for broad applications in practice. We develop a nonparametric Q-learning procedure to estimate the optimal DTR within this class. We establish its consistency and rate of convergence. We demonstrate the performance of the proposed method using simulations and a clinical dataset. ","The discovery of relationships between gene expression measurements and phenotypic responses is hampered by both computational and statistical impediments. Conventional statistical methods are less than ideal because they either fail to select relevant genes, predict poorly, ignore the unknown interaction structure between genes, or are computationally intractable. Thus, the creation of new methods which can handle many expression measurements on relatively small numbers of patients while also uncovering gene-gene relationships and predicting well is desirable. We develop a new technique for using the marginal relationship between gene expression measurements and patient survival outcomes to identify a small subset of genes which appear highly relevant, produce a low-dimensional embedding based on this small subset, and amplify this embedding with information from the remaining genes. We motivate our methodology by using gene expression measurements to predict survival time for patients with diffuse large B-cell lymphoma, illustrate the behavior of our methodology on carefully constructed synthetic examples, and test it on a number of other gene expression datasets. ","This poster presents work on the project Distant Seeing TV, which applies computational techniques to the study of television series. Our initial set of interest consists of twelve American sitcoms from the Network Era, spanning the 1950's through the 1970's. In order to focus the academic questions that we are interested in exposing, all of the selected shows feature leading women characters and have previously been the topic of academic studies. The poster will outline the kinds of academic questions we hope to answer with our study, the computational methods currently available for answering these questions, our novel extensions of these methods, and initial results. ","Generalized fiducial inference (GFI) is a relatively new approach for conducting statistical inference and performing uncertainty quantification,  although its original form was introduced by Fisher back in the 1930s. In this project, we describe an alternative technique to investigate nonparametric function estimation problems in the GFI framework via reproducing kernel Hilbert space (RKHS).  ","Surveys of microbial communities (microbiota), typically measured as relative abundance of species, have illustrated the importance of these communities in human health and disease. Yet, statistical artifacts commonly plague the analysis of relative abundance data. Here, we introduce the PhILR transform, which incorporates microbial evolutionary models with the isometric log-ratio transform to allow off-the-shelf statistical tools to be safely applied to microbiota surveys. We demonstrate that analyses of community-level structure can be applied to PhILR transformed data with performance on benchmark datasets often surpassing standard tools. By decomposing distance in the PhILR transformed space, we identified neighboring clades that may have adapted to distinct human body sites. Decomposing variance revealed that the covariation of bacterial clades within human body sites increases with phylogenetic relatedness. Together, these findings illustrate how the PhILR transform combines statistical and phylogenetic models to overcome compositional data challenges and enable evolutionary insights relevant to microbial communities.  ","Gaussian processes provide a flexible framework for modeling functional responses without assuming a parametric form. Many scientific disciplines use Gaussian process approximations to improve prediction and make inference on latent processes and parameters. When prediction is desired on unobserved covariates given realizations of the response variable, this is called inverse prediction. Because inverse prediction is often mathematically and computationally challenging, predicting unobserved covariates often requires fitting models that are different from the hypothesized generative model. We present a novel computational framework that allows for efficient estimation of a Gaussian process approximation to generative models. Our framework enables scientific learning about how the latent processes co-vary with respect to covariates while simultaneously providing predictions to the inverse problem. The proposed framework is capable of exploring the high dimensional, multi-modal latent spaces that arise in the inverse problem efficiently. To demonstrate flexibility, we apply our method to a generalized linear model to predict latent climate states given multivariate count data.  ","Spatial non-Gaussian data are common in many environmental disciplines. Spatial generalized linear mixed models (SGLMMs) are flexible models for such data but they are computationally intensive. Moreover, SGLMMs inflate the variance of fixed effect (regression coefficient) estimates.  We explore fast maximum likelihood inference for methods that reduce the computational cost while also alleviating the confounding issue between fixed and random effects. We study a projection-based methodology that represents the high-dimensional spatial random effects by reduced-dimensional random vectors. We develop a Monte Carlo Expectation Maximization (MC-EM) algorithm for efficient inference for these models. Our approach applies to both discrete-domain (Gaussian Markov random field) as well as continuous-domain (Gaussian process) spatial models. ","Recidivism prediction instruments provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses a fairness criterion originating in the field of educational and psychological testing that has recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate how adherence to the criterion may lead to considerable disparate impact when recidivism prevalence differs across groups. ","It has become a standard practice to conduct a nonresponse follow-up (NRFU) survey on the unit nonrespondents to a survey.  What to do next is not clear.  A NRFU survey was conducted for the National Pilot of the Residential Energy Consumption Survey (RECS)  Virtually all unit nonrespondents to the National Pilot were sent a short mail questionnaire containing 18 key items from the full survey.  Two methods of adjusting variables collected on the NRFU for unit nonresponse were compared.   Either only the weights for respondents to the full National Pilot survey had their weights calibration adjusted to compensate for nonresponse, or only the NRFU-survey respondents' weights were adjusted for nonresponse using a analogous calibration weighting scheme, while weights for the respondents to the full survey were not adjusted. When the estimates from the two methods were significantly different, the latter was added as a calibration variable when adjusting for unit nonresponse to the full sample.  Otherwise, the mean of the two added as a calibration variable when readjusting for nonresponse to the full sample.  The theory behind this practice and its repercussions are discussed.  ","A rapidly changing survey environment today requires a nimble, flexible design that leverages multiple data sources in a multi-mode data collection, produces quality data and optimizes cost allocation over the survey life cycle. Adaptive survey design has been responsive to these changes to minimize survey cost and optimize data quality. We have organized an invited panel to discuss the latest advancements and challenges remaining in adaptive and responsive survey design by assembling editors and authors of papers to be published in the Journal of Official Statistics Special Issue on Adaptive Design, as well as researchers collaborating between academia and governmental sectors. The panel is expected to answer the following overarching questions: What theoretical contributions and innovations have been feasible with adaptive design to advance survey methodology? Under what conditions can administrative records or big data be adaptively used to supplement responsive design? What stopping rules of data collection can be used across major phases of the survey life cycle? How could adaptive design guide surveys while controlling for total survey errors? ","Imputation is an effective way to handle missing values. In this paper, we propose a nonparametric multiple imputation procedure that makes use of multiple outcome regression models and/or multiple nonresponse models. Our procedure leads to a multiply robust point estimator in the sense that it remains consistent if anyone of these multiple models is correctly specified. A variance estimate is readily obtained by applying the customary rule advocated by Rubin (1987). The asymptotic properties of the proposed method are established. Results from a simulation study, assessing the proposed method in terms of bias, efficiency and coverage probability, support our findings. ","Detailed breakdown of totals items are collected in surveys. Detail proportions can vary greatly by sample unit, and the multinomial distributions can likewise vary by imputation cell.  Consequently, although it might be feasible to develop viable parametric imputation models for the total, it is challenging for the collective set of detail items. Instead, a common practice is to use some form of hot deck imputation to match donor and recipient records, then impute the donor's complete set of proportions. Nearest neighbor imputation is useful when the set of proportions is correlated with unit size.  This approach preserves the correlation between the detailed items within imputation cell, as long as the number of donors is greater than or equal to the number of recipients. Unfortunately, this condition often does not hold in practice. Collapsing imputation cells is not an attractive alternative. We explore unrestricted usage of the donor records in the original cell versus the usage of a random draw from the donor record's multinomial distribution via a limited simulation study using historic data in selected industries from the 2012 Economic Census.  ","Attrition is one of the major methodological problems in longitudinal studies. It not only reduces the sample size, but also can result in biased estimation and inference. It is crucial to correctly understand the attrition mechanism and appropriately incorporate it into the estimation and inference procedures.  Traditional methods, such as the complete case analysis and imputation methods, are designed to deal with missing data under unverifiable assumptions of MCAR and MAR. The purpose of this paper is to identify and estimate attrition parameters under the nonignorable missing assumption utilizing the refreshment sample.  In particular, we propose a semiparametric method to estimate the attrition parameters by comparing the marginal density estimator using the model (Hirano et al. 1998) with the one using only the observed information from the refreshment sample.  For a bivariate Gaussian model, we also considered a parametric approach by applying the adaptive Gaussian quadrature to overcome the difficulty in the integration. Our simulation has shown that both methods are able to estimate the attrition parameters with relatively better MSE. ","Analysis of sample survey data often requires adjustments for missing values in the variables of interest. Standard adjustments based on item imputation or on propensity weighting factors rely on the availability of auxiliary variables for both responding and non-responding units. Their application can be challenging when the auxiliary variables are numerous and are themselves subject to incomplete-data problems. This paper shows how classification and regression trees and forests can overcome these difficulties and compares them with likelihood methods in terms of bias and mean squared error. The development centers on a component of income data from the U.S. Consumer Expenditure Survey,which has a relatively high rate of item missingness. Classification trees and forests are used to model the unit-level propensity for item missingness in the income component. Regression trees and forests are used to model the conditional mean of the income component. The methods are then used to estimate the mean of the income component, adjusted for item nonresponse. Thirteen methods for estimating a population mean are compared in simulation experiments.  ","In this research, we consider whether sales and income tax data collected in the Public Sector Quarterly Tax Survey (QTAX) is comparable to that collected in Annual Survey of Local Government Finance (ALFIN).  Ideally, the sales tax collected from QTAX in a year should be similar to that of ALFIN.  The key element in assessing this question is the source(s) from which respective surveys obtain their data.  As it stands, the collection sources are very different---QTAX data represents responses from state government representatives and ALFIN from state level annual reports or administrative records.  In this research, we investigate the magnitude of the differences and discuss how to improve the data quality. ","The Florida Youth Survey project is a composite of three school-based surveys administered using paper-based instruments annually since 2002.  ICF recently conducted a web-based pilot of the 2017 Florida Youth Substance Abuse Survey with high and middle school students in 10 counties.  The goal of the study was to determine the impact of the administering the survey via a web-based system and to examine the impact of a series of methodological tests, including the use of skip logic and changes to the standard response options.  Our findings on student's responses and teacher impressions as well as challenges and recommendations for internet administration are presented.   ","Moving existing establishment surveys from face to face, mail and telephone to the web can make data collection cheaper and more efficient and allow the use of techniques like dependent interviewing, plausibly checks and complex filters. However, there is little research into the effect of switching to web data collection on data quality and performance rates. We conducted a survey of establishments in Germany which experimentally varied three mode conditions: web only, mail only and a web/mail choice condition. By comparing responses to administrative records, we can report the measurement error in each condition. We also compare the unit and item nonresponse rates across the conditions. We also explore whether the best mode varies by establishment size and/or industry. Our results are relevant for establishment surveys that wish to move to the web, but are worried about measurement error and nonresponse effects on the data.  ","The 2015 Residential Energy Consumption Survey (RECS) is a household level survey used to estimate energy consumption behaviors of U.S. residents. Initially, the 2015 RECS was administered solely using Computer-Assisted Personal Interviewing (CAPI).  However, mid-way through the collection process, it became evident that relying on CAPI as the only collection method was not prudent, so a mixed-mode approach was adopted, adding Computer Assisted Web Interview (CAWI) and Paper and Pencil Interviewing (PAPI) alongside CAPI. This research discusses the impact a last-minute switch to a mixed-mode design may have had on the 2015 RECS. Specifically, we investigated to what existent complex skip patterns increased skip logic error rates. This paper also considers how the layout of the PAPI instrument may have played a role. The findings and opinions of this paper are solely that of the authors and do not necessarily reflect the beliefs of the U.S. Energy Information Administration.   ","The 2015 Residential Energy Consumption Survey (RECS) was a stratified multistage cluster survey of housing units (HUs). RECS was designed for computer-assisted personal interviewing (CAPI) as the method of data collection. Because of difficulties experienced in the field, CAPI data collection was terminated and replaced with a Web/Mail data collection protocol.  Nonrespondents and unfinished cases from CAPI were transferred to Web/Mail, and HUs in reserve replicates of sample were released to Web/Mail. This change imposed a challenge for weighting the combined CAPI and Web/Mail data. In this paper we discuss the weighting class method to adjust for bad addresses and drop points, a latent-variable technique to predict the probability of an address corresponding to an occupied HU, and logistic regression models to estimate the probability of a HU being a primary residence. We used a calibration method to adjust for unit nonresponse and to poststratify the nonresponse-adjusted weights to the estimated number of occupied HUs from the 2015 American Community Survey for specified HU characteristics.  ","Mixed-mode surveys have grown in popularity due to concerns of falling response rates and coverage bias; however, treatment of the inherent mode effects threaten the integrity of these studies. There is an ongoing debate in the survey community about the validity of performing mode adjustments. While several diverse attempts have been made to adjust for survey mode effects with one time period, the literature reveals a demand for the development of theory which extends to longitudinal studies. Adjustment methods are based on the framework of creating counterfactuals of reassigning respondents to one mode in order to establish consistency for ease of comparison over time. These procedures assume either independence or maximal dependence between an individual's hypothetical responses to different modes. Although individual effects are inestimable within a single survey study, the definition and treatment of correlation structure may affect the conclusions we draw and how accurate they are. In this paper we explore a Bayesian prior on the within subject effect to create a flexible unified adjustment model that explicitly parametrizes the within-respondent correlation. ","The USDA's National Agricultural Statistics Service is exploring the use of an electronic mobile mapping instrument in an effort to incorporate newer technologies of its June Area Survey (JAS) data collection.  The JAS is based on an area sampling frame comprised of segments of land that make up the sampling units.  JAS enumerators use a paper aerial photograph to locate and interview all operators within the segment boundary.  Then, they draw off all fields by hand on the aerial photograph and fill out a paper questionnaire. Research conducted in 2014, using a mobile mapping prototype, indicated that drawing fields during the interview took longer than is operationally feasible.  Testing in 2016 focused on providing enumerators with pre-delineated fields in the mobile mapping instrument in order to reduce interview time.  Completion times were compared to current procedures using the paper aerial photograph.  Using a Latin-square design, enumerators recorded previous year's JAS data using a mock interview format in two states.  Research results are discussed in this paper.  ","Model-based standardization uses a statistical model to estimate a standardized, or unconfounded, population-averaged effect. With it, one can compare groups had the distribution of confounders been identical in both groups to that of the standard population. We develop two methods for model-based standardization with complex survey data that accommodate a categorical confounder that clusters the individual  observations into a very large number of subgroups. The first method combines a random-intercept generalized linear mixed model with a conditional pseudo-likelihood (CPL) estimator of the fixed effects.  The second method combines a between-within generalized linear mixed model with census data on the cluster-level means of the individual-level covariates. We conduct simulation studies to compare the two approaches. We apply the two methods to the 2008 Florida Behavioral Risk Factor Surveillance System (BRFSS) survey data to estimate standardized proportions of people who drink alcohol, within age groups, adjusting for measured individual-level and unmeasured cluster-level confounders.   ","In this study we introduce a multilevel pseudo maximum likelihood (MPML) method for weighted count data from a complex survey. We conduct a simulation study to assess the finite sample performance of this estimation method under several scaling methods of the survey sampling weights. In addition, different approaches for estimating the variance of parameter estimates corresponding to covariates that are included in the model are considered and compared in the simulation studies. The simulation results show that MPML estimation with scaled sampling weight provide better performance in terms of relative bias and 95% confidence interval coverage compared to estimations by standard approaches that use sampling weight or that ignore sampling weight. Thus, we recommend MPML to analyze multilevel weighted count data. Among the four variance estimation approaches we considered, the empirical variance estimator provides values that reflect the true variance of the estimators. We demonstrate the proposed methods using real data from national surveys of three countries in sub-Saharan Africa. ","Multilevel complex surveys are widely used in large-scale research.  In a multilevel complex survey, clusters and units within the sampled clusters may be sampled with unequal probabilities. Multilevel models that take into account hierarchical structure are commonly used to analyze multilevel complex survey data. For example, a pseudolikelihood approach extends the multilevel model framework to incorporate weights at different levels. The increasing use of this approach has led software such as SAS, R and Stata to develop their own package or procedure to analyze complex survey data with sampling weight. In this study, we demonstrate how to estimate multilevel models for complex survey data using SAS, R and Stata. We clarify the weight options offered in each package in SAS, R and Stata, and provide the syntax for estimating multilevel model in each package. We then compare the results of these different software under different scaling weights.   ","In Public Health we strive to predict disease incidence or mortality for a particular area, such as state or county. At an aggregate level, we can predict spatially correlated count data using a Generalized Linear Mixed Models (GLMM) framework with a Poisson outcome variable with an auxiliary variable in a bivariate relationship. A cokriging structure is applied in a GLMM setting which includes a Poisson outcome variable and an auxiliary variable with a named distribution, where the outcome variable and auxiliary variable are spatially correlated and correlated with each other. This methodology is examined in a real data setting with applications in public health, predicting West Nile virus incidence, and cancer incidence and mortality at the county level. Environmental auxiliary variables considered in the West Nile virus prediction are counts of infected mosquitos, birds, and percent irrigated farmland. In cancer incidence and mortality prediction we consider environmental variables, and socio-demographic variables such as racial distribution, education, and income as auxiliary variables. To evaluate the prediction performance, cross-validation is employed. ","Attrition or dropout is a common phenomenon in surveys and questionnaires. One theory is that attrition occurs in three distinct types of phases: a curiosity plateau, a dropout phase, and then stable participation. We generalize this theory to either phases of stable participation or phases of dropout and aim to empirically estimate phase transitions with user-specified thresholds, using a generalized linear mixed model (GLMM), and applying discrete time survival analysis (DTSA). We simulate survey responses for 200 participants in a ten item questionnaire under three attrition patterns. Using these methods, we test the null hypothesis of no distinct attrition phases (linear attrition) against the alternative hypothesis that distinct attrition phases exist. ","Sample attrition is a common problem in longitudinal studies. This study examines how paradata, particularly observations documented by interviewers in the course of data collection, can improve the accuracy of prediction of attrition from one round of data collection to the next. We modeled round-to-round attrition using questionnaire data and paradata from rounds of data collection in the 2016 Medicare Current Beneficiary Survey (MCBS), a survey of a nationally representative sample of the Medicare population, conducted by the Centers for Medicare &amp; Medicaid Services (CMS) through a contract with NORC at the University of Chicago. We show that attrition can be effectively predicted with a model incorporating covariates such as number of contact attempts, length of previous interview, and respondent demographics. The predictive validity of the model was improved by adding predictors related to interviewer observations from the previous survey round. We describe the process of transforming text from interviewer observations into model predictors and explore how other longitudinal studies could use such data. ","Declining participation in government surveys is a concern for data users who rely on such programs for high-quality population and household statistics.  In recent years, a rising trend in sampled persons who refuse to participate in government surveys is alarming, since increased unit non-response requires more reliance upon adjustment methods that - when used excessively - can generate bias and reduce data quality.  While there are many factors that may affect one's decision to participate in a survey, the authors are concerned with the influence of economic and political conditions.  In a prior analysis, the authors built upon an earlier paper (Harris-Kojetin and Tucker, 1999) to explore the relationships of these external factors with refusal rates of the monthly Current Population Survey (CPS).  The time series regression model of the 1999 paper was replicated, and the scope of that model was expanded to 2015.  However, analysis of the more recent data indicated that the model was not ideal for studying modern refusal patterns. In this follow-up paper, the authors consider new covariates for the model and devise an alternate construction of the autocorrelated error.   ","Efficient sampling is important in many statistical applications. When sampling units is expensive or time-consuming, procedures are needed which limit the size of the sample taken while maintaining a high level of precision. Adaptive sampling is one way to efficiently sample from a population. In adaptive sampling, the sampling design is modified as data is collected. Another sampling method is Neyman allocation, which is known to give the optimal sample size allocation in stratified sampling under certain conditions. The difficulty of performing Neyman allocation in practice is that it requires some knowledge of population parameters.  ","Survey organizations are increasingly using interactive data visualization tools to monitor organizational performance. This paper highlights the design and development of a dynamic web-based tool that provides survey field staff with real-time information about their collection activities and allows field managers to monitor and manage staff performance. A cross-disciplinary team of field staff, programmers, and methodologists identified strategic objectives, prioritized metrics, and gathered requirements. We used free, open-source software (R Shiny) to graphically represent the key performance metrics with interactive features (e.g., filtering, linking and brushing), and created a prototype performance dashboard. We discuss technical and methodological challenges of this project, and provide practical considerations for aligning performance and quality control goals (e.g., in terms of level of effort, productivity, representativeness, etc.) of the survey with decisions about metric and graphical display selections. ","The U.S. Census Bureau is investigating adaptive nonresponse follow-up (NRFU) strategies for single unit businesses in the 2017 Economic Census. This paper describes an embedded split-panel field experiment in the 2015 Annual Survey of Manufactures that tests two adaptive NRFU designs. With the first design, nonresponding establishments in the experimental group received a reminder letter either by certified mail (expensive) or standard mail (inexpensive) based on an optimal allocation that assigns a higher proportion of the certified letters to domains that initially have low unit response rates. This targeted allocation procedure ensures that all units receive some form of NRFU, but saves cost over the current procedure that sends a certified letter to all nonresponding units. The second studied adaptive design restricts the NRFU for the probability subsample of nonrespondents selected for the targeted allocation. In this paper, we compare the quality effects of the two studied adaptive NRFU designs examining effects on response, respondent sample balance, and collected data quality. ","Current solutions for unit nonresponse focus on improving nonresponse follow-up and enhancing post-survey weighting adjustment. We propose an alternative inferential paradigm to adjust for unit nonresponse using micro-level auxiliary data that captures the same features of the target population, referred to as 'benchmark'. We describe a benchmark-driven mitigation and imputation (M&amp;I) strategy, in the context of a multi-phase survey, that sequentially guides the sampling and estimation to improve survey inferences regardless of the nonresponse mechanism. The M&amp;I strategy employs a high quality benchmark to mitigate undesirable nonresponse patterns through a benchmarked sequential sampling (BSS) design; and to impute population information through benchmarked multivariate imputation (B-MI) by chained equations. The performance of the M&amp;I strategy will be evaluated by simulation experiments to mimic adaptive design under various nonresponse mechanisms including missing not at random (MNAR). We report on the preservation of marginal and joint distribution for population estimates of the M&amp;I strategy from respondent data and completed data. ","Unlike traditional probability sampling, the sample size of respondent driven sampling is a random variable with little to no information about its distribution and is dependent on the number of seeds and participants' recruitment propensity. Therefore, RDS sampling productivity is neither controllable nor predictable. Low recruitment propensity will lead to the final sample size being much smaller than desired. With high recruitment propensity, one may achieve the target sample size quickly. However, if starting with a large number of seeds, recruitment chains may be too short to reach the equilibrium state.  This study examines sampling productivity of RDS applied to a Web survey of foreign-born Koreans in the U.S. Specifically, we examine sample size growth over waves and recruitment chain length and whether participants' characteristics (e.g., the number of peers) and survey features (e.g., reminders) play a role. Using follow-up study data, we also examine how participants recruit their peers in practice and whether recruitment success is related to participants' recruitment strategies.  ","Demands for data collection of small and rare population subgroups are on the rise. However, traditional sampling methods may not provide practical solutions for such demands. The present study investigates a new sampling method, known as respondent driven sampling (RDS), applied to an immigrant group that is rare in numbers. The best way to evaluate RDS is empirically comparing estimates from RDS data to those from traditional sample data or population data. Specifically, this study uses an RDS dataset that is collected for non-US-born Koreans in the U.S. and compares its estimates to benchmarks from the American Community Survey (ACS) in order to assess error properties in the estimates. In doing so, we propose new inference approaches for univariate and multivariate statistics that are subject to fewer assumptions than existing approaches, while being more flexible to reflect realities of RDS recruitment processes. This study warrants further research about innovative estimation methods. ","  A \"cold call\" or random screening to identify and sample a specific, rare, and/or a difficult to find population is prohibitively expensive. Thus, the survey organizations need to develop new screening/sampling strategies in order to keep field efficiency high and to minimize the associated field costs. This paper reports on a strategy that uses data from multiple sources (existing survey data, commercial data and census information), imputation and modeling strategies to predict probability of a household satisfying the eligibility criteria: (1) Households with a 3 to 10 year old and (2) income strata defined as low, medium and high based on the tertiles of the household income. A two-stage sampling strategy was developed to oversample in 3:2:1 ratio from the low, medium and high income ?strata of ?households and with children between the ages of 3 and 10. A pilot study was conducted to evaluate this strategy to estimate the actual eligibility rate. It is estimated that nearly 70% efficiency can be gained using this strategy (29% eligibility) when compared to random screening (20% eligibility).  ","Traditional sampling techniques to study small or rare population often are inefficient due to the low number of eligible participants on the general population. One strategy to improve efficiency is to increase eligibility rates by targeting likely eligible participants. However, to implement this strategy the sampling frame requires to include a set of variables that correlate with eligibility. This paper presents the use of external data at cluster and household level to improve eligibility rates for sampling late baby boomers. Census data was used at the cluster level to select area with higher density of late baby boomers with higher probability. At the household level, commercial data was attached to households. This is information was used to stratified households in selected cluster. Likely baby boomers according to the commercial data were sampled at higher rate. The use of external data proved to be effective on increasing efficiency of selecting late baby boomers.  ","Independent surveys that measure the same quantity typically produce different estimates. While sampling error is one obvious explanation, there are many non-sampling error sources that could contribute to an even greater variation across different studies. In this paper, the authors compare important tobacco-use estimates from several national surveys. They explore variations in the definitions, timing, instrument wording, interview modes, and other methodological dissimilarities that might contribute to those differences. The authors' objective is to inform users of these tobacco-use estimates about notable differences between the surveys and to show the impact these differences can make on the resulting estimates. ","Snijkers and Willimack (2011) have identified a gap in questionnaire development, where questions may be written without a full understanding of the underlying concept. Subject experts may have a different understanding of key terms and feasibility of data collection than respondents. Cognitive testing of questions written without understanding of the concept can reveal a need for substantial changes. Stettler and Featherston (2010) discuss early stage scoping (ESS): interviews that learn how respondents understand the survey's key concepts, before the questions are written. In practice, however, subject experts often prepare initial questions before the development process begins. While the questions may seem reasonable, testing can reveal that respondents do not understand the conceptual objectives. When this issue is discovered, it may be too late for a full round of ESS. This paper looks at the development of a survey that experienced such issues, and how methodologists and analysts integrated aspects of ESS into subsequent rounds. We also discuss how this may be a way forward in making the practice of early stage scoping a more indelible part of the development process. ","Social desirability bias results from answering questions in a way that is designed to paint oneself in a positive light or to avoid association with negatively viewed attitudes or behaviors. This bias pervades a variety of self-reported data. Indirect questioning is a low burden technique that may reduce this bias. It involves asking participants to report about the attitudes or behaviors of \"other people\" rather than themselves under the assumption that participants project their own attitudes and behaviors onto others and feel more comfortable reporting socially undesirable responses about others. There has been little research on the effectiveness of indirect questioning (Fisher, 1993) and whether it may be more or less effective among specific subgroups. With data from an Indiana University School of Public and Environmental Affairs web survey of 2,833 US adults from GfK's KnowledgePanel, we compare univariate and multivariate regression estimates of support for a local fracking project using indirect and direct questions. We also model the difference in responses as a function of attitudinal, demographic, and behavioral covariates to explore effectiveness by subgroup. ","This presentation will summarize efforts to expand military family data by leveraging an existing large national survey of the noninstitutionalized civilian population. Other national surveys could model this effort; however, addressing the data quality concerns is required. In 2015, the National Survey on Drug Use and Health (NSDUH) added questions to identify whether respondents had immediate family members who were currently serving in the U.S. military, regardless of where those family members were currently living. Although the military questions were pretested, these items had about 9% missing data. The NSDUH military population estimates were compared to Department of Defense personnel record population data to assess coverage. Based on this comparison, the 2015 data were analyzed while the 2016 NSDUH questions were redesigned to reduce missing data. Changes were made to the military questions to address the weaknesses in the questions regarding the definition of military and the definition of immediate family.  ","Economic data are often constrained by additivity conditions, where a set of item values (detail items) are required to sum to an associated total value. The set of detail items and their respective total is referred to as a balance complex. When these additivity constraints are not met, changes must be made to either the total or the set of details. Raking proportionally adjusts each detail item by the same amount. If each item's reporting error is random and has variance proportional to its value, then raking minimizes a chi-squared statistic. However, raking was developed for strictly positive data and can produce erroneous values when negative data are included. Modifications have been developed to address this situation, but implementation is not straightforward and does not always yield a feasible solution. In this paper, we develop separate linear and nonlinear programs that minimize loss functions under specified additivity constraints that work with negative data and include item reliability weights. We apply the proposed methods to examples from the Quarterly Financial Report conducted by the U.S. Census Bureau, examining statistical properties of the resultant solutions. ","We consider the dual problems of choosing between competing small area models and validating model assumptions in an area-level model. Many classes of small area models result in an estimate that is a convex combination of the direct and the marginal estimate for a given area. Therefore, competing models may share the same direct estimates, but give different marginal estimates as well as relative weight on the estimates. We discuss diagnostics to choose between competing models and parametric bootstrap methods to check for model validity and goodness of fit. We use the example of small area models related to the Voting Rights Act Section 203(b), which are used to estimate the number of limited English proficient and illiterate persons in certain language minority groups within jurisdictions using 5-year data from the American Community Survey. ","Recent work in small area estimates at sub-county level has focused on estimating the rate of poverty in school-aged children (e.g. Franco 2015). To obtain counts of children in poverty require having known population counts, often assumed without error, for these small areas. In general, this is not always the case and the uncertainty due to not knowing the true population counts is often not reflected in the predictions. The sampling distributions from simulating the American Community Survey design appear more similar to over-dispersed Poisson distributions than standard Poisson.  We propose a small area model for over-dispersed Poisson count data to model the number of children in poverty at the census tract level. Modeling assumptions will be tested using the simulated samples from design-based simulation. ","Following its justification in widely cited papers (McCarthy 1969, Krewski &amp; Rao 1981, Fay 1984, 1989), Balanced Repeated Replication (BRR) has become a standard method for variance estimation in large complex surveys, especially in the US. However, it is also known that BRR variance estimates for very small domains are unreliable. Survey point estimates for small domains are often based on empirical-Bayes small area estimation models (Rao and Molina 2015), with variances estimated through parametric-bootstrap methods. This paper presents theory and practical details for a novel hybrid approach, in which variances are estimated via parametric-bootstrap replications nested within BRR weight-replications. The method is presented first in general settings where categories are modeled within larger (but sometimes still small) domains. Then the results are specialized to the Dirichlet-multinomial hierarchical model describing small outcome proportions developed in the recent estimation from 2010-2014 American Community Survey data of language-minority and English proficiency characteristics in support of alternative-language ballot assistance determinations under the terms of Section 203(b) of the Voting Rights Act of 1965.  ","Sociodemographic and health surveys have become routinely geocoded in federal statistical agencies, which means that we could have both individual characteristics of survey respondents from the survey itself but also their geographic context that might have great influence on their individual social, economic and health behaviors.  Thus, we are developing and validating an innovative multilevel regression and poststratification (MRP) approach that applies multilevel models to geocoded surveys; takes account for both individual characteristics and area level factors at multiple geographic levels; predicts individual-level social, economic and health outcomes in a multilevel modeling framework; and estimates the geographic distributions of population socioeconomic and health outcomes. We applied this innovative multilevel approach for small area estimation using geocoded American Community Survey (ACS) data. We will demonstrate that MRP provides a flexible statistical linkage and modeling platform that makes full use of geocoded ACS data and available geodemographic data to generate small area estimates of percentages of the population without health insurance coverage. ","With the threat of climate change looming, identification of communities at the highest risk of devastation based not only on geographic features but also on social characteristics is imperative. Indices of community social vulnerability can be created by summarizing large sets of social indicator data using spatial factor analysis, assuming all variables arise from a common set of spatial units; however, current spatial factor analysis methodology is ill-equipped to handle spatially misaligned data. In this paper, we introduce a joint spatial factor analysis model that can accommodate spatial data from multiple levels of nested areal units and identify a common set of latent factors arising from the smallest areal units represented in the data. This model provides a statistically sound method for summarizing spatially misaligned data and allows for results and inference at the highest possible spatial resolution. We demonstrate the superiority of this model over competing methods and use it to construct an index of community social vulnerability in Louisiana based on a set of misaligned social indicator data. This social vulnerability index is shown to improve on an existing one. ","Current health policy calls for greater use of evidence based care delivery services to improve patient quality and safety outcomes. Care delivery is complex, with interacting and interdependent components that challenge traditional statistical analytic techniques. Methods for analyzing interrupted time series (ITS) do not account for potential changes in variation and correlation following the intervention, and require a pre-specified intervention time point with an instantaneous effect. This is a key limitation since it is plausible to have either anticipatory or delayed change, which can influence determination of overall effectiveness. In this paper, we describe and develop a novel 'Robust-ITS' model that overcomes these omissions and limitations. The Robust-ITS model formally performs inference on the change-point; pre- and post-intervention correlation; variance of the outcome measure, and pre- and post-intervention trajectory. We illustrate the proposed method by analyzing patient satisfaction from a hospital that implemented and evaluated a nursing care delivery model. The Robust-ITS model is implemented in a R Shiny toolbox, freely available to the community.  ","In longitudinal studies, marginal structural models (MSM) are widely used to estimate the effect of time-dependent treatments in the presence of time-dependent confounders. Under a sequential ignorability assumption, MSM yield unbiased treatment effect estimates by weighing each observation by the inverse of the probability of the observed treatment given the history of observed covariates. However, these probabilities are typically estimated by fitting a model and the resulting weights can fail to adjust for observed covariates due to model misspecification. Also, these weights tend to yield very unstable estimates if the predicted probabilities are close to zero. To address these problems, instead of modeling the probabilities of treatment, we take a design-based approach and directly find the weights of minimum variance that adjust for the covariates across all possible treatment histories. For this, we analyze the role of weighting in longitudinal studies of treatment effects and pose a convex optimization problem that we can solve efficiently. In a simulation study we show that this approach outperforms standard methods, providing less biased and more precise effect estimates. ","Estimates of population characteristics such as domain means are often expected to follow qualitative assumptions. Domain mean estimation and inference have been showed to be improved when monotone constraints are incorporated along with sampling design of surveys, in comparison with common methods that do not take them into account. However, assuming incorrect shape constraints could lead to biased estimators. We develop the Cone Information Criterion for Survey Data (CICs) as a diagnostic method to measure monotonicity departures on population domain means. ","When sharing data among researchers or releasing data for public use, there is a risk of exposing sensitive information of individuals who contribute to the data.  Data synthesis (DS) is a statistical disclosure limitation technique for releasing synthetic data sets with pseudo individual records. Traditional DS techniques often rely on strong assumptions on a data intruder's behaviors and background knowledge to assess disclosure risk. Differential privacy (DP) formulates a theoretical approach for strong and robust privacy guarantee in data release without having to model intruders' behaviors. Efforts have been made aiming to incorporate the DP concept in the DS process. In this paper, we examine current DIfferentially Private Data Synthesis (DIPS) techniques, compare the techniques conceptually, and evaluate the statistical utility and inferential properties of the synthetic data via each DIPS technique through extensive simulation studies. Our work sheds light on the practical feasibility and utility of the various DIPS approaches, and suggests future research directions for DIPS. ","Conducted since 2007 in 17 states with funding from the Centers for Disease Control and Prevention, the Medical Monitoring Project (MMP) provides national and local estimates of the experiences, behaviors, access to medical care and HIV medications, and needs of U.S. adults living with HIV. Initially a three-stage cluster sample of states, clinics, and patients, in 2015 MMP began to sample persons with a diagnosis of HIV reported to state surveillance registries, regardless of medical care status. We describe changes to the sampling frames and sample design, then outline  two weight adjustment methods (propensity weighting vs. weighting class models) that were compared for two stages of non-response (sequential adjustments for non-contact and nonresponse vs. a single adjustment for nonresponse). We explain the use of updated surveillance registry data for non-coverage adjustments and for post-stratifying estimates to reduce bias. We describe how estimates available for this expanded study population differ from past MMP estimates and complement information from CDC'S National HIV Surveillance System. ","Population-based HIV Impact Assessment (PHIA) surveys are being conducted in 12 sub-Saharan African countries to measure HIV incidence, prevalence, and other key impact indicators by ICAP at Columbia University in collaboration with ministries of health and the Centers of Disease Control and Prevention and other partners. We use data from the first three PHIA surveys -in Zimbabwe, Malawi, and Zambia- to study the impact of survey weights on estimates of the prevalence of HIV. In developing the survey weights, decisions made about the variables to use for nonresponse adjustments, and about the formation of nonresponse and poststratification cells, may affect survey estimates and their standard errors. The nonresponse adjustments in PHIA surveys include adjustments for household and person nonresponse, and for failure to obtain analyzable blood samples. The paper describes the use of LASSO and CHAID for variable selection in making the nonresponse adjustments. It then examines the effects on selected survey estimates of the use of different sets of variables employed in the nonresponse adjustments and of the use of alternative poststratification adjustments. ","In 2015, the Medical Monitoring Project (MMP) was redesigned to sample individuals diagnosed with HIV in order to provide national and local estimates of various characteristics of persons living with HIV. Weight adjustments for use with this new stratified multistage sample design needed to be developed. We compared weight adjustments generated from a weighting class model with those from a propensity weighting model. Two components of nonresponse were accounted for: non-contact and nonresponse among those contacted. Weight adjustments methods were applied separately to each to allow for differing predictors by nonresponse component. The propensity method has greater flexibility to incorporate continuous variables and more than two variables into the nonresponse adjustment. However, this flexibility led to only minor improvements in model fit. We examine how the magnitude and variance of key estimates differ between methods at each stage of adjustment. Adjusted weights from both methods resulted in similar weighted estimates with small differences in variances.  ","Nonresponse weighting adjustment using propensity score (PS) is a popular tool for handling unit nonresponse. However, including all the auxiliary variables into the propensity model can lead to inefficient estimation and the consistency is not guaranteed if the dimension of the covariates is large. In this paper, a new Bayesian method using the Spike-and-Slab prior  is proposed to handle the sparse propensity score estimation under the parametric model assumption on the response probability. The proposed method does not assume any model for the outcome variable and is computationally efficient. Instead of doing model selection and parameter estimation separately as in most frequentist methods, the proposed method simultaneously selects the true sparse response probability model and provides consistent parameter estimation and corresponding inference, which can be quite involved in the frequentist methods. The finite-sample performance of the proposed method is investigated in limited simulation studies, including a partially simulated real data example from the Korean Labor and Income Panel Survey. ","The Medical Expenditure Panel Survey (MEPS) is an ongoing household survey that yields national estimates of various health care metrics; including health care use, expenditures, and insurance coverage. The MEPS also includes a medical provider component (MEPS-MPC) that is designed to collect information from the health care service providers reported by the household. To address the increased demand for data on organizational characteristics of providers and/or health care practices, the Robert Wood Johnson Foundation sponsored a pilot Medical Organization Survey (MOS). This survey is an extension of the MEPS-MPC and collects this type of data from a subset of MEPS sample medical providers. The MOS pilot was first fielded in 2016 and the data were made available through the release of a public use data file (PUF) in spring 2017. This paper discusses the construction of the analytic weights included on the MOS PUF. ","The dual-frame RDD telephone surveys have been widely used to avoid coverage bias due to cell phone-only populations in many countries. But country-specific characteristics of the cell phone numbering system have led to a variety of drawbacks in conducting telephone surveys. For example, the area code of cell phone numbers in the U.S. has become increasingly unreliable for surveys at the state or local level due to the regional portability of the numbers. In another instance, cell phone numbers in South Korea or some other countries raise a serious concern about RDD sampling and estimation at both national level and sub-national levels because they do not involve area codes useful to select random or stratified samples based on geographies. In this paper, we first present cell phone RDD sample designs to overcome this problem in South Korea. Next, we present several weighting procedures, including post-stratification weighting using respondents' self-reported location and simple weighting to solve the overlap problem in the dual frame. Then, we show the state of data quality at national or provincial levels in the National Adult Tobacco Survey conducted for applying these methods. ","In this paper, we show that the new concept for tuning design weights in survey sampling developed by Singh, Sedory, Rueda, Arcos and Arnab (2015: Elsevier) leads to an innovative Tuned Ratio Unbiased Mean Predictor (TRUMP) with the assistance of a model. It is shown theoretically that the proposed TRUMP is more efficient than the ratio estimator due to Cochran (1940). Although there is no need of empirical investigations, but a small scale simulation study will be discussed.  The proposed TRUMP has potential to be extended to the regression predictor and other complex survey sampling designs.    ","In this paper, we discuss the role that data processing and collection have for the measurement of misallocation. First, we turn to the newly accessible self-reported data from the US Census of Manufactures firms, reflecting what can be found in most developing countries. In the raw US data, measured misallocation (following Hsieh and Klenow 2009) is substantially higher than for any other country for which we have census data. For instance, if Indian firms had the same dispersion of distortions as measured in the reported US data, TFP in the Indian manufacturing sector would decrease by around two thirds. Second, we follow a different strategy for editing and imputing missing data than what is used by the US Census Bureau, by using a method that seeks to replicate the true variance in the underlying data generating process known as Classification and Regression Trees (CART). Relative to the current imputation strategy, this change raises the potential gains from removing misallocation in the United States manufacturing sector by around 10%.  ","The Physical Activity Monitor component was introduced into the 2003-2004 National Health and Nutrition Examination Survey (NHANES) to collect objective information on physical activity including both movement intensity counts and ambulatory steps. Due to an error in the accelerometer device initialization process, the steps data were missing for all participants in several primary sampling units, typically single counties or groups of contiguous counties, who had intensity count data from their accelerometers.  To avoid potential bias and loss in efficiency in estimation and inference involving the steps data, we adopted a multiple imputation approach based on Additive Regression, Bootstrapping and Predictive mean matching methods to accurately impute the missing values for steps collected in the 2003-2004 NHANES. This paper describes several real data analyses using the multiply imputed data and compares them with the before imputation results.   ","The National Survey on Drug Use and Health (NSDUH) is sponsored by the Substance Abuse and Mental Health Services Administration (SAMHSA) of the U.S. Department of Health and Human Services (DHHS) and has been conducted by RTI International since 1988. It provides data on the use of tobacco, alcohol, illicit drugs (including non-medical use of prescription drugs) and mental health in the U.S. at both the national and state-level. Analysts often fit regression and other models to data from this complex survey. We provide a guide to analysts interested in fitting regression models using data from the NSDUH by providing them with scientifically defensible methods for handling missing item values in regression analyses (MIVRA). To this end, a simulation experiment was performed that evaluated several MIVRA methods using NSDUH data. Combining the results of a literature review and the simulation experiment, we offer advice to analysts on which methods are best to use in which situations. ","Multiple matrix sampling is a component of survey design that divides a questionnaire into subsets of questions, and administers these subsets to different subsamples of respondents, so that data are missing completely at random by design. The promise of the method is in reducing survey burden and its associated problems such as unit and item nonresponse, and satisficing. There are two matrix sampling design challenges. The first is to determine the optimal allocation of individual items or scales to each subset of questions or questionnaire form. The second is to determine the optimal number of completed forms per subsample to achieve sufficiently stable survey estimates. This paper provides the power analysis framework for regression modeling with matrix-sampled data, and derives the asymptotic variances of regression estimates that use full information maximum likelihood estimation methods, such as structural equation modeling with missing values, or multiple imputation. Using selected psychological traits from the Big Five Inventory as an example, we demonstrate how to obtain optimal multiple matrix sampling plans that maximize the precision of the regression estimates, discuss sensitivity to the assumptions, and implications for survey practice. ","Ratio estimation is often useful for Official Statistics regarding energy, and for agriculture, econometrics, and perhaps many other applications in business, social science, and other areas. Notably, ratio estimation is very often useful for highly skewed establishment survey populations where, per Brewer(2002), mid-page 111, there should be at least as much implicit heteroscedasticity as for that of the classical ratio estimator (CRE). The concepts of design-based and model-based ratio estimation and sampling are reviewed and compared.  Meaningfulness might be enhanced by understanding this comparison.  Note that here the design-based case is actually model-assisted, but is being contrasted with the strictly model-based methodology, where probability of selection does not enter into the estimation, actually 'prediction,' of totals, and may or may not be used for sample selection.  These model-based and design-based interpretations of the CRE, their corresponding concepts of variance and bias, with relation to sampling and estimation, are reviewed, and extensions of these estimators are also considered. Simple random sampling, cutoff, and unequal probability of selection methodologies are of interest. Stratification is often highly useful with either approach.  Even if a regression model is not explicitly considered, this review considers the role it still plays.  The relationship of heteroscedasticity, explicitly addressed in model-based estimation, to cluster sampling for unequal-sized censused clusters, is a point of interest: Each observation in a model-based sample may be treated as a cluster unit for which we have a census.  (Again, see Brewer(2002), mid-page 111.) ","Occupation coding, an important task in official statistics, refers to coding a respondent's text answer into one of many hundreds of occupation codes. To date, occupation coding is still at least partially conducted manually, at great expense. We propose three methods for automatic coding: combining separate models for the detailed occupation codes and for aggregate occupation codes, a hybrid method that combines a duplicate-based approach with a statistical learning algorithm, and a modified nearest neighbor approach. Using data from the German General Social Survey (ALLBUS), we show that the proposed methods improve on both the coding accuracy of the underlying statistical learning algorithm and the coding accuracy of duplicates where duplicates exist. Further, we find defining duplicates based on ngram variables (a concept from text mining) is preferable to one based on exact string matches. ","In 2016, a probability sample of Oregon drivers was selected to participate in a satisfaction survey for the Oregon Department of Transportation. This study was conducted by the Survey Research Center at Oregon State University using a stratified sampling design. The survey was delivered using a mixed mode methodology, which randomized the sampled population into a web + mail group and an all mail group. In addition, within each of these groups, the effect of question order was examined. Two versions of the questionnaire were used to study the order of a general and specific question (i.e., examing a contrast effect)  to determine whether or not the influence of question order was consistent between Web and paper respondents. Our findings show that responses for overall satisfaction are different between surveys where item order is switched. We now wish to will examine whether or not these results change when considering different survey modes. This poster will use chi-square tests to investigate the interaction between question order and other factors of interest-namely, delivery mode and demographic information-and their influence on overall satisfaction.  ","The Neighborhood Environment Survey was designed for the purpose of estimating the relationship between a household's aircraft noise exposure and the probability that the household reports being highly annoyed by aircraft noise. Unlike most surveys, in which at least some of the primary outcomes are means or totals, here the primary outcome is a logistic regression function that estimates the dose-response relationship. The first stage of the design uses balanced sampling to select airports. The household selection at the second stage of sampling takes a stratified sample of households from an address-based sampling frame. At most airports, there are relatively few households that are close to the airport and have high noise exposure levels, but there are many households that are farther away from the airport and have low noise exposure levels. We use optimal design results of Chaloner and Larntz (1989) to inform the second-stage sample allocation to noise exposure strata at each airport. The resulting sampling weights for households have high variation, and we discuss the choice of weights for estimating the dose-response curve and for nonresponse adjustments. ","Researchers can be challenged by data sets published at incongruent levels of aggregation. However, there exists the need to combine such data while maintaining its integrity and geographic relationships. We explore two approaches with trade-offs in accuracy and efficiency, with a focus on ZIP codes and US Census tracts. Our first method uses geographic information systems (GIS) to weight tract-level data from the American Community Survey (ACS) based on spatial overlap. The weights are the percent of area overlap for each tract intersecting each ZIP code. This method avoids the duplication of data caused by allocating all of a tract's data to each ZIP code it intersects and allows for a more nuanced distribution of data over matching the tract centroid to the ZIP code. Secondly, we describe a framework that uses calibration techniques to estimate overlapping regions based on published margin totals. The overlap proportion is used to allocate a portion of each tract to the ZIP code. Exploratory analysis provides insight into the strengths and weaknesses of each approach. ","Nonresponse occurs when a sampled subject fails to complete either part or all of a survey. It can be classified as either item nonresponse, if the subject does not respond to certain survey questions, or as unit nonresponse, if the subject fails to participate in the entire survey. Nonresponse reduces sample size and study power and can cause biased estimates if significant differences exist between those who respond and those who do not. The usual approach to unit nonresponse bias detection and adjustment is to utilize weighting adjustment methods that are based off auxiliary information, or more recently, use propensity score weighting. In this study, we examine the impact of link misspecification on propensity score weighting when used in unit nonresponse adjustments through various simulations. Researchers should investigate the link that best fits their data before using propensity score weighting.    ","Survey nonresponse has been increasing in the National Health and Nutrition Examination Survey (NHANES). Adjusting sample weights for nonresponse may reduce the bias of health estimates.  However, as survey responses for non-respondents and the reasons for non-responding are not available, both the magnitude of nonresponse bias and whether sample weight adjustment reduce bias are unknown.  NHANES collects participants' information in three stages: screening at home, interviewing at home, and examination in a mobile examination center (MEC).  The screening stage determines survey eligibility.  Demographic, socioeconomic, and health-related information is obtained in the home interview. In this simulation study, nonresponse models using interview information are used to create informative missingness.  Using the simulated nonresponse data, sample weights with nonresponse adjustments based on the screening information are created.  To evaluate whether sample weight adjustments reduce bias, the weighted health estimates from the simulated data are compared to those from the whole sample. ","Background: We sought to develop and validate an assessment tool using behaviorally descriptive response items to measure effectiveness of physicians interpersonal and communication skills (ICS). Methods: A multistep validity-driven approach was used: Literature review findings, structured input by subject matter experts and end-user focus group review used for questionnaire development. Psychometric analysis included exploratory factor analysis (EFA), confirmatory factor analysis (CFA), and item analysis. Results: First order EFA from 389 evaluations yielded a 2-factor model (RMSEA = 0.033; CFI = 0.995): 6 items in factor 1 (physician engagement in the care process) and 7 in factor 2 (physician information delivery). To test factor structure and scale quality, CFA was conducted on calibration (n=1222) and validation (n=1190) samples. A 2 factor structure was confirmed with satisfactory fit for both samples (RMSEA ? 0.051, TLI=0.98 CFI =0.98, WRMR ? 0.98). Cronbach's ? was 0.91 for factor 1 and 0.85 for factor 2. Conclusion: This qualitatively and statistically validated assessment tool is a reliable tool for pediatric patient caregivers to evaluate physicians ICS. ","We explore different strategies for combining identifying assumptions to handle nonignorable missing data.  In the first strategy, the main idea is to use a sequential identification procedure, whereby we specify potentially different missingness mechanisms for different blocks of variables.  In its most general form, this strategy consists in expanding the observed-data distribution sequentially by identifying parts of the full-data distribution associated with blocks of variables, one block at a time. In the second procedure we exploit the availability of auxiliary marginal information on the distribution of some of the variables recorded in the sample, which allow us to identify missingness mechanisms where the nonresponse for these variables may directly depend on the value of the variables.    ","The fundamental non-verifiable nature of the missing-data mechanism is discussed, in selection, pattern-mixture, and shared-parameter frameworks. It is shown how, in all three frameworks, MAR can be formulated to provide a useful analysis starting point. From there, parametric, semi-parametric, and non-parametric sensitivity analysis routes are discussed. It is indicated how non-parametric bounds can be usefully considered to provide a background for (semi-)parametric sensitivity analyses. ","In longitudinal clinical trials, one often encounters informative missingness. It is well-known that informative missingness introduces fundamental identifiability issues, resulting in unidentified intention-to-treat effects; the best one can do is conduct a sensitivity analysis to assess how much of the inference is being driven by missingness, anchored at some clinically meaningful baseline assumption such as missing-at-random (MAR). We introduce a class of baseline identifying restrictions, and argue that these assumptions are more appropriate for the analysis of non-monotone missingness than MAR. Deviations from baseline assumptions are handled through either transformation approaches or exponential tilting. The class of identifying restrictions so obtained results in a model which is nonparametrically unidentified, but is sufficient to identify the effects which are of clinical interest. We implement these ideas in a Bayesian nonparametric framework, where our identifying restrictions are shown to interact conveniently with posterior consistency. Advantages of our approach include a flexible modeling framework, access to simple computational methods, and theoretical support. ","Missing records are a perennial problem in analysis of complex data of all types, when the target of inference is some function of the full data law. In simple cases, where data is missing at random or completely at random, well-known adjustments exist that result in consistent estimators of target quantities.  Assumptions underlying these estimators are generally not realistic in practical missing data problems. Unfortunately, consistent estimators in more complex cases where data is missing not at random, and where no ordering on variables induces monotonicity of missingness status are not known in general, with some notable exceptions. In this paper, we propose a general class of consistent estimators for cases where data is missing not at random, and missingness status is non-monotonic. Our estimators, which are generalized inverse probability weighting estimators, make no assumptions on the underlying full data law, but instead place independence restrictions, and certain other fairly mild assumptions, on the distribution of missingness status conditional on the data. The assumptions we place on the distribution of missingness status conditional on the data can be viewed as ","Maximum likelihood multiple imputation (MLMI) is a form of multiple imputation (MI) that imputes values conditionally on a maximum likelihood estimate of the parameters. MLMI contrasts with the most popular form of MI, posterior draw multiple imputation (PDMI), which imputes values conditionally on an estimate drawn at random from the posterior distribution of the parameters. Despite being less popular, MLMI is less computationally intensive and yields more efficient point estimates than PDMI.  ","Fractional imputation has been developed as a general tool for handling missing data in survey sampling. We introduce some basic concepts of fractional imputation and fractional hot deck imputation. New R-package for fractional hot deck imputation will be also introduced. A comparison with multiple imputation will be presented using a real data example. ","Hot deck imputation is a nonparametric imputation method which replaces missing values with observed responses from \"similar\" units, often from within imputation cells. The hot deck has long been used by the U.S. Census Bureau and is one of the most common imputation methods for survey data. When the hot deck is used for single imputation, explicit variance formulae exist for simple cases that may not be realistic in practice. Resampling methods have also been proposed, including both jackknife and bootstrap approaches. However, when used as the basis for multiple imputation, hot deck imputation is improper, since simply drawing multiple donors per missing value from an imputation cell does not fully propagate uncertainty. Modifications to the hot deck have been proposed that make it proper, including the Bayesian Bootstrap and the Approximate Bayesian Bootstrap. In this talk I will review and contrast these various strategies for obtaining valid standard errors after hot deck imputation through a detailed example. I will also discuss ongoing work to use the newly proposed maximum likelihood multiple imputation (MLMI) in the context of hot deck imputation. ","Record linkage, as a research topic, is approaching its semicentennial anniversary.  Over its lifetime--and specifically, during the past few decades--record linkage has become a cross-disciplinary research field. However, as often happens when different scientists approach the same problem, separate bodies of literature have grown concurrently. Gathering together the research related to record linkage requires an understanding of its many alternative names: entity linking, entity resolution, probabilistic matching, duplicate detection, data linking, and, of course, record linkage, among others. In addition, subfields related to record linkage have also developed, such as privacy preserving record linkage (PPRL), informed consent for record linkage, temporal record linkage, and differential record linkage. This paper will attempt to provide as comprehensive a review as possible of the literature related to record linkage across its many disciplines.  ","The Fellegi-Sunter record linkage paradigm specifies the functional relationship between agreement probabilities and match weights for each identifier. This paradigm assumes conditional independence of identifier agreements. However, many identifier agreements are, in fact, dependent. For example, within the set of non-matched pairs, if we know the first names agree, then it is more likely that last names also agree since name distributions vary by ethnicity. In this paper we present an approach to specify and estimate agreement probabilities, the relationship between them, and the total number of links without the use of training data. This, in turn, yields estimates of match rates (i.e., the proportion of matches among a set of pairs) for a given identifier agreement pattern. ","Many data sets, like surveys, are publicly available for analysis. Linking such public data sources to internal or private data sets allows richer analysis to be performed. Without common identifiers across the two files, linking often involves matching on a set of variables common to both files. However, data quality concerns, such as inaccurate field values or missing data, can hinder the linking process. We present a Bayesian file linking methodology designed to link records using continuous matching variables, called MVs, in situations where we do not expect values of these MVs to agree exactly across matched pairs. The method involves a linking model for the distance between the MVs of records in one file and the MVs of their linked records in the second. This model is conditional on a vector indicating the links. We specify a mixture model for the distance component of the linking model, as this latent structure allows the distance between matching variables in linked pairs to vary across types of linked pairs. Finally, we specify a model for the linking vector. We use the approach to link public survey information and data from the U.S. Census of Manufactures. ","The National Center for Health Statistics (NCHS) is the principal health statistics agency in the U.S. Its mission is to provide statistical information that can be used to guide actions and policies to improve the health of the American people. NCHS conducts several population-based and health-care establishment surveys that allow the publication of widely-used, reliable statistics on the health status of the U.S. population and selected subgroups.  ","Spatial smoothing models play an important role in the field of small area estimation (SAE). In the context of complex survey designs, the use of design weights is indispensable in the estimation process. Recently, efforts have been made in these spatial smoothing models, in order to obtain reliable estimates of the spatial trend. However, the concept of missing data remains a prevalent problem in the context of spatial trend estimation as estimates are potentially subject to bias. In this paper, we focus on spatial health surveys where the available information consists of a binary response and its associated design weight. Furthermore, we investigate the impact of nonresponse as missing data on a range of spatial models for different missingness mechanisms and different degrees of missingness by means of an extensive simulation study. The results show that weight adjustment to correct for missingness has a beneficial effect on the bias in the missing at random (MAR) setting for all models. Furthermore we estimate the geographical distribution of perceived health at the district level based on the Belgian Health Interview Survey (2001). ","Assuming  sample survey framework of two domains, one quantitative auxiliary variable with lognormal distribution, ratio-synthetic estimator was proved to be more efficient than BLUP under mixed model and unconditional inference. Similar results were obtained for sample surveys under unified model.  In household surveys methods for estimation of total households and persons in domain of interest, which can be used for ratio-synthetic estimation in survey practice, were reviewed (Ghangurde, P.D.;JSM(2016)). In this paper the results on efficiency are extended to sample surveys in which post-stratification of sample units is done. For example, in a sample survey of retail trade sector with NAICS code 44711 value of annual sales is x-variable and employee salaries and wages is y-variable in the model; sample establishments are post-stratified as small (less than 100 employees) and medium (100 to 500 employees) using survey data. Efficiencies  of ratio-synthetic estimator, assuming various sample sizes in domain, total sample size and values for parameters in the model, are obtained by unconditional inference assuming lognormal x-variable with unequal medians in post-strata.         ","Estimates of agricultural cash rental rates at different geographic levels are used for rental agreement formulation, farm program administration and other applications. The USDA's National Agricultural Statistics Service (NASS) has been producing estimates of average cash rental rates at the state level since 1997. In 2009, the agency fully implemented an annual Cash Rents Survey (CRS) to obtain estimates of average cash rental rates for counties with at least 20,000 acres of cropland or pastureland in three land use categories. Since sample sizes for a number of counties were too small for reliable direct estimation, NASS developed a model-based estimation method (implemented in 2013) that incorporates auxiliary information and involves separate area-level modeling of the average and difference of rental rates over two survey years. With the 2014 Farm Bill specifying that the survey be conducted \"no less frequently than every other year\", the current focus is on evaluating how this estimator would perform with survey data from two years apart (when the survey is skipped in the intervening year). We describe the results of an empirical study addressing that question and discuss future directions for research.   ","The American Community Survey (ACS) publishes annual poverty estimates for large counties. In response to the demand for poverty estimates for U.S. counties and school districts, the Census Bureau's Small Area Income and Poverty Estimates (SAIPE) program produces poverty estimates for all U.S. counties and school districts from area-level Fay-Herriot models. This paper focuses on estimating county-level poverty rates for the year 2014, using the unit-level logistic mixed effects model. We fit an unweighted multilevel logistic regression (MLR) model with demographic predictors and state- and county-level random effects. To account for the design of the ACS survey, we consider models with weights scaled based not only on the person sample size (PSS), but also on the housing unit sample size (HUSS). Given the individual demographic characteristics, we estimate the predicted probability that an individual is in poverty. An aggregation within county is made to generate the corresponding county-level poverty rates using the U.S. Census Bureau postcensal population estimates. In comparing ACS direct estimates, SAIPE estimates, and the estimates from the MLR models, the distributions of all three estimates are similar for large counties. For small size counties, the distributions of the estimates produced via MLR models tend to have smaller ranges compared with the distribution of ACS direct estimates. MLR models with weights scaled based on the person sample size almost always yield estimates with smaller mean absolute differences with the ACS. ","Residual maximum likelihood estimation (REML) is a likelihood-based method of estimating error components of a linear mixed model (LMM) that yields consistent and asymptotically normal estimators even with unbalanced datasets. It is also known that the asymptotic variance of the error components is larger in the case of a unbalanced dataset under comparable conditions for the fixed effects. Ahrens and Pincus (1981) derived a measure of unbalancedness that is bounded between 0 and 1. We express the asymptotic variance covariance matrix of a one-way error component model (which has been variously derived by Jiang (1998), Searle (1970) and others) as a function of the Ahrens-Pincus measure to demonstrate the growth of the asymptotic variance of the error component as the degree of unbalancedness grows. These results are further demonstrated through  a simulation exercise.  ","Repeated surveys of a population over time can provide reliable estimates of many population quantities, as well as on changes over time. However, there is often a significant delay between a survey being administered and the release of a final data product. Other, less statistically robust data sources are often available more quickly. Incorporating non-survey auxiliary information into a model may allow for providing more timely and more accurate predictions and estimates. We develop a unit-level Markov model for categorical data, which incorporates survey observations and auxiliary information. The model provides simple, closed-form estimates of transition probabilities between categories for each unit, and incorporates more recent auxiliary information with the survey data. Survey weights can be naturally incorporated to provide estimates of population totals. We demonstrate the model on data taken from the Natural Resources Inventory, using the Cropland Data Layer for auxiliary unit level information.  ","The Annual Survey of Public Employment &amp; Payroll (ASPEP), conducted by the U.S. Census Bureau, provides statistics on the number of federal, state, and local government civilian employees and their gross payrolls. The universe of ASPEP is about 90,000+ state and local government units. Every five years Census Bureau conducts a Census of Governments: Employment. Between two consecutive censuses, Census Bureau conducts the ASPEP, a nationwide sample survey covering all state and local governments in the United States.  The ASPEP survey is designed to produce reliable estimates, for example, the number of full-time and part-time employees and payroll at the national level for large domains.  However, it is also required to estimate the parameters for individual function codes within each state. This requirement prompted us to develop a methodology that employs Small Area Estimation (SAE) methods.  The outlier treatments (Trinh &amp; Tran, JSM 2017) will be discussed briefly in this research to improve the quality of the estimates. ","The Sampling Program for Survey Statisticians (SPSS) was founded by Leslie Kish in 1961 to train statisticians, especially from developing countries, on survey sampling methods. The SPSS has trained more than 1,000 participants from more than 110 countries. This presentation is to highlight the role of the SPSS in strengthen the sampling-related capacity of survey statisticians from all over the world. We will present results of a survey conducted from a sample of the SPSS participants. The survey explored how the SPSS contributed in their career and life in general. ","Although commonly used in many surveys as a strategy to deal with unit nonresponse, substitution is frequently criticized and has received very little attention by survey researchers. In fact, there is evidence to suggest that the performance of substitution as a strategy to mitigate nonresponse is comparable to other adjustment methods, such as weighting or imputation (Vehovar, 1999; Rubin and Zanutto, 2002). However, as with many other nonresponse adjustment methods, research on and applications of this method has been limited to ignorable nonresponse mechanisms. This paper presents a substitution procedure that incorporates a nonignorable nonresponse mechanism in the selection of the substitutes of the nonresponding units through the use of pattern-mixture models. This method can be employed to perform sensitivity analysis of a range of missingness models using additional real data of the substitutes, as opposed to other methods that use predicted values under a model or data from hot-deck donors already present in the responding sample. This methodology is evaluated and compared to other nonresponse adjustment methods through a simulation study. ","Cancer surveillance research requires estimates of the prevalence of cancer risk factors and screening for small areas such as states and counties. To obtain such estimates, Raghunathan et al (2007) has developed an approach to combine two national surveys, the BRFSS and NHIS where the population in each small area was divided into two strata, households with landline telephones and those without. Estimates from 1997-2000 were obtained from a Bayesian hierarchical model. The potential noncoverage and  noresponse biases in the BRFSS were took into account. When applying this approach to newer data in the year of 2004-2006, we had to consider another stratum of the population, the cell-only households. We extended the above approach to obtain estimates for three sub-populations for each small area and weighted them by the proportions in each sub-population. The proportion of cell only households, landline households, and no landline or cell phone households for the small areas were obtained and the variances of these estimates were incorporated into the final  small area estimates on cancer risk factors and screening via Markov Chain Monte Carlo method.  ","Public health surveys often seek to measure racial disparities in health conditions, but estimates related to race can be clouded by the effects of interviewers. This paper focuses on the effect of interviewers generally (i.e., interviewer variance) and interviewer race on estimates of health characteristics in a survey of rural African Americans. Interviewers were assigned to respondents using a semi-interpenetrated design allowing estimates of interviewer variance to be mostly disentangled from geographical sample point effects. Questions in the survey (n = 292) were rated for their racial sensitivity by 6 independent reviewers who had a range of ethnic, racial, and linguistic backgrounds. Interviewer effects were assessed for racially-sensitive and racially-nonsensitive questions. Results show that interviewer race affects responses to a range of survey questions, particularly those that are racially-sensitive (e.g., reports of discrimination), but also those that are racially-nonsensitive (e.g., self-rated health). Surprisingly, it also affected blood pressure readings, suggesting a possible physiological link in the mechanism of interviewer effects.  ","The National Survey on Drug Use and Health (NSDUH) provides annual data on alcohol use, tobacco use, illicit drug use, substance use disorders, mental health, receipt of services for behavioral health conditions, and other related measures of interest (e.g., risk perceptions related to substance use). When analyzing cross-sectional survey data such as NSDUH's, the \"trend\" depicts the general underlying pattern of change of an outcome variable over time in a finite population. This presentation will summarize the current methods used for analyzing trends in NSDUH and the trend testing methods used by other federal agencies. A variety of methods for trend analysis has been used, such as pairwise testing and statistical regression. However, no formal guidelines have been available for NSDUH and other studies on how to select an appropriate method in terms of fitness for use under certain constraints. Although the literature contains alternative methods, open questions remain about inclusion of survey design features into complex regression and time series models, and the sensitivity of analyses to outliers, moving time windows, and the availability of new data each year. ","The NSDUH produces and monitors annual prevalence estimates which are comparable over time in the population. Assessing the significance of changes across time is often referred to as trend analysis or trend testing. The trend testing literature and the current practices for NSDUH and other federal data collections were thoroughly reviewed. Emphasis was placed on trend testing practices for cross-sectional designs from a complex sample. Then, a series of simulations with ten years of NSDUH data were conducted to compare the statistical properties (e.g. classification error rates) of the most common trend testing approaches as identified through the review. These include pairwise testing that compares the current year estimates with prior year estimates, linear and quadratic orthogonal contrasts, statistical regression with time-dependent covariates, and joinpoint regression method. Outcome variables with a wide range of prevalence in the population were considered. The goal of this study is to facilitate the choice of trend testing methods for different statistical products in terms of credibility, ease of implementation and interpretation, and time implications.  ","Annually since 1973 the National Crime Victimization Survey (NCVS), and its predecessor the National Crime Survey (NCS), have collected self-report victimization data. Typically trend analyses have been restricted to data collected after 1992, when the survey underwent a large methodological redesign. However, in 2012 the Bureau of Justice Statistics (BJS) and Research Triangle Institute (RTI) initiated the Historical Trends project to integrate the historical NCS and contemporary NCVS into a single cross-sectional concatenated data file spanning 41 years. The project was a methodologically complex multiyear undertaking involving challenging decisions on-data linkage, variance estimation, continuity of variables over time, and importantly, analytical approaches for discussing 40-year trends in a meaningful way. As data collections from federal statistical agencies like BJS continue to mature, there will be increased demands to apply formal methods suitable for analysis of long term trend data. This presentation examines the use of Joinpoint Analysis to analyze long term trend data on violent and property victimization. Considerations and implications of using JPA are discussed. ","One challenge with conducting trend analysis for long running studies is organizing and adjusting the data to account for changes over time to key measures of analysis. The Bureau of Justice Statistics (BJS) has been collecting data on personal and household victimization since 1973 through the National Crime Victimization Survey (NCVS) and its predecessor, the National Crime Survey (NCS). During this time, the NCVS and NCS have undergone a number of survey protocol and questionnaire changes. In addition, although the victimization survey data are publicly available, working with more than 40 years of data can be challenging, making it difficult for researchers to generate historical estimates. To alleviate this burden on analysts, BJS undertook the NCVS Historical Trends (NHT) project to provide BJS stakeholders with ready access to the entire series of victimization survey data. We discuss the challenges faced during this undertaking, including the development of common constructs to account for questionnaire changes, the handling of missing and/or incorrectly coded data, and adjustment methods to account for a break in the comparability of estimates. ","This paper examines nonresponse in a longitudinal establishment survey by using regression trees to model the effect of time in survey on data collection response propensities. We built regression trees using the R Package \"Recursive Partitioning for Modeling Survey Data (RPMS),\" which fits a linear model to the data conditionally on variables selected through recursive partitioning. The RPSM tree model provides estimates for the average response rate (across time), the intercept (initial response rate), and the slope (the effect of time in survey) for each end node. Using the RPSM package, we modeled the effect of time in survey on response rates in the Job Openings and Labor Turnover Survey (JOLTS) of the Bureau of Labor Statistics, so we could identify establishment characteristics and subgroups of establishments that were least likely to respond throughout the data collection process and thus determine which establishments required more follow-up over the course of the survey to ensure that they continue responding.  ","The National Youth Tobacco Survey (NYTS) is a cross-sectional, school-based, self-administered, pencil-and-paper questionnaire administered to U.S. middle school and high school students. A three-stage cluster sampling procedure is used to generate a nationally representative sample of U.S. students attending public and private schools in grades 6-12.  A probability based sample is used at each of the three stages: primary sampling units (a county, part of a county, or groups of small counties), schools, and classes. This paper presents the results of a review of the current NYTS sampling methodology, including a summary of potential refinements. The review assessed sampling design features such as sampling stages, sampling units, stratification, allocation, and subgroup sample sizes. Two identified areas for refinement include: 1) the ability to expand the sampling frame without negative impacts on school participation rates or the current budget; and 2) the feasibility of developing precise sub-national estimates by grade, sex, and race/ethnicity for middle and high school separately. These findings could help enhance the accuracy and utility of future waves of NYTS. ","Online self-reported surveys present an alternative to traditional collection modes. As such, Statistics Canada is gradually moving to online surveys for the majority of its household surveys. This new collection mode however does present challenges in the need to randomly select a household member to complete the survey. Currently, an initial paper invitation is sent by mail and the household membership selection is done through rostering. If the selected member is not the person who completed the roster, a handing-off of the survey to the selected person is needed, which may however increase the survey's non-response rate. Alternative methods for individual selection before accessing the online application are proposed to avoid this hand-off request and two of them were tested: the last birthday method and an age-order method. The selection is done using instructions on the paper invitation so that only the selected household member will have to access the online application. Response rates and selection inaccuracy rates were compared between the two alternative methods as well as with the traditional roster method. Results of the comparisons will be presented and discussed.  ","There is no standard method for estimating fare evasion and fare compliance in North American transit agencies. Most transit agencies report on fare evasion as discovered by fare enforcement officials. Fewer transit agencies report on randomized sampling of transit users to estimate fare evasion and fare compliance. Reporting of the methodologies and results of the various sampling techniques are not shared widely. This paper examines the attempt of a North American transit agency to estimate fare evasion and fare compliance rates using a multi-stage stratified cluster sampling methodology of its light rail transit lines. A baseline estimate was performed in 2014. Sampling was conducted again in 2016. This paper reports on the results of the sampling methodology and provides recommendations for implementing statistically sound fare evasion sampling in transit agencies. The substantive results of the study varied depending on the type of ticket/pass, time of day, rail line, zone, location and direction of travel. ","Survey researchers are often confronted with the question about the length of the field period. A longer fielding time can increase participation but it requires more effort from survey managers. In this paper we study how sample composition changes during the field period. Our main research question is whether a longer fielding time reduces the risk of nonresponse bias. We use data from the GESIS Panel, a probability-based mixed-mode panel of the general population in Germany (N=4938). We analyze online and offline samples, in which respondents were invited to bi-monthly surveys with a field period of 2 months. Using the information from the recruitment interview and the first self-administered survey we predict panelists' response propensities. We then analyze how nonresponse bias indicators vary over the field period using coefficients of variation and R-indicators. We find differences in participation propensities by mode: underrepresented propensity strata begin to increasingly participate online after about 2 weeks and offline after about 40 days. Hence, decreasing the length of the field period would increase the risk of bias in the online mode more than in the offline mode. ","Example of \"doubly robust\" estimators for missing data  include augmented inverse probability weighting (AIPWT) models (Bang and Robins 2005) and penalized spline of propensity prediction (PSPP) models (Zhang and Little 2009).  Under missing at random (MAR) mechanisms, consistent estimation of a population mean can be obtained by weighting by the inverse of the probability of response conditional on observed covariates, or by prediction from those covariates.  Double robustness refers to models in which if either the response propensity or the mean is modeled correctly, a consistent estimator of a population mean is obtained.  Doubly robust estimators can perform poorly when modest misspecification is present in both models  (Kang and Schafer 2007).  Here we consider extensions of the AIPWT and PSPP models that use Bayesian Additive Regression Trees (BART; Chipman et al. 2010) to provide highly robust propensity estimation.  We consider their behavior via simulations where propensities and/or mean models are misspecified, and show that BART applied to PSPP provides efficient and robust estimation.  We also consider an application to the National Automotive Sampling System (NASS). ","A good imputation model leverages relationships in the complete data to make predictions for missing values. However, there is some disagreement about how to handle imputed values in analyses when the imputation is driven by a single strong predictor. One common situation is when subjects are assessed at two time points (T1 and T2), but some subjects are missing scores at one or both time points. Other auxiliary data are available for all subjects. The T2 score for each subject is typically the strongest predictor of the T1 score in the imputation model, but there is concern about \"circularity\" if the planned analyses then use the T1 score to predict the T2 score. Suggested approaches in the literature include a multiple imputation then deletion (MID) approach, where all missing values are imputed but observations with imputed outcomes are dropped from analyses; or using all observations (AO), including those with imputed outcomes, for analyses following imputation. This paper investigates the conditions under which circularity may be a concern, studies the performance of the MID and AO methods under different settings, and makes recommendations for practice. ","Standard applications of multiple imputation (MI) techniques are based on the assumption that the data are Missing at Random (MAR). However, in many situations it seems very realistic that the missing values follow a Missing Not at Random (MNAR) mechanism. In this case, usual implementations of MI are not sufficient either and may lead to biased estimates. We will present an approach to multiply impute non-ignorable binary missing data in the framework of Fully Conditional Specification (FCS). The suspected MNAR mechanism will be considered and modeled during the imputation process by applying a censored bivariate probit model as imputation model. For allowing the consideration of a present multilevel structure in the data during the imputation process, the model is expanded by a random intercept term. In order to assess the performance of this imputation technique, different simulation studies were conducted. The method performed well in all considered situations in terms of coverage and bias and outperforms alternative methods. Finally, in order to evaluate its applicability the approach is employed on empirical data of the National Educational Panel Study (NEPS). ","National health surveys, such as the National Health and Nutrition Examination Survey (NHANES), are used to monitor trends of nutritional biomarkers. These surveys try to maintain the same biomarker assay over time, but there are a variety of reasons the assay may change. In these cases, it is important to evaluate the potential impact of a change so that any observed fluctuations in concentrations over time are not confounded by changes in the assay. To this end, a subset of stored specimens previously analyzed with the old assay are retested using the new assay. These paired data are used to estimate an adjustment equation which is then used to 'adjust' all the old assay results and convert them into 'equivalent' units of the new assay. In this paper, we present a new way of approaching this problem using modern statistical methods designed for missing data.  Using simulations, we compare the proposed multiple imputation approach with the adjustment equation approach currently in use. We also compare these approaches using real NHANES data for 25-hydroxyvitamin D. ","Income and asset (IA) survey questions are known to have a high non-response rate. To mitigate this, surveys often allow respondents to report a value-range in lieu of an exact value. Still, most population based surveys implement item non-response imputation based on an appropriate respondent donor pool to enhance IA data quality. Prior research of elderly populations has suggested that those who provide value-range responses to asset questions have higher asset values than exact-value responders. Moreover, asset value-range respondents are more representative of item non-responders. We will examine whether IA exact-value respondents differ from value-ranges respondents in IA-related key demographic variables in the Medicare Current Beneficiary Survey (MCBS), a continuous, multipurpose survey of a nationally representative sample of the Medicare population that implemented a new IA questionnaire in 2015. In addition, we will compare IA item non-response imputation using a donor pool of all respondents versus only value-range respondents and see if restricting the donor pool would increase the imputed IA means. Outcomes will provide guidance for future MCBS IA imputation. ","We have collaborated on several surveys and studies in the area of dental research in the past decade. These include studies on the efficacy of a prenatal oral health program for mothers and their children, the use of sealants on molars of school children in Jamaica, the impact of measures to address stress in a Dental College, a comparison of school-based and community-based dental clinics, and a study of simple predictors of carious teeth in children. The data sets involved in these analyses had small amounts of missing data. Missing data can impact analyses through causing bias in and increasing variance of estimators. In these studies, there usually are variables that are correlated with missing outcomes and predictor variables which can be used to build models for imputation of missing values. These models range in type from parametric statistical models to procedures for matching subjects to find donors. Another approach, related to survey post stratification, to addressing missing information is to weight the respondent data within categories of subjects. In this paper, methods of handling missing data are studied through simulations based on contexts of published studies.   ","The Truth Initiative Longitudinal Cohort Study is designed to evaluate the impact of a television and digital campaign on youths' smoking-related knowledge, attitudes and beliefs, perceived social norms, and behaviors over time. The study administers surveys to participants over six waves between 2014 and 2017 and uses multivariate statistical models to evaluate the effectiveness of the media campaign. The survey is subject to nonresponse, which can bias estimates for the evaluation. We evaluate different methods of imputing missing data in the context of a longitudinal study. Hot deck and model-based approaches are compared for both their performance and practicality, and multiple imputation is used to account for the uncertainty in estimates due to missing data. To evaluate different imputation approaches, we use the framework of He and Zaslavsky (2012), which involves duplicating the dataset and comparing the distributions of observed and imputed data. ","Four publicly reported composite measures (care of patients, communication between providers and patients, specific care issues, and overall rating of care) are used to produce an overall summary measure for each agency. The \"star\" rating assignment process consists of three main steps: 1) computing the Euclidian distance of the measure scores between each pair of home health agencies, 2) grouping the measure scores into an initial set of clusters using Ward's minimum variance method, and 3) refining the selections to produce a final set of clusters. This presentation will describe the methodology used to produce five-star quality ratings for home healthcare agencies. Additionally, a review of the steps used to evaluate the ratings will be presented including a brief review of clustering methods considered, determining if five groupings was a good fit, and testing size limits for health care agencies. Potential improvements that may be taken will also be presented.  Data from the Home Health Consumer Assessment of Healthcare Providers and Systems (HHCAHPS) is used to demonstrate the clustering methodology.  ","The recent tendency of growing cost and nonresponse of traditional randomized surveys and rapid proliferation of web surveys and administrative data calls for developing a standard framework for inferences from nonrandom data samples. Approaches relying either on a propensity score model or on a predictive model of an outcome variable are overly sensitive to model assumptions. This paper proposes to: (a) supplement an initial nonrandom sample with a reference random sample, having missing detail target variables but containing core covariates shared with the nonrandom sample; (b) define imputation classes using both propensity and prediction scores, and impute target variables from the nonrandom to the random sample; and (c) use a delete-a-group version of the adjusted jackknife variance estimator, proposed by Rao, Shao (1992) for imputed data. Since imputation classes are defined by both propensity and predictive models, the proposed framework exhibits double-robust property against misspecification of either model. Reference samples, complete with imputed data and jackknife replication weights, can be released to end-users as public use files, allowing for any kind of inferences. The proposed paradigm for inferences from nonrandom samples may legitimize their use in official statistics.  ","Hinkins et al. (1997) introduced inverse sampling as a way to aid analysts navigating complex sample designs. One intent was to provide users a set of inverse samples that could each be analyzed using methods designed for simple random samples and then combined for inference. These techniques assume one has knowledge of the complex sample design and can properly invert the sample. For public use data, unless inverse samples are provided, a data user would be hard-pressed to create inverse samples based on the complex sample design. Our current research empirically investigates the performance of inverse sampling by comparing the resulting estimates to estimates that use the original sample and incorporate the survey design properly. We study the performance of inverse sampling both when the sampling design is known and when only the survey weights are known and thus only approximate inverse samples can be obtained. The results show that inverse sampling performs well for producing unbiased estimates of model coefficients, but caution is needed for estimating standard errors. ","The Heckman selection model proposed by Heckman (1979) has been used extensively in economics to correct selection bias. It is closely related to nonignorable nonresponse problems. In previous literature, inference based on the Heckman selection model with complex survey data has not been rigorously studied. Results from the Heckman model by assuming simple random sampling can be biased and misleading. In this paper, we rigorously studied theoretical properties of estimates from Heckman model by incorporating sampling design features. In addition, we propose an efficient weight smoothing approach to further improve efficiency of the estimates under informative sampling. Simulation studies assess the impact of this approach and show the benefits of our proposed methods. Empirical study has been done by using a real data application.  ","Familial aggregation is considered as an important aetiology of disease so that many studies focus on the analysis of familial aggregation. In recent studies, marginalization approach is considered as a better method to analyze the familiar aggregation of varying family sizes. The purpose of our research is to combine the marginalization approach with complex survey design, analyzing the familial aggregation for the families with different familial relationships and varying family sizes. Weights are added to recent model to obtain the parameter estimators and the robust variance estimators. The recurrence risk is what we use to represent the familial aggregation. We apply our model to diabetes disease data collected by the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC) in 1976. Network sampling method is used and propensity score weighting is also applied to adjust for the compounding variables. Also, simulation studies are conducted to examine the parameter estimates and variance estimators in the weighted model. ","We previously reported that age-specific mortality of participants in the National Health and Nutrition Examination Survey (NHANES) 1999-2010 differed by survey cycle (better survival for later survey cycles), which was attributed to the longer followup time for early survey cycles rather than a true secular trend. In the current work, using Cox regression, we explore alternative methods to adjust for the followup time-mortality association, which was found to be more pronounced for older age during followup, in the analysis of blood lead and cardiovascular disease (CVD) mortality. We examined whether time-related factors including followup time (and its functions interacted with age during followup), age at followup start, calendar time at followup start, age during followup, and calendar time during followup) operate as effect modifiers of the blood lead-CVD mortality association. Our preliminary analyses indicate that the time-related factors are not effect modifiers of blood lead-CVD mortality association. ","National tracking polls and state-specific polls have long been a popular method of forecasting national voting intention for elections in the United States. The 2016 US Presidential election introduced the widespread use of high-volume national tracking polls, which attempted to infer voting intention estimates at the state level. Such tracking polls relied on rim or post-stratification and achieved poor results. Thus indicating that corrective methods, which pollsters typically rely on, would not alone yield accurate estimates. This paper presents multilevel regression with poststratification (MRP) as a tool for reducing such error. We examine data from the Reuters and Google mega-polls to compare different techniques for adjusting survey data. Using the Stan language we are able to implement a fully Bayesian regression models and compare the results to the estimates produced by traditional methods. As expected, we find that MRP produces significant accuracy gains at the state level over traditional methods. Additionally, we explore ways of integrating external information into the prior to enhance MRP; something that is not possible through empirical Bayes MRP. ","The 2010 Census Coverage Measurement (CCM) program measured the coverage of the 2010 Census enumeration of persons and housing units. The CCM independently listed housing units and persons in a sample of geographies. Census and CCM listings were compared and field operations were conducted to resolve differences. The Final Housing Unit Followup (FHUFU) was the last of the CCM field operations; it primarily processed late changes to the Census inventory of housing units. The research described in this paper assessed what the impact would be of not conducting the FHUFU field operation on the 2010 CCM estimates of housing unit coverage. It informs a decision whether to remove the FHUFU from the 2020 Coverage Measurement program. In the study, housing units which went to FHUFU for resolution were assigned unresolved match and housing unit statuses. These unresolved match and housing unit statuses were then imputed to simulate what would have happened if FHUFU had not been conducted. The modified data with the imputations were used to generate alternative estimates. The alternative estimates were compared to the 2010 Census coverage estimates to assess the impact of removing the FHUFU. ","Loh, Eltinge, and Cho (2016) demonstrated that the Generalized, Unbiased, Interaction Detection, and Estimation (GUIDE) software package and other classification and regression tree algorithms can be used to impute missing data. Advantages of the tree algorithms for imputation are that they are less sensitive to model assumptions because they are non-parametric in nature, and that they can more easily handle a large number of variables and potentially numerous interaction terms in the imputation model. We want to expand current knowledge of the emerging tree-based imputation technique by comparing its performance under actual and artificially generated population datasets with several existing software packages, especially including AutoImpute, which was developed by Westat. ","In multiple imputation (MI), the total variance (T) is estimated by U+(1+1/m)B, where U is the within-imputation variance, B the between-imputation variance, and m the number of imputations. The expected value of U is not affected by a proper MI, whereas the extra variance B can be captured only by MI but not by single imputation (SI). Whether B is large enough to cause a meaningful change in T may have an effect on people's perspective towards the value of MI as compared to SI. This paper evaluates how data analysis affects the impact of MI (IMI), measured as IMI = 100(B/T)1/2. MI trials were conducted using the data of the 2012 Physician Workflow Mail Survey. Difference in analytic models had differentiated effects on B and U. Our results suggest that, for the same MI and the same data, IMI may be negligible (&lt;1%) in one analysis but substantial (&gt;5%) in another.  ","The KDQOL-36 is a short form version of the Kidney Disease Quality of Life Instrument, including 12 items comprising the SF-12 Physical and Mental Component Summary scales (PCS and MCS). We administered the survey to 9,071 Medicare beneficiaries with end-stage renal disease participating in a quality improvement demonstration program.  At least one SF-12 item was missing for 21% of respondents, resulting in missing PCS and MCS scores.  We will compare three data imputation approaches: item substitution (substituting a correlated item from the same respondent); multiple imputation; and mean imputation, repeated for a maximum number of missing items ranging from 1 to 12.  We will simulate missing values from surveys with complete SF-12 data, replicating missing data patterns observed in the original data.  We will impute missing values, and compare true MCS and PCS scores against those calculated from imputed data.  Evaluation criteria will include bias, precision measured by mean square error, and sample size recovered.  We will evaluate the pros and cons of each approach, considering ease of implementation and replicability. ","The Medical Expenditure Panel Survey (MEPS) is an ongoing set of surveys that yield national estimates of health care use, cost, and insurance coverage. The MEPS Medical Provider Component (MEPS-MPC) collects data from providers of medical care to MEPS households. The RWJ Foundation sponsored the Medical Organization Survey (MEPS-MOS) to collect data on practice characteristics from MEPS-MPC providers that are usual sources of care. Surveys of medical care facilities are often subject to low cooperation rates. The MEPS-MOS data collection strategy can potentially improve cooperation and results will provide critical information for future provider and medical practice data collection.  This paper will present data on characteristics associated with unit nonresponse for organizations providing medical care to MEPS respondents; analyze factors that predict item nonresponse; and examine the quality of responses. External data sources will be used to obtain characteristics of non-responding providers and identify correlates of item nonresponse. Results of the survey will be benchmarked to external data sources.  ","     This paper analyzes estimates from the Household Component of the Medical Expenditure Panel Survey (MEPS) matched with the National Health Interview Survey (NHIS) and uses practical tools to inform MEPS nonresponse estimates. MEPS is a nationally representative panel survey studying health care use, access, expenditures, source of payment, insurance coverage, and quality of care sponsored by the Agency for Healthcare Research and Quality. Each year a new panel begins and each panel has 5 rounds of data collection over 2 \u00bd years. The MEPS sample is a subsample of the previous year's NHIS sample; NHIS is sponsored by the National Center for Health Statistics, Centers for Disease Control and Prevention (CDC).       The goal of this paper is to explain MEPS nonresponse rates for 2010 to 2012. We use demographic and other personal characteristics from NHIS and MEPS to predict MEPS nonresponse rates. Data used are from the 2009 and 2011 NHIS matched with the 2010 and 2012 MEPS files along with additional paradata.  ","After 26 rounds of data collection, attrition in the NLSY79 remains remarkably low. Over 77 percent of those still living participated in the round 26 interview in 2014-15. The most significant reason for the high retention rate in the NLSY79 is likely the innovation of attempting to interview all baseline sample respondents in each round. The NLSY79 also collects data on employment and other topics in an event-history format, which fills in important information if respondents miss an interview, but are then interviewed again in a later round. In logits examining the probability of participating in later rounds, we find that attrition from the NLSY79 is fairly random with respect to basic demographics and labor market behavior, marital status, and number of children at age 30. We also estimate the effects of educational attainment and other characteristics at age 30 on labor force participation, earnings, and family income at that age. We find that survey participation in the most recent rounds is not related to these outcomes. Attrition does not appear to lead to biased estimates in models of important economic relationships. ","This paper provides a comprehensive treatment on design-based empirical likelihood inferences for complex surveys with estimating equations. Our settings are very general: (i) The estimating functions can be smooth or non-differentiable, covering descriptive population parameters, linear and generalized linear models, population quantiles as well as quantile regression models; and (ii) The dimension of the estimating functions can be higher than the dimension of the parameters, allowing additional calibration constraints. Our theoretical results have two prominent features: (a) Asymptotic distributions of the empirical likelihood ratio statistics are derived for arbitrary sampling designs, with simplified results presented for single-stage unequal probability sampling; and (b) Empirical likelihood ratio tests for general linear or nonlinear hypotheses are developed for model building, and a penalized empirical likelihood method is proposed for design-based variable selection with oracle properties. Finite sample performances of our proposed methods are examined through simulation studies and a real survey data set is used to test the method for variable selection. ","This paper reviews the development of survey sampling practice and theory from the beginning of the twentieth century. The theoretical foundations of survey sampling design-based inference were presented in Neyman's 1934 landmark paper. This design-based approach is the norm for large-scale surveys; model-assisted methods are widely used in design and analysis but inference does not depend on the validity of statistical models. Since then, a variety of sampling methods has been devised and the associated theory developed. The need to estimate the precision of a survey's estimates from the selected sample has led to the development of theory for variance estimation for complex sample designs, including various replication methods. Over the years the issue of design-based versus model-based inference has often been debated. Even with the design-based approach, survey estimates are becoming more model-dependent because of rising nonresponse rates. Greater reliance on model-dependent inference has also occurred because of user demands for small area estimates, because of the desire to study hard-to-survey populations, and through the use of nonprobability methods for web surveys. ","The paper introduces global-local shrinkage priors for random effects in small area estimation. These priors with earlier success in Bayesian multiple testing and Bayesian variable selection can capture potential sparsity, which in the present context means lack of significant contributions by many of the random effects. In addition, these priors can quantify and assess disparity among the random  effects. The basic idea is to employ two levels of parameters to express prior variances of random effects. One is the global shrinkage parameter, which is common for all the random effects and introduces an overall shrinkage effect. The others are local shrinkage parameters, which act at individual levels and prevent overshrinkage. We show via data analysis and simulations that the global-local priors work as well or even better than some of the other priors proposed earlier. ","The distribution of first-digits obtained from many natural and economic datasets seem to follow a consistent distribution. The desire to find anomalies such as detecting fraud in financial and scientific datasets are common, and applications of \"Benford's Law\" have been developed to find these anomalies. In our work with applying these methods to determine interviewer anomalies we found that interviewer's assigned caseloads contained data where stratified subsets of first-digits follow consistent distributions that are like Benford's, but not specifically Benford's. To observe an interviewer objectively, we created a profile distribution by subsampling a mixture from available distributions to match individual interviewer's profile distribution. Using the interviewer's proportion of first-digits as a test statistic, we are able to determine bootstrapped p-values for first-digits in a way that allows us to flag interviewer results as suspicious and in need of closer scrutiny. ","The Current Employment Statistics State and Area (CESSA) program produces monthly industry employment estimates for subnational areas based on a survey of about 634,000 nonfarm worksites. Before estimates are published, they go through several screening procedures at the micro (individual report) and macro (estimation cell) level. CESSA adapted a process based on the Fay-Herriot model for extreme outlier detection at the macro level. The standardized difference between the sample-based estimate (Y1) and the synthetic part (Y2) of the model are used to identify significant deviations as candidates for macro editing. In those cases where the standardized difference exceeds a given threshold and analysts cannot find economic reasons to support the extreme movement, the modeled estimate is used to replace the direct sample-based estimate. This paper examines the process that CESSA uses to identify extreme macro outliers and its application to employment estimates at the state and area level. The effect of this procedure on error variance and bias when adjusting extreme estimates at different standardized cut-off levels is explored in an empirical study.   ","The Bureau of Labor Statistics Current Employment Statistics (CES) survey leads to the creation of measures which are used to create Principal Federal Economic Indicators. One of those measures is the monthly change in establishment jobs, which CES estimates for detailed industries as well as geographic domains such as states and metropolitan areas. CES calculates sampling errors using Balanced Repeated Replication, but estimates of sampling variance can be volatile and may not be available as readily as desired. Generalized Variance Functions (GVF) are fit using regression models to existing direct estimates of sampling variance to improve estimates of those variances. A GVF should provide good fits for data used to construct the model, and it should provide good estimates of variances for other observations not used in the model. This paper develops a GVF model for ratio estimators and considers two main characteristics: accuracy in terms of confidence interval coverage, and stability. Metrics and tests for each characteristics are developed. We also consider and test new model types to further increase coverage and stability of the GVF variances. ","Survey estimates may be susceptible to the influence of sample units having large design weights associated with unusual observed values. Especially in smaller samples, these sample units can influence estimates disproportionately causing them to be very unstable. In this paper, we consider several model-based approaches for weight smoothing where the design weights are modeled as a function of observed survey quantities. Using these modeled weights, one hopes to reduce volatility in the weights, thus producing better estimates. In this paper we extend prior work on the Current Employment Statistics Survey (CES). Several prospective models are used for the weights, including LOESS curves and Bayesian methods. The new \"smoothed\" weights are then used to create new survey estimates and we compare these estimates to the true value. Analysis of the fitted weights is performed in the end to find cases where \"smoothed\" weights may give worse estimates. ","USDA's National Agricultural Statistics Service (NASS) employs the June Area Survey (JAS) to produce annual estimates of U.S. farm numbers. The JAS is an area-frame-based survey conducted every year during the first two weeks of June. NASS also publishes an independent estimate of the number of farms from the quinquennial Census of Agriculture. Studies conducted by NASS have shown that farm number estimates from the JAS can be biased, mainly due to misclassification of agricultural tracts during the pre-screening and data collection processes. To adjust for the bias, NASS has developed a capture-recapture model that uses NASS's list frame as the second sample, where estimation is performed based on records in the JAS with matches in the list frame. In the current paper, we describe an alternative capture-recapture approach that uses all available data from the JAS and the Census of Agriculture to correct for biases due to misclassification and to produce more stable farm number estimates. ","We present a parametric approach for the estimation of enumerative finite population characteristics such as totals and means for complex survey samples in the presence of full response. We go beyond the model-assisted design approach by estimating the parameters of the distribution of the working model that generates the finite population while taking into account the sample design. Through statistical tests of parameters and model goodness of fit, we are able to determine the functional form of the estimator and the relevant set of auxiliary variables. Since this approach makes use of an explicit model for the finite population generating process, it can incorporate most recent developments in modeling from classical statistics. This approach is only based on the observed sample and does not rely on simulation studies for comparing models and evaluating estimators. We describe the foundations of the parametric estimation for finite populations based on sample-based maximum likelihood, present examples for finding efficient estimators and selecting auxiliary variables, and describe the effect on model misspecification on the statistical properties of estimates. ","Data from an extensive survey conducted by the National Center for Education Statistics (NCES) is used for predicting qualified secondary school teachers across public schools in the U.S.  The SAS survey family of procedures such as Proc Surveyfreq and Proc Surveylogistic is used for all model building and analysis.  The residuals from a logistic regression do not necessarily follow the normal distribution that is so often assumed in residual analysis.  Furthermore, in dealing with survey data, the weights of the observations must be accounted for, as these affect the variance of the observations.  To adjust for this, rather than looking at the difference in the observed and predicted values, the difference between the actual and expected counts is calculated by using the weights on each observation and the predicted probability from the logistic model for the observation.  A simulation study is also performed to better understand the correct distribution of the residuals accounting for the complex survey design.  The purpose is to identify which type of residuals best satisfy the assumption of normality while also accounting for the complex survey design.   ","This paper describes an easy-to-implement method for producing confidence intervals (CIs) for population attributable fraction (PAF) statistics using survey data. The PAF is used by epidemiologists and policymakers to assess how much of the disease burden in a population could be reduced if the exposure to certain risk factors were eliminated. The PAF is defined as p(rr-1)/rr , where p  denotes the proportion of cases exposed to a risk factor and rr  denotes the model-based relative risk comparing the proportion of cases among the exposed group with the proportion of cases among the unexposed group. The rr  is obtained by modeling the log of the prevalence of the disease as a linear function of covariates where exposure to the risk factor is included as one of the model covariates. The proposed methodology is based on the Taylor series linearization and properly accounts for survey design features in estimating the variances and covariances of the estimated quantities. The methodology is implemented using the VARGEN procedure of SUDAAN\u00ae version 11.0 software on 2013 Behavioral Risk Factor Surveillance System data to produce state-by-age group PAF estimates and CIs of hypertension with diabetes as the risk factor.   ","The CDC conducts the National Health Interview Survey (NHIS) and maintains a database of air quality estimates from USEPA's Bayesian space-time Downscaler (DS) fusion model, which combines output from the Community Multi-scale Air Quality model with monitor-based measurements from USEPA's Air Quality System. Specifically, DS-based annual predictions and estimated standard errors of 24-hour average PM2.5 concentrations (\u00b5g/m3) for 2010 U.S. Census tract centroid locations were linked to NHIS data. Estimated standard errors varied across locations, depending on distances to air monitors. Nonparametric regression with measurement errors was used to associate population health with annual air quality estimates. A deconvolution estimator with heteroscedastic error in R package \"Decon\" was used. Bootstrap confidence interval (CI) bands for \"Decon\" results were generated using R package \"Boot\".  Several alternative methods to compute CI bands were applied and compared. Point and CI estimates across methods were compared for models with and without measurement errors.  ","Online self-reported surveys present an alternative to traditional collection modes. As such, Statistics Canada is gradually moving to online surveys for the majority of its household surveys. This new collection mode however does present challenges in the need to randomly select a household member to complete the survey. Historically, an initial paper invitation is sent by mail and selection of a household member is done through rostering. If the selected member is not the person who completed the roster, a handing-off of the survey to the selected person is needed, which may however increase the survey's non-response rate. Alternative methods for individual selection before accessing the online application are proposed to avoid this hand-off request. We present two such methods that were tested: the last birthday method and an age-order method. For these methods, the selection is done using instructions on the paper invitation so that only the selected household member will have to access the online application. Response rates and selection inaccuracy rates were compared between the two alternative methods as well as with the traditional roster method. Results of the comparisons will be presented and discussed.  ","There is no standard method for estimating fare evasion and fare compliance in North American transit agencies. Most transit agencies report on fare evasion as discovered by fare enforcement officials. Fewer transit agencies report on randomized sampling of transit users to estimate fare evasion and fare compliance. Reporting of the methodologies and results of the various sampling techniques are not shared widely. This paper examines the attempt of a North American transit agency to estimate fare evasion and fare compliance rates using a multi-stage stratified cluster random sampling methodology of its light rail transit lines. A baseline estimate was performed in 2014. Sampling was conducted again in 2016. This paper reports on the results of the sampling methodology and provides recommendations for implementing statistically sound fare evasion sampling in transit agencies. The substantive results of the study varied depending on the type of ticket/pass, time of day, rail line, zone, location and direction of travel. ","The Truth Initiative Longitudinal Cohort Study is designed to evaluate the impact of a television and digital campaign on youths' smoking-related knowledge, attitudes and beliefs, perceived social norms, and behaviors over time. The study administers surveys to participants over six waves between 2014 and 2017 and uses multivariate statistical models to evaluate the effectiveness of the media campaign. The survey is subject to nonresponse, which can bias estimates for the evaluation. We describe and examine different methods of imputing missing data in the context of a longitudinal study. Hot deck and model-based approaches are compared for both their performance and practicality. Examining income, the variable with the highest item nonresponse rate, we find that using either hot deck or model-based estimation helps correct for nonresponse bias in estimates from complete case analysis, and we demonstrate how multiple imputation can help account for the uncertainty in estimates due to imputation. ","In many large-scale studies, it is difficult to use the most accurate measurement technique for measuring a variable of interest due to cost or logistical reasons. Imperfect instruments may be used to measure such variables in the main study and carefully designed auxiliary studies may be used to calibrate the accurate and imperfect instruments. All the studies, taken together create a variety of patterns of missing data and, thus, can be handled using multiple imputation. However, unlike the standard missing data problem, only the subjects in the main study are used in the ultimate analysis. This paper discusses a variety of modeling and analysis issues related to using the multiple imputation approach. Examples include standard measurement error calibration, mixed mode surveys, combining information from multiple surveys and harmonization of variables to a common standard. ","Often in surveys, key items are subject to measurement errors. Given just the data, it can be difficult to determine the distribution of this error process, and hence to obtain accurate inferences that involve the error-prone variables. In some settings, however, analysts have access to a data source on different individuals with high quality measurements of the error-prone survey items. We present a data fusion framework for leveraging this information to improve inferences in the error-prone survey. The basic idea is to posit models about the rates at which individuals make errors, coupled with models for the values reported when errors are made. This can avoid the unrealistic assumption of conditional independence typically used in data fusion. We apply the approach on the reported values of educational attainments in the American Community Survey, using the National Survey of College Graduates as the high quality data source. In doing so, we account for the informative sampling design used to select the National Survey of College Graduates. We also present a process for assessing the sensitivity of various analyses to different choices for the measurement error models. ","The missing pattern of data fusion implies that the variables that are specific to the data sets are never jointly observed. When applying standard imputation techniques, independence conditioned on the common variables is implicitly assumed. In general, however, this assumption does not hold; consequently, the estimated correlations between the fused specific variables are usually biased toward zero. We argue that in the absence of further information, a correlation lying well within the bounds of the conditional independence assumption (CIA) and one specific measurement error model is a significantly more sensible assumption. This argument is derived from a simple trivariate model and empirically supported by data from various fields.   ","In lifestyle intervention trials, where the goal is to change a participant's weight or modify their eating behavior, self-reported diet is a longitudinal outcome variable that is subject to measurement error. We propose a statistical framework for correcting for measurement error in longitudinal self-reported dietary data by combining these data with auxiliary data from external biomarker validation studies where both self-reported and recovery biomarkers of dietary intake are available. In this setting, dietary intake measured without error in the intervention trial is treated as missing data and multiple imputation is used to fill-in the missing measurements. Since validation studies are cross-sectional, they do not contain information on whether the nature of the measurement error changes over time or differs between treatment and control groups. We use sensitivity analyses to address the influence of these unverifiable assumptions involving the measurement error process and how they affect inferences regarding the effect of treatment. ","The term 'Data Fusion' describes a particular missing-data pattern in combination with a particular analysis objective. The pattern emerges when two data sources A and B are stacked over each other, yielding three sets of variables: a set of variables (X) observed in both sources, a set of variables (Y) only available in source A, and a set of variables (Z) only available in source B. The analysis objective is to draw inference about the joint distribution of Y and Z which are not jointly observed. At first glance, this missing-data pattern resembles the missing-data pattern of the potential outcomes framework (Rubin, 1974). Propensity Score Matching (PSM) (Rosenbaum &amp; Rubin, 1983) is a popular method for causal inference with observational data, and it is tempting to apply his method to data fusion problems, especially, since 'Statistical Matching' is used synonymously for combining data from different sources. In order to investigate its suitability for data fusion settings, we compare PSM with parametric and non-parametric imputation method in a simulation study. ","Geosampling is a probability-based, multistage, sample design in which geographic areas are partitioned into nested layers of GIS grid cells that are used as sampling units and boundaries for fieldwork. Prior applications of this methodology have relied on a simplified sampling and weighting approach, in which sampling units defined by the second layer of GIS grid cells (\"SGUs\") were sequentially inspected for households and accepted into the sample only if deemed residential. This process ceased once the desired sample size was achieved. As the total number of nonresidential units was unknown, weights assumed a simple random sample and a uniform distribution of the population across grid cells. This paper reviews the sensitivity of weights and estimates to the simplified approach using data from a new geosampling-based survey in two states of Nigeria. In the new study, complete residential inspection of SGUs was possible for a random set of clusters. Weights are calculated using both the simplified approach and new methods, and results are compared. Potential bias and variance impacts of the simplified approach are discussed and recommendations are provided for future studies.  ","In a series of papers, Little and Vartivarian (2003, 2005) argued that basing survey nonresponse adjustments on propensity to respond could increase the sampling variance of the estimates while not reducing bias, if the predictors of response were unrelated to key survey outcomes. Applying this idea to a 2014 military workforce survey, RAND researchers used machine learning approaches to develop a two-step method for nonresponse adjustment. The two step method comprises (1) a model for the key outcome variables based on respondents and (2) a response propensity model using the predicted key outcome variables as predictors for all sampled units. At the 2016 JSM, we presented simulation results assessing the predictive performance of competing machine learning algorithms in the first of the two steps. In this paper, we investigate the circumstances necessary for the two-step method to outperform nonresponse approaches in common practice, most of which can be regarded as single-step methods. ","The National Immunization Survey (NIS) tracks state and national vaccination coverage through a dual-frame landline and cell telephone Random Digit Dial (RDD) sample design. Demographic data are collected during telephone interviews, with a mail survey sent to providers to collect vaccination histories when consent is obtained. We investigate a method that collapses three of the initial NIS weighting adjustments associated with the CASRO response rate (nonresolution of phone numbers, screener nonresponse, household interview nonresponse) into a single step using adjustment cells, while keeping the remainder of the weighting process intact. By collapsing three adjustments into one, the sample weights were expected to be somewhat less variable and yield smaller variances for NIS vaccination coverage estimates. This method was evaluated using the 2015 NIS data and found little impact on variances or their estimates, with larger effects on intermediate weighting steps. These results raise the question whether there is an advantage in making separate adjustments for all CASRO factors involved in constructing NIS weights. ","Beginning with the 2011-2012 cycle, NHANES drew a special one-third subsample of adults comprising all current tobacco smokers and a random selection of non-smokers from Subsample A. This Smoking Subsample was intended to augment biomonitoring of smokers by essentially oversampling this group, but many of the same biomarkers were measured in the Smoking Subsample as in Subsample A. Combining these subsamples may enhance overall statistical power, and application of full-sample weights would enable sample-weighted analysis. Using NHANES 2011-2014 urine concentration data for thiocyanate (N=8096), a tobacco smoke biomarker, this study assessed the performance of using full-sample weights for analysis of combined subsamples nested within the full sample. Sample-weighted variances estimated with the bootstrap (Rust &amp; Rao 1996; 1000 replicates) for the sum of urinary thiocyanate found confidence limits of the combined subsamples improved between 28-36 percent over the subsamples individually. Commensurate improvements were observed for the sample-weighted mean and median, as well as the univariate regression slope for thiocyanate's association with urine flow. ","The random group (RG) method of variance estimation has been used in the Medical Expenditure Panel Survey - Insurance Component (MEPS-IC) since the beginning of the survey in 1996.  However, this method was found to be less reliable for certain types of estimates so the variance estimation method was changed to the Taylor Series Expansion (TS) method starting in 2014.  This paper will present a comparison of standard error estimates using the RG and TS methods for a variety of MEPS-IC estimates and will discuss the situations where the RG method may be less reliable than the TS method. ","One-per-stratum and Two-per-stratum designs are common survey sampling designs. The one-per-stratum design generally has the smallest variance, but no unbiased estimator of variance is available. We present an intermediate design which is a combination of the one-per-stratum and two-per-stratum designs, and for which an unbiased variance estimator is available. A closely related design is a one-per-stratum design with supplemental observations for variance estimation. We compare the variance and variance of estimated variance for versions of the intermediate design and for the supplement design. ","This paper provides a two-step approach to estimate the standard errors of parameter estimates in a trend analysis. This analysis used single-year American Community Survey (ACS) Public Use Microdata Sample (PUMS) data from years 2000 to 2015 to examine the trend of the race/ethnicity diversity of Registered Nurses (RNs). In estimating the standard errors of parameter estimates in logistic regression, the existing methods provided by U.S. Census Bureau doesn't apply. A two-step method was developed to estimate variance for the data across 16 years. This method estimated adjustment factors based on data years with replicate weights available (2005-2015), and then apply the adjustment factor to the estimates based on the data across all years (2000-2015). Using the idea of the design factor approach, this two-step approach takes advantage of all the available data.  ","Data from the National Health and Nutrition Examination Survey (NHANES) have been linked to the Center for Medicare and Medicaid Services' Medicaid Enrollment and Claims Files. As not all survey participants provide sufficient information to be eligible for record linkage, linked data often includes fewer records than the original survey data. This project presents an application of multiple imputation (MI) for handling missing Medicaid status due to linkage refusals in linked NHANES-Medicaid data using linkages of 1999-2004 NHANES survey years. By examining multiple outcomes and subgroups among children, the analyses compares the utility of a multi-purpose dataset from a single MI model to that of individualized models. Outcomes examined here include obesity, untreated dental caries, attention deficit hyperactivity disorder (ADHD), and exposure to second hand smoke. ","The multiple imputation (MI) method was originally invented under the Bayesian paradigm that ignores survey weights. However, the general practice of using MI for complex survey data clearly involves weights in all analysis. For example, the U.S. Centers for Disease Control and Prevention (CDC) provides five multiple imputed data sets for missing body fat mass data so that analysts may apply survey weights to each imputed data set and combine results using Rubin's rules. In this process, analysts already employ both the Bayesian-driven MI method and the weights that calibrate characteristics of a sample to approximate those of a target population. In this talk, we provide some insights for the MI paradigm for complex surveys and propose a way of calibrating imputation models with survey weights modified by nonresponse mechanism. The purpose of the calibration is to balance distributions of covariates between the missing and the observed data. With the evidence of balanced covariate distributions, calibrated imputed data are generated for complete case analysis. The calibrated MI method is applied for the analysis of NHANES' incomplete sexuality data. ","This presentation focuses on computationally feasible ideas on developing imputation models. Our main motivation is to employ these ideas in data structures with unique complexities such as large number of incompletely-observed categorical variables, multilevel data and/or clustered data with multiple membership. Our computational routines modify computationally advantageous Gaussian-based methods that use Markov Chain Monte Carlo techniques to sample from posterior predictive distribution of missing data. Specifically, we propose rounding rules to be used with these existing imputation methods, allowing practitioners to obtain usable imputation with negligible biases. These rules are calibrated in the sense that values re-imputed for observed data have distributions similar to those of the observed data. The methodology is demonstrated using a sample data from the NewYork Cancer Registry database. ","The publication of official statistics at different levels of aggregation requires a benchmarking step. Difficulties arise when a benchmarking method needs to be applied to a triplet of related estimates, at multiple stages of aggregation. For ratios of totals,  external benchmarking constraints for the triplet (numerator, denominator, ratio) are that the weighted sum of denominator/numerator/ratio estimates equals to a constant. The benchmarking weight applied to the ratio estimates is a function of the denominator estimates. For example, the United States Department of Agriculture's National Agricultural Statistics Service's county-level, end-of-season acreage, production and yield estimates need to aggregate to the corresponding agricultural statistics district-level estimates, that also need to aggregate to the corresponding prepublished, state-level values. Moreover, the definition of yield, as the ratio of production to harvested acreage, needs to hold at the county level, at the agricultural statistics district level and at the state level. We discuss different approaches of applying benchmarking constraints to a triplet (numerator, denominator, ratio), at multiple stages of aggregation, where estimators are constructed for two of the three quantities, the third being derived as a result. County-level and agricultural statistics district-level, end-of-season acreage, production and yield estimates are constructed and compared using the different methods. Results are illustrated for a subset of the sampled commodities and states, in the year 2014. ","Small area estimation using area-level models may benefit from covariates observed with error, such as when using other survey estimates as covariates. Given estimated variances of these errors, one can account for uncertainty in the covariates using measurement error models (e.g., Ybarra and Lohr 2008). Two types of measurement error models have been introduced to deal with such errors. The functional measurement error model assumes that the underlying true values of the covariate are fixed but unknown quantities. The structural measurement error model assumes that these true values follow a model, implying a multivariate model for the covariates and the original dependent variable. We compare and contrast these two models under different underlying assumptions. We also explore the consequences for prediction mean squared errors that result from ignoring measurement error when it is present (na\u00efve model) rather than using a functional or structural measurement error model. Comparisons done both analytically and via simulations yield some surprising results. We illustrate the results using data from the U.S. Census Bureau's Small Area Income and Poverty Estimates Program. ","We implement the techniques of small area estimation (SAE) to study a positively skewed welfare indicator, consumption. A logarithmic transformation that may be problematic is usually suggested for positively skewed data to build a model. In our study, we have developed hierarchical Bayesian models without log-transformation. We applied our model to the  Nepal Living Standards Survey, 2003/04 consumption data, an aggregate of all food and nonfood items consumed in the past twelve months. Since, the respondent has to recall the consumptions in the past twelve months, we assume that data are recorded with noises. For the noisy data, we fit three special cases of generalized beta distribution of the second kind (GB2) models using the Metropolis Hastings sampler. After fitting Bayesian models for SAE, we show how to select the most plausible model and perform the Bayesian data analysis. ","In sample surveys with sensitive items, sampled individuals either do not respond  (nonignorable nonresponse) or they respond untruthfully. For example, responders usually give negative answers to sensitive items when the responses should actually  have been positive, thereby leading to an estimate of the sensitive population proportion that is too small. For this study, we have sparse binary data on college cheating, collected under an unrelated-question design, from several locations (small areas) on a US campus. We have used  a hierarchical Bayesian model to capture the variation in the observed binomial counts from these locations and to estimate the sensitive proportions for all  locations. For our application on college cheating, there are significant reductions in the posterior standard deviations of the sensitive proportions under the small-area model in comparison to an analogous individual-area model. A simulation study confirms the gain in precision and, surprisingly, shows the estimates under the small-area model are closer to the truth than the corresponding estimates under the individual-area model.   ","The development and evaluation of a hierarchical multinomial and a hierarchical uniform model for making small area estimates is presented.  By limiting these types of models to include constraints at a higher geographic level, many of the design-based characteristics retained by a multinomial or a uniform model may still be retained while borrowing strength from a higher geographic-level model.  An evaluation using data from the 2008-2012 NHIS to make estimates of access to care for the counties in States that border Mexico is presented.  Here, the model uses sample segments as the unit-level and accounts for the differential selection of segments from within-PSU strata.  Covariates available from the U.S. Census Bureau's \"American Factfinder\" are used in the county-level model. ","This paper assesses weighting methods for a national panel sample data selected with non-probability sampling from a mobile panel.  In addition to a raking method, we use a propensity model method which relies on a large probability sample as a reference sample.  The non-probability sample for the study, the Child Immunization Panel Survey (ChIMPS), was selected using a very large mobile panel which is representative of the US population along some key demographics.  The probability sample is the 2014 National Immunization Survey (NIS), a large probability sample of US households with children in the target age range. We match on demographics, geography and telephone characteristics, and use a propensity model for the combined sample.  The logistic regression models include the survey weights for the NIS; weight adjustments are computed as the inverse of propensity scores.  We compare weighted estimates of immunization outcomes with those computed with raking for the ChIMPS data. The comparisons also look at variances and weight variability. We address the challenge that the NIS is already a dual frame RDD sample where the cell component is more comparable with the ChIMPS sample.  ","Consider estimation of the population total ???? for an outcome or study variable ?? from a low-budget purposive sample ??~ with the aid of an ongoing high-budget reference probability sample ??* with no data on ?? but data on common auxiliary variables or covariates ??. Using Royall's model-based approach, a prediction estimator can be constructed from ??~ with known totals of ?? or their estimates from ??* under the postulated assumption of model holding for ??~ and its complementary part; i.e., nonselected units. Using S\u00e4rndal's design-based approach, GREG (generalized regression) can be constructed from ??~ after estimating the sample inclusion propensities using the calibration approach under the postulated assumption of model holding for ??~ and its complement. By treating the problem as a complete missing data problem for ??*, a new estimator iGREG (i for imputation) can be constructed from ??*after imputing ?? for all units in ??* by using ??~ as the donor dataset under a model whose validity can be partially tested using ?? observed in both samples. Analogous to the quasi-design based approach in probability samples with nonresponse, we start with a design-based approach using the reference sample ??*, but build over it by integrating ??-information from the purposive sample ??~ under an imputation model. This approach is termed model-over-design (MOD) integration following Singh (2015). The information on the differences between imputed and observed values of ?? provide extra covariates with the constraints of zero control totals to reduce the imputation bias via weight calibration. Variance estimates for iGREG can be obtained by extending results under the reverse framework for nonresponse imputation in probability samples (where the respondent subsample serves as the donor dataset) to the case of complete missingness by design where an external dataset (??~) serves as the donor dataset. Limited simulation results are presented for illustration.  ","Given the high cost associated with probability samples, there is increasing demand for combining larger non-probability samples with probability samples to increase sample size for low incidence studies and/or key analytic subgroups. Given bias and coverage error inherent in non-probability samples, use of traditional weighted survey estimators for data from such surveys may not be statistically valid. In this paper, we discuss the use of small area models and estimation methods to combine a probability sample with a non-probability sample assuming the (smaller) probability sample yields unbiased estimates. We consider two distinct small area models: (a) Fay-Herriot model with the probability sample point estimate as the dependent variable and the non-probability sample point estimate as a covariate in the model, and (b) Bivariate Fay-Herriot model that jointly models the probability sample point estimate and the non-probability sample point estimate, and accounts for the bias associated with the non-probability sample. ","Probability samples are the preferred method for providing inferences that generalize to a larger population. When a small subpopulation is of interest, this approach is unlikely to yield a sample size large enough to produce precise inferences. Convenience sampling may provide the necessary sample size, but selection bias may compromise the generalizability of results. We propose methods of statistical weighting that may be used to combine data from a probability sample with data from a convenience sample. Methods are proposed which involve direct estimation of inclusion probabilities for both samples via propensity scores and which involve calibration weighting.  We propose a distinction between methods that blend the samples simultaneously and methods that blend each sample separately.  Simultaneous blending invokes a smaller design effect; a test for the adequacy of blending is proposed that uses weights found via disjoint blending. Motivating the exposition is a survey of military caregivers. Simulations show that the gain in precision provided by the convenience sample is lower in circumstances where the outcome is strongly related to the auxiliary variables used for blending ","Obtaining probability samples of homeless youth is challenging as it is difficult to generate a complete sampling frame listing all the members of the population of interest. Therefore, statisticians resort to alternative sampling methods such as location sampling. Location sampling consists of developing a list of locations where homeless youth are known to hang out.  Location sampling introduces other challenges such as the multiplicity issue. The multiplicity issue arises because homeless youth can enter the sample in multiple ways. When the population of interest is homeless youth and not homeless youth-visits it is important to devise corrections to the sampling weights so that a sample of homeless youth is obtained. We will analyze the effects of these corrections on the estimates of smoking outcomes and background characteristics of homeless youth. We will compare those homeless youth who visit the sites in the frame frequently versus those who don't. The results of this analysis will speak to the importance of correcting for the multiplicity issue and how sensitive the results are when these corrections are computed in different ways.    ","Nonresponse and increasing survey costs continue to influence traditional methods of data collection. In recent years, researchers are exploring alternative methods of designing surveys and data collection to reduce bias and survey cost. According to the Pew Research Center telephone surveys, the Internet use among the U.S. adults has increased from 14% in 1996 to 88% in 2016. Therefore, Web surveys and multimode data collections from households and establishments have become a common, alternative cost-saving approach. The National Health Interview Survey (NHIS) has been collecting information on the Internet and email usage among adults since 2012. Among 2014-2015 NHIS adult respondents, 74.1% report using the Internet or emails. This paper presents an assessment of bias in selected estimates using data from a random sample of self-reported Web users (defined as an NHIS respondent who uses the Internet or email) and compares with the overall NHIS estimates. ","As part of the ongoing effort to improve its economic surveys, the U.S. Census Bureau is exploring alternative data collection methods with the goal of reducing respondent burden and enhancing the efficiency of data processing.  Some of these methods belong to the category of passive data collection, in which the respondent either has little awareness of the data collection effort or does not need to take any explicit actions.  Examples include scraping data from respondents' websites and obtaining respondent data from third parties that have already collected it.  Other methods belong to the category of system-to-system data collection, which involves respondents transferring data directly from their computer systems to the Census Bureau's systems.  In this paper, we outline the Census Bureau's data collection vision for its economic programs and describe recent work on exploring the potential of alternative methods.  We also explain how machine learning can be used to assist in collecting and processing data, especially data scraped from websites.  Lastly, we describe concerns and challenges associated with all of these methods. ","Economic data products produced by the United States Census Bureau have long served as a high-quality benchmark for data users.   To maintain this quality and enhance the foundation of its economic programs, the Census Bureau has begun exploring the potential of Big Data sources to supplement current products, create new products, and improve current methodologies. Research done with Big Data sources such as credit card transaction data, point-of-sale data, and publicly available building permit data have shown potential to improve the timeliness, geographic detail, and product-line coverage of Census Bureau economic data products. The big data work also has found promising enhancements to current seasonal adjustment methodologies including holiday effects and trading day weights. This paper covers the Big Data findings that the Economic Directorate of the Census Bureau has discovered thus far as well as plans for the future. ","In 2015 BEA released the first official set of Personal Consumption Expenditures (PCE) by state statistics. These statistics are largely based on Economic Census receipts of establishments, which reflect the location of sale rather than the residence of the consumer, thus adjustments need to made for cross-border purchases. To address this issue, we use credit card data from FirstData, the largest credit card intermediary in the country containing millions of establishments throughout the country.  The data include the location of the establishment and information about the location of the consumer.  Based on this information, we build estimates of consumption flow patterns across the U.S. for a wide variety of retail industries.  We demonstrate that the consumption flows show predictable economic patterns across markets with consumers preferring nearby locations, larger economic markets, and popular vacation destinations.  Next, we show how this consumption flow information may be used to correct for the border-crossing problem in our regional PCE state estimates and could even be applied to create a new product of PCE at the MSA level. ","In survey sampling, multiple imputation provides an effective way to handle missing data. When a large number of possible models are under consideration for the data, the multiple imputation is typically performed under a single selected model from the large pool of candidates. This model selection approach ignores the model uncertainty and so leads to underestimation of the variance of multiple imputation estimator. In this paper, we propose a new multiple imputation procedure for accounting for the model uncertainty under informative sampling design. We use the Bayesian model averaging (BMA) to incorporate all possible models into the imputation procedure. Some theoretical properties of the proposed method are investigated. A simulation study demonstrates that our model averaging approach provides better estimation performance than a single model selection approach under complex sampling design. ","Skewed data are common in sample surveys. In probability-proportional-to-size sampling, we proposed a Bayesian predictive method for estimating finite population quantiles. We assumed the survey outcome to follow a skew-normal distribution given the observed selection probability. To allow a flexible association between the survey outcome and the probability of selection, we modeled both the location and the scale parameters of the skew-normal distribution as a penalized spline function of the selection probability. Using a fully Bayesian approach, we can obtain the posterior predictive distributions of the non-sample units in the population, and thus the posterior distributions of the finite population quantiles. We showed through simulation that our proposed method is more efficient and yields shorter confidence interval with better confidence coverage than alternative methods in estimating finite population quantiles. We demonstrated the application of our proposed method using a real establishment survey. ","Survey weighting adjusts for differences between the collected samples and the target population. However, classical weights have lots of problems. Extreme values of weights will cause high variability and blow up the estimates. In practice, weighting construction requires arbitrary choices about selection of weighting factors and interactions, pooling of weighting cells and weight trimming. The general principles of Bayesian analysis imply that models for survey outcomes should be conditional on all variables that affect the probability of inclusion, which are the variables used in survey weighting. We propose to include weighting variables in the model for survey outcomes under the framework of multilevel regression and poststratification at much finer levels with structural prior specification. The procedure will yield the model-based weights after smoothing, which are evaluated via simulations comparing with classical weights. We use Stan for computation and illustrate the performances via the application of the New York Longitudinal Survey of Poverty study.  ","Every three years the Survey of Consumer Finances (SCF) is conducted to collect personal income and family finance data from a national area probability and list sample with a lengthy and complex survey instrument. The survey faces challenges in gaining cooperation from households due to the sensitive nature of the study. Since 2004 the field period had to be extended to reach both the targeted response rate and targeted number of completed cases. For the 2016 Survey of Consumer Finances (2016 SCF), and most surveys seeking high response rates, the pursuit of elusive respondents is necessary and both a lengthy and labor intensive process. Last year we presented findings from our examination of the 2013 SCF data to inform and target special efforts designed to shorten start to finish time and reduce total labor associated with these most challenging respondents.  We will describe our experiences on the 2016 SCF:  the challenging places in which there is a higher percentage of hard-to-contact cases, our analysis of the characteristics of these places and households,and determine whether we were able to reduce the level of effort and improve the outcomes of these more difficult cases. ","To achieve a high response rate for the triennial Survey of Consumer Finances (SCF) amidst a national trend of declining participation in U.S. household surveys, researchers at the Board of Governors of the Federal Reserve System (FRB) and NORC at the University of Chicago (NORC) have historically offered escalated monetary incentives late in the field period and extended data collection beyond initial projections. A new strategy for the 2016 SCF involved using propensity modeling to inform respondent incentive escalation.  We developed an \"escalation need\" score based on contact outcomes for the SCF and a measure of low response propensity from the Census Planning Database. In this paper we share our methodology for developing the escalation need score, incorporating the score in an algorithm to identify households that present the most challenges for data collection, and offering escalated incentives in a 2-staged release. We then examine results for the 2016 SCF and assess the utility of escalated incentives on the overall probability of survey completion.  ","The Survey of Consumer Finances (SCF) collects expansive financial data from households. Field interviewers are trained to collect complex financial data, alleviate the concerns of respondents nervous about identity theft, and engage those uninterested. While the concerns of many sample members are alleviated upon learning about NORC's long-standing reputation as a legitimate research organization and the extensive security measures in place to protect their data, some remain skeptical even after agreeing to participate. Others agree to participate yet have little interest in the study. Are the data provided by skeptical or indifferent respondents of lower quality than those who participate more willingly? This research will examine the 2016 SCF data to identify hesitant respondents, as defined by cases completing after a higher than average incidence of number of contacts, receiving targeted gaining cooperation materials, and/or assessed as having high suspicion or low interest by the field interview upon completion of the interview. Data quality for such respondents, determined by how extensively a case needs to be edited for missing or incomplete data, will be evaluated.  ","As an in-person data collection endeavor, the Survey of Consumer Finances (SCF) attempts to reach survey respondents living in all types of housing arrangements.  Gated communities and locked buildings are two examples of housing types that present special challenges for SCF interviewers.  Both of these residence types have very specific barriers, such as guardhouses, locked gates, and/or doormen, which make it difficult for interviewers to reach potential respondents.  In-depth comparisons between types of physical entry barriers and contact success from the 2013 SCF have been presented previously, along with a review of select Census characteristics of geographic areas with various types of barriers.  We update these earlier findings with results from the 2016 SCF.  We will advance the research by studying the relationship between urbanicity and entry barriers, and our study of Census variables will expand to examine interactions between Census variables.  We will also discuss practical applications of our findings for use in future interviewer trainings. ","The Survey of Consumer Finance (SCF) is a dual frame national survey conducted triennially for the Board of Governors of the Federal Reserve System, and provides unique and important insights into the assets, liabilities and net worth of American households. NORC at the University of Chicago collected survey observations for the 2016 SCF using face-to-face interviewing techniques designed to address more widespread challenges to participation. This paper outlines the contacting strategy and the utilization of operational paradata elements to inform interviewer outreach when household-level gatekeeper scenarios are encountered preventing access to potential respondents. We will discuss the application of refined operational strategies utilized in 2016 to address known systematic barriers to reaching respondents, including gatekeeper identification and engagement, as well as efforts to connect with respondents and ensure that an informed decision related to participation can be made. ","The Medical Expenditure Panel Survey (MEPS) is a nationally representative survey conducted annually by the Agency for Health Research and Quality (AHRQ).  Respondents to the Household Component (HC) of MEPS provide detailed information on health care use and expenditures, as well as health insurance coverage, access to care, demographic and socioeconomic characteristics. For a subset of respondents, medical providers associated with medical events reported by the household are contacted to obtain more precise expenditure information. While the primary motivation for conducting this follow-back survey, called the Medical Provider Component (MPC), is to collect data to improve the quality and completeness of expenditure data, we leverage data in the MPC to determine the extent to which HC respondents may be mis-reporting the number of medical events. We treat the MPC as a validation data set and use machine learning methods to build a model to identify correlates of reporting accuracy. We use this model predict reporting accuracy for those respondents for whom provider data were not collected. ","The State Finance and Tax Statistics Branch conducts the Quarterly Summary of State and Local Government Tax Revenue (QTax). The QTax survey is comprised of local property tax, state taxes, and local nonproperty taxes. In this paper, we describe the investigation into whether the Public Sector Quarterly Tax (QTax) time series are good candidates for seasonal adjustment. We review the four QTax item codes: Sales Tax (T09), Income Tax (T40), Corporate Income Tax (T41), and Property Tax (T01) at the National, State, and local levels, to determine whether seasonal adjustment helps to interpret series changes that occur over time. We make these adjustments by using the seasonal adjustment software X-13ARIMA-SEATS. With this software, we employ the semiparametric X-11 method and the model-based SEATS method, and compare results and adjustment quality. We also consider whether to adjust the National series directly or indirectly. We undertook the research knowing that the adjustments would be additional information that we could provide to data users, not replacing the current data products. ","The Bank of Canada 2015 Retailer Survey on the Cost of Payment Methods (RCPM) faced low response rates and outliers in sample data for two of its strata: chains and large independent businesses. In the presence of outliers, Chambers (1986) and Beaumont, Haziza and Ruiz-Gazen (2013) suggested the use of a bias-corrected estimator based on an appropriate choice of the tuning constant. In this paper, we develop a parametric bootstrap procedure to determine the tuning constant which minimise an estimator of the mean square errors (MSE). Monte Carlo simulations are performed to compare the performance of the robust-estimators for several choice of the tuning constant, including the adaptive method proposed by Beaumont, Haziza and Ruiz-Gazen (2013). The results show that for all the simulation set-up considered in this paper, the estimator based on the conditional bias with the adaptive choice of the tuning constant proposed by Beaumont, Haziza and Ruiz-Gazen (2013) performs better in terms of both bias and efficiency. The methodology is applied to the RCPM survey. ","This paper explores the use of time-to-event models under right censoring in the Model-Calibration estimator framework. Model-assisted estimation has been widely used for sample surveys. One class of model-assisted estimators is Model-Calibration estimators. These estimators have been explored for Generalized Linear Models but not time-to-event models under right censoring. Adaptations to the current theory need to be made through the use of a counting process approach to time-to-event analysis. Here, both parametric and semi-parametric time-to-event models will be explored.  ","Collecting information from sampled units over the Internet or by mail is much more cost-efficient than conducting interviews. These methods make self-enumeration an attractive data collection method for surveys and censuses. However, self-enumeration data collection can produce low response rates compared to interviews. To increase response rates, nonrespondents are subject to follow-up treatments, which influence the resulting probability of response. Because response occurrence is intrinsically conditional, we primarily record response occurrence in discrete intervals, and we characterize the probability of response by a discrete time hazard. This approach facilitates examining when a response is most likely to occur and how the probability of responding varies. Because response rates are presumed to be low, a widely used approach is to consider a second-phase of data collection, where only sub-sampled nonrespondents are followed-up. However, in practice, data collection from self-enumeration and from follow-ups are done in parallel, which makes sub-sampling from nonrespondents difficult to apply. In this case, excluding late self-enumeration responses ? not obtained from the follow-up subsample after follow-up has been started ? is recommended in the literature to avoid a nonresponse bias. Finally, we study the estimator of the finite population total that use all observed responses. Simulation results on the performance of the proposed estimators are also presented. ","Longitudinal data are collected over several time periods for the same units and allow for modelling both latent heterogeneity and dynamics. Unfortunately, the analyst is often faced with serious estimation problems that arise due to missing data. In a dynamic setup the dependent variables also appear as explaining variables in later periods, and item nonresponse induces an even higher loss of information and potential to inefficient estimation, if the missing-data mechanism is not taken into account. The suggested linear Bayesian fixed and random effects estimation provides a Gibbs sampler routine with a data augmentation step to treat the item nonresponse by drawing the missing values iteratively from their full conditional posterior distribution. To handle possible nonresponse within additional covariates, we incorporate a Classification and Regression Trees (CART)-imputation into the sampler. Also, we provide non-nested model comparisons in terms of the marginal likelihood from the Gibbs output. The properties and efficiency gains of the suggested approaches are illustrated by means of a simulation study and an empirical application. ","When the response mechanism is believed to be nonignorable or not missing at random (NMAR), a valid analysis requires stronger assumptions about the data than do standard statistical methods. Semiparametric estimators have been developed under the correct model specification assumption for the response mechanism.  In this talk, we consider  a scheme for obtaining the optimal estimation for the parameters  such as the mean and propose two semiparametric adaptive estimators that do not require any model assumptions except for the response mechanism. Asymptotic properties of proposed estimators are discussed.  "],"title":["NaN","Building Regression Trees on Survey Data","Tree-Based Methods To Model Dependent Data","Nonparametric Density Estimation from Censored Data","Can\u00a0Survey Bootstrap Replicates Be Used for Cross-Validation?","Effect of Spanish Two-Way Immersion Programs on Kindergarten Students' Attitudes: A Study of Treatment and Control Schools","Matching Four Groups of Postsecondary Education Institutions Using Propensity Scores","Methods for Analyzing Agreement on Ordinal Ratings Between Self- and Peer-Evaluations in a Medical School Student Survey","Modeling of Probability of Infant Survival (Survival Rates)","Deal with Excess Zeros in the Discrete Dependent Variable, the Number of Homicide in Chicago Census Tract","An Examination of the Relationships Between University Student Retention and High-School Coursework Among At-Risk Students Using Regression Analysis","Using Respondent Information and Psychometric Methods to Estimate Response Propensities","Reducing Error in Psychological Research: Averaging Data To Determine Factor Structure of the QMPR","Comparison of Students' Scores Between Public and Private Schools: Analysis of PISA Data","A Brief History of Survey Classification Error Models","Survey Respondent Incentives: Research and Practice","Randy Sitter's Contributions to Resampling Methods in Sample Surveys","A Statistical Approach to Quantifying the Elastic Deformation of Nanomaterials","Using Empirical Likelihood Methods To Obtain Range Restricted","Evaluating the Uncertainty in HIV/AIDS Infection Models","Recent Background and Influences on the Direction of Research in Public Health Biosurveillance","Statistical Considerations in Large-Scale Screening Programs: Impacts on the Public","Assessing the Effects of Air Pollution on Human Health","Multiple Semiparametric Imputation","Multiply Imputing Potential Outcomes To Estimate Individual Causal Effects","Parametric fractional imputation for missing data analysis","Probabilistic and Fuzzy Matching as Applied to Record Linkage and Computerized Coding","Rank and Set Restrictions for Homogeneity Analysis in R","Sensitivity Analysis for Observational Studies: The ObsSens Package for R","SAFAL: Statistical Analysis Functions Automating Language","An Open Source Library for the Estimation and Evaluation of ACD Models","Algorithmic Errors in the Estimation of Tobit II Models and the Corresponding Failure To Recognize Selection Bias","GLUMIP 2.0: SAS/IML Software for Planning Internal Pilots","Old Tabulations, Old Files, and a Brief History of Individual Tax Return Sampling","Statistics of Income Sales of Capital Assets Sample Redesign for Tax Year 2007","Attrition in the Tax Years 1999--2005 Individual Income Tax Return Panel","Methodological Limitations in Producing Subnational Tabulations of Unincorporated Business Activity That Partnerships and Sole Proprietorships Report on Returns","Testing for Qualitative Interaction of Multiple Sources of Informative Dropout in Longitudinal Data","Overdispersed Continuous-Time Models with Epidemiological Applications","Multilevel Modeling Technique and Bootstrap Variance Estimation in Longitudinal and Cross-Sectional Complex Survey Data Sets","Correction of Bias from Nonrandom Missing Longitudinal Data using auxiliary information","Modeling Change in Longitudinal Studies: Use Age Only or Initial Age and Time?","Comparison of Generalized Estimating Equations and Quadratic Inference Functions in Longitudinal Analysis of Data from the NLSCY Database","Sample Refreshment, Optimal Allocation, and Other Issues in the Redesign of Panel Surveys","American Community Survey (ACS) Sample Design Issues and Challenges","Redesigning the American Community Survey (ACS): Computer-Assisted Personal Interview Sample","Some Research on Sampling Design Alternatives for a Redesigned 2010 National Hospital Discharge Survey ","Assessing Oversampling of Older Persons in a National Health Survey","Implementation of Controlled Selection in the National Compensation Survey Redesign","Reinterview Revisited","Evaluating data collection quality after protocol changes","Simulation Modeling of Field Operations in Surveys","Using Paradata to Actively Manage Data Collection Survey Progress","A Microlevel Latent Class Model for Measurement Error in the Consumer Expenditure Interview Survey","How To Prioritize Recontacts","Use of a Likelihood-Based Mixed-Effects Model Repeated Measures Analysis for a Clinical Trial in Major Depressive Disorder","Longitudinal Data Analysis: Repeated Measures Analysis Versus Random Coefficient Models","Mixture Model for Individual and Combined Data Using Estimating Equations","A Comparison of Methods for Pretest/Post-Test Trials with Small Samples","MMRM Analyses with and Without Titration Visits","Modeling Phase-Dependent Effects and Volatile Longitudinal Responses via Geometric Brownian Motion Process","Surveying Cell Phone Numbers in the United States","Test-Score Ceiling Effects and Value-Added Measures of School Quality ","The Sensitivity of Teacher Effect Estimates to Decision Rules for Establishing Student-Teacher Links","Does a Teacher's Value Added Require Data To Be Missing at Random?","Estimating Teacher Effects from Longitudinal Data Without Assuming Vertical Scaling","Estimating Dynamic Panel Data Models with Measurement Errors","Canonical Correlation Analysis of Longitudinal Data","Inverse Regression from Longitudinal Data","State Space Representation of an Autoregressive Linear Mixed Effects Model for the Analysis of Longitudinal Data","Estimation of Disease Progression in Amyotrophic Lateral Sclerosis Using Muscle Electrical Properties","Comparison Between Analysis of Variance and Spectral Decomposition in the Mixed-Effects Models","Marginalized Transition Models for Longitudinal Count Data","Applications of Ranked Set Sampling to Genetic Studies","Some Further Generalizations of Ranked Set Sampling","Nonparametric Maximum Likelihood Estimator of Bohn-Wolfe Model","Confidence Intervals Estimation of the Location Parameter of the Logistic Distribution Using Ranked Set Sampling","Sequential Unbalanced Ranked Set Sampling","Using the National Health and Nutrition Examination Survey (NHANES) To Evaluate Health Disparities","Health Care Disparities Research: Using the National Health Care Surveys","Vital Statistics: Vital to the Measurement of Health Disparities","Using National Health Interview Survey Data and State and Local Area Integrated Telephone Survey Data To Study Health Disparities","Translating Disclosure Risk of Geographic Units to Survey Respondents: A Hierarchical Assessment of Contextual Data","Multiple Imputation Methods for Disclosure Limitation in Longitudinal Data","The Effect of Measurement Error and Under-Coverage Error on Disclosure Risk Assessments","Measuring Disclosure Risk for a Synthetic Data Set Created Using Multiple Methods","Achieving Analytic Potential While Protecting Data Confidentiality","A Unified Theory of Randomized Response Surveys of Binary Characteristics","Hybrid Logistic Regression and Random-Response Model in a Matched Pairs Study","Nonresponse in the American Time Use Survey","Using a Large-Scale Field Study To Estimate Nonresponse and Noncoverage Biases in an RDD Survey","A Study of SES Bias and Nonresponse in a Large-Scale Household Survey","Sensitivity Analysis of Nonresponse Bias in the Current Population Survey","Proxy Pattern-Mixture Analysis for Survey Nonresponse","A Picture Is Worth a Billion Words: Visualizing Mega-Parameter Models from Giga-Scale Textual Data","Ceci N'Est Pas Une Carte","Tufte Without Tears","An Introduction to Multiple Imputation","Analyzing Coarse Data","Examining Sensitivity of Small-Area Inferences to Uncertainty About Sampling Error Variances","Robust Estimation of Monthly Employment Growth Rates for Small Areas in the Current Employment Statistics Survey","Robust Small-Area Estimation Under Unit-Level Models","Relaxing the Autonomous Independence Assumption for Census Coverage Measurement Dual System Estimates","Using Continuous Variables as Modeling Covariates for Net Coverage Estimation","Direct Estimates as a Diagnostic for Dual System Estimators Based on Logistic Regression","Assessing Synthetic Error via Markov Chain Monte Carlo Techniques","Demographic Analysis in 2008 and Beyond","Application of Confirmatory Factor Analysis To Establish the Validity of a Practice-Analysis Survey","Testing the Proportional Odds Assumption for Complex Data","Bid Design and Its Influence on Stated Willingness To Pay and Participation in a Deposit Refund Program in a Survey of Nonrefillable Plastic Pesticide Users","Imputation Methods for Missing Data in a Longitudinal Family Study","Imputation for Missing Physiological and Health Measurement Data: Tests and Applications","A New Approach to Estimation of Response Probabilities When Missing Data Are Not Missing at Random","Model-Assisted Hot-Deck Imputation","Imputing and Jackknifing Scrambled Responses","Analysis of Using the SERVQUAL Model for Student Faculty Course Evaluations","Factorial Invariance and Robustness to Low Variability: Maximum Likelihood Factor Analysis vs. Correlation Constraint Analysis","A Quality-Control Approach for Statistical Computer Programs","Measurement of Perceived Health Status: Respondent and Mode Effects in a National Health Survey","Cluster Size in Multilevel Models: The Impact of Sparse Data Structures on Point and Interval Estimates in Two-Level Models","County-Level Small-Area Estimation Using the National Health Interview Survey (NHIS) and Behavioral Risk Factors Surveillance System (BRFSS)","Variance of the With-Replacement Sample Variance","The Use of Paradata for Evaluating Interviewer Training and Performance","Which Incentives Work Best for Respondents in Today's RDD Surveys?","Year-to-Year Correlation in National Health Interview Survey Estimates","Hardy-Weinberg Equilibrium for Unequal-Probability Genotype Samples","Surveying Parts To Construct the Whole: Sampling and Estimation Issues","Response Quality Among Reluctant Respondents","Cell Phone--Only Research at Arbitron: Statistical Analyses","Controlling Effect of Sample Design on Principal Components Analysis: A Simulation Study","Coevolution of Multivariate Optimal Allocations and Stratum Boundaries","Accounting for Sampling Design in Complex Surveys: A Jamaican Example","Effects of Sampling and Screening Strategies in an RDD Survey","The Impact of Income Imputation Using Cascading Partial Income Information in California Health Interview Survey","Election Statistics Results","Bayesian Analysis for the Difference of Two Poisson Rates with Data Subject to Under-Reporting","A Bayesian Cox-Regression Model with Informative Censoring","Effects of Missing and Censored Data for Nonlinear Models Involving ODEs","Multiple imputation in multiple classification and multiple-membership structures","Bayesian Semiparametric Density Regression with Measurement Error","Bayesian Variable Selection for Large Data Sets with Missing Covariates","Spatial Multipurpose Designs","On the Effect of Collocation on the Quality of Multivariate Spatial Prediction","Sampling Strategies for Estimating the Spatial Mean of Temporal Trends","Spatial Network Design To Detect Regional Trends in PM2.5","Spatial Sampling Design Using Support Vector Machines","SAIPE County Poverty Models Using Data from the American Community Survey","Model Utility in a Time Series with Interventions: A Case Study","Multivariate Analysis of Waiting and Treatment Time in Emergency Departments","Tracking Consumer Energy Price Change: An Overview of Federal Data Sources and Methodologies","Estimating Valid Signatures on a Petition: Power To Choose Between W and A Goodman Type","A Smoothing Approach to Data Masking","GeoFrame: A Technological Advancement in Field Enumeration","Using Digital Imagery To Update the Measures of Size of Area Segments","Success Using an Age-Targeted List Sample for the National Immunization Survey - Adult","Challenges and Methods in Creating Secondary Sampling Units for Area Probability Samples","Assessing the Filter Rules for Extracting Addresses from the Master Address File To Construct a Housing Unit Frame for Current Demographic Surveys","Evaluating the American Community Survey as a Potential Sampling Frame for the National Survey of College Graduates","Stratification and Allocation for the Estimation of a Complex Statistic with Auxiliary Data","Sampling Issues in Biomarker Studies of Nevus Phenotypes","Assessing the Impact of Intervention","Noncompliance-Corrected Effect of Randomized Highly Active Antiretroviral Therapy on Incident Aids or Death Using Inverse Probability-of-Censoring Weights","The Exposure-Controlled Retrospective Study: Application to High-Incidence Diseases","Semiparametric-Efficient Estimation for Multistage Case-Control Studies","A Steady-State Effect of a Cancer Screening Intervention","Estimation in Hidden Population Using Social Network","The False Discovery Rate in ACS: Helping Users Understand Estimates for Small Domains","Spatial Modeling and Prediction of County-Level Employment Growth Data","Small-Area Estimation for Alcohol Drinking Among Teenagers","Hierarchical Model Selection Using a Benchmark Discrepancy","Further Developments in a Hierarchical Bayes Approach to Small-Area Estimation of Health Insurance Coverage: State-Level Estimates for Demographic Groups","The Connection Between Bayesian- and Resampling-Based Inference in Small-Area Models","Evaluating the Performance of the 2008 Pre-Election Polls in the Primaries","Sources of Variation in Pre-Election Polling","Understanding and Communicating Sources of Measurement and Operational Error in Opinion Polls: Beyond Sampling Error","Keeping Up with Survey Respondents","Communicating with Survey Respondents at the UK Office for National Statistics","Customer and Respondent Outreach Initiatives at the Bureau of Labor Statistics","Preventing Falsified Survey Data","Evaluation of Alternative Prediction Models To Oversample Low-Income Persons in the Medical Expenditure Panel Survey (MEPS)","Comparison of Imputation Adjustment Techniques on Variance Estimation in the Medical Expenditure Panel Survey (MEPS)","Event Reporting in the Medical Expenditure Panel Survey (MEPS)","Sample Redesign and Conversion to a Windows-Based Survey Instrument: Impact on Part-Year Estimates from the 2007 Medical Expenditure Panel Survey (MEPS)","Comparison of Direct, Mixed Model, and Bayesian Metropolitan Statistical Area Estimates for the Insurance Component of the Medical Expenditure Panel Survey","Variance Estimation for Statistics Canada's Small Household Surveys in the Context of the Household Survey Strategy","Implementing Resampling Methods for Design-Based Variance Estimation in Multilevel Models: Using HLM6 and SAS Together","Performance of Bootstrap Variance Estimation for a Dual Frame Household Survey: Evidence from the German Panel Survey","BRR versus Inclusion-Probability Formulas for Variances of Nonresponse Adjusted Survey  Estimates","Application of Fay's Method for Variance Estimation in the National Compensation Survey Benefits Products","The Method of Laplace and BRR: A Hybrid Variance Estimation Method in Surveys","Bootstrap Variance Estimation for Predicted Individual and Population-Average Risks","Child Survival Theory and Practice: How Do They Match?","Child Mortality: What We Count Counts","How Can Statistics Save Lives?","So Why Are Millions of Children Needlessly Dying?","Designing an Audit System To Increase Voter Confidence in Elections","Engaging the Unengaged Voter: Vote Centers and Voter Turnout","Measuring Voting System Failures: Survey Evidence of the Frequency of Voting Problems in the 2006 and 2008 U.S. Election","Local Post-Stratification and Diagnostics in Dual System Accuracy and Coverage Evaluation for U.S. Census","The Prior Selection and Approximations in a Hierarchical Bayes Approach: An Application to the Small-Area Income and Poverty Estimation","Bayesian Penalized Spline Model-Based Estimation for Finite Population Proportion in Unequal Probability Sampling","Model Selection with Partially Synthetic Data","Calculating Cell Bounds in Contingency Tables Based on Conditional Frequencies","Using an Informative Missing Data Model To Predict the Ability To Assess Recovery of Balance Control After Spaceflight","Pattern Mixture Models Incorporating Reasons for Dropout","A Unified Capture-Recapture Model","Assessing Bias Associated with Missing Data from Joint Canada/U.S. Survey of Health: An Application","Modeling of Longitudinal Biomarker Data with Dropout and Death Using a Weighted Pseudo--Maximum Likelihood Method","Respondent-Driven Sampling in an HIV at-Risk Population: The Analogy with Markov Chain Monte Carlo","Statistical Challenges Facing Cell Phone Surveys","Adjustment for Noncoverage of Nonlandline Telephone Households in an RDD Survey","Cell Phone--Only Household in a National Mail Survey: Who Are They?","RDD Telephone Surveys: Reducing Bias and Increasing Operational Efficiency","Comparison of the Wireless-Only and Landline Populations in a Small Pilot Immunization Study","Responsive Design for Random Digit Dial Surveys Using Auxiliary Survey Process Data and Contextual Data","Where Have All the Smokers Gone? A Simple Strategy for Targeting Rare Subgroups for RDD Studies","Imputation of Missing Data for the Pre-Elementary Education Longitudinal Survey","A Study of Imputation Alternatives for the Quarterly Financial Report","Extension of Fractional Imputation to General Missingness Patterns using Maximum Likelihood","Study of Item Nonresponse for a Sample of Questionnaires for the 2007 Commodity Flow Survey (CFS)","Data-Driven Post Modeling Sensitivity Analysis To Detect Missing Data Mechanism","The Fraction of Missing Information as a Tool for Monitoring the Quality of Survey Data","Measuring Income and Poverty in Four Surveys: An Overview","How Much Is It Worth: Comparing the Policy-Analytic Value of Wealth Data from Four Surveys","Measurement Issues Associated with Using Survey Data Matched with Administrative Data from the Social Security Administration","A Comparison of the Health Insurance Coverage Estimates from Four National Surveys and Six State Surveys","When Good Weights Seem Bad: Nuances of Sample Weights in the NHANES","Producing Local Area Estimates for NHANES","Challenges and Consequences from Combining National Health Surveys with Air Monitoring Data","Comparing Methods for Left-Censored Data Using Two-Parameter Lognormal and Johnson's SB Distributions: The Case of Environmental Data from NHANES","Methods and Issues in Trimming Extreme Weights in Sample Surveys","Examining Mode Differences in the Health Information National Trends Survey","An Analysis of Mode Effects in Three Mixed-Mode Surveys of Veteran and Military Populations","Using Multiple Data Sources To Identify Types and Sources of Coverage Errors on an American Indian Reservation","Using Administrative Records to Improve Within Household Coverage in the 2008 Census Dress Rehearsal","Bilingual Census Questionnaire Design Test","Impact of New Income Questions on Reported Income in the National Survey on Drug Use and Health (NSDUH)","Calendarization of the Goods and Services Tax Data: Issues and Solutions","Efficiency Comparison for Constrained Generalized Least Squares Estimators in a Panel Survey","Matching Surveys in Longitudinal Studies","Exploration of the Use of Empirical Bayes Procedures for Estimating Changes in Occupancy Rate and Persons per Household","Using the SIMEX Method To Estimate Temporal Change for a High-Scoring Group","Efficient Calibration for Some Variants of Double Sampling","Analysis of an Outcome-Dependent Enriched Sample Using Semiparametric Likelihood Method","Providing Access to Business Microdata: The International Experience","Defining Business Data Needs","Providing Remote Access to Business Micro Data: Lessons Learned","Microdata Access","The Measure of Mollie Orshansky","Hierarchical Bayes Estimation for Bivariate Binary Data with Applications to Small-Area Estimation","Can Calibration Be Used To Adjust for 'Nonignorable' Nonresponse?","NMAR Nonresponse with Limited Covariate Information","Variations in Effect Estimation Techniques To Assess Graduated Driver","Quasi-Likelihood Generalized Linear Regression Analysis of Fatality Risk Data","Estimating Nonresponse Bias in the Omnibus Household Survey","Safety Analyses of Signalized Intersections Using Bayesian Hierarchical Spatial Models","Analyzing Time Delay of Highway Construction Transportation Contracts: An Exploratory Study","Single Phase Variance Estimation Approach to Two-Phase Designs","Some Notes on Cell Collapsing and Its Effect on Replicate Variance Estimates with the Delete-a-Group Jackknife Variance Estimator","Variance Estimation for an Estimator of Between-Year Change in Totals from Two Stratified Bernoulli Samples","Analyzing Generalized Variances for the American Community Survey 2005 Public Use Microdata Sample","Estimating Observation Outlyingness Using Statistical Distances in National Resources Inventory","An Alternative to the Logit-Wald Method for Inference with Models for Proportions","A Simulation Study of Post-Adjustment Bootstrapping of Final Weights in the General Social Survey","Minimizing Conditional Global MSE for Health Estimates from the Behavioral Risk Factor Surveillance System for U.S. Counties Contiguous to the United States-Mexico Border","Results from the 2006 Canadian Census Weighting Process","Weighting Class versus Propensity Model Approaches to Nonresponse Adjustment: The SDR Experience","Response Patterns Among New Businesses","Multilevel Analysis with Informative Weights","Weighting Methods in School-Based Surveys","What Are the Crucial Error Rates To Consider in Sample-Size Analysis?","A Good N Despite a Bad Start: A Practical Guide to Easy Internal Pilots","Best Practices in Collecting Survey Data on Sexual Orientation","Evaluation of Multiple Imputation by Ordered Monotone Blocks in an Anthrax Vaccine Trial","Evaluation of Imputation of Covariates in an Impact Analysis with Regression Adjustment","Multiple Imputation for Protecting Data Confidentiality: Applications by the German Institute for Employment Research","Heavy Children in Motor Vehicle Crashes: Imputation for Covariates with Missingness and Measurement Errors","Developing a Sampling Frame for Postdoctoral Researchers","Assessment of Lists for Building a Sample Frame of Academic Postdoc Employers","Frame Improvements for the 2007 Commodity Flow Survey","Getting an Establishment Survey to the Right Person Within the Organization","Applying Federal Data Security Policy to Statistical Agency Practices","U.S. Census Bureau's Approach To Address OMB's Guidelines and Rules Relating to Privacy Breaches in Federal Statistical Agencies","The Impact of a Privacy Breach on Survey Participation in a National Longitudinal Survey","Iraq Data on Mortality: What Can We Believe?","Assessing Mortality in Conflicts: A Comparison of Surveys from Iraq, Darfur and the Democratic Republic of Congo","Nonviolence-Related Mortality in Iraq: Evidence from Iraq Family Health Survey (IFHS)","Ethical and Data-Integrity Problems in the Second Lancet Survey of Mortality in Iraq","Diagnostic Measures for Generalized Linear Models with Missing Covariates","An Exact Noniterative Sampling Procedure for Discrete Missing Data Problems","Bayesian Variable Selection for Modeling Repeated Binary Responses and Time-Dependent Missing Covariates","A Joint Modeling Approach to Repeated Measures of Mixed Data Types","Bayesian Structural Equations Models for Longitudinal Surveys Data with Missing Responses and Covariates","Weight Smoothing Models in Clustered or Cross-Classed Sample Designs","Weight Development for Outliers in a Panel Sample","Alternative Methods To Adjust for Nonresponse in the American Community Survey (ACS)","Evaluating Use of Alternative Population Controls for American Community Survey Weighting Methodology","Analyzing Weighting Methods in the Federal Human Capital Survey","The 2006 National Health Interview Survey Paradata File: Overview and Applications","Analyzing Field Notes Systematically To Better Understand Respondent Participation","An Exploration into the Use of Paradata for Nonresponse Adjustment in a Health Survey","Do You Really Mean What You Say? Doorstep Concerns and Data Quality in the National Health Interview Survey (NHIS)","Characterization, Evaluation, and Management of Systemic Risk in Surveys","Mode Effects on In-Person and Internet Surveys: A Comparison of the General Social Survey and Knowledge Networks Surveys","Comparing Estimates and Data Quality from the School Health Policies and Programs Study (SHPPS) 2006 for Mail and Telephone Data Collection: District-Level Questionnaires","Assessing Mode Bias in a Mixed Mode Reproductive Health Survey","2006 Canadian Census Internet Mode Effect Study","Evaluating Frames and Modes of Contact in a Study of Individuals with Disabilities","Measuring Impact of Nonignorability in Longitudinal Data with Nonmonotone Nonresponse","A Two-Stage Approach to Survival Analysis with Time-Dependent Covariates from a Time-Varying Means Model: Application of Time to Initiation of Therapy in HIV-Infected Women vs. Men","Weighting Method for Binary Longitudinal Data with Incomplete Covariates and Outcomes Incorporating Auxiliary Information","Variance Estimation for Correlated Data","Overview of the Survey of Occupational Injuries and Illnesses Sample Design and Estimation Methodology","Evaluating the Within-Household Selection Procedures for in-Person U.S. Adult Literacy Surveys","Developing Statistical 'Twins' Methodology for Selecting Sites for Qualitative Case Studies","Sample Design and Sample Selection for Adult Multivitamin Mineral Study","Optimized Whole-Sample Procedures vs. Traditional Draw-by-Draw Procedures","Intraclass Correlation Patterns of Cognitive and Behavioral Measures of Illicit Drug Use Within Six Major Metropolitan Areas","Drawing a Sample from a Given Distribution","Update on Use of Administrative Data To Explore Effect of Establishment Nonresponse Adjustment on the National Compensation Survey Estimates","Empirical Evaluation of Raking Ratio Adjustments for Nonresponse","The Bitter End? The Close of the 2007 SCF Field Period","Identifying, Collecting, and Using Auxiliary Variables To Adjust for Nonresponse Bias in Organizational Surveys","Incorporating Multiple Reasons for Attrition into Analysis of Longitudinal Data","Subjective and Objective Numeracy and Disclosure Risk in Surveys","An Overview of the 2010 Redesign Program at the U.S. Census Bureau","An Overview of Primary Sampling Units (PSUs) in Multistage Samples for Demographic Surveys","Using a MAF-Based Frame for Demographic Household Surveys","Innovations in Survey Redesign at the U.S. Census Bureau","Monitoring Quality Control: Can We Get Better Data?","Measurement Error Adjusted Effects of A-Bomb Radiation Dose on Cancer Risk","Assessing the Impact of Measurement Error in Modeling Change: A Sensitivity Analysis Approach","Modeling Biases in Observational Data Using Bayesian Graphical Models To Combine Multiple Data Sources: Application to Low Birth-Weight and Water Disinfection Byproducts","Bounds on ACE and Unmeasured Confounding Bias in Observational Studies","Misclassification Adjustment in Threshold Models for the Effects of Subject-Specific Exposure Means and Variances","Statistical Methods for Biodosimetry in the Presence of Both Classical and Berkson Measurement Error","Disclosure Risk Assessment for Queriable Web-Based Reporting Systems in a Public Health Context","Controlling Disclosure Risk in Synthetic Public Use Microdata: The Longitudinal Business Database","Producing Partially Synthetic Data to Avoid Disclosure","Selective Rounding: A Better Alternative to Conventional Rounding of Tabular Data","Truncated Triangular Distribution for Multiplicative Noise and Domain Estimation","Evaluation of Error Components in a Simulation-Based Evaluation of a Survey Procedure","A Protocol Calibration Experiment in a Longitudinal Survey with Errors-in-Variables","Relating Self-report and Accelerometer Physical Activity with Application to NHANES 2003-2004","An Application of Matrix Sampling and Multiple Imputation: The Decisions Survey","Multiple Imputation for Latent Variables in Classical Test Theory for Cluster Sample ","Adaptive Matrix Sampling for the Consumer Expenditure Interview Survey","Imputing Responses to Nonexistent Questions","National Election Scorecard","Voter Confidence and the Election Day Voting Experience","How ASA Members Are Helping States Improve Elections","Releasing Microdata: Disclosure Risk Estimation, Data Masking, and Assessing Utility","Web-Based Data Query Tools: Meeting User Needs","New Ways To Provide More and Better Data to the Public While Still Protecting Confidentiality","Models for Callback Procedures and Mode Effects in Household and Establishment Surveys","Efficiency and Bias in Differential Contact Strategies","Nonresponse Bias Analysis Using Critical Item-Only Respondents for the National Survey of Recent College Graduates","Properties of Callback Procedures Under Moderate Deviations from Specified Models","Estimation and Inference for Group-Randomized Trials with a Binary Outcome","Power for Clustered Gaussian Data","Estimation and Inference for Clustered Gaussian Data","Internal Pilot Designs for Cluster Samples","Assessing the Effect of Calibration on Nonresponse Bias in the 2005 Agricultural Resource Management Survey Phase III Sample Using Census 2002 Data","Adjustments for Mode Effect Bias for the Canadian Community Health Survey","Methods To Promote Deeper Processing on Survey Questions","Use of a Listed Sample To Supplement and Improve the Accuracy of a Probability Sample","Fuzzy Matching of Federal Science and Technology Program Data","Maximum Likelihood Estimation for Categorical Data with an Almost-Missing-at-Random Mechanism","Medicaid Enrollment: The Relationships between Survey Design, Enrollee Characteristics, and False-Negative Reporting","Reconciling Employment Differences Between Administrative and Survey Data","Comparison of Business Revenues from Two Administrative Files","Reconciling Differences in Income Estimates Derived from Survey and Tax Return Data","Estimating the Distribution Function Using Ranked Set Samples with Imperfect Ranking","Using Regression Discontinuity Design for Program Evaluation","Charactering the Propensity To Volunteer in America","B-Splines and Bootstrapping for Piecewise Logistic Regression in Complex Samples","Inference in Sampling Problems Using Regression Models Imposed by Randomization in the Sample Design - Called Pre-Sampling ","Robust Predictions Based on Model-Based Approaches","The Two Sample Problem","Improved Ratio Estimators in Adaptive Cluster Sampling","Under-Reporting of Energy Intake in the Canadian Community Health Survey","Analysis of Survey-Based Usual Intake Nutrition Data: The Issue of Within-Person Variation","National Nutrition Data: Methodological and Analytic Experiences of NHANES","NaN","NaN","NaN","Response Analysis Survey: Examining Reasons for Employment Differences Between the Quarterly Census of Employment and Wages and the Current Employment Statistics Survey","Application of Piecewise Quadratic Density Estimator to OES Wage Data","Getting to the Top: Reaching Wealthy Respondents in the SCF","The Effect of Late-Filed Returns on Population Estimates: A Comparative Analysis","An Analysis of e-Commerce and American Consumers","An Investigation on Response Rate for Best Survey Estimates of Inflation Expectations","Guiding Young Professionals to Be Successful in Government, Academia, and Industry","Estimated-Control Post-Stratified Variance Estimators for Proportions","Efficacy of Post-Stratification in Complex Sampling Designs","Variance Estimation in Complex Surveys with One PSU per Stratum","An Investigation of Stratified Jackknife Estimators Using Simulated Establishment Data Under an Unequal Probability Sample Design","Identification of Functional Forms and Predictor Variables in Generalized Variance Functions for Price Indexes","Developing Guidelines Based on CVs for When One-Year Estimates Can Be Used Instead of Three-Year Estimates in the American Community Survey","Evaluating the Asymptotic Limits of the Delete-a-Group Jackknife for Model Analyses","OnTheMap: An Innovative Mapping Tool","Statistical Consulting Within the Internal Revenue Service","Recent Developments in Internet Data Collection Methods Used at EIA","A Fresh Approach to Agricultural Statistics: Data Mining and Remote Sensing","Correlates of Data Quality in the Consumer Expenditure Quarterly Interview Survey","Comparison of Web-Based versus Paper-and-Pencil Administration of the YRBS: Participation, Data Quality, and Perceived Privacy and Confidentiality by Mode of Data Collection","Mode Effects and Data Quality on a Survey of New Businesses","Bayesian Multiscale Multiple Imputation with Implications to Data Confidentiality","Summary of Methods and Preliminary Assessment of the SIPP Synthetic Beta, Version 5.0","Responsible Data Releases","Examining the Robustness of Fully Synthetic Data Techniques for Data with Binary Variables","Modeling the Difference in Interview Characteristics for Different Respondents","An Evaluation of Nonresponse Bias Using Paradata from a Health Survey","Use of Paradata to Manage a Field Data Collection","Using the Fraction of Missing Information to Monitor the Quality of Survey Data","Subunit Nonresponse in the National Health Interview Survey (NHIS): An Exploration Using Paradata","Use of Statistics at the Centers for Disease Control and Prevention and National Cancer Institute: Estimation of the Number of Deaths Associated with Body Weight","How Numbers Rule: Pitfalls and Practicalities from Health Policy","Using Statistical Analyses to Inform Congressional Decisions","Building Statistical Capacity Globally","A Modeler's Perspective on Survey Weights","Accounting for Complex Sample Designs via Mixture Models","Power-Shrinkage: An Alternative Method for Dealing with Excessive Weights","Using Multivariate Spatial Statistics in the Modeling of Rate-Based Diffusion Processes: An Extension and Replication of Cohen and Tita","Extended Bootstrap Bias Corrections with Application to Multilevel Modeling of Survey Data Under Informative Sampling","Adaptive Hierarchical Bayes Estimation of Small-Area Proportions","The Prior Selection and Approximations for the Nested Error Regression Model: Estimation of Finite Population Mean for Small Areas","The Generalized ANOVA: A Classic Song Sung with Modern Lyrics","Evaluation of PPS Sampling for the Medical Expenditure Panel Survey","Evaluation of Design Effects for Selected Estimates in the Medical Expenditure Panel Survey","Assessment and Evaluation of Nonresponse Error in the Medical Expenditure Panel Survey","Event Reporting in the Medical Expenditure Panel Survey (MEPS) by Event Type","Comparison of Imputation Adjustment Techniques on Variance Estimation in the Medical Expenditure Panel Survey","Toward Quantifying Disclosure Risk for Area-Level Tables When Public Microdata Exists","Measures of Data Utility for Complex Survey Data","Investigation of Variance Properties of Noise-Infused Estimates for the Survey of Business Owners (SBO)","Data and Metadata from the Terminological Perspective","GeoFrameTM: A Technological Advancement in Field Enumeration (Part 2)","General Discrete-Data Modeling Methods for Producing Synthetic Data with Reduced Reidentification Risk That Preserve Analytic Properties","Efforts to Assist Users with American Community Survey Data","Sample Design for the Census 2010 Experimental Program","Coverage of the 2011 Canadian Census of Agriculture","'You Really Have to Puzzle This Out': Checking Residence and Coverage Duplications on a Census 2010 Questionnaire","Overview of Evaluations of the 2010 Census Coverage Measurement Program ","Missing Data Methods for CCM Component Error Estimation","Is There an Undercount of Medicaid Participants in the ACS Field Test?","Redesign of SOI's Individual Income Tax Return Edited Panel Sample","Can Taxpayer Characteristics Determine the Possibility of Filing Future Tax Returns?","Analysis of the Distributions of Income, Taxes, and Horizontal and Vertical Equity Using Cross-Section and Panel Data from Individual Tax Returns","Getting to Know US Taxpayers: Selected Tax Data by Occupation and Industry, Tax Years 2003--2005","Using Sample Data to Reduce Nonsampling Error in State-Level Estimates Produced from Tax Records","Model versus Randomization-Based Inference from Group Randomized Trials","Real-Life Ethical Dilemmas Encountered in the Practice of Statistics: Resolution Leading to Policy Change","Ratio Estimation of the Mean with Unequal Probability Samples: Hansen, Madow, and Tepping Revisited","Modified Ratio Estimators in Simple Random Sampling","An Introduction to Pre-sampling Inference","Model-Based Approach to Small Area Estimation of Disability Count and Rate Using Data from the 2006 Participation and Activity Limitation Survey","Some Methods of Model-Based Sampling ","Rethinking Privacy and Disclosure Limitation from a Cryptographic Perspective","Rethinking the Risk-Utility Tradeoff Approach to Statistical Disclosure Limitation","Rethinking Official Statistical Disclosure Limitation Procedures from a Cryptographic Privacy Perspective","Speeding Up the Asymptotics When Constructing One-Sided Coverage Intervals for Survey-Weighted Estimates","Estimation Using Gaussian Replicates of the Pivotal Based on Weighted Quasi-Score Vector","Empirical Likelihood--Based Calibration Methods for Missing Data Problems","Options for Allocating the American Community Survey Sample","Evaluation of the Effectiveness of the American Community Survey Family Equalization Project","Using Sub-County Population Estimates as Controls in Weighting for the American Community Survey","Usability of the American Community Survey Estimates of the Group Quarters Population for Substate Geographies","Assessment of Data Release Rules on the Reliability of Multi-Year Estimates in American Community Survey Data Products","A Simulation Study of Alternative Weighting Class Adjustments for Nonresponse When Estimating a Population Mean from Complex Sample Survey Data","An Evaluation of Weighting Methodology in an RDD Survey with Multiple Population Controls","Evaluation of Randomization-Based Estimation and Inference Methods ","An Empirical Study of Nonresponse Adjustment Methods for the Survey of Doctorate Recipients","A Simulation Study to Compare Weighting Methods for Survey Nonresponses","Design and Weighting Issues for the Dual-Frame Household Panel Survey 'Labor Market and Social Security'","Overview of Software that Will Produce Sample Weight Adjustments","Research on Quarterly Benchmarking for the Current Employment Statistics Survey","Using Current Employment Statistics (CES) Survey Data to Estimate Employment in Expanding and Contracting Establishments: Preliminary Results and Issues","Numbers as Pictures: Examples of Data Visualization from the Business Employment Dynamics Program","Size Class Dynamics: Small and Large Firms in the 2008 Recession","Dynamics of Business Growth","Improving the Job Openings and Labor Turnover Survey's Sampling Procedure","The Impact of High Variances at the Lowest Aggregate Levels on the CPI's All-US-All-Items Variance","Election Polling Challenges: Cell Phones, the Bradley Effect, and Voter Turnout","Methodological Issues in ABC News 2008 Pre-Election Polling","Limitations of Recorded-Voice Telephone Polling in Election 2008: Is This Method of Data Collection Doomed in 2012?","Inferences from Matched Samples in the U.S. National Elections from 2004 to 2008","Socioeconomic Differences in Life Expectancy","Using Linked Survey and Administrative Data to Build Imputational Models to Adjust Survey Estimates of Medicaid Coverage","Are Expenditures Higher for Those Entering Medicare at Age 65 Having Been Previously Uninsured?","One More Step: NHIS-Linked Mortality Data Combined with EPA Air Quality Data","Fitting Models and Estimating Model Parameters Using Data from Complex Surveys","Health Surveys and the Survey Statistician ","The Quality of ACS Estimates for Small Population Groups","Evaluating the Age Dimension at the PUMA Level in the Three-Year Estimates from the American Community Survey","Improving the Utility of Three-Year ACS Data: A Transportation Perspective","Posterior predictive checking of imputation models","Analyses Based on Combining Similar Information from Multiple Surveys","Performance of Sequential Imputation Method in Multilevel Applications","The Practice of Imputation Methods with Structural Equation Models","Imputation of Gaps in Transaction Sequences","Imputation of Income, Poverty, and Medicaid Status in the Ohio Family Health Survey","Imputation of Nominal Variables Using Gaussian-Based Routines","Response Improvement Strategies for the 2007 Economic Census","Strategies to Improve Response Rates for Current Economic Programs","Improving Response Rates for the 2007 Census of Government Employment","Taking the Hard Work Out of Privacy: Interactive Data Analysis with End-to-End Privacy Guarantees for All","Integrating Differential Privacy with Statistical Theory","Differentially Private Categorical Data Analysis ","Valid Statistical Analysis for Logistic Regression with Multiple Sources","Exploratory Data Analysis at the Boundary of Statistics and Engineering Design Optimization","Planning Surveillance for a Stockpile That Might Degrade","Scatterplots for Sampling Weighted Data","Disclosure Limitation Techniques for Tabular Data","The Relationship Between Reliability and Misclassification in Physician Quality and Cost Profiles","Innovative Guidance for the Design of New Surveys or Major Revisions of Existing Surveys","Assessing Nonresponse Bias in the International Price Program's Import and Export Price Index Surveys","Effects of Imputation on CPS Income and Poverty Series: 1981 - 2007","Modeling Nonresponse and Under-Reporting in Response in Surveys of the Arrestee Population","Analysis of Nonresponse Bias in the Early Head Start Research and Evaluation Project","Effect on Oral Health Estimates of Response Disparities: Results from the Survey of Oral Health Status, Maryland School Children 2005--2006","Effect of Survey Modes on Sampling Design, Coverage, and Nonresponse in Surveys of Veteran and Military Populations","Computing Response Rates for Probability-Based Web Panels","A Model-Based Method for Estimating Serious Mental Illness Using Household Interview Data Combined with Clinical Interviews","What Is 'Misuse' of Prescription Drugs, and How Do We Measure It?","Searching for a Gold Standard: Comparisons of NSDUH Data with External Sources to Assess Coverage and Validity","Evaluating the reliability of NSDUH data on drug use, mental health and demographics","Model and Variable Selection in Hierarchical Small Area Models","Small Area Estimation Under Fay-Herriot Models with Nonparametric Estimation of Error Variances","Extended Small Area Models for Analysis of Nonlinear Effects and Extremes","Small Area Estimation for Population Counts in the German Census 2011","Using Survey Data to Investigate Changes Over Time in Health Indicators","Weighting Methods for Population-Based Case-Control Study with Complex Sampling","Using Projection Splines to Explore Racial Differences in Gestational Age Distribution Among Very Low-Risk Women","Log-Linear Modeling of Health Services Use in a Sample Survey","Cross-Validation in Survey Estimation: Model Selection and Variance Estimation","Bayesian Benchmarking with Applications to Small Area Estimation","A Bayesian Adjustment for a Selection Bias in Genetics","New Developments in Model-Based Small Area Estimation","Why Is Survey Research 20 Years Behind?","Optimizing Call Time of Day in an RDD Survey","Using Sample Statistics to Quantify Community Building","Imputation Methods for Adaptive Matrix Sampling","Testing Hardy-Weinberg Equilibrium Using Family Data from Complex Surveys","Pretesting 2010 Census Questionnaires for People with Atypical Living Situations","One-Time Contact for a Census Internet Survey: Is It Sufficient? Washington State Professional Health Worker Census Internet Mode Effect and Response Rate Study","Are 'Do Not Include' Statements Helpful?","Visualization of Complex Survey Data","Variability in Life-Table Estimates Based on the National Health Interview Survey Linked Mortality Files","Model-Based Methods in Analyzing Complex Survey Data: A Case Study with National Health Interview Survey Data","Estimating the Variance of Between-Year Change in Domain-Level Totals ","Characteristic Function for and Moments of the Truncated Triangular Distribution","Developing models of respondent fatigue to guide the order in which to ask survey questions","Nonresponse Bias: Telephone Point-of-Purchase Survey","Alphabetical Placement in Surveys of Persons at Institutions: A Simulation Study","A Weighting Methodology for Complex Surveys","Reproducing Nonresponse Adjustments in Replicate Weights","Bayes Model for Inference on Vehicular Traffic Density at the Main Campus of University of Lagos","Sequential Modeling for Contact and Cooperation Propensity for the United States Military Health Survey","A Noninformative Bayesian Approach to Small-Area Estimation","Towards a Unified Framework for Inference with Aggregated Relational Data","Developing the Dual Frame Design for the 2010 National Survey of College Graduates","Missing Data and Complex Samples: The Impact of Listwise Deletion vs. Subpopulation Analysis on Statistical Bias and Hypothesis Test Results When Data Are MCAR and MAR","A Comparison of Address-Based Sampling and Random Digit Dialing Sampling","Assessing Survey Estimates of Intent to Leave with Personnel Data","Model Selection in Linear Mixed Effects Models","Translation of Agreement Answer Scales: Effects on Category Response Curves","Assessing the Convergence of Multiple Imputation Algorithms Using a Sequence of Regression Models","Optimal Probability Weighting Methods in Longitudinal Models for Data with Nonignorable Unequal Selection and Nonignorable Wave Nonresponse","Sampling Weights for Analysis of Couple Data in Demographic and Health Surveys","Issues Associated with Measuring Activities Associated with Seeing and Hearing Across National Surveys","Effectiveness of Nesting Age, Race, and Gender for Weighting Estimates of Radio Listening","Analysis of Data from Complex Survey Samples: Propensity Scores and Survey Weights","Confidence Intervals for Radio Ratings Estimators","Analysis of Nonresponse in a Social Survey with the Sharp Bounds Method","Propensity Score Methodology for Nonignorable Nonresponse","Responsive Design for Random Digit Dial Surveys Using Auxiliary Survey Process Data and Contextual Data","The Rao, Hartley, and Cochran Scheme with Dubious Random Nonresponse in Survey Sampling","Propensity Score Matching to Correct Telephone Surveys for Cell Phone Nonresponse","Simulation Studies of a Latent-Class Selection Model for Nonignorable Missing Data","Are We Adjusting Response Rates or Survey Variables? The Effects of Multiple Auxiliary Variables on Nonresponse Adjustment","A Simulation Study of the Distribution of Fay's Successive Difference Replication Variance Estimator","Coverage Bias and Sampling Error in a Study Using 1000-Series RDD Sampling","Modeling County-Level Vaccination Coverage Rates Across Time","Small-Area Variance Modeling with Application to County Poverty Estimates from the American Community Survey","A Small Area Procedure For Estimating Population Counts","Using Predictive Marginals to Produce Standardized Estimates","A National Evaluation of Coverage for a Sampling Frame Based on the Master Address File","Content and Coverage Quality of a Commercial Address List as a National Sampling Frame for Household Surveys","Examining Blocks with Lister Error in Area Listing","Impact of Master Address File Coverage on Survey Estimates","Models for the Characterization and Management of Costs and Risks During Changes in Survey Design","Measurement Error Models for Physical Activity Measures","How Misclassification of Race/Ethnicity Categories in Sampling Stratification Affects Survey Estimates","Estimating Response Bias and Response Variance (Simple and Correlated) from Reinterviews","Effects from Respondent Location on Telephone Survey Estimates","Using Statistical Models for Sample Design of a Reinterview Program","Identifying Outliers When Creating an Imputation Base for the Quarterly Financial Report","Identifying Sources of Survey Errors: The 2007 Classification Error Survey for the U.S. Census of Agriculture","Bayesian Inference of Finite Population Distribution Functions and Quantiles from Unequal Probability Samples","Estimating Variance Components Using Random Forest","An Adaptive Method for Collapsing Strata Using Regression Trees on Data from a Complex Design","Simultaneous Calibration and Nonresponse Adjustment","Collinearity Diagnostics for Complex Survey Data","Dan Horvitz: His Life and Work as a Statistician","Some Generalizations of the Horvitz-Thompson Estimator","Horvitz-Thompson Estimation and Unit Nonresponse ","Imputation and Estimation Under NMAR Nonresponse with Limited Covariate Information, Revisited","Federal Policy on Collecting and Using Social Security Numbers for Statistical Purposes","The Effect on Linkage Rates and Mortality Ascertainment of Partial SSN Matching to the National Death Index","Tracing Survey Respondents Without SSNs","Use of Income Tax Data of Individuals for Demographic Purposes","Register Data Are a Good Backbone but Sometimes Restricted for Policymakers: Experiences from Finland Over Decades","Small Area Estimation of the Prevalence of Cancer Risk Factors and Screening via Bayesian Methods Using Combined Information from Two Surveys","The Role of Meta-Analysis in Assessing Adverse Drug Effects","Bayesian Shrinkage Models for Associations Involving Sparse Data","Evaluating the 2010 Census","Measuring Coverage in the 2010 U.S. Census","An Integrated Approach to Communications","An Overview of Progress on the 2010 Census","Mean and Sensitivity Estimation in Optional Randomized Response Models","Survey Mode Effects in Two Military Surveys","Data Collection Mode Effects: Empirical Comparisons Between CATI-Only vs. Mixed Mode in the 2003 National Survey of Recent College Graduates","The Franklin's Randomized Response Model for Two Sensitive Attributes","Sample Surveys with Sensitive Questions: A Nonrandomized Response Approach","Looking for a Needle in a Haystack: A Multi-Mode Approach to a Survey of Rare Subpopulations","Response Mode and Bias Analysis in the IRS' Individual Taxpayer Burden Survey","Using Addresses as Sampling Units in the 2007 Health Information National Trends Survey","Indirect Sampling in Context of Multiple Frames","A Comparative Evaluation of Traditional Listing vs. Address-Based Sampling Frames: Matching with Field Investigation of Discrepancies","Operational Results from an Address-Based Sampling Pilot for the National Immunization Survey","Cross-Community Comparison and Multi-Frame Weighting in REACH US","The Use of Address-Based Sampling (ABS) with Multi-Mode Data Collection: The Case of REACH (Racial and Ethnic Approaches to Community Health) US Risk Factor Survey","Bayesian Sensitivity Analysis of Incomplete Data Using Pattern-Mixture and Selection Models Through Equivalent Parameterization","Semiparametric Estimation for Longitudinal Data with Nonignorable Nonmonotone Missing Responses","Extensions of Proxy Pattern-Mixture Analysis for Survey Nonresponse","A semi-parametric approach to fractional imputation for nonignorable missing data","Link-Tracing Sampling: Estimating the Size of a Hidden Population in Presence of Heterogeneous Nomination Probabilities","Adjusting Sampling and Weighting to Account for Births and Deaths","Sample Design and Estimation of Volumes and Trends in the Use of Paper Checks and Electronic Payment Methods in the United States","An Application of Two-Way Stratification Procedure for Selection Under Three-Way Stratification","PPS Network Sampling with Partial PSU Replacement","Longitudinal Surveys versus Continuous Surveys and Surveys with Flexible Periodicity","A Russian Invasion, Skeptical Villagers, and Controversial Elections: The 'Power' Plays Faced by Randomized Impact Evaluations in Developing Countries","A Study of the Composite Estimator for Change Rate","A Simulation Study of Treatments of Influential Values in a Monthly Retail Trade Survey","Multiple Imputation for Missing Items in Multi-Section Questionnaires","A Bayesian Approach to Predicting the Electoral Vote Totals for the 2008 Presidential Election","Estimation Procedure for New Public Employment Survey Design","Election Day 2008 Voter Scorecard","New Information for Policymakers: National Ambulatory Medical Care Survey and National Hospital Ambulatory Medical Care Survey","Future of Random Digit Dial Telephone Surveys","Imputation Methods for Complex Survey Data: Best Practices and Next Steps","CANCELLED - ASA Advocacy Day","Methods for Analysis of Recall of Fires in a Retrospective Survey","Developing Macro Edits for the Census and Annual Survey of Government Finance","Empirical Evaluation of Imputation Methods on Quarterly Census of Employment and Wages Data","Imputation Variance Estimation by Multiple Imputation Method for the National Hospital Discharge Survey","Evaluation of Alternative Imputation Methods for the Public Libraries Survey","Methodological Issues in the Conversion from RDD to Address-Based Sampling","Address-Based Sampling and the 2008 American National Election Survey: A New Paradigm for In-Person Household Surveys","Recruiting Probability-Based Web Panel Members Using an Address-Based Sample Frame: Results from a Pilot Study Conducted by Knowledge Networks","Using Address Frames to Identify Cell-Phone-Only and Cell-Phone-Primary Households","Coverage Rates and Coverage Bias in Housing Unit Frames","Informing Potential Respondents About Disclosure Risk and Survey Participation","Data Access for the National Children's Study: Preliminary Plans","Disclosure Review Issues of Genetic Data","Application of the Truncated Distributions and Copulas in Masking Data","An Overview of Uncertainty Creation to Protect Statistical Data","Fence Method for Prediction Problems and Its Application in Small Area Estimation","Borrowing Strength Over Space in Small Area Estimation: Comparing Local and Spline-Based M-Quantile Models with Spatial Autoregressive Random Effects Models","Robust Small Area Estimation Using Penalized Spline Mixed Models","Design-Based Model Consistent Estimator for the Nested-Error Regression","A Random Pattern Mixture Model for Longitudinal Binary Outcome with Informative Dropouts","Outfluence: The Impact of Missing Values","Multiple Imputation for Drop-Out in Longitudinal Studies","Multiple Imputation in Right-Truncated Multivariate Normal Distribution, with Applications to RT-PCR","Robust Variable and Model Selection with Missing Data","A Three-Phase Model of Census Inclusion","Assessing the Effects of Imputation on Income Estimates in the U.S. Consumer Expenditure Survey: Comparison with the Current Population Survey","The Impact of Allocations as a Tool for Statistical Editing of Corporate Taxpayer Administrative Records","Propensity Scoring: An IRS Case Study in Determining the Relationship Between Bank Products and Noncompliance","Trends in Early Childhood Vaccination Coverage: Progress Toward U.S. Healthy People 2010 Goals","An Assessment of the Effect of Calibration on Nonresponse Bias in the 2006 Agricultural Resource Management Survey","Assessing Nonresponse Bias in the Consumer Expenditure Interview Survey","Conducting Nonresponse Bias Analyses for Two Business Surveys at the U.S. Census Bureau: Methods and (Some) Results","Components of Error Analysis in the Current Employment Statistics Survey","An Overview of Coverage Adjustment for the 2007 Census of Agriculture","Overview of the Census of Agriculture","Searching for Donors: Refining an Imputation Strategy","2007 Census of Agriculture Nonresponse Methodology","Impact of Outreach Initiatives on the 2007 Census of Agriculture","Achieving the Unique Objectives of the Canadian Health Measures Survey","Sample Redesign of Canadian Local Government Surveys","Reducing the Public Employment Survey Sample Size","National Sample Reallocation for the Occupational Employment Statistics Survey","Evaluation of Methods for Increasing Precision in DAWN: Stratification and Ratio Estimation","Progress on the Redesign of the Quarterly Tax Survey","Exploring Statistical Issues of Annual Sampling for the Current Population Survey","Statistical Estimation for Persons with Disabilities as Measured in the Current Population Survey","Labor Force Analysis of Those Identified as Disabled in the CPS","An Evaluation of Efforts to Use a Work Disability Question to Evaluate the Labor Force Situation of Persons with a Disability","The Addition of Questions on Disability to the Current Population Survey","The Changing Petroleum Marketing Industry as Viewed Through an Attribute Frame","Improvement of Data Quality Assurance in the EIA Weekly Gasoline Prices Survey","Improving the Edits in Petroleum Supply Surveys","An Improved Imputation Methodology Derived Through Regression Trees","Studying Millions of Rescued Documents: Sample Plan at the Guatemalan National Police Archive","Weighting for the Guatemalan National Police Archive Sample: Unusual Challenges and Problems","A Statistical Analysis of the Guatemalan National Police Archive: Searching for Documentation of Human Rights Abuses","Nonparametric Regression Estimators in Survey Sampling","Penalized Balanced Sampling: Nonparametric Guidance for Survey Design","Nonparametric Regression and the Two Sample Problem","The Federal Statistical System: Opportunities and Challenges for the Next 5--10 Years","Partial Synthesis of the Longitudinal Employer-Household Dynamics Database","The Synthetic Public-Use Release of the Longitudinal Business Database","Generating Multiply Imputed Synthetic Data Sets for a German Establishment Survey: The Agency's Perspective","Improving the Usefulness of Synthetic Data: Some Recent Advances","Using the National Health Interview Survey to Monitor Health Insurance and Access to Care","Access to Care and Objective Measures of Health: Results from the National Health and Nutrition Examination Survey (NHANES)","Health care access data for children and adolescents from the National Survey of Children with Special Health Care Needs and the National Survey of Children's Health","Using the National Health Care Surveys to Monitor Use and Access to Health Care","Imputing with Confidence","Multiple Imputations Quality Assessment for Survey Data","Diagnosing Imputation Models by Applying Target Analyses Under Posterior Replications of Observed Data","Multiple Imputation Framework for Combining Data from Multiple Sources","Establishing and Monitoring Statistical Standards in USDA-NASS","Implementing and Revising NCES Statistical Standards","Revising Statistical Standards to Keep Pace with the Web","American Community Survey Sample Size Research","Evaluation of the Effect on Cost and Variances of the Group Quarters Cluster Size","Analysis of the Variances of American Community Survey Estimates of the Group Quarters Population","An Analysis of Alternate Variance Estimation Methods for the American Community Survey Group Quarters Sample","Investigation of Data Release Rules for Medians and Zero Estimates for the American Community Survey","Network Model-Assisted Prevalence Estimation from Respondent-Driven Sampling Data","Marginally Specified Hierarchical Models for Networks and Relational Data: An Application to Health Policy","Modeling Networks When Data Are Missing or Sampled","Degrees of Uncertainty with Uncertain Degrees in Respondent-Driven Sampling ","Building-Block BLUPs for Aggregate-Level Small-Area Estimation for Survey Data","Empirical Likelihood for Small-Area Estimation","Robust Small Area Estimation Using a Mixture Model","Small-Area Estimation for Business Surveys","Variance Modeling in the U.S. Small Area Income and Poverty Estimates Program (SAIPE)","Small-Area Estimation Approach to Estimating the Association Between Traffic-Generated Air Pollution and Early Childhood Respiratory Problems","Methods for Optimizing the Overlap of Two Consecutive Surveys While Preserving First-Order Inclusion Probabilities","Regression-Based Data Fusion: Robust Residual Imputation","Integrating Sample Designs for Environmental Industry and Occupational Employment Surveys","More for Less? Comparing Small-Area Estimation, Spatial Microsimulation, and Mass Imputation","Statistical Disclosure Risk of Synthetic Datasets","An Estimation Method for Matrix Survey Sampling","Census Coverage Measurement Initial Housing Unit Matching Activities","Fast Record Linkage of Very Large Files in Support of Decennial and Administrative Records Projects","Current Records Linkage Research and Practice at the U.S. Census Bureau","Modelling the UK Labour Force Survey using a Structural Time Series Model","Using Point and Intervalized Data in Occupational Employment Statistics Survey Estimates","Determining the Appropriate Implementation of a Repeated Measures Analysis Comparing Two Periodic Estimates on Economic Survey Data","Evaluating the Effect of Dependent Sampling on the National Compensation Survey Earnings Estimates","Cell Phone Surveying in the United States: An Update from a 2010 AAPOR Task Force","Can You Maintain Confidentiality and Have Useful Data at the Same Time?","Statistical Graphics of Pearson Residuals in Survey Logistic Regression Diagnosis ","Evaluating Imputation Methods and Software for Missing Vaccination Data in the National Immunization Survey","Robust Covariate Control in Cluster-Randomized Trials with MPLUS and WinBUGS","Estimating the R-Indicator, Its Standard Error, and Other Related Statistics with SAS and SUDAAN","Causal Inference Using Semi-parametric Imputation","Multiple Imputation for causal Inference","Building Regression Trees on Data from a Complex Sample","Logistic Generalized Regression (LGREG) Estimator in Cluster Samples","Modeling Log-Linear Restricted Conditional Probabilities for Prediction in Surveys","Bayesian Quantile Regression in Stratified Sampling","Methods for the Analysis of Cognitive Interviews","Effects of Unbounded Interviews, Time-in-Sample, and Recency on Reported Crimes in the National Crime Victimization Survey","Measuring Time Use in Surveys: A Novel Validation of Survey Questions Through Experience Sampling","Latent Class Analysis in Computer Expenditure Reports","Verbal Paradata and Survey Question Content: How Question Sensitivity and Cognitive Complexity Influence the Way Answers (and Nonanswers) to Survey Questions Are Delivered","Roads to Rome: A Comparison of Alternative Approaches for Evaluating Survey Error by Integrating Data from the National Immunization Survey and State Immunization Registries","Split Sample Reinterview: Some Relationships","A Comparison of Mail and Face-to-Face Responses in a Dual-Mode Survey of Physicians","The Effects of a Mixed-Mode Experiment on Response Rates and Nonresponse Bias in a Survey of Physicians","Response and Mode Switching in a Sequential Mixed-Mode Design","Comparison of Different Models for Predicting the Probability of Interview Completion in Complex Telephone Surveys","Modeling and Simulation of Survey Collection Using Paradata","Contact Histories from the Survey of Grouped Individuals: Multivariate Multilevel Survival Analysis of Grouped Censored Data with Competing Risks","Providing Actionable Information from Sample Monitoring Data for Special Subsamples","An Alternate Approach to Assessing Misclassification in JAS","Using the Census of Agriculture List Frame to Assess Misclassification in the June Area Survey","Adjusting the June Area Survey for Non-response and Misclassification","Modeling Misclassification in the June Area Survey","A New Approach to Protect Confidentiality for Census Microdata with Missing Values","Procedures to Reduce the Risk of Disclosure in Health Surveys","Disclosure Avoidance for Census 2010 and American Community Survey Five-Year Tabular Data Products","(In)Effectiveness of Independent Rounding of Discrete Tabular Data as Statistical Disclosure Control Strategy","Data Access and Confidentiality Policies of the National Children's Study ","Joint Modeling of Epigenetic Data, Transcription Factor Binding Data, and Expression Data","Incorporating Chromosomal Spatial Correlation Through 3D Structures Into Microarray Data Analysis","Cancer Marker Identification via Penalized Integrative Analysis","Bayesian, Frequentist, or Both? Model-Robust Regression and the 'Sandwich' Estimator","Analyzing Methods of Imputation","Parallel Statistical Computing: Are We Embracing the Scalable Concurrency Revolution?","Generic Framework for Parallel Statistical Computing","Sequential Application of a Third-Order Test Design in Six Dimensions Incorporating an Orthogonality Constraint","Parallel Implementation of Response Surface Regression on R","Mode-Based Clustering with Applications to Information Visualization","A Comparative Study of Variable Screening Methods: Univariate vs. Multivariate Screening","Efficient Classification for Longitudinal Data","Adaptive Confidence Intervals for the Test Error in Classification","Energy Functions for Dimension Reduction and Graph Visualization","Grouping Pursuit and Feature Selection Over a Graph","Classfication Using Matrix Predictors with Application in Toxicant Identification","ROC Regression Using Percentile Values for Event Time Outcomes","Hill's Matrix Augments the Barell Matrix for Identifying Knee Injuries in Active Duty U.S. Army","A latent space representation of overdispersed relative propensity in ``How many X's do you know?' data","Development of Imaging Biomarkers for Clinical Trials: Applications in Glioblastoma Multiforme","Development of Imaging Biomarkers for Clinical Trials: Applications in Rheumatoid Arthritis","Comparison of Characteristics of Cell-Only, Cell-Mostly, and Landline Households","Dual-Frame Sample Sizes (RDD and Cell) for Future Minnesota Health Access Surveys","Dual-Frame Weights (Landline and Cell) for the 2009 Minnesota Health Access Survey","Modeling Multi-Time-Period State-Level Estimates for Wireless-Only, Wireless-Mostly, Landline-Mostly, and Landline-Only Households","The Use of Paradata to Monitor and Manage Survey Data Collection","Design and Operational Changes for REACH US","The Influence of Prior Experiences in Managing Current and Future Risks During Survey Transition Points on the National Survey on Drug Use and Health (NSDUH)","Evaluation of the Innovations Implemented in the 2009 Canadian Census Test","GIS for Automated Case and Segment Assignment for In-Person Studies","What Can 179 Million Phone Calls Indicate About the Possibilities for the Future of Telephone Data Collection?","Characteristics of Falsified Interviews","Minimizing Survey Error Through Interviewer Training: New Procedures Applied to the National Health Interview Survey","Time Varying Covariates in Markov Latent Class Analysis: Some Problems and Solutions","Incorporating Sampling Designs Into a Grade of Membership (GoM) Model","Generalized Structured Component Analysis with Latent Interactions","A Method for Improving List Building: Cluster Profiling","Using an Action-Research Model to Move from Conversational to Hybrid Standardized Interviewing: A Case Study","Using the Research Data Center to Analyze Genetic Data ","Reliability of Relative Standard Errors Computed from NHDS Public Use Data Files","Erosion Prediction with USLE and RUSLE2","Methodology Used for Measuring Use of Electronic Medical Records in Physician Practices in 2008 and 2009 NAMCS","Equivalence of Conditional Logistic Regression and Ordinary Logistic Regression via Infinite Replication of Observations","Time Series Analyses of Price Indices","Application of a Fay-Modified Balanced Repeated Replication Method to the National Health Interview Survey (NHIS)","The Impact of Small Cluster Size on Multilevel Models: A Monte Carlo Examination of Two-Level Models with Binary and Continuous Predictors","A Simulation to Evaluate the Impact of Design on Model-Based Methods for National Health and Nutrition Examination Survey (NHANES) Data Linked with Environmental Exposures","Spline Models for the Population Total from PPS Samples: The Precision Gain from Knowing the Sizes of Nonsampled Units","How Does Unusual Local Weather Affect Public Opinion About Climate Change?","Effective Sampling Methodology for Program Evaluation in Developing Countries","Improving the Propensity Score Equal Frequency Adjustment Estimator Using an Alternative Weight","Characteristics of Sampling Plans for Food Inspection in Japan","Incorporating and Managing Paradata Into Survey Operations for Quality Control and Cost Containment","Estimating Characteristics of Energy Expenditure using Measurement Error Models","Estimating Technology Adoption and Aggregate Volumes from U.S. Payments Surveys in the Presence of Complex Item Nonresponse","Assessing Nonresponse Bias and Measurement Error Using Statistical Matching","A Semiparametric Approach to Inference with Nonignorable Missing Data Using Surrogate Information","Analyzing Nonresponse Bias Among Kindergarten Teachers in the Head Start Family and Child Experiences Survey (FACES)","Incorporating Nonresponse Follow-Up Into a Main Survey Data Set","On Modeling and Estimation of Response Probabilities When Missing Data Are Not Missing at Random","Examining the Challenges of Missing Data Analysis in Phase Three of the Agricultural Resource Management Survey","Innovative Imputation Techniques Designed for the Agricultural","An Assessment of Imputation Methods for the USDA's Agricultural Resource Management Survey","Using Copula Methods to Create Synthetic Data Sets for Agricultural Applications ","Can Post-Stratification Adjustments Do Enough to Reduce Bias in Telephone Surveys That Do Not Sample Cell Phones? It Depends","Change in Nontelephone Household Measurement by the U.S. Census Bureau and Its Impact on Telephone Surveys Using a Nontelephone Household Post-Stratification Adjustment","Comparing Vaccination Estimates from Four Survey Designs: Vaccination Estimates from RDD, RDD+Cell, ABS, and Area Probability Sampling","Comparison of Influenza Vaccination Rates in Cell-Only, Cell-Mostly, and Landline Households in the National 2009 H1N1 Flu Survey","Profiles of Responses Over the 2006 Canadian Census Collection Period: What Are the Differences for Early and Late Responses?","Nonresponse in a Survey of Military Personnel: What Record Data Can Tell Us","2003 National Survey of College Graduates Nonresponse Bias Analysis","Do Characteristics of RDD Survey Respondents Differ According to Difficulty of Obtaining Response?","Try, Try Again: Response and Nonresponse in the 2009 SCF Panel","Assuming 'e'","Experimenting with Pre-Contact Strategies for Reducing Nonresponse in an Economic Survey","The Global Adult Tobacco Survey (GATS): Sample Design and Related Methods","Subsegments for One-Unit Acceptance-Rejection Sampling for Covering Missed Housing Units","Expected Number of Random Duplications Within or Between Lists","Sample Design Methodology for Studying Patients Using Registry Data","Is There Nonrandom Selection in Matched Administrative Earnings Data? Evidence from the Health and Retirement Study","Understanding the Effect of Job Satisfaction Moderated by Job Involvement on Interviewer Turnover","On Quality of Ancillary Data Available for Address-Based Sampling","Assessing Concurrent Effects of Substance Abuse Treatment Modalities Using Marginal Structural Models","Understanding Time-Varying Moderation Effects When Evaluating Substance Abuse Treatment Modalities Using Structural Nested Mean Models","Analysis of Rolling Group Therapy Data Using Conditionally Autoregressive Priors","Latent Class-Profile Analysis: An Application to Stage-Sequential Process of Under-Age Drinking Behaviors","Methodological Comparison of Estimates of Ambulatory Health Care Use from the Medical Expenditure Panel Survey and Other Data Sources","Evaluation of Health Care Utilization Data from the National Survey on Drug Use and Health","Estimating Total Emergency Department Visits: A Comparison of Two Data Sources","Application of Small-Area Estimation Methods to Emergency Department Data from the National Hospital Ambulatory Medical Care Survey","Assessing the Accuracy of Prescription Drug Data for Medicare Beneficiaries in the Medical Expenditure Panel Survey ","Providing Double Protection for Unit Nonresponse with a Nonlinear Calibration Routine","Doubly robust inference with missing data in survey sampling","Weights, Double Protection, and Multiple Imputation","Methodological Issues in the Meta-Analysis of Observational Studies","Issues in the Meta-Analysis of Observational Data","A Framework for the Meta-Analysis of Survey Data","The U.S. Federal Statistical System 2.0: Future Directions ","Inference for Changes in the Kappa Statistic in Multi-level Clustered Binary Data","Variable Selection for Multiply-Imputed Data ","Multiple Imputation of Disease Status Based on a Two-Phase Sample Design ","Spline-Based Models for Prediction in Survey Samples","Paradata in the European Social Survey: Studying Nonresponse and Adjusting for Bias","Assessing Contact History Data Quality and Consistency Across Several Federal Surveys","A Study of Modeling Longitudinal Nonresponse Using Paradata in the Survey of Labour and Income Dynamics (SLID)","A Response Propensity Modeling Navigator for Paradata","Address-Based Area Sampling: An Efficient Hybrid Between Address-Based Sampling and Area Sampling","Predicting the Coverage of Address-Based Sampling Frames Prior to Sample Selection","The Presence and Characteristics of Households at Addresses Obtained by Traditional Field Listing and from USPS-Based Lists","An Evaluation of Delivery Sequence File Sufficiency in Rural and Suburban Segments: When Is Listing Required?","Using a \"Match Rate\" Model to Predict Areas where USPS-Based Address Lists May Be Used in Place of Traditional Listing","The Implications of Geocoding Error on Address-Based Sampling ","Address-Based Sampling and the National Survey on Drug Use and Health: Evaluating the Effects of Coverage Bias","Selection of Replacement Samples for Nonresponding Units in Surveys That Use PPS Selection","Effect of Benchmark Cells Collapse Patterns on the National Compensation Survey Earnings Estimates","Weight Calibration across Subject-Specific Samples in the National Assessment of Educational Progress","Iterative Weighting Procedures for School Surveys: Trimming and Raking as One Algorithm","A Generalization of Maximum Entropy Weighting (MAXENT) for the Analysis of Internet Panel Data","Assessment of Alternative Weighting Methods for the National Health Interview Survey","Communicating Qualitative Research Findings to a Statistical Audience","CANCELLED:  Best Practices for Asking Questions on Sexual Orientation on Surveys","Sampling and Weighting Issues in the National Immunization Survey Evaluation Study","Modeling H1N1 Vaccination Rates","Using the National Inpatient Sample (NIS) to Identify Injury Patterns from External Causes of Injury ","Ambient Air Pollutants and Mortality: A Meta-Analysis","Look Back in Anger? Ten Years on: The Statistical Issues in Bush vs. Gore","Logistic Regression Analysis of Disabled Employee Data","Serial Comparisons in Small Domain Models: A Residual-Based Approach","Multiple Frame Sample Design and Estimation for the 2008 National Sample Survey of Registered Nurses - Theory, Implementation, and Assessment","Frame Construction and Sample Maintenance for Current Economic Surveys","Selecting Kindergarten Children by Three Stage Indirect Sampling","Pilot Survey Results from the Canadian Survey of Household Spending Redesign","Evaluating Sample Design Issues in the National Compensation Survey","Challenges in the Design of the Canadian Community Health Survey on Healthy Aging","Eliminating Invisible Boundaries in the National Children's Study While Preserving Selection Probabilities","Measurement Error in Measures of Change: Role of Dependent Interviewing and Other Questioning Techniques","Panel Conditioning in the Consumer Expenditure Interview Survey","The Relationship Between Nonresponse and Data Quality in the Current Population Survey","Studying Recall Error in Survey Data Using Administrative Records","Seam Effects in Quantitative Responses","Estimation Bias in Complete-Case Analysis in Crossover Studies with Missing Data","Modeling Longitudinal Dyadic Data with Informative Dropout","A Test of Missing Completely at Random for Regression Data with Nonresponse","Robust ROC Analysis Using Auxiliary Variables in the Presence of Missing Biomarker Values","Addressing the Missing Data Problem in Clinical Trials","Balanced Random Imputation for Estimating Coefficients of Correlations in Surveys","Imputation Methods for Wave Nonresponse in Panel Surveys","Evaluation of the Quality of Imputation of Goods and Services Tax (GST) Revenue for Late Transactions","Using SRMI for Nonresponse Adjustments in IRS' Taxpayer Compliance Studies","Bootstrap Variance Estimation in the Presence of Imputed Data","Optimal Survey Design: A Review","Probability-Proportional-to-Size Sampling from a Rare Population","Multi-Objective Evolutionary Algorithms for Multivariate Optimal Allocation","Optimal Design for Consistent Rating of Classroom Instructions","Where Youth Buy Cigarettes: Clustering Patterns and Efficient Design","Using Stratification Trends to Keep Tobacco Products Out of the Hands of Minors","Adaptive Contact Strategies in Telephone and Face-to-Face Surveys","U.S. Census Coverage Measurement Demographic Analysis Plans","U.S. Census Coverage Measurement Survey Plans","Census-Coverage Studies in Canada: A History with Emphasis on the 2011 Census","Coverage Assessment in the 2011 UK Census","Adjusting for Treatment Disparities in Observational Research","Collider Stratification Bias as a Threat to Validity in U.S.-Based Health Disparities Research","Measuring Health Disparities in a Geographic Context: Spatial Estimates of Local Health Disparities","A Symmetric, Entropy-Based, Relative and Quasi-Absolute Measure of Health Disparities: An Example Using Dental Caries in U.S. Children and Adolescents","Underlying Properties of Health Disparity Indices","Horvitz-Thompson Variance Weights: Exact vs. Approximate","Further Simulation Results on the Distribution of Some Survey Variance Estimators","Evaluation of Methods for Second-Stage Variance Estimation for the Drug Abuse Warning Network Survey","Variance Estimation for a Small Number of PSUs","Alternative Variance Estimators for a Measurement Error Model","Simulation of Incorporating the Variance from Missing Data in Census Coverage Measurement (CCM) Variance Estimates","Conducting Nonresponse Bias Analysis for Business Surveys","Measuring Nonresponse Bias in the National Crime Victimization Survey","Conducting Nonresponse Bias Analyses in Federal Surveys","Multivariate Imputation Methods for Agricultural Resource Management Survey (ARMS) Data","Precise Estimates of the Number of Farms in the United States","Advancing Statistical Methodology for USDA Surveys and Their Analysis","Evaluation and Management of Cost Structures in Statistical Work with Administrative Record Data","Fitting Multilevel Models to Complex Sample Survey Data","Arbitron's Hybrid Sampling Frame Approach Balances Coverage and Response","Sampling Children and Teens in a Cell Phone Health Survey","Noncoverage Bias in Household Landline Telephone Surveys: The BRFSS Experience","Is ABS a Viable Alternative to RDD?","An Examination of the Bias Effects with a Two-Phase Address-Based Sample","Comparing Web Panel Samples vs. Non-Panel Samples of Medical Doctors","Evaluating the Need for Purging Listed Business Phone Numbers","Coordinated Collection and the Quality of Paradata  for CAPI Surveys at Statistics Canada ","Probabilistic Approach to Editing","Evaluating Incentive Effects in the National Health Study for a New Generation of  U.S. Veterans Survey","Respondent Incentives: Do They Bring Different Respondents to the Data Table?","Estimating Nonsampling Errors in Estimates of Omissions and Erroneous Enumerations in the 2010 Census","Ensuring Data Quality in the 2011 Canadian Census","Nonparametric and Semiparametric M-Quantile Inference for Longitudinal Data","Nonparametric Regression in Some Nonstandard Sampling Situations","Semiparametric Marginal Mean Models for Longitudinal Survey Data","Nonparametric Endogenous Poststratification Estimation","Using the American Community Survey (ACS) to Implement a National Academy of Sciences (NAS)-Style Poverty Measure","Development of the Wisconsin Poverty Measure:  Methods and Findings for 2008","Imputation Variance Estimation Protocols for the NAS Poverty Measure: The New York City Poverty Measure Experience","The Analysis of the TANF and Food Stamp Programs with Matched Administrative and Survey Data","Maximizing Response for Households with Children in a Two-Phase Address-Based Sample","Findings from a Two-Phase Mail Survey for a Study of Veterans","A Pilot Test of a Dual Frame Mail Survey of Recreational Marine Anglers ","Implementing Two-Stage Interviewing in an RDD Survey","An Application of Calibration Approach in Weight Trimming for Stratum Jumpers","Creating a 100-Year Time Series of Tax Data with Consistent Income Ranges from Existing Historical Tabulations and Current Microdata - Part 1: Why and How","Developing an Optimal Approach to Account for Late-Filed Returns in Population Estimates","Applying Alternative Variance Estimation Methods for Totals Under Raking in SOI's Corporate Sample","Alternative Weight Trimming Methods to Estimate Totals in Single-Stage Sample Designs","Two Approaches to Design Effects","A Preliminary Assessment of Cultural Variability in Respondent Actions","Design Effects and Misspecification Effects for Cross-National Comparisons Where Sample Design Varies Between Countries","A Framework for Monitoring Quality in Cross-National Survey Research","Random Group Variance Estimators for Survey Data with Imputation","The Impact of Measurement Error in Auxiliary Variables on Model-Based Estimation of Finite Population Totals: A Simulation Study","New Data Dissemination Approaches in Old Europe: Synthetic Data Sets for a German Establishment Survey","Descriptive Comparison of Populations by Rates: The Analysis of Departures from the Equirates Model","Applying a Family Equalization Adjustment to Weighting for the Current Population Survey","Sample Correlations of Current Population Survey Unemployment Statistics","Current Population Survey Correlations Across and Within the 1990/2000 Phase-In/Phase-Out Period","Evaluating Alternative Criteria for Primary Sampling Units Stratification","Using Latent Class Models to Better Understand Reliability in Measures of Labor Force Status","Issues in Cross-Cultural and Comparative Questionnaire Design, Pretesting, and Administration ","Developing Multilingual Questionnaires: A Sociolinguistic Perspective","Analysis of Chinese Speakers' Responses to Survey Intention Questions","A Mixed-Method Approach for Measurement Construction for Cross-National Studies","Design Effects for Totals in Multi-stage Samples","An Improved Method for Constructing Confidence Intervals for Quantiles from Survey Data","Constrained Estimation of Cell-Only Households by Block Group Using Iterative Calibration","Improving Efficiency of Ratio-Type Estimators Through Order Statistics","Some Theory for the Propensity Scoring Adjustment Estimator","An Introduction to Presampling Inference II","On Sample Sizes in a Longitudinal Survey","Measuring Elusive Populations with Multiple Systems Estimation: A Case Study in Casanare","Challenges in Sampling Elusive Populations","Measuring Lethal Counterinsurgency Violence in Amritsar District, India Using a Referral-Based Sampling Technique","Current Developments in Small-Area Estimation at Statistics Canada","An Evaluation of Housing Unit and Block Cluster Effects on Small-Area Census Coverage Variability in the 2006 Census Test","An Empirical Bayes Approach for the National Agricultural Statistics Service's County Estimation Program","Allegations of Undercounting in the BLS Survey of Occupational Injuries and Illnesses","Using Capture-Recapture Analysis to Identify Factors Associated with Differential Reporting of Workplace Injuries and Illnesses","Employer Interviews: Exploring Differences in Reporting Work Injuries and Illnesses in the Survey of Occupational Injuries and Illnesses and State Workers' Compensation Claims","Work-Related Injury and Illness Surveillance Through Multiple Data Sources, a Pilot Project","Inverse Probability Weighting for Clustered Nonresponse","Use of Propensity Weights and Categories in a Two-Stage Survey","Using Call-Back Data to Adjust for Nonignorable Nonresponse: Results of an Empirical Study","Use of Single Years of Age in the National Survey on Drug Use and Health (NSDUH) Weighting to Improve Drug Prevalence Estimates","Nonparametric Estimation of Response Probabilities in Survey Sampling: An Empirical Investigation","Obtaining Population Representation with an Address-Based Sampling Design","Qualities of Coverage: Who Is Included or Excluded by Decisions of Frame Composition?","On the Quality of Ancillary Data Available for Address-Based Sampling","The Best of Both Worlds: A Sampling Frame Based on Address-Based Sampling and Field Enumeration","'Unmatched' RDD and ABS Sample: Are We Reaching the Same Population?","NaN","NaN","Federal Funding for Statistics in 2011","The Role of Statistics in Comparative Effectiveness Research","Comparing the Coverage of Alternative Address Frames for the 2009 Residential Energy Consumption Survey","Choices of Frame Construction on the National Children's Study: Impacts on Address Quality and Survey Results","A Comparison of Data Collected Using Address-Based Sampling to Those from Alternate Sampling Techniques in the National Children's Study","Best Practices for Matching Address-Based Samples to Telephone Numbers","Toward a Structural Approach to Questionnaire Design","The 2012 Commodity Flow Survey: Lessons Learned from a Precanvass Survey of Establishments","Computer Simulations to Improve Quality Control Strategies for Census Data Capture","Consideration of Variance Estimation in the Sample Design of the U.S. Census Bureau's Service Annual Survey","Hierarchical Simulation Model for Indian PES","Estimating Health Care Savings from Fall-Related Injuries Prevented by the Connecticut Collaboration for Fall Prevention","Real-Time Sampling Methods for Patient Surveys","Hospital Re-Admission Risk Prediction and Stratification","Standard Errors: Statistical Consequences of Health Care Provider Insurance Risk Assumption","Investigation of the Consequence of State Mandated Insurance Coverage for Autism","A Bayesian Analytical Study of the Impact of Information Therapy Events Using Patient-Level Claims Data","Investigating the Effect of Interviewer Job Attitudes on Turnover and Job Performance in Centralized Telephone Interviewing Facilities","Information Flowing in Two Directions: How Respondents Learn by Responding to the National Health Interview Survey","Where Do Interviewers Go When They Do What They Do? An Analysis of Interviewer Travel in Two Field Surveys","OPTIMIZING CATI WORKLOAD TO MINIMIZE DATA COLLECTION COST","Highlights and Lessons from the First Two Pilots of Responsive Collection Design for CATI Surveys","New Resampling Method for Two-Phase Sampling Designs","Finite Population Correction (FPC) for NAEP Variance Estimation","Variance Estimation for Measures of Trends with Rotated Repeated Surveys","Using Successive Difference Replication for Estimating Variances","Incorporating a First-Stage Finite Population Correction (FPC) in Variance Estimation for a Two-Stage Design in the National Assessment of Educational Progress (NAEP)","Cleaning and Using Administrative Lists: Enhanced Practices and Computational Algorithms for Record Linkage and Modeling/Editing/Imputation","To understand the Possibilities of Administrative Data you must change your Statistical Paradigm!","Pandata Systems to Enhance Survey and Census Systems: Employing Administrative Data to Better Inform Public Policy","Some Applications of Administrative Data in Health, Plus a Glimpse of the Future","Use of Paradata to Assess the Performance of a Multi-Mode ABS Study","Multi-Mode Address-Based Sampling and Mode Effects in REACH U.S.","Address-Based Sampling with a Field Component for the CICPE 2010","Cost Efficiency: Which Design Is Cheapest?","Issues in Weighting the 2009 Residential Energy Consumption Survey","Analysis of Multiple Imputation Methods for the Survey of Local Election Officials","Imputation of Multiple-Response Items in SESTAT and Its Component Surveys","Multiple Imputation for Missing Covariates in Regression Models with Interactions","A Comparison of Approximate Bayesian Bootstrap and Weighted Sequential Hot Deck for Multiple Imputation","IRT Summarized Pattern Mixture Model for Data Not Missing at Random","Assessing Several Hot Deck Imputation Methods for Several Economic Programs","2020 Census: How Can the Cost and Complexity of the 2020 Census Be Controlled?","How Much of Interviewer Variance Is Really Nonresponse Error Variance?","Nonresponse and Measurement Error: Relationship, Relative Magnitude, and Correction","Challenges in Minimizing Nonsampling Errors in an International Assessment of Adult Competencies","Using Uncertainty Bounds for Regression Imputation in Statistical Matching","Robust Multiple Imputation for Discrete Data in Missing-by-Design Settings","Bayesian Analysis of Binary Probit Models: The Case of Measurement Error and Sequential Regression Modeling for Missing Explaining Factors","Statistical Matching of Administrative and Survey Data: An Application to Wealth Inequality Analyses","Displaying Uncertainty In Data Fusion By Multiple Imputation","The National Children's Study: Expansion of Methods, Research, and Analysis","Exploring Nonresponse Bias In A National Health Expenditures Survey Of Institutions","Logistic Regression with Variables Subject to Post-Randomization Method","Bridging Livestock Survey Results to Published Estimates Through State-Space Models: A Time-Series Approach","Modeling Health Insurance Coverage Estimates for Minnesota Counties","Application of Hierarchical Bayesian Models with Poststratification for Small-Area Estimation from Complex Survey Data","A Three-Part Model for Survey Estimates of Proportions","Methods and Results for Small-Area Estimation Using Smoking Data from the 2008 National Health Interview Survey","The Development of State Estimates from a National Health Survey","State and Local Wireless and Landline Estimates","Estimation of Poverty at the School District Level Using Hierarchical Bayes Modeling","A Bayesian Zero-One Inflated Beta Model for Estimating Poverty in U.S. Counties","Application of Small-Area Estimation for Annual Survey of Employment and Payroll","An Empirical Best Linear Unbiased Prediction Approach to Small-Area Estimation of Crop Parameters","A 'Virtual Population' Approach to Small-Area Estimation","Additive Random Coefficient (ARC) Models for Robust Small Area Estimation","Quasi-BLUPs for Reducing Over-Shrinkage in Small-Area Estimation","Exploring Missing Data in Value-Added Models in Education","Modeling Score-Based Student Achievement Data with Many Ceiling Values","Teachers' Attitudes Toward Job Conditions: An Index Created by an Unfolding IRT Model","Assessing Goodness of Fit of Item Response Theory Models Using Generalized Residuals","Modeling Outcomes for Elementary Science Education: The Science Writing Heuristic Project","Dropout Factories and College Enrollment: How School-Level Rates of On-Time Grade Promotion Affect Matriculation","Sampling Design for the 2010-2012 National Hospital Ambulatory Medical Care Survey","Innovative Northern Design Improvements in the Canadian Labour Force Survey","Early Childhood Longitudinal Study: Kindergarten Class of 2010-2011 - Sample Design Issues","Predicting Violent Crime Rates for the 2010 Redesign of the National Crime Victimization Survey (NCVS)","Update on the Evaluation of Sample Design Issues in the National Compensation Survey","Alternative Probability Proportionate to Size Sampling Methods for the IPP Sample Design","Sample Design Changes For The NHANES 2011-2014 Annual Surveys","Distinguishing Mode Selection Effects from Mode Response Effects in the Consumer Assessments of Healthcare Providers and Systems (CAHPS) Survey","Framing Inference from Mixed-Mode Surveys Using Causal Inference Framework","Can We Drive Respondents to the Web? Experiemntal Tests of Multi-Mode Survey Implementation and Varied Survey Benefit Appeals","Piggyback Survey Respondents and Mode: Lessons Learned from Design and Operations","Using a Multi-Mode Survey Design on a Panel Study of New Businesses","Secondary Analysis in GWAS","A Wavelet-Based Historical Functional Linear Mixed Model for Examining the Acute Health Effects of Pollution Exposure","A Bayesian Model for Estimation of Population Proportions","Confidence Intervals for the Ratio of Two Poisson Rates","Significance Analysis and Statistical Dissection of Variably Methylated Regions","Genome-Wide Epistasis Screening for Asthma-Associated Traits","A New Method for Detecting Associations with Rare Variants for Complex Disease","Haplotype-Based Association Studies Under Complex Sampling","Measurement Error in Survey Paradata","Relationship of Smoking and Cancer in Women: A National Health and Nutrition Examination Survey (NHANES) 2005-2006 Study","Estimating Sibling Recurrence Risk In Population Sample Surveys","Mantel-Haenszel Estimators for Complex Survey Data","Analysis of Case-Control Studies with Sample Weights","Integrating Data From Two Surveys To Estimate The Prevalence Of Cervical Cancer Screening And Its Associated Factors In The U.S.-Mexico Border Region","Survey Design and Results from UNICEF Project in Sierra Leone","Use of GPS-Enabled Mobile Devices to Conduct Surveys","Methods to Infer Hard-to-Reach Populations","Is It Feasible to Use a Sampling List Frame to Evaluate Misclassification Errors of an Area-Frame-Based Survey?","Adjusting an Area Frame Estimate for Misclassification Using a List Frame","Estimating Change for a Dynamic Target Population","Developing a Set of Decision Rules for a Responsive Split Questionnaire Design","A Randomized Experiment Comparing Response Quality in a Single and a Mixed Mode Design Health Survey","Does Survey Mode Make Differences? A Comparative Evaluation of the Department of Defense Survey of Health-Related Behaviors and the U.S. Air Force Community Assessment Survey","Comparing Cell Phone and Web for a Student Survey","Respondent Effects in a Dual-Mode Survey of Physicians: 2008-2009","Missing Data in Record-Linked Data Sets: Comparing the Performance of Different Missing Data Techniques","An Integrated Adaptive Approach to Data Fusion","The Mali Road Project: A Case Study in Paired Sampling","Comparing Measure-Level Sampling and Bed-Level Sampling for Group Quarters Sample Redesign","Composite Size Measures in Surveys of Rare or Hard-to-Reach Populations","Two Methods of Sampling New Construction for the Demographic Surveys Sample Redesign","Using Order Sampling to Achieve a Fixed Sample Size After Nonresponse","Exploration of Data Quality in Work with Administrative Records and Sample Surveys","Re-Weighting the National Health and Nutrition Examination Survey Linked to Medicare Administrative Records","Building A Comprehensive Data Set On Every Teacher:  The Nces Teacher Compensation Survey","Responsive Designs for Rare Subpopulations Subject to Misclassification","Estimation with Nonignorable Missing Covariates","Propensity Score Adjustment for Nonignorable Nonresponse","Assessment of Nonresponse Bias Through Interviewing Effort Analysis in a Dual-Frame RDD Telephone Survey","Nonresponse Adjustment for a Vector of Outcomes and the MAR Assumption","Analysis of Nonresponse in the Statistics of Income's 1999 Individual Tax Return Panel","Representativeness (R-Index) and Nonresponse Bias Patterns in Household Surveys","Reweighting in the Presence of Nonresponse in Stratified Designs","Benchmarking the Mixed Linear Model for Longitudinal Data","GEE Analysis of Clustered Binary Data with Diverging Number of Parameters","Threshold Estimation Using P-Values","Hierarchical Spatial Regression Models for Change Point Analysis","Joint Estimation of Multiple Gaussian Graphical Models by Nonconvex Penalty Functions with an Application to Genomic Data","A Semi-Parametric Roc Approach To Assessing Biomarkers Subject To A Measurement Error And Limit Of Detection","Multiple Imputation of High-Dimensional Mixed Incomplete Data","A Measurement Error Model for Self-Reported Physical Activity","Maximum-Likelihood-Based Multiple Imputation","Bayesian Analysis of Between-Group Differences in Variance Components in Hierarchical Generalized Linear Models","Combining Information from Multiple Complex Surveys","Variance Inflation Factors in the Analysis of Complex Survey Data","Synthetic Data Generation for Small-Area Estimation in the American Community Survey","Imputation and Estimation Under Nonignorable Nonresponse for Household Surveys with Missing Covariate Information","Cell Phone Versus Landline Respondents: Who's More Cooperative After Screening?","Given Different Mobile Phone Device Data Collection Options, What Will Respondents Choose?","Development and Validation of a Questionnaire to Measure Respondents' Perceived Risks of Web Surveys","Testing Gender Difference in Perceived Time Risk of the Web Surveys Using MIMIC Model","The Impacts of Respondents' Perceived Privacy and Time Risks on the Web Surveys Reply Intention","Managing Response Burden by Controlling Sample Selection and Survey Coverage","Beyond Response Rates: Exploring the Impact of Prepaid Cash Incentives on Multiple Indicators of Data Quality","The Microdata Analysis System at the U.S. Census Bureau","Research Access to Statewide Longitudinal Data Systems","Disclosure Risk Assessment for Population-based Cancer Microdata","Simulating Geography for Microdata Disclosure via Sparse Multinomial Probit Models","Applicability of Basic Separability Principles to Enhance the Operational Efficiency of Synthetic Tabular Data Generation Procedures in Multidimensional Table Structures","Research at the U.S. Census Bureau","Survey Quality Indicator Measures: Response Rates and Alternatives","Jumping the Informed Consent Hurdle: Federal Agency Experiences","Historical data linkage of tax records on earnings: The case of the Living in Canada Survey pilot","Record Linkage in the Survey of Health, Ageing, and Retirement in Europe","Using Imputation Methods to Improve the American Community Survey Estimates of the Group Quarters Population for Small Geographies","Using Statistical Process Control Techniques in the American Community Survey","Incorporating a Finite Population Correction Factor in American Community Survey Variance Estimates","Improving Weighted Person-Level Estimates from the American Community Survey's Public Use Microdata Sample","Mapping American Community Survey Data ","The Use of Longitudinal Analysis to Model PSU Level One-Year Price Change in the Consumer Price Index","Revising Replicate Selection in the CPI Variance System","Record Linkage Methodology in Longitudinal Database of Quarterly Census of Employment and Wages","Improving Estimates of Employment in Expanding and Contracting Businesses","Who Is Eligible for U.S. Employer-Sponsored Pensions?","Effects of the Pretrial Supervision Phase on Children of Defendants: Opportunities for New Data Collection and Analysis","Proposed Indicators to Assess Interviewer Performance in CATI Surveys","Comparing CAPI Trace File Data and Quality Control Reinterview Data as Methods of Maintaining Data Quality","Using Statistical Process Control to Understand Variation in Computer-Assisted Personal Interviewing Data","An Attempt to Reduce Survey Costs via Logistic Regression and Paradata","Analyzing Interviewer Call Record Data Using a Multilevel Multinomial Modeling Approach to Understand the Process Leading to Cooperation or Refusal","A Functional Method for Longitudinal Data with Missing Responses and Covariate Measurement Error","Generating Multiple Imputations from Multiple Models to Incorporate Model Uncertainty in Nonignorable Missing Data Problems","Empirical Likelihood-Based Method Using Calibration for Longitudinal Data with Drop-Out","Bayesian Modeling and Inference for Data with Informative Treatment Switching or Dropout","Pre-Sampling Model-Based Inference III","Rule of Thumb Regarding the Use Weights in Survey Sampling Analyses","Hierarchical Design-Based Estimation in Stratified Multipurpose Surveys","A Coverage Approach to Evaluating Mean Square Error","Approximate Confidence Intervals for a Parameter of the Negative Hypergeometric Distribution","Resampling Variance Estimation for a Two-Phase Sample","The Impact of Visual Design in Survey Cover Letters on Response and Web Take-Up Rates","Comparison of Variance Estimates in a National Health Survey","Combined Methods for Imputing School Variables in Principal Data Files","Estimating the Bias Resulting from the Exclusion of Cell Phone-Only Respondents","Using Isotonic Regression to Estimate Order-Restricted Health Indicators for the 1997-2006 National Health Interview Survey","Overcoming Challenges to Sample Design in Iraq","An Overview of Following Rules in Household Panels and Their Effect on Sample Size","Day-of-Week, Time-of-Year, and Meteorological Effects on Items on a Local Health Survey","Decomposing the Fraction of Missing Information Into Auxiliary Variable Contributions for Monitoring Survey Data","Comparison of Sampling Methods for a School-Based Population","The Nonverbal Response Card Method for Soliciting Responses to Sensitive Questions","Designing Minimum-Cost Multi-Stage Sample Designs","A Generalized Epsem Two-Phase Design for Domain Estimation","Assigning PSUs to a Stratification PSU","An Enhanced Approach for Solving the Overlap Problem in Dual-Frame RDD Surveys","Updating the Measures of Size of Local Areas Late in the Decade Using USPS Address Lists","Calibrating Non-Probability Internet Samples with Probability Samples Using Early Adopter Characteristics","Nonresponse Follow-Up Allocation for Domains","NSCG Sampling Issues When Using an ACS-Based Sampling Frame","NSCG Estimation Issues When Using an ACS-Based Sampling Frame","Evaluating 2003 NSCG Dual-Frame Estimates for 2010 NSCG Planning Purposes","Variance Estimation Issues in the 2010 NSCG Two-Phase Sample Design","Microdata Imputation and Macrodata Implications: Evidence from the Ifo Business Survey","Look Again: Editing and Imputation of SCF Panel Data","Generalized Estimating Equation Model for Binary Outcomes with Covariates Missing by Design","Approaches for Handling Missing Sexual Identity Data","Imputation of Categorical Data with Small Subcategories: Assessing the Results of Imputing Race in the 2007 - 2008 National Ambulatory Medical Care Survey (NAMCS)","Evaluation of Improvements to the Imputation Routine for Health Insurance in the CPS ASEC","Erosion Analysis with USLE and RUSLE2","Analysis of Longitudinal Binary Data with Nonignorable Dropout Using Shared Parameter Models","Analysis of Longitudinal Data with Non-Random Missingness Using Shared Random Effects Models","Longitudinal Data Analysis with Both Monotone and Non-Monotone Missing Data","Mixed-Effects Models for the Effects of Partially Observed Covariates on Dependent Recurrent Events Data","Joint Modeling of Longitudinal and Survival Data with Missing and Left-Censored Time-Varying Covariates","Evaluating the Effects of Treatment Regimes in the Presence of Drop-Outs: Application to Depression Data","A Broad Framework for Joint Modeling","Evaluating Energy Efficiency of Demand-Side Management Programs","Methods of Estimation in Random Effects Meta-Regression","The Role of Legal Orientation Program in Affecting Immigration Court Case Processing Times","A Case Study in Using Household Information from Administrative Databases","Sources of Revision in Statistics Canada's Service Indices","Utilizing Automated Statistical Edit Changes in Significance Editing","Trends in Record Linkage Refusal Rates: Characteristics of National Health Interview Survey Participants Who Refuse Record Linkage","Fifty Years of Sample Redesign at the U.S. Census Bureau","Frame Improvement for the Demographic Surveys Sample Redesign","Sample Design Research in the 2010 Sample Redesign","How the U.S. Census Bureau's Redesign of Household Survey Sampling Impacts Field and Clerical Operations","Initial Efforts to Extend the EZS Noise Methodology to the Quarterly Census of Employment and Wages","Applying Cell Suppression to Inter-Related Tables","Modernizing Cell Suppression Software at the U.S. Census Bureau","Using Tau-Argus and sdcTable to Conduct Secondary Cell Suppression for Linked Tables","Estimation of Finite Population Domain Means: Then and Now","Calibrated Bayes Inference for Sample Surveys","Bayesian Predictive Inference for Small Areas with Uncertain Unit-Level Models","Bayesian Predictive Inference for a Proportion with Nonresponse","The General Social Survey (GSS) and the Methodology for Studying Societal Change","Assessment of Measurement Error and Nonresponse Error Using Respondent-Provided Paradata from Paper Self-Administered Questionnaires","Designing Estimators of Nonsampling Errors in Estimates of Components of Census Coverage Error","Using Interviewer Observations Related to the Interview Process for Measurement Error and Nonresponse Error Adjustment","Assessing and Adjusting for Response Error Using a Multi-Phase Survey Approach in the Residential Energy Consumption Survey","Eliciting Illicit Work: Item Count and Randomized Response Technique Put to the Test","General-Specific Questions in Survey Research: A Confirmatory Factor Analysis Approach","Calibration Estimation and Longitudinal Survey Weights: Application to the NSF Survey of Doctorate Recipients","Exploring Calibration of NIS Weights Utilizing Telephone Status from the NHIS","WTADJX Is Coming: Calibration Weighting in SUDAAN When Unit Nonrespondents Are Not Missing at Random and Other Applications","Simultaneous Calibration and Nonresponse Adjustment with Explicit Variance Formulas","Two-Stage Bayesian Benchmarking for Small-Area Estimation","How Large Should a Statistical Sample Be?","Is Housing Unit Undercoverage Random?","Comparing Cell Phone and Internet Coverage Use in the USA and South Korea","RDD Unplugged: Findings from a Household Survey Using a Cell Overlap Design","Goodness of Fit Tests in Dual-Frame Surveys","Maintaining Privacy in Data.gov Data Files","The Challenges of Consolidating Federal Government Information Systems","Statistical Methods for Protecting Personally Identifiable Information in Aggregate Reporting","Evaluating a Constrained Hotdeck to Perturb American Community Survey Data for the Census Transportation Planning Products","Exploring New Methods for Protecting and Distributing Confidential Research Data","No Group Quarters Left Behind: The New 2010 Census Group Quarters Validation","Characteristics of the 2010 Census Nonresponse Follow-Up Operation","Characteristics of the 2010 Census Nonresponse Follow-Up Operation","Evaluating the 2010 Census Nonresponse Follow-Up Reinterview","Outcome of the 2010 Vacant/Delete Check and Nonresponse Follow-Up Residual Census Enumeration Operations","Variance Estimation for Census Transportation Planning Products with Perturbed American Community Survey Data","An Empirical Evaluation of Easily Implemented, Nonparametric Methods for Generating Synthetic Data Sets","An Evaluation Of Bls Noise Research For The Quarterly Census Of Employment And Wages","A Comparative Assessment of Disclosure Risk and Data Quality Between MASSC and Other Statistical Disclosure Limitation Methods","Modeling Aggregates for Reliability and Confidentiality of Output with Application to the QCEW Program of BLS","A New Optimal Estimator of Population Proportion in Randomized Response Sampling","Estimation of Finite Population Variance Using Scrambled Responses in the Presence of Auxiliary Information","The United States Experience: Streamlining the Decennial Census and Meeting Data User Needs","The First Fully Register-Based Census in Sweden","A Review of Caribbean Population and Housing Census Experiences Using the Long and Short Forms","Enhancing the Value of Census Data Through Data Integration","Bias-Variance and Breadth-Depth Tradeoff in Respondent-Driven Sampling","Using Signal Detection Theory to Verify Performance Standard Setting on Michigan English Language Proficiency Assessment","Causal Inference in Transportation Safety Studies: Comparison of the Potential Outcomes and Causal Bayesian Networks","Using a Density-Variation/Compactness Measure to Evaluate Redistricting Plans for Partisan Bias and Electoral Responsiveness","Modeling Single-Establishment Firm Returns to the 2007 Economic Census","Recursive Partitioning for Racial Classification Cells","Plotting Likert and Other Rating Scales","Methods for Fitting a Markov Latent Class Analysis for the National Crime Victimization Survey","Latent Class Analysis of Measurement Error in the Consumer Expenditure Survey","Survey Incentive Fees, Microeconomic Data Quality, and Nonresponse","Using Quality Indicators to Manage Collection and Editing in Business Surveys","Measuring Green Industry Employment: Developing a Definition of Green Goods and Services","Measuring Green Industry Employment","Maximizing Sample Overlap between Two Independent Surveys","Counting Green Jobs: Developing the Green Technologies and Practices Survey","The Weighting Process Used in the Employer Costs for Employee Compensation Series for the National Compensation Survey","Evaluation of the National Longitudinal Survey of Children and Youth Weighting Methodology","Relative Importance of Poverty and Education in Medical Expenditure Panel Survey Weighting Adjustment","Estimation and Replicate Variance Estimation of Deciles for the Survey of Construction","Bayesian nonparametric estimation of finite population quantities in absence of design information on nonsampled units","Using Frame-to-Population Linkage to Determine Design Weighting Effects on Estimates and Variance","Selection of prior distributions for multivariate small area models with application to small area health insurance estimates","Beyond Simple Descriptive Statistics: Utility of Multivariate Techniques in Survey Research","Multiple Regression Analysis with Data from Complex Survey","A Comparison of Several Methods for Determining the Number of Factors in Exploratory Factor Analysis","Modeling High-Dimensional Survey Data Using Latent Structure Analyses","A Noninformative Bayesian Approach to Cluster Sampling","Inference for the Zenga Inequality Index","NaN","NaN","Using 2010 Census Data to Evaluate Imputation Methods to Improve the American Community Survey Estimates of the Group Quarters Population for Small Geographies","Lessons Learned from Use of  Administrative Data and Ethnographic Observation of Frail Population: Health Related Facilities","What Counts as Group Quarters? A Glimpse of Total Coverage Errors and Costs Model in Census","Design Effect Anomalies in the American Community Survey","Introduction to Regulatory Statistics: Principles for Drug Approval at FDA","Partnerships to Engage and Train the Next Generation: Challenges and Opportunities","SMACS: Invisible Sciences with Impeccable Impact","Application of the Shewhart Philosophy and Methodology to Create Insights from Analytics","Nurturing Research and Recruitment Partnerships Between a Federal Statistical Agency and Academic Teams","Inference for Finite Population Quantiles of Non-Normal Survey Data Using Bayesian Mixture of Splines","A Bayesian Test of Independence in a Two-Way Contingency Table Under Two-Stage Cluster Sampling","Bayesian Inference for the Finite Population Total from a Heteroscedastic Probability Proportional to Size Sample","Design-Based Evaluation of Bayesian Methods for Complex Survey Data","Using the Fraction of Missing Information to Tailor Survey Design","Synthesizing Bipartite Graphs: An Application to Employer-Employee--Linked Data","Generating Synthetic Graphs Under Differential Privacy","Protecting Interrelated Time Series with Synthetic Data Models","Estimating Identification Disclosure Risk Using Mixed Membership Models","Using Address Based Sampling Frames in Lieu of Traditional Listing: A New Approach","Quasi Address-Based Sampling: A New Cost-Efficient Rejective Sampling Design for Reducing Undercoverage, Nonresponse Rate, and Nonrespondent Substitution Bias","Using New IT in Area Sampling: An Experience in Korea","Unpacking the DSF in an Attempt to Better Reach the Drop Point Population","A Review of the Cell Phone Sample Component of the California Health Interview Survey","Geographical Screening in Cell Phone Surveys: Sampling Variability and Bias in the March 2011 National Flu Survey","Exploring Error Properties of Respondent-Driven Sampling","Errors of Non-Observation: Dwelling Nonresponse and Coverage Error in Traditional Censuses","The 2011 UK Census and Its Coverage","United States Census Coverage Survey Results","Small-Area Estimation Combining Information from Several Sources","Small-Area Confidence Bounds on Small Cell Proportions in Survey Populations","Benchmarked Small Area Prediction","Small-Area Estimation Under Measurement Error Models","State-Level Estimates of Phosphorus and Nitrogen Levels in Wadeable Streams","Thinking About Unauthorized Population Estimates as Residuals: The Method and Policy Implications","Estimating the Number and Characteristics of Illegal Migrants via the Center Sampling Technique: Methodology and Application to Italian Data","Estimating Unauthorized Immigrant Populations in Sub-State Areas","Household Probability Sampling of Legal and Unauthorized Migrants","Unauthorized Immigrants, Trends, and Characteristics: Comparison of Estimates Based in the CPS and ACS","Orphan Survey or Orphaned Survey? Design Effects in a Survey of Orphans and Vulnerable Children","Survival Estimation from Current Status Data","Analysis of the Mobile Phone Survey of the Haitian Population","Report on Current Statistical Capacity Building Efforts in Albania, Ethiopia, Kenya, Papua New Guinea, and Madagascar","Inappropriate Use of Statistical Measures in the Name of Balancing Data Quality and Confidentiality of Tabular Format Magnitude Data","Disclosure Avoidance at the National Center for Health Statistics","The Role of the NCES Disclosure Review Board in Balancing Confidentiality and Data Quality","A Renewed Understanding of Complementary Cell Suppression","The Empirical Properties of Survey Quality Indicators Under Nonignorable Nonresponse","Assessing Nonresponse Bias in the Green Goods and Services Survey","A Study Comparing CHAID Analysis to Traditional Weighting","Using Soft Refusal Status in the Cell Phone Sample Nonresponse Adjustment","Adjusting for Survey Measurement Error with Accuracy Variables","Assessment of Total Survey Error for the National Immunization Survey (NIS) 2009--2010","Examining the Use of EWMA Charts for Monitoring the National Crime Victimization Survey","Allocation of Sample for the 2010 Redesign of the Consumer Expenditure Survey","A 'First Cut' at Forming Rural PSUs","Complex Survey Sample Design in IRS' Multi-Objective Taxpayer Compliance Burden Studies","State and Local Government Sample Design for the National Compensation Survey","Sample Allocation for the Redesigned National Compensation Survey","Evaluation of Alternative Measures of Size for Sampling of Establishments in the National Compensation Survey","History of Sampling Methodology in the HUD Assisted Housing Household Rent Quality Control Studies","Asymptotic Variance Estimation and Comparison of Model-Assisted Regression Estimators in Sample Surveys","Anomalies Under Jackknife Variance Estimation Incorporating Rao-Shao Adjustment in the Medical Expenditure Panel Survey Insurance Component","Replicate Variance Estimation in a Two-Phase Sample Design Setting: Simulation Study with National Survey of College Graduates Data","Post-Imputation Calibration Under Rubin's Multiple Imputation Variance Estimator","Adjusting for Nonresponse in the Occupational Employment Statistics Survey","Comparing Item Nonresponse Rate in SIPP and SIPP-EHC: Case of Snap Eligibility Measurement","On Weighting of Propensity Models When Analyzing Nonresponse","Combining Model-Based and Hot-Deck Imputation to Fill Gaps in Longitudinal Surveys","Nonresponse Bias Adjustment in Establishment Surveys: A Comparison of Weighting Methods Using the Agricultural Resource Management Survey (ARMS)","ABS and Cell Phones: Appending Both Cell Phone and Landline Phone Numbers to an Address-Based Sampling Frame","Scope and Coverage of Landline and Cell Phone Numbers Appended to Address Frames","Augmented RDD/Mail Sampling: In-Person Recruiting","Coverage and Nonresponse Relationship in Mixed-Mode Surveys That Use Enhanced ABS Frames","Feasibility of a Single-Frame Cell Phone RDD Sampling Design","Optimizing Call Patterns for Landline and Cell Phone Surveys","A Cost-Benefit Analysis for Reducing Call Attempts for the National Immunization Survey","Double Protection for Imputation Using a Tree-Based Methodology","Judgment Post-Stratification Estimation of Population Proportion with High Missing Data Rate","Enhanced Tipping-Point Displays","Alternative Indicators for the Risk of Nonresponse Bias: A Simulation Study","MissMech: An R Package for Testing Homoscedasticity, Multivariate Normality, and Missing Completely at Random","Analysis of Nonresponse Bias in the 2010 Post-Election Survey of Uniformed Service","Several Scenarios for Influential Observations and Methods for Their Treatment","Energy Consumption Surveys: Re-Energizing for an Energy- and Data-Dependent Society","Monitoring the Effects of the Affordable Care Act (ACA) Using NCHS Ambulatory and Hospital Care Statistics","Monitoring Physician Capacity Constraints ","Using the National Health Interview Survey to Monitor the Early Effects of the Affordable Care Act","Monitoring Health Care Access and Utilization Following Implementation of the Affordable Care Act Using the National Health Interview Survey","How to Share Research Data: Views from Practitioners","The Will of the People and the Luck of the Draw: Using Statistics to Limit the Risk of Wrong Electoral Outcomes","Challenges of Data Analysis in Transportation","A Showcase of Statistical Activities at Federal Statistical Agencies","Representative Measures of Veteran Health Factors","Housing Affordability for Low-Income Households in New Jersey, Maryland, and West Virginia: A Comparative Analysis","Statistical Methods Used in Generating U.S. State-Level Life Tables","Use of Labor Market Indicators in Small-Area Poverty Models","Tests for Homogeneity of Multinomial Proportions for Sparse Data","Geographic Area Effect in the CPI Variance Model","Bayesian Methods in Poverty Mapping","Generalized Maximum Likelihood Method in Linear Mixed Models with an Application in Small-Area Estimation","On Estimation of Mean Squared Errors of Benchmarked Empirical Bayes Estimators","Confidence Interval Estimation of Small-Area Parameters Shrinking Both Means and Variances","Small-Area Estimation with Uncertain Random Effects","Causal Inference with Longitudinal Outcomes and Nonignorable Drop-Out","Missing Value Imputation for Predictive Models on Large and Distributed Data Sources","Exploring Missing Data Substitions for Binomial Response Student Test Data","Hosmer-Lemeshow Goodness-of-Fit Test for Multiply Imputed Data","Model-Based Cluster Analysis of Incomplete Continuous Data with Intermittent Measurement Errors","The Effect of Prepaid Cash Incentives on Satisficing Behaviors in a Telephone Survey","Household Early Bird Incentives: Leveraging Family Influence to Improve Household Response Rates","Improving Response Rates in Military Surveys","Modeling Response Rates for Telephone Surveys","Impact of Different and Reduced Incentive Amounts on Longitudinal Survey Response Rates and Overall Cost","Bayesian Analysis of Randomized Response Models","Post-Stratification Based on the Choice of Use of a Quantitative Randomization Device","Estimation of Complex Small-Area Parameters with Application to Poverty Indicators","A Hierarchical Bayes Estimation of Poverty Rates","Semi-parametric Small Area Estimation Based Prediction Methods to Track Poverty: Validation and Applications","Inference from Multilevel Versus Single-Level Methods with Complex Survey Data: An Example Using the National Youth Risk Behavior Survey","The Challenge of Using Data from Multiple Informants: An Illustration Using a Longitudinal Study of Children with Attention-Deficit/Hyperactivity Disorder (ADHD)","A Model-Based Method for Estimating Serious Emotional Disturbance Using a Mental Health Screener from Household Interview Data Combined with Clinical Interviews","Sample Design Issues in The National Children's Study","40 Years and Counting: CNSTAT's Role in Government and Social Statistics","Predictors of Retention and Re-Engagement in the Health and Retirement Study","Dynamics of Web Panel Attrition Across Cohorts, 2008--2010: A GfK-Knowledge Networks Case Study","Measuring Attrition in Long-Term Longitudinal Surveys","Assessment Through Simulation of Respondent-Driven Sampling Estimates When Used in Telephone Survey Sampling","An Exploration of the Application of PLS Path Modeling Approach to Creating a Summary Index of Respondent Burden","Panel Conditioning: Change in True Values vs. Change in Self-Report","Methods for Responsive Split Questionnaire Panel Surveys","A Semiparametric Approach to Dimension Reduction","Uncertainty Analysis for Computationally Expensive Models with Multiple Outputs","Nonparametric Tests for the Frailty Distribution in Proportional Hazards Models with Shared Frailty","Using Nonparametric Regression to Derive Predictors in Analytic Inference from Complex Surveys","Results of the 2010 Census Coverage Measurement Field and Matching Operations","Once and Only Once: Searching Near and Far for Person Duplications in the 2010 Census","Household Contact and Probing to Collect a Complete Address List and Household Roster in the Census Coverage Measurement Program","Recall Bias on Reporting a Move and Move Date","Benchmarking Small-Area Estimates","Benchmarking Small-Area Estimates: A Minimum Discrimination Information Approach and Other New Perspectives","A Hierarchical Mixture Model for Small-Area Estimation","A Bayesian Zero-One Inflated Beta Model for Small-Area Shrinkage Estimation","The Use of Sampling Weights in Bayesian Hierarchical Models for Small-Area Estimation","Small-Area Estimation in the Presence of Measurement Error Using Corrected Scores Approach","A New Adjusted Residual Likelihood Method for the Fay-Herriot Small Area Model","Overview of Project Talent","Determinates of Tracking Success After an Extended Hiatus: Results from the Project Talent Pilot Test","Methods for Linking Administrative Records When Unique Identifiers Are Not Available: Using Project Talent and the Death Master File","Examining the Effects of Differential Coverage of Three Mortality Record Sources","Bootstrap Confidence Bands for the CDF Using Ranked-Set Sampling","Isotonic Mean Estimator for the Judgement-Post Stratified Samples","Optimal Allocation for the Mean of the Paired Ranked Set Samples","Ranked Set Sampling and Its Applications in Educational Statistics","The Discarded Information of Ranked-Set Sampling and Judgment Post-Stratification","Methods of Modeling Tax Units to Estimate Tax Credits","Evaluating the Construct of a Canberra Household Income Definition Using CPS ASEC","A Look at CPS Non-Response and Trends in Poverty","An Evaluation of Retirement Income in the CPS Using Form 1099-R Microdata","What About the NonMatches?","Pilot Study on Combining Direct Estimates of Income and Poverty from the American Community Survey with Predictions from a Model","Application of a Small-Area Model for a Voting Rights Act Tabulation","Developing Replicate Weight-Based Methods to Account for Imputation Variance in a Mass Imputation Application","Understanding the Causes of the Differences Between the 2010 American Community Survey and the 2010 Decennial Census Vacancy Rates","Results of Monitoring the American Community Survey Using Statistical Process Control Methodologies","Multi-Level Data Methods to Detect and Adjust for Nonresponse Bias in Sample Surveys","Integrating Data Sources","Quality/Quantity Assessment of Patient Registries Versus Questionnaire Self-Reports","Comparing Injury Data from Administrative and Survey Sources: Methodological Issues","Evaluation of Risk Factors in Population-Based Incidence of Clostridium Difficile Infection Across Multiple U.S. Geographic Locations","Combining Cluster Sampling and Link-Tracing Sampling: Estimating the Size of a Hidden Population in the Presence of Heterogeneous Link-Probabilities Modeled by a Latent-Class Model","Weak Identifiability in Latent Class Analysis","Estimating Record Linkage Error Rates Without Training Data","Pre-Sampling Model-Based Inference IV","Results from the Survey of Health Insurance and Program Participation","Redesign of the National Ambulatory Medical Care Survey to Support State Estimates","The Intersection of Response Propensity and Data Quality in the National Health Interview Survey","Impacts of Question Format on Prevalence Estimates, Item Nonresponse, and Response Times: A Split-Ballot Experiment of Disability Questions on the National Health Interview Survey","Coverage Implications of Targeted Lists for Rare Populations","Haplotype-Based Statistical Inference for Population-Based Case-Control Studies with Complex Sample Designs","Aggregate-Level PUF as a New Alternative to the Traditional Unit-Level PUF for Improving Analytic Utility and Data Confidentiality","Total Survey Error in Practice","2010 Census Deadline Messaging and Compressed Mailing Schedule Experiment","2010 Alternative Questionnaire Experiment Race and Hispanic Origin","Paid Media and the 2010 Census Communications Campaign: An Experiment to Assess Increased Spending","The 2010 Census Alternative Questionnaire Experiment: Replication of the 2000 Census Questionnaire","2010 Census Nonresponse Follow-Up Contact Strategy","Linking Medical Expenditure Panel Survey Data to the National Health Interview Survey: Weighting and Estimation Considerations","The Utility of the Integrated Design of the Medical Expenditure Panel Survey to Inform Mortality-Related Studies","Dealing with 'Incompletely Linked' Data in Linked Survey/Administrative Databases: An Empirical Comparison of Alternative Methods","Nonresponse Adjustment Methodology for NHIS-Medicare--Linked Data","Multiple Imputation Of Linked Data: A Case Study","Justice by the Numbers: Rwandan Prison Survey","The Need for Sound Statistical Analysis of Issues Pertaining to Class Certification: The Problematic Studies Submitted in Dukes v. Walmart","Effective Command or Minor Aberrations: Statistical Analyses of Police Administrative Data Systems in Human Rights Cases from Chad and Guatemala","Semiparametric Functional Linear Model with High-Dimensional Covariates","Functional Principal Component Analysis of Spatial-Temporal Point Processes with Applications in Disease Surveillance","Two-Phase Studies of Gene-Environment Interaction","Automatic Structure Selection in Semiparametric Models","Jon Rao and Survey Sampling","The Rescaling Bootstrap and Extensions","Small Area Estimation under Density Ratio Model","Extensions of Rao-Scott Tests: Pseudo Likelihood-Ratio Tests for Survey Data","Making Instrumental Variables Look More Like Experimental Design","Calibrated Sensitivity Analysis for the Instrumental Variables Method for Observational Studies","Mediation Analysis on General Outcomes with Application to Dental Studies","Making Public Performance Reporting Meaningful and Useful: Large Observational Studies of the Role of Electronic Health Records in Chronic Disease Care","Response Surface Matching for Survey Nonresponse","Optimal Designs for School Surveys","Exploring Geographic Clustering Methods in National School-Based Student Survey Samples","A New Approach for Multiway Stratification: School Sampling in Charting the Progress of Education Reform: An Evaluation of the Recovery Act's Role","Effectiveness of a Composite Size Measure for Sampling Students with Disabilities","Research and Development for Methods of Estimating Poverty for School-Age Children","Weighting in the Dark: What to Do in the Absence of Benchmarks","Using Cognitive Interviewing to Detect Privacy and Confidentiality Concerns","Estimates by Firm Size Using the CES Survey","Analytical Highlights of CES Firm Size Employment Data","Development of JOLTS Firm Size Estimation Methodology","Analysis of JOLTS Research Estimates by Size of Firm","Choosing Size Classes for Industry Employment Estimates by Firm-Size Class","Coherence Structures of Mortality Time Series Data for All Causes of Deaths for Selected Neighboring States 2004--2006","Time-Series Cross-Section Approach for Small-Area Poverty Models","Utilizing Changepoint Detection to Improve Boundary Tracking in Noisy Images","Application of GEE and MRM in Evaluation of the Efficacy of an HIV Prevention Intervention","Analyzing Nonresponse Bias in PPI Data","Using Contact History to Adjust for Nonresponse in the Current Population Survey","A Prediction/Pseudo-Empirical Likelihood Raking Approach to Estimating Small-Area Occupational Employment Change Over Time","Small Area Estimation Alternatives for the National Crime Victimization Survey","Evaluation of Small-Area Estimates of Substance Use in the National Survey on Drug Use and Health","A Search for Robust Model-Based Small-Area Estimation Methods for the National Ambulatory Medical Care Survey (NAMCS)","Estimation for Detailed Publication Levels in the Current Employment Statistics Survey","An Examination of the Relative Variance of Replicate Weight  Variance Estimators for Ratios through First Order Expansions","Nonlinear mixed effects cross-sectional models for estimation of smoking proportions using the National Health Interview Survey","Small-Area Estimation in Household Surveys When Auxiliary Variable Totals Are Known","Estimating Agreement Coefficients from Sample Survey Data","Cell Phone Coverage in an ABS Multi-Mode Design","An Empirical Investigation of the Role of the Email Contact in Web Survey Response Rates","Sampling Design for the 2011--13 National Hospital Care Survey","Improving Small-Area Health Data: Developing an Outcome Screening Procedure to Support Small-Area Estimation Models","An \"approximately unbiased\" estimator may be uniformly larger than an\" overestimate\"","Assessing the Impact of Simplified Design Assumptions When Analyzing Data from Public-Use Complex Surveys","Measurement Error on Dual-Frame Estimation","Weighting Methods in Survey Sampling","Continuous Measures of Response Latency in Evaluating Nonresponse Bias in a Large Randomized Community Trial of College Students Using the Continuum of Resistance Model","Assessing Coverage and Accuracy of Population Subgroups using an Address-Based Sample Frame","Refining Cellular Phone Samples Using Switch Centers in a Multiregional Survey","Comparison of Imputation Procedures Once the Data Sets Contain Nonresponses","A Modeling Approach: Use of Survey Contact Day and Time to Model Response Propensity in the Medical Expenditure Panel Survey","Modeling and Displaying the Precision of Radio Audience Estimates","Revisiting the Survey Form: The Effects of Redesigning the Current Employment Statistics Survey's Iconic One-Page Form with a Booklet-Style Form","Small-Area Estimation for Government Surveys","Coupling X-12-Arima-Seats with a Multidimensional Direct Filter Approach for Signal Extraction in Nonstationary Seasonal Time Series","Evaluating AICC Tests in X-13Arima-Seats","Differences in Performance Between Concurrent and Projected Seasonal Factors for Weekly UI Data","Complementary Properties of an F-Test and an Empirical Spectral Test for Identifying Seasonality in Unadjusted or Seasonally Adjusted Series","Moving from X-12-Arima to X-13Arima-Seats: The UK Experience","Data Fusion via Multiple Imputation","Multiple Imputation for Missing Body-Scan (DXA) Data in the National Health and Nutrition Examination Survey","Multiple Imputation for Disclosure Limitation","Multiple Imputation for Correcting Rounding Errors from Heaped Income Data","Encouraging Record Use in an Online Survey","Cognitive Aspects of Dependent Verification in Survey Operations","Outliers: an evaluation of methodologies","Statistics and Audit Sampling","Identifying Survey Interviewer Applicants with a High Probability of Early Termination","Evaluation of Interviewer Effects on Nonresponse Bias","What's the Chance? Interviewers' Expectations of Response in the 2010 SCF","Propensity Score Weight Adjustment for Dual-Mode or Dual-Frame Longitudinal Surveys","Calibration Adjustment for Nonresponse in Cross-Classified Data","Calibrated Maximum Likelihood Design Weights in Survey Sampling","What's New in SUDAAN 11?","Practical Issues When Calibrating Weights for Multiple Skewed Variables","Longitudinal Survey Weight Calibration: Theory and Methods","Fixing the Sample and the Sample Weights When Problems Arise in the Frame: Experiences with a Survey of Physicians","Improvements for Dual-Frame RDD Sampling and Weighting Methodology","Venn Diagrams, Probability 101, and Sampling Weights Computed for Dual-Frame Telephone RDD Designs","Finding an Optimal Composite Factor: Weighting Data from a Household Survey Using a Cell Overlap Design","An Approach for Incorporating an Undersampled Cell Phone Frame in a Dual-Frame Telephone Survey Through Weight Attenuation","Propensity Score Weight Adjustment with Several Follow-Ups","Semiparametric Fractional Imputation for Complex Survey Data","Exploring Some Uses for Instrumental-Variable Calibration","Unified Methods for Calibration in Some Missing Data and Sampling Problems","Decisionmaking About Redistricting ","Methodological Aspects of Pre-Election Polling","Preventing Fraud Versus Preventing Voting","Statistical Estimation and Election Night Decisionmaking","Estimates of Correct and Erroneous Enumeration with Duplicates in the 2010 U.S. Census","Approaches to Designing and Conducting Research on Hospitalization in View of the Reduced Samples in the 2008--2010 National Hospital Discharge Surveys","Meta-Analysis of Sample Survey Data","Linking the Teacher's Attitude Toward Job Condition 2003--04 Scale to 2007--08 Scale Using Concurrent Calibration","Racial Disparities in Delay of Diagnosis and Treatment Among Cancer Patients: Data Management Challenges for the SEER-Medicare Linked Database","The Rise in Multiple Births in the U.S.: An Analysis of a Hundred-Million Birth Records with R","Parametric Test of Equality of Two Frequency Distributions or Matrices","Overview of Current Population Survey Methodology","The Impact of Different Rotation Patterns on the Composite Estimator","Changes in Panel Bias in the U.S. Current Population Survey and Its Effects on Labor Force Estimates","Modeling Variances to Determine Sample Allocation for the Current Population Survey","Linkage of National Data on Assisted Reproductive Technology with Vital Records in Florida, Massachusetts, and Michigan","Classifications of Women's Menstrual Patterns and Related Epidemiology Explanations","Neighborhood Context Effects on Childhood Obesity: A Multilevel Analysis of Geocoded NSCH 2007","Optimum Grouping of Children Based on Early Childhood Growth Patterns","Bayesian Zero-Inflated Poisson Model and Its Application in Age-Specific Fertility Rate Pattern Estimation","Measuring Social Smoking: Challenges for Analysis and Implications for Surveys and Intervention","A Robust Bayesian Approach to Assessing Average Indirect (Mediation) Effect in Multilevel Models, with Application to a Smoking Study","New Composite Indicator for the Business Tendency Survey","On the Performance of Consumer Confidence Measurement","Does Moving the Condition Questions to the Beginning of Round 1 in the Medical Expenditure Panel Survey Produce Different Condition Estimates?","Concurrent Criterion-Related Validity of a Multidimensional Model of Symptoms of Depression in Organ Transplant Recipients","Methodological Considerations in Estimating Adolescent Substance Use","Kappa Statistic for Surveys and Complex Data","Challenges with Linking Survey and Administrative Data Sets","Argentina: An Update on Credibility in CPI and Other Official Statistics and Perils Endured by Some Statisticians","Human Rights and Statistics: A Reciprocal Relationship","International Responses to Governmental Targeting of Statisticians and Research Organizations in Argentina","Bayesian Friends with Benefits: Partial Shrinkage","Shrinkage Adjustment for Model Selection","Robust Minimax Shrinkage Estimation for Spherically Symmetric Distributions Under Concave Loss","Understanding Shrinkage in a Decision Theoretic Framework","Multiple Imputation for Nonignorable Attrition in Panel Studies with Refreshment Samples","A Bayesian Degree-Corrected Stochastic Block Model for Community Detection","How Far off Is Implicit Modeling in Multiple Imputation?","Bayesian Transition Models for Multivariate Event Histories","A Bayesian Analysis of the Spatial Distribution of Alcohol Outlets and Assaultive Violence","Model Selection Strategies Applied to Customer Research Data","Hierarchical, Continuous-Time Models for Network-Based Event Data","A Simple Approach to Sample Allocation for Multivariate Stratified Sampling","A Hierarchical Clustering Algorithm for Multivariate Stratification in Stratified Sampling","The Equivalence of Neyman Optimum Allocation for Sampling and Equal Proportions for Apportioning the U. S. House of Representatives","Two Measures for Assisting Sample Size Discussions","An Equivalence of Conditional and Unconditional Maximum Likelihood Estimators via Infinite Replication of Observations for Matched Pairs Designs","Interval Estimation for Small-Area Proportions with Small True Proportions from Surveys with Stratified Random Sampling Designs","Significance Testing for Two Cluster Samples with Identical Clusters and Different Units","Multiple Imputation for Hierarchical Data Sets","A Two-Step Semi-Parametric Method to Account for Survey Weights in Multiple Imputation","Estimation of Two-Level Hierarchical Generalized Linear Models Given a Mixture of Ordinal and Continuous Missing Data","Imputation Models to Estimate Mean Fatty Acid Percentages in NHANES","Investigating the Bias of Alternative Statistical Inference Methods in Sequential Mixed-Mode Surveys","Using Imputation to Adjust for Seam Bias in a Rotating Panel Survey","Fitting Additive Hazards Models for Case-Cohort Studies: A Multiple Imputation Approach","Particulate Matter Is Not Killing Californians","\tA Closer Look at Air Pollution-Mortality Relationships for California Members of the American Cancer Society Cohort","Assessing Variable Importance in an Environmental Observational Study","Improving the Scientific Advice Provided by the Clean Air Scientific Advisory PM Subcommittee","Reducing Revisions in Real Time Trend-Cycle Estimation","Frequency Domain Analysis of Seasonal Adjustment Filters Applied to Periodic Labor Force Survey Series","Theoretical and Real Trading-Day Frequencies","Testing for Hardy Weinberg Equilibrium in National Household Surveys That Collect Family-Based Genetic Data","A Randomized Experiment to Increase Response Rates to a Health Care Survey Among Individuals with a High Predicted Probability of Preferring Spanish","Conditional Logistic Regression Estimators for Ordinal or Multinomial Outcomes and Complex Survey Data","Outcome Vector--Dependent Sampling with Longitudinal Continuous Response Data: Stratified Sampling Based on Summary Statistics","Linear Rank Tests for Survival Outcomes in Complex Sample Survey Data","Transitioning from RDD to ABS in the National Household Education Surveys Program","An Evaluation of Incentive Experiments in a Two-Phase Address-Based Sample Mail Survey","Mail Surveys Including the Spanish-Speaking Population","The ABS Frame: Quality and Considerations","Development of the Sample Design for the International Survey of Doctorate Recipients (SDR)","Integration of the National and International 2008 SDR: Bridging Effects and Expected Improvements to the Time Series Data","Utilizing a Logistic Regression Approach for Weighting Adjustment in a Longitudinal Data Set","Migration Patterns of U.S.-Trained Doctorate-Holders","Small Population, Big Impact: Improving the Measurement of the Group Quarters Population in the American Community Survey","Functional Dynamic Factor Models with Application to Yield Curve Forecasting","Robust Association Studies Using Density Power Divergences with Application to Consumer Expenditure Survey and Survey of Consumer Finance","The Implications of Ignoring the Uncertainty in Control Totals for Generalized Regression Estimators","Regression Coefficient Estimation in Dual Frame Surveys","New Model-Optimized Sampling Techniques","Model-Assisted Lasso Regression Estimator","Inference in Cutoff Sampling","Using Administrative Records to Augment Unit Nonresponse to Facilitate Hot-Deck Imputation","Imputation Using the Other Pair Member","NORCSuite_Impute: Two-Way Search Hot-Deck Imputation Macro Using SAS IML","Semi-Parametric Imputation of Panel Surveys","Parametric Fractional Imputation using Adjusted Profile Likelihood for Linear Mixed Models with Nonignorable Missing Data","Jackknife Empirical Likelihood Method for Inference with Imputed Data","Climate, Health, and Vulnerability in Urban Populations","Spatial Uncertainty Estimation and Public Health Data","The Association Between Ambient Ozone and Cardiac Arrest at a Fine Spatial and Temporal Scale","A Bayesian Model-Averaging Approach for Estimating the Relative Risk of Mortality Associated with Heat Waves in 105 U.S. Cities","ACS Issues for Small Jurisdictions","An Early Look at Small-Area Data from the American Community Survey","Measuring Neighborhood Poverty with the American Community Survey","What Demographic Analysis Tells Us About the Undercount of Children in the 2010 Census","Generalized Linear Models with Variables Subject to Post Randomization Method","A Discussion of Uncongeniality for Synthetic Data","Asymptotic Inferences in Synthesized Microdata via Multiple Imputation","More Effective Database Construction: Enhancing the Utility of Secure Databases","Propensity Score Adjustments Using Covariates in Observational Studies","A Model-Based Approach to Assessing and Mitigating Nonresponse Bias for the Monthly Wholesale Trade Survey","Assessing Effectiveness of Nonresponse Adjustment Methods: Response Propensity Weighting and Generalized Regression Calibration Estimation","A Cautionary Note on Post-Stratification Adjustment","Comparing Alternative Weight Adjustment Methods ","Adjustment for Unit Nonresponse in the National Health Interview Survey","Evaluation of Two-Step Nonresponse Adjustment in the Mental Health Surveillance Study","Endogenous Post-Stratification Estimation Using a Random Forests Model","Can Replicate-Based Methods Be Used in Variance Estimation for Cut-Point Estimators Derived Through ROC Analysis?","Properties of Smoothed Design-Based Variance Estimators from Complex Sample Surveys","Variance Estimation for the Multidisciplinary Treatment Planning (MTP) Survey","Comparing Recent Approaches For Bootstrapping Sample Survey Data: A First Step Towards A Unified Approach","Single Versus Replicate Survey Weights: Toward Guidance for Choosing an Optimal Approach","A Unified Theory of Empirical Likelihood Ratio Confidence Intervals for Survey Data with Unequal Probabilities","Confidence Intervals for Quantile Estimation from Complex Survey Data","Dual-Frame Sample Weighting for a Telephone Survey of Children's Vaccination Status","New Directions for State and Local Telephone Surveys","Transitioning from RDD to ABS with Mail as the Primary Mode","Recruitment and Nonresponse Challenges with Unmatched Cases in an Address-Based Sample Design","Beyond the USPS CDs File: Extending the Coverage of ABS Frames","Challenges in the Treatment of Nonresponse for Selected Business Surveys","Using Paradata to Understand Effort and Attrition in a Panel Survey","Chasing the Nonrespondents: Use of the Conditional Bias","Methods for Adjusting Statistical Analyses for Record Linkage Error","Methods of Computing Optimal Record-Linkage Parameters","Two-Step Imputation of Linked National Health Interview Survey and Medicare Data Files","Layne, M.: Estimating Record Linkage Error Rates Using Administrative Records Data ","Parameter Estimation for Record Linkage","A Procedure for Evaluating and Comparing Small-Area Variability of Binary Outcomes","Small Area Prediction of the Mean of a Binomial Random Variable","Validity Testing for Coverage Properties of Small-Area Models for Cell Proportions","Small Area Estimates from the National Crime Victimization Survey","An Empirical Artificial Population and Sampling Design for Small-Area Model Evaluation","Applying Bivariate Binomial/Logit Normal Models to Small-Area Estimation","Standard Regression Model-Based Small-Area Domain Estimation in Household Surveys","Estimation Methodology for Weekly Surveys of Influenza Vaccination Rates","Weighting Adjustments for Panel Nonresponse","Bias Analysis of Average Weekly Earnings in the Current Employment Statistics Survey","An Investigation of Decennial Census Effects on Estimates of Substance Use and Mental Illness from the National Survey on Drug Use and Health (NSDUH)","Subsampling the Medical Expenditure Panel Survey for High-Expenditure Cases","NSHAP's Wave 2 Nonresponse Weight Adjustment with Some Wave 1 Nonresponses","Weighting Methods for the 2010 Data Collection Cycle of the Medical Monitoring Project","The Remarkable Robustness of Ordinary Least Squares in Randomized Clinical Trials","Shrinkage for Improved Inference in Factorial Experiments","Valid Post-Selection Inference","Uses and Limitations of GEEs and GLMs for Social Network Data","A Model-Averaging Approach to Improve the Efficiency of Teacher Value-Added Estimates","Revisions Revisited: Data-Driven Approaches for Detection in Quarterly Financial Report Macro-Level Data","Multivariate Selective Editing in the Integrated Business Statistics Program","Applicability of the Outlier Review Tool to Manufacturing, Mining, and Construction Sectors of the Economic Census","Setting Thresholds for Selective Edit: A Methodological Approach Using Process History from Establishment Surveys","Overview and Taxonomy of Technique for Privacy-Preserving Record Linkage","Privacy-Preserving Record Linkage and Privacy-Preserving Blocking for Large Files with Cryptographic Keys using Multibit Trees","Encrypted Versus Plain: Comparison of Record Linkages Using Privacy Preserving Probabilistic Record Linkage (P3RL): Results from a Simulation Study","Kuk's Model Adjusted for Efficiency and Protection Using Two Non-Sensitive Questions Unrelated to the Characteristic of Interest","Quasi-Empirical Bayes Estimates in Randomized Response Sampling","Alternative Variance Estimators for Data Perturbed for Confidentiality Protection","Likelihood-Based Finite Sample Inference Based on Synthetic Data","Measures of Disclosure Risk for Functions of Totals","Alternative Disclosure Limitation Methodologies for Small Establishments in the Quarterly Census of Employment and Wages Program","The Role of Risk Perception in Determining Consumer Cash Balances: A Study of Three Countries","Cash Versus Cards: The Role of Budget Control","Experimental Results from a Consumer Diary","Bayesian Solutions to Missing Data Problems in a Consumer Payments Survey","Measuring Payment Choice from Bank Survey Data: Can We Identify Consumer and Business Payments?","Single-Stage Generalized Raking Weight Adjustment in the Current Population Survey","Composite Estimation in Current Population Survey","Analysis of Longitudinal Complex Survey Data Using Parametric Bootstrap","Calibration and Evaluation of Generalized Variance Functions","Variance Estimation for High-Income Tables","Estimating the Variance of a Two-Phase Estimator with Sudaan 11","An Evaluation of Successive Difference Replication Variance Estimation for Systematic Sampling","Weighted Least Squares Estimation with Sampling Weights","Evaluation of Model-based Methods in Analyzing Complex Survey Data: A Simulation Study using Multistage Complex Sampling on a Finite Population","Aerial-Access Creel Surveys with Incomplete Matching of Aerial and Access Components","Properties of Some Size-Based Sample Designs Based on Imperfect Frame Information","Cellular RDD Sampling Enhancements for the Behavioral Risk Factor Surveillance System","Some Almost-Forgotten SCF Days, Imputed, Occasionally Multiply","An Enduring Partnership: Incorporating Administrative Data Into Sample Design for the Survey of Consumer Finances","Stay Out of the Emergency Room: A History of Coping and Improving in the SCF","Identifying Data Problems and Improving Data Quality in the Survey of Consumer Finances","Using the Survey of Consumer Finances in Federal Reserve Board Policy Analysis","How Representative Are Google Consumer Surveys?: Results from an Analysis of Google Consumer Survey Questions Relative to National Level Benchmarks","How Changes to Improve the Reliability of Telephone Status Items Affect Telephone Status Estimates","Nationwide Surveys in Mexico Based on a Mobile Phone Sampling Design","2013 National Census Contact Test","Weighting for Dual-Frame Designs Where Nonresponse Differs Between Frames","Simulation of Alternative Single-Frame and Dual-Frame Sample Designs for the NIS","Location, Location, Location: Incentives and Geography","Calibration Weighting: What We Know Now, What We Still Need to Know","Simulation Study to Validate Sample Allocation for the National Compensation Survey","Study and Sample Design Plan Review for Federal Compliance Programs: NHTSA's State Seat Belt Use Study","Expanding the Number of Primary Sampling Units for the National Health Interview Survey","Impact on Weights and Sampling Errors of Using Hybrid Frame and Composite MOS","Changes in the Selection of Dwellings in the Labour Force Survey of Argentina: A Simulation","Redesigning the Sample of the Company Organization Survey Using Predictive Modeling","Indirect Sampling in Case of Asymmetrical Link Structures","Inter-Agency Energy Data Standardization: Combining Multi-Source Data with Imperfect Information","Researching Forecast Models for Item Imputation in an EIA Survey","A Hierarchical State-Space Model for Short-Term Forecasting of Residential Electricity Demand","Examining and Estimating Power Plant Operations and Maintenance (O&amp;M) Costs","Face-to-Face Screening Interviews: Tradeoffs Between Coverage and Nonresponse Errors","Does Data Quality Decrease Over the Course of an Expenditure Survey?","Design Principles for the Use of Filter Questions","Interviewer Behavior and Survey Data Quality: The Case of Social Network Data","Interviewer Effects on a Network-Size Filter Question","Data Management of Confidential Data","Administrative Data and Replicable Science: Does It Fit?","Managing Disclosure Risks in the Curation and Dissemination of Research Data","Meeting the Long-Term Needs of Scientific Progress Through Data-Collection Standards","A Comparison of Methods for Estimating Confidence Intervals for Proportions in Clustered Surveys","Analysis of Variance as a Basis for Sample Surveys","Pre-Sampling Model-Based Inference V","Partially Linear Models in Dual-Frame Surveys","A Comparison of Design-Based and Calibrated-Bayes Estimates Using Data from a Health Survey","An Empirical Study to Evaluate the Performance of Synthetic Estimates of Substance Use in the National Survey on Drug Use and Health","A Comparison of the Performance of Four Confidence Intervals for Population Size Based on a Capture-Recapture Design","Can Randomized Response Techniques Play a Role in the Era of Big Data?","Design Effects in Surveys That Require Oversampling of Certain Subpopulations","The Relative Statistical and Operational Plausibility of Multiple-Frame Sampling for Rare Population Subgroups","Using Targeted Lists for Studies of Rare Populations: The Super Wealthy","Sampling Designs for Populations at High Risk for HIV","Weighting Methods for a Study of Men Who Have Sex with Men (MSM3)","Sampling Designs for HIV Patient Populations","Sensitivity Analysis of Respondent-Driven Sampling","Data Smearing: An Approach to Disclosure Limitation for Tabular Data","Nonparametric Bayesian Models for Generating Synthetic Household Data","Balancing Use of Weights, Predictions, and Locality Effects in a Model-Assisted Constrained Hot Deck Approach for Perturbation","Generating Synthetic Graphs Under Differential Privacy","Developing Interviewer Observations of the Neighborhood and Sample Unit for the National Health Interview Survey","The Implications of Differential Measurement Error in Interviewer Observations for Nonresponse Adjustment of Survey Estimates: A Simulation Study","Assessing Interviewer Observations in the NHIS","Evaluating Interviewer Observations in the National Health Interview Survey: Associations with Response Propensity","Methods for Studying Variability as a Predictor of Health Status","Bayesian Mixed-Effects Location Scale Models for the Analysis of Objectively Measured Physical Activity Data from a Lifestyle Intervention Trial","Exploring the Relations Among Different Levels of Intraindividual Variability  and Longitudinal Change Using a Mixed Effects Location Scale Model","A Location Scale Item Response Theory (IRT) Model for Analysis of Ordinal Questionnaire Data","Detangling the Effect Between Rate of Change and Within-Subject Variability in Longitudinal Risk Factors and Associations with a Binary Health Outcome","Challenges Faced in the Daily Modeling of Survey Responses","Monitoring Key Estimates from the National Health Interview Survey throughout the Realignment of Census Bureau Regional Offices","The Effect of the U.S. Census Bureau Realignment on the National Crime Victimization Survey and the Consumer Expenditure Quarterly Interview Survey","Applications of Statistical Models That Detect Daily Changes Using Key Estimates from the American Community Survey Due to the U.S. Census Bureau Regional Office Restructure","Demographic Data Monitoring System: Technology Used to Track Survey Quality","A Proposed Revision of Wage Imputation Methods for the Occupational Employment Statistics Survey","An Innovative Multiple Imputation Method to Accommodate Complex Sample Design Features","Quantile Estimation After Multiple Imputation","Comparison of Imputation Techniques for Item Missing Data in the Survey of Income and Program Participation","Evaluating and Redesigning Imputation Methodologies for the 2015 American Housing Survey","Making Inference from Multiply Imputed Data Sets Using Mixture Distributions","On the Choice of Tuning Constants for Winsorized Estimators","Setting M-Estimation Parameters for Detection and Treatment of Influential Values","Aggregating Comparable Categorical Responses to the Unit of Observation in Employer Surveys","On the Effects of Degree-Day Base Temperatures on Estimates of Residential Energy End Uses","The Estimation Methodology of the 2011 National Household Survey","Calculating Adjusted Survival Functions for Complex Sample Survey Data and Application to Vaccination Coverage Studies with National Immunization Survey (NIS)","Using Mixture Distributions to Predict Radio Listening","Practical Guidelines for Dual-Frame RDD Survey Methodology","What Statistical Problems Are Not Missing-Data Problems?","Joint Modeling of Incomplete Data with Mixed Variable Types Using Latent-Variable Models","Regression Discontinuity Designs and Potential Outcomes","The Role of Covariates and Secondary Outcomes in Causal Studies with Intermediate Variables","Using Publically Available Administrative Data to Improve Direct Estimates of Income and Poverty from the American Community Survey","Coverage of American Indian and Alaska Native Persons and of the Population in American Indian and Alaska Native Areas in the American Community Survey","Investigation of Anomalies in Derived Variances for Estimates from The American Community Survey Public Use Microdata File","Approaches to Modeling the Characteristics of Undeliverable-as-Addressed Addresses in the American Community Survey","Sample Representivity in the American Community Survey","Methods for Producing Consistent Control Totals for Benchmarking in Survey Sampling","Two-Step Calibration of Design Weights in Survey Sampling","Improved Sampling Weight Calibration by Generalized Raking with Optimal Unbiased Modification","Dealing with Nonresponse Using Follow-Up","Pseudo-Population Bootstrap Methods for Imputed Survey Data","Preserving Relationships Between Variables with MIVQUE-Based Imputation for Item Nonresponse in Surveys","Standardizing Imputation Methods for the Dairy Products Program","Validation of Prediction Models in the Presence of Missing Data","How Does Online Survey Mode Affect Answers to Customer Feedback Loyalty Surveys?","Imputation Methods for Surveys: A Demonstration of the Impute Procedure in Sudaan","Creating Intuitive Editing Interfaces for the Survey of Consumer Finances (SCF)","Creating an Automated Edit and Imputation System for the Survey on Quebec Accommodation Establishment Occupancy","Estimation of Glomerular Filtration Rate in South Asians: A Study from the General Population in Pakistan","Should the Proxy-Respondents Be Surveyed When Assessing the Regular Smoking Initiation Age?","Weighting Strategy in the Quebec Survey on the Experience of Health Care","Variance Estimation of the Design Effect","ARIMA and General Regression Neural Network for Forecasting Rice Production in Sri Lanka","Comparisons of K-Mean and K-Medoid General Regression Neural Network for Handling Missing Data","Methodological Experiences from a Register-Based Census","On Simultaneous Interval Estimating the Relative Prevalence of Forward Shifting in Reported Regular Smoking Initiation Age","Imputing Ordinal Data with One Predominate Category","Web Collection in the Quarterly Census of Employment and Wages Program","Use of R-Indicators to Assess Survey Response Representativeness","The Impact on Response Rates of Adding a Survey Supplement","Model-Based Methods for Missing Data in Surveys with Post-Stratification Information","Bootstrap Estimation of Variance from ROC Curve Analysis of Complex NHANES Survey Data","2012 NHANES National Youth Fitness Survey","Analyzing Student Perceptions of Teaching with Quantile Regression","Effects of Response Format on Race and Ethnicity Measurement in the U.S.","What Makes Us Exploit the Community? The Influence of Individual Characteristics on Committing Tax Evasion and Insurance Fraud","Restricted Latent Class Multiple Imputation Method of Categorical Missing Data","Efficient Estimation of Partially Observed Clustered Data Using Multiple Imputation","Comparison of Weighting Approaches for Longitudinal Data with Time-Dependent Cluster Sizes","Imputation of Family Income and Maximal Utilization of Auxiliary Data: A Case Study of the 2012 Ohio Medicaid Assessment Survey (OMAS)","Applications of Survey Regression Models to Estimate the Degree of Data Agreement","Projected Variance for the Model-Based Classical Ratio Estimator: Estimating Sample Size Requirements","Bayesian Nonparametric Finite Population Inference ","Estimating Prices from a Natural Gas Monthly Survey","Analysis of Large Survey Data Sets Using Dynamically Generated SQL","Hot Deck Imputation of Nonignorable Missing Data with Sensitivity Analysis","Reliability and Stability of the Six-Question Disability Measure in the Survey of Income and Program Participation","Response Rates Revisited","Understanding Egypt's Telephone Owning Population","New Computer-Based Training for National Center for Education Statistics Complex Survey Data Sets","Modeling Smoking and Heaping Patterns in Self-Reported Cigarette Numbers by a Finite Mixture Approach","Using the Constrained Ordinal Models for Likert-Based Outcomes","Social Security Numbers in State Medicaid Records: Completeness and Quality","Linked NCHS-Medicaid Data Files","Evaluating Race and Ethnicity of Medicaid Participants Using Census Data","Combining Paradata and Survey Responses to Identify Sources of Measurement Error in Medical Event Reporting","Using GPS and Other Data to Assess Errors in Level-of-Effort Data in Field Surveys","Impact of the 2012 Computer Audio Recorded Interviewing Application on Survey of Income and Program Participation Event History Calendar Response Rates and Item-Level Responses","Comparisons of CPS Unemployment Estimates by Rotation Panel","Measurement Error Properties in an Accelerometer Sample of U.S. Elementary School Children","Using Response Time to Investigate Students' Test-Taking Behaviors","Strategies for Processing Tabular Data Using the G-Confid Cell Suppression Software","Dealing with Negative Contributions in Protecting Tabular Data","Synthesizing Truncated Count Data for Confidentiality","Estimation for Cells Suppressed in Tabulation with Application to Output Disclosure Treatment of the NSF Survey of Earned Doctorates","Analysis of Tables Containing Suppressions","The Analysis of Survey Data Using the Bootstrap","Parametric Bootstrap Confidence Intervals for Survey-Weighted Small-Area Proportions","A Simulation Study on Bootstrap Variance Estimation of Sample Quantiles Under Doubly Protected Hot Deck Imputation","Targeting Minorities Using Address-Based Sampling: A Simulation Study","Using Imputation Procedures to Enhance the DSF Frame","Sample Allocation Using Vendor-Provided Demographic Data","Examining Components of Coverage for the 2010 U.S. Census for Several Census Operations","Using New IT for Area Sampling in a Metropolitan Household Survey","An Alternative Approach for Dealing with Inaccessible Sampled Persons in Registry-Based Samples","Administrative Records and the 2010 U.S. Census Coverage Measurement Estimates: A Comparison","Within and Across County Variation in SNAP Misreporting: Evidence from Linked ACS and Administrative Records","Deciphering Duplicity: Characterizing Persons with Multiple Protected Identification Keys (PIKs) in the National Change of Address (NCOA) Database to Facilitate Migration Research","The 2010 National Survey of College Graduates (NSCG) Weighting","Evaluating the Consistency Between Responses to the 2010 NSCG and the 2009 ACS","Leveraging the American Community Survey (ACS) in Current Estimation for the National Survey of College Graduates (NSCG)","Monitoring Methods for Adaptive Design in the National Survey of College Graduates (NSCG): A Retrospective Appraisal","GIS and Survey Research","The Limit of Linkage: What Happens When Records Lack PII","Strategies for Enhancing the Linkage of National Center for Health Statistics' Surveys with Death Indices for Mortality Followup","Transitive Probabilistic Deduplication of Record Systems Using a Stochastic Blockmodel","Some Advances on Bayesian Record Linkage and Inference for Linked Data","Identifiability and Estimation in Generalized Linear Models with Nonignorable Missing Data","Is It MAR or NMAR?","Propensity Score Adjustment Method for Nonignorable Nonresponse","Maximum Empirical Likelihood Estimation for Nonignorable Missing Data Problems","Evaluation of Selective Editing for the Census Bureau Foreign Trade Data","A Visual Proof, a Test, and an Extension of a Simple Tool for Comparing Competing Estimates","Counting Persons Once and Only Once at the Right Location in the Census: Techniques and Challenges Unduplicating People Experiencing Homelessness","Evaluation of a New Edit Methodology for the Common Core of Data Nonfiscal Surveys","Simplified Census Edit and Imputation Based on Statistical Principles","Ratio Edits Based on Tolerance Intervals","Knocking on Respondents Doors: Interviewers and Unit Nonresponse in a Large Wealth Survey","Survey Mode Effects on Income Inequality Measurement","Estimating Population Size with Link-Tracing Sampling","Optimal Recall Period Length in Consumer Payment Surveys","Measuring Household Spending and Payment Habits: The Role of 'Typical' and 'Specific' Time Frames in Survey Questions","Respondents: Who Art Thou? Comparing Internal, Temporal, and External Validity of Survey Response Propensity Models Based on Random Forests and Logistic Regression Models","Using an Item Response Theory Approach to Measure Survey Mode of Administration Effects: Analysis of Data from a Randomized Mode Experiment","The Role of Mode Preference Questions in Predicting Mode-Specific Response Propensities","Investigating the Bias of Alternative Statistical Inference Methods in Mixed-Mode Surveys","Mode Effect Analysis and Adjustment in a Split-Sample Mixed-Mode Web/CATI Survey","Assessing Nonresponse Bias in the Green Technologies and Practices Survey","Nonparametric Small-Area Estimation","Benchmarked Empirical Bayes Small-Area Estimators Under Multiplicative Models","Small-Area Estimation Method with Covariates Subject to Measurement Error","Blending Domain Estimates from Two Victimization Surveys with Possible Bias","A Study of Data-Collection Rules Involving Real-Time Imputation for Adaptive Survey Design","Multivariate Linear Mixed-Effects Models for Missing Data Applied to a Business Survey","Building a Complete History for Respondents in Longitudinal Surveys Through Imputation","Analysis on Generalized Variance Function Estimators from Complex Sample Surveys","Construction of Replicate Weights for Project TALENT","Multiple Imputation of Missing or Faulty Values Under Linear Constraints","Using Internet Panel Surveys for Behavioral Health Surveillance","Numerical Impact of Topcoding on CE Microdata Utility","Data Entrepreneurs' Synthetic PUF: A Working PUF as an Alternative to Traditional Synthetic and Nonsynthetic PUFs","The CMS Public Use File Reidentification Experience","Aggregate-Level PUF with High Data Confidentiality and Analytic Utility for Descriptive Analyses from Medicare Claims Data","Query-Based PUF for Disclosure-Safe Remote Analysis from Medicare Claims Microdata","Integrative Analysis of Longitudinal Studies of Aging (IALSA): A Collaborative Network for Parallel and Pooled Data Analysis of Within-Person Change","Harmonizing Individual Participant Data for Collaborative Research: How Should We Foster Such an Agenda?","Multilevel Item Response Theory: Modeling Changing Measurement Structures in Longitudinal Studies","Practical Issues with Aggregate and Individual-Level Meta-Analytic Methods for Combining Results from Longitudinal Studies","Might a Social Integration Navigator Guide Response Propensity Models in Nonresponse Follow-Up?","When Do \"Do it Yourself-ers\" Really Do It Themselves? Using Paradata to Explore the Preference for Self-completion Modes in a Multi-mode Survey","Model-Based Mode of Data Collection Switching from Internet to Mail in the American Community Survey","Using Paradata to Understand Panel Effects in the Current Population Survey","Opening the Box: What We Already Know and Don't Know Yet About the Response Process in the European Social Survey","Estimating Mode Effects Without Bias: A Randomized Experiment to Compare Mode Effects Between Face-to-Face Interviews and Web Surveys","Analyses of a Mixed-Mode ABS Nationwide Survey of Trust and Confidence","Assessment of Total Survey Error for the 2011 National Immunization Survey","Exploring Organizational Characteristics and Design Features Affecting Nonresponse in Surveys of Nonprofit Organizations","Reducing Survey Nonresponse Through Enhanced Administrative Cooperation: An Experience in Korea","Simulation Approach for Determining Use of Mahalanobis Distance to Reduce Nonresponse Bias","Predicting Proxy Status in Nonresponse Follow-Up Workload","Will They Answer the Phone if They Know It Is Us? Using Caller ID to Improve Response Rates","Treatment of Outcome-Related Nonresponse in an International Literacy Survey","Calibration-Weighting Methods for Complex Surveys","Longitudinal Survey Weight Calibration with Estimated Totals","Some Thoughts on Calibration Applications for Multipurpose Estimations","A Design Effect Measure for Calibration Weights in Single-Stage Samples","NaN","A Simplified Approach to Administrative Record Linkage in the Quarterly Census of Employment and Wages","Project Talent: Weighting Adjustments Comparison for Nonresponse and Tracking Loss in a Follow-Up Survey 50 Years Later","Protecting survey design information by combining strata and accounting for the realized sample selection.","Opening the Doors to U.S. Department of Education Data: Program, Grant, and Statistical Data","Computer-Based Training for NCES Complex Survey Micro-Data Sets: 2014 Update","Modeling Clustering Design Effects When Cluster Sizes Vary","Understanding Egypt's Telephone Using Population Using RDD and Face-to-Face Surveys","Using Bayesian Statistical Inference to Improve the Measurement of Adequacy of Mental Health Care Utilization in a Nationally Representative Sample","Implementation and Results of an Experiment Using Mahalanobis Distance in Responsive Design to Reduce Nonresponse Bias","Weight Smoothing Using Laplace Priors","Clustering of Dietary Patterns in Pregnant Women and Children Living in the Seychelles","Automated Survey Coding for German Occupations","Big Data and Advanced Analytics in Radiology Precision and Pre-Emptive Patient Care","Follow-Up Survey Response Rates in Women at Risk for Breast Cancer","Modeling Population Psychometric Characteristics of a Speech-in-Noise Task: Using a Large Cross-Sectional Study to Explore Associations Between Cognition and Listening","Comparison of Estimates for Lifetime Depression Using the National Survey on Drug Use and Health (NSDUH) and Behavioral Risk Factor Surveillance System (BRFSS)","Using Contact Networks and Mortality Patterns to Estimate Epidemiological Process Parameters","Sampling Strategies Based on Existing Information in Nested Case Control Study","Monte Carlo Simulation to Examine the Uncertainty in Autism Spectrum Disorder Prevalence Estimates Derived from Postcensal Population Estimates","Releasing Synthetic Microdata for Magnitude Tabular Data","Evaluation Protocol for International Education Systems","Negative Binomials Regression Model in Analysis of Wait Time at Hospital Emergency Department ","Descriptive and Analytic Inferences from Input and Output De-Identified Data","Inference for Exponential-Family Random Graph Models Based on Egocentrically Sampled Data","A Pseudo-Likelihood Approach for Hierarchical Linear Models Under Complex Designs","Combining State Behavioral Risk Factor Surveillance System Data: Weighting for National Estimates","A New Quasi-Empirical Bayes Estimate in Randomized Response Technique","ABS and Demographic Flags: Examining the Implications for Using Auxiliary Frame Information","The Effect Size for Simultaneous Item Bias Test","Imputing Biomarker Outcome Refusals and Unasked Questions in a Nonresponse Follow-Up for Environmental Exposure Models","A Data Quality Program for the Survey of Consumer Finances","Hierarchical Bayesian Methods for Combining Surveys","Missing Data in Afghanistan: A Latent Class Examination of Item Nonresponse","Sample Selection Bias in Consumer Payment Surveys","A Comparison of Weighting Adjustment Methods for Nonresponse","Using Response Rates to Adjust a Dual Sample Design","Protecting Survey Design Information in Public Use Files by Constructing Combined Strata Accounting for the Realized Sample Selection","A Domain-Based Estimation Framework for Measuring Risk and Utility for Both Input and Output De-Identified Data","Using Paradata from Interviewer Voices to Examine Nonresponse Error and Measurement Error","The Global Impact of Statistics Without Borders and StatCom: Can We Make It Enduring?","Analyzing Survey Nonresponse","Designing and Architecting a Shared Platform for Adaptive Data Collection in Surveys and Censuses","Dirty and Unknown: Statistical Editing and Imputation in the SCF","Single-Stage Generalized Raking Application in the American Housing Survey","Defining and Updating the National Health Interview Survey Variance Estimation Structure Over a Sample Design Period","Case Reassignment: When Making Contact Is a Two-Person Job","Estimating the Cost of the 2020 Census Nonresponse Followup with Administrative Records","An Imputation Model Database and Its Relevance to Analysis","Dual-Frame Telephone Sampling for a National Survey with State Estimates","Evaluating Record Linkage Quality in the NCHS Linked Mortality Files","Joe Steinberg and the Census Class of 1940","Medicaid Undercount in the American Community Survey","Unresolved Matched Records in Capture-Recapture Methodology","Longitudinal Assessment of Measurement Error on the Consumer Expenditure Interview Survey: 1996--2010","Combining Two Sources of Crime Data to Improve County-Level Estimation","Methodological Challenges in the Redesign of the 2015 Survey of Prison Inmates","Using Administrative Records to Reduce Census Nonresponse Followup Operations","Substitution of Nonresponding Primary Selection Units","Estimating Characteristics of Strip Shopping Center Buildings in the 2012 CBECS","Cash Sources and Sinks in Mexico","Model-Assisted Estimation of Unemployment Rates for Longitudinal Surveys with an Application in the Current Population Survey","Edit and Imputation Processing for Ethnocultural Variables: The Experience of the 2011 Canadian National Household Survey","Random Data Swapping as an Approach for Statistical Disclosure Limitation and Its Effects on Utility of Data","The Benefits of Sampling Clusters with Probability Proportional to Size in Cluster-Randomized Experiments","Comparison of Alternative Imputation Methods in the National Teacher and Principal Survey","Evaluating a New Approach for Estimating the Number of U.S. Farms with Adjustment for Misclassification","Reduction of Survey Nonobservation Errors Through Adaptive Sampling Design","WITHDRAWN: Assessing Two Nonresponse Follow-Up Strategies: Shortened Questionnaire and Administrative Data Collection","Iterative Hot-Deck Multiple Imputation with a Distance-Based Donor Method for the Current Population Survey","On Estimating Mean Squared Prediction Error of Small Area Estimators in Basic Area Level Model with Unknown Sampling Variance by Parametric Bootstrap","Imputation for National Hospital Care Survey","Parametric Bootstrap Procedures for Small Area Prediction Variance","Planning and Implementing International Early Grade Reading Assessments","A General Framework for Research Design: Monte Carlo Simulation Methods for Sample Size Planning","Small Area Estimation Methods for Binary Variables in the Behavioral Risk Factor Surveillance System ","The Backstory of Deff and Deft","Adaptive Integration of Survey Data with Other Information Sources","Managing Sample Release in Social and Economic Surveys","Data Analysis Using NHIS-EPA--Linked Files: Issues with Using Incomplete Linkage","Stop Chasing Your Tail: Reining in Hard-to-Reach Respondents","An Application of Calibrated Bayes Methods to Estimate Vaccination Coverage Rates from the NIS","Can We Avoid Problems with Movers? Some Analytical Issues with National Data Linked with State-Level Data","What Paradata Can Tell Us About the Annual Survey of Jails","A Comparison Study of Weighting Adjustment and Multiple Imputation for Missingness Due to Nonlinkage: A Study of the National Health Interview Survey Linked to Medicare Data Files","A Snapshot of Primary Classroom Experiences in Nigeria","Nonparametric Multiple Imputation","Examining the Threshold: Experiences in Evaluating the DSF When Listing May or May Not Be Necessary","Classification Error in Measuring Sexual Victimization Among Inmates: The National Inmate Survey","Predicting Initial Response Mode in Advance of Data Collection in the NSCG","Study of Error in Survey Reports of Move Month Using the U.S. Postal Service Change of Address Records ","Modeling Compositional Time Series from the Brazilian Labour Force Survey","Identifying Business and Consumer Payment Patterns from Bank Account Data: Estimating Partially Observed Distributions from Survey Data","Using Household Surveys in International Education Research","Experiences with the Use of Address-Based Sampling in Two In-Person National Household Surveys","A Bayesian Approach to Incorporating Uncertainty in Record Linkage ","Designing Flexibility for State Samples into the Redesigned 2016 National Health Interview Survey (NHIS)","Qatar National Education Reform ","Seeking to Reduce Motivated Underreporting to the Health and Retirement Study Screening Interview","Weighted Least Squares Estimation with Simultaneous Consideration of Variance and Sampling Weights","Implementation of an Efficient Sampling and Case Processing Method","Analyzing Potential Mode Effects in the National Crime Victimization Survey","Retail Payment Innovations and Cash Usage: Accounting for Attrition Using Refreshment Samples","Predictive Ratio Matching Imputation of Nested Compositional Data with Semicontinuous Variables","Measuring Risk in Tables Where the Study Variable May Be Negative","A New Way to Multiply Impute Nonignorable Missing Outcomes","Analysis of the Sources of Group Quarters Enumeration Data in the 2010 Census","Accessing Data from the Census Bureau API","Four Men Who Were Presidents in the History of the American Statistical Association: Selected Vignettes","Women Presidents in the History of the American Statistical Association: Selected Vignettes","Black Magic Using Randomized Response Techniques","Project Talent: An Overview of Characteristics and Performance of Students from Immigrant Families","A Class of Dual Frame Survey Sampling Estimators in the Presence of a Covariate: How Amy Predicts Her President","Impact of Changing the Number of Interview Waves in the NCVS","Survey Estimators That Respect Natural Orderings","Experiences with Face-to-Face Surveys in Africa","Optimal AK Composite Estimators for the Current Population Survey","Nonresponse Bias Correction via Calibration for the Brazilian National Household Sample Survey","Testing Goodness-of-Fit with Survey Data","Bayesian Models for Binary Responses with Markov Dependence","Multilevel Regression and Poststratification for Small Area Estimation of Population Health Outcomes: Validation and Application","Small Area Estimation for the Tobacco-Use Supplement to the Current Population Survey","Nested Dirichlet Process Model for Household Data Set Synthesis","Research on Rotation Design and Estimation in the Current Population Survey","Interval Estimation of a Finite Population Proportion Using Randomized Response","Investigating Nonresponse Error in Open-Ended Survey Items and Their Potential for Enriching Validity of Closed-Ended Measures","Semiparametric Estimation for Recurrent Event Data from Complex Survey Designs","Model-Based Estimation in Official Statistics: Applying Small Area Estimation to the Dutch National Crime Victimisation Survey","New Technologies for Privacy Protection in Data Collection and Analysis","Semiparametric Generalized Linear Mixed Model for Localized Health Estimates","A Brief History of the American Statistical Association: 1839--2014, Part 2","Defining the Optimal Incentive Amount: Does Money Talk?","Hospital Based Sampling: Lessons Learned from the SAFE Study","A Brief History of the American Statistical Association: 1839--2014, Part 1","Practical Tools for Designing and Weighting Survey Samples","PPS Subsampling from NHIS to MEPS: Effect on Precision of MEPS Estimates","Comparison of Traditional Weight Adjustments to Calibrated Weights in the Medical Expenditure Panel Survey for Both Non-Response and Post-Stratification","Decomposing the Variance of Child Outcomes in Multistage Sample of Head Start Children","Contacting Strategies and Incentives During the Field Period: Evidence from the Survey of Consumer Finances","NHTSA's Data Modernization Project","Examining interviewer-respondent interactions in the Survey of Consumer Finances (SCF)","Empirical Likelihood Confidence Intervals and Significance Test for Regression Parameters Under Complex Survey Sampling Designs","Empirical Likelihood Confidence Intervals under the Rao-Hartley-Cochran Sampling Design","Bayesian Post-Stratification Models Using Multilevel Penalized Spline Regression","Using Data from the American Community Survey to Better Understand Coverage Measurement Results in the 2010 Census","Using Integer Programming to Determine the Optimal Assignment of Strategies for Mode Switching from Internet to Mail","Using the Fraction of Missing Information (FMI) to Identify Auxiliary Variables for Imputation Procedures via Proxy Pattern-Mixture Models","The Use of Indicators to Assess the Quality of Business Survey Returns During Data Collection","Model-Assisted Domain Estimation When Combining Survey Data with Administrative Records","An Investigation of Interviewer Effects on Measurement Error","Adjustments for Survey Imputed Data Sets to Achieve First- and Second-Order Properties","Item Nonresponse: Modeling the Impact of Probing Don't Knows","Synthetic Longitudinal Business Databases for International Comparisons","Nearest-Neighbor--Based Approaches for Multiple Imputation of Unordered Categorical Variables","Statistical Matching Using Fractional Imputation","Transitioning a Random Digit Dialing Health Survey to Address-Based Sampling","Issues Concerning Imputation of Hispanic Origin Due to Potential Administrative Record Enumeration for the 2020 Census","Evaluating Imputation Techniques in the Monthly Wholesale Trade Survey","Coverage Properties of Confidence Intervals for Proportions in Complex Sample Surveys","Modeling Frame Deficiencies for Improved Calibrations","Strategies for Subsampling Nonrespondents for Economic Programs","Examining Economic Census Reporting Patterns","A Comparison of Methodologies for Classification of Administrative Records Quality for Census Enumeration","Tools for Analyzing Nonresponse Adjustments in Survey Sampling","Balancing Timeliness, Data Quality, and Cost by Optimizing Data-Collection Strategies","Lessons Learned from Implementing Response Propensity Models in the 2013 Census Test","An Objective Stepwise Bayes Approach to Small Area Estimation ","Different Ways of Dealing with Missingness in Hierarchical Data Sets","Effects of Imperfect Unit Size Information on Complex Sample Designs and Estimators","Nonresponse Patterns and Bias in the American Time Use Survey","Valid Analytic Properties and Disclosure Limitation for Microdata","Multiple Imputation for Poverty Rate Estimation from Rounded Income Data","Analysis of Interval-Censored Data Using Competing Risks Model and Multiple Imputation","A Design Effect Measure for Calibration Weighting in Cluster Samples","A Simple Method of Exact Optimal Sample Allocation Under Stratification with Any Mixed Constraint Patterns","Predicting Response Mode During Data Collection in the National Survey of College Graduates","Mental Health Estimates Computed Directly from the Clinical Sample of the Mental Health Surveillance Study and Measures of their Standard Errors","Quality and Analysis of Sets of National Files","Non-Probability Samples from Populations That Are Identified Through Costly Screening","Ordered Sample Scatterplots for Displaying Survey Data ","Panel Analysis of Household Nonresponse and Person Coverage in the Current Population Survey","Combining Information from Multiple Sources in Bayesian Modeling","Using the National Provider Identification File as the Sampling Frame for a Physician Survey ","Multivariate Sample Design Optimization for NHTSA's New National Automotive Sampling System","Composite Measure of Size Evaluation and Primary Sampling Unit Formation for NHTSA's New National Automotive Sampling System","Interviewer Effects on Survey Questionnaire Response Times: A Hierarchical Bayesian Analysis of Multivariate Survival Paradata","Handling Frame Problems with Address-Based Sampling and In-Person Household Surveys","The Coverage-Nonresponse Trade-Off","Strategies for Selecting a Follow-Up Sample of Nonrespondents in Business Surveys","Measuring Impact of Top-Coding on the Utility of Consumer Expenditure Microdata","Improving the Socioeconomic Ranking of Small Areas Using the Estimates from the American Community Survey","Changes in Interviewer-Related Error Over the Course of the Field Period: An Empirical Examination Using Paradata and Behavior Codes","Small Area Estimation for the National Survey of College Graduates New Cohort","On Weight Smoothing in the Current Employment Statistics Survey","Interpolating and Standardizing Time Series Data Covering Various Fiscal Intervals Using Splines","Spatio-Temporal Modeling of U.S. State-To-State Migration Flows","EM and Data Augmentation Algorithms for Social Network Analysis with Missing Data","Analyzing Open-Ended Survey Questions Using Unsupervised Learning Methods","A Fully Bayesian Approach for Generating Synthetic Marks and Geographies for Confidential Data","The Poisson Change of Support Problem with Applications to the American Community Survey","Three-Stage Optimal Sampling Plans for Group Testing Data","Estimating Population and Design Parameters for Nhtsa's New National Automotive Sampling System (Nass)","Creating a Flexible and Scalable Psu Sample for Nhtsa's New National Automotive Sampling System","Address-Based Sampling Frames for Beginners","Replication Variance Estimation for Balanced Sampling: An Application to the PIAAC Study","Small Area Estimation of Complex Parameters Under Unit-Level Models with Skew-Normal Errors","Bayesian Item Response Analysis of Method-Of-Payment Habits in Banking Surveys","Flexible Bayesian Methodology for Multivariate Spatial Small Area Estimation","Improving the American Community Survey Margins of Error Through Data-Driven Regionalization","Undergraduates' Statistics Anxiety and Mathematics Anxiety: Are They Similar or Different Constructs?","Noncoverage Adjustments in a Single-Frame Cell-Phone Survey: Weighting Approach to Adjust for Phoneless and Landline-Only Households","Geographic Oversampling for Race/Ethnic Minorities Based on Data from the 2010 U.S. Population Census","Estimation of Dynamic Models with Nonignorable and Nonmonotone Drop-Out","Designing the National Immunization Survey to Account for Geographic Misclassification of the Cell Phone Sample","A Weight-Trimming Approach to Achieve a Comparable Increase to Bias Across Countries in the Programme for the International Assessment of Adult Competencies","Evaluation of Sample Designs Using Results from the Programme for the International Assessment of Adult Competencies","Reducing Quality Variation Across Countries in an International Survey","The Sample Overlap Problem for Systematic Sampling","Considerations for Selection and Release of Reserve Samples for In-Person Surveys","An Evaluation of the Impact of Missing Data on Disclosure Risk","Comparison of Methods Utilizing Immunization Information Systems (IIS) Data for Sample Frame Construction in the National Immunization Survey (NIS)","Evaluation of Efficiency of Standard Regression Mixed Model--Based BLUP Estimators in Household Surveys Assuming Unequal Error Variances","Examining the Impact of Data Collection Interventions on Data Quality, Cost, and the Risk of Nonresponse Bias","An Overview of Adaptive Survey Design","Using Paradata to Inform Collection Instruments","Using Population Data to Improve Public Sector Frame Coverage","Using Paradata and the Nonresponse Follow-Up Dashboard Score Function to Prioritize Workload","Evaluating Calibration Estimators for the Annual Survey of Local Government Finance","Using Paradata to Calibrate the Quarterly Summary of State and Local Government Tax Revenue","Using Paradata to Design the Quarterly Tax Sample","Adaptive Mixed-Mode Survey Designs Accounting for Mode Effects","Calibrating the Empirical Bayes to Decision-Based Estimates in the Annual Survey of Public Employment and Payroll","An Evaluation of Different Small Area Estimators for the Annual Survey of Public Employment and Payroll","From Flagging a Sample to Framing It: An Exploration of the Utility of Information That Can Be Appended to RDD and Address-Based Samples","Evaluation of Combining Consumer Marketing Data Used for Address-Based Sampling","Adaptive Design Features for Using Address-Based Sampling in a National CATI Survey of Households with Children","The Use of Targeted Lists to Enhance Sampling Efficiency in Address-Based Sample Designs: Age, Race, and Other Qualities","Limiting the Risk of Nonresponse Bias by Using Regression Diagnostics as a Guide to Data Collection","NaN","Variance Estimation for Survey-Weighted Data Using Bootstrap Resampling Methods: 2013 Methods-of-Payment Survey Questionnaire","Estimation and Calibration from Multiple Data Sources: Linking Payment Volumes and User Type Allocations from Different Survey Vantage Points","Assimilating Dual-Panel Surveys to Generate Population Estimates","Validating Survey Data Using Benford's Law","In Love with the Debit Card, but Still Married to Cash","Mode Effects in American Trends Panel: A Closer Look at the Person-Level and Item-Level Characteristics","A Close Look at the Interview Length for Cell and Landline Telephone Surveys: The Case of the California Health Interview Survey","Analyzing Mode Effects by Using R-Indicators of Propensity Models","Comparison of Landline and Cell Phone Response Patterns from a Call-Back Telephone Survey: Behavioral Risk Factor Surveillance System (BRFSS) and Asthma Call-Back Survey (ACBS)","Does the Timing of the Mode Switch Matter in a Mixed-Mode Survey? Results from an Experiment","Survey Treatments and Response Modes: Bayesian Survival Analysis with Competing Risks","Effect of Data-Collection Mode on Response Rates and Data Quality in Voting Survey of Active Duty Military","How Can We Produce Estimates When We Can't Call You? Revisiting Methods to Adjust for the Phoneless Population","Got a Phone Number? Examining the Reliability and Accuracy of Phone Number Append Propensity Models for ABS Samples","ABS Coverage Evaluation: Recommendations for Evaluating the Household Coverage of Address-Based Sampling (ABS) Frames","Assessing the Impact of Using a Single-Frame Cell Phone Sample Design for the National Immunization Survey","Can We Hit the Mark? Using Commercial and Publicly Available Data to Target Specific Populations","An Evaluation of Ported Telephone Numbers in the 2013 California Health Interview Survey","2016 Sample Redesign of the National Health Interview Survey","Do Interviewers with High Cooperation Rates Behave Differently? Interviewer Cooperation Rates and Interview Behaviors","Decomposing Mobile Versus PC Web Mode Effects in a Probability Web Panel","The Effects of Nonresponse Error and Measurement Error on Estimates of Regression Coefficients","To Allow or Disallow Smartphone Participation in Web Surveys: Choosing Between the Potential for Coverage and Measurement Error","Effective Strategies for Collecting Interviewer Observations to Be Used for Nonresponse Adjustment","Imputation of Missing Data in the State Inpatient Databases","Sample Allocation to Increase the Expected Number of Publishable Cells in the Survey of Occupational Injuries and Illnesses","Investigating the Effect of Mode Assignment on the Response Rate in the Current Population Survey","Exploring Regional Effects on Establishment Nonresponse Using Hierarchical Linear Modeling","Are Proxy Responses Better Than Administrative Records?","Studying the Association of Environmental Measures Linked with Health Data:  A Case Study Using the Linked National Health Interview Survey and Modeled Ambient PM2.5 Data","Applying Pattern-Mixture Models for Estimation from Multiple Data Sources","Re-Contact Within the Justice System: Integrating Multiple Data Sources Through Record Linkage","Changes in Labor Market Behavior Due to Panel Conditioning in a German Panel Study","Treatment of Missing Data in the FBI's National Incident Based Reporting System: A Case Study in the Bakken Region","Alternative Methods for Sampling and Estimation in the National Immunization Survey (NIS): Utilizing Immunization Information Systems (IIS) Data","Small-Area Estimates of Crime Rates for States and Large Counties Based on the NCVS","Estimation of the Difference of Small-Area Parameters from Different Time Periods","An Evaluation of Different Small-Area Estimators and Benchmarking for the Annual Survey of Public Employment and Payroll","Analysis of Basic Area-Level Models: The Extensions of the Fay-Herriot Model","Evaluation of Small-Area Estimation Method Used in AskCHIS Neighborhood Edition","Combining Time Series and Cross-Sectional Data for the Current Employment Statistics Estimates","Spatial Bayesian Hierarchical Model for Small-Area Estimation of Categorical Data","Sequential Design for Computerized Adaptive Testing That Allows for Response Revision","Data-Collection Strategies for the Application of Conditional Dynamic Network Models","Constructing Cross-Sectional Weights for the German Panel on Household Finances","Estimating Taxes for the Redesigned CPS ASEC Sample","Optimal Adaptive Sequential Design with Application to Crowdsourcing","Text Analysis: Further Work on Computer-Assisted Techniques","Old SSS Proceedings Papers Online!","Information Criterion for Nonparametric Model-Assisted Survey Estimators","Variable Screening in Multicategory Classification","New Feature Screening Method for Time-Varying Coefficient Model with Ultrahigh-Dimensional Longitudinal Data","Simultaneous Inference for the Mean of Functional Time Series","Convergence Analysis of Kernel Canonical Correlation Analysis","Bayesian Small-Area Unit-Level Modeling: A Discussion of Viable Approaches","Inference About Small-Area Distributions Using Area-Level Tabulations","Small-Area Estimation for High-Dimensional Multivariate Spatio-Temporal Count Data","On Borrowing Information Over Time in Small-Area Estimation","Estimating the Prevalence of Psychiatric Disorders Among National Guard Service Members Using a Bayesian Post-Stratification Model ","Multilevel Regression and Post-Stratification for Survey Weighting","Bayesian Predictive Inference for Skewed Survey Data in Unequal Probability Sampling","Novel Application of Statistical Tools for Big Data Analyses of Solar Physics","Uplift Model vs. Propensity Model","The Torgegram for Fluvial Variography: Characterizing Spatial Dependence on Stream Networks","Bias Correction for CSP: Better Border Biosecurity Estimates","Providing Weight to Unit-Weighting: Generalizability of Unit-Weighted Factor Scores","Partially Missing at Random and Ignorable Inferences for Parameter Subsets with Missing Data","Differences in Student Debt Among Demographic Groups in Those Recently Graduating with a Bachelors Degree 2011--2012","Generalization of Conditional Logit Choice Model Using Gaussian Copula","Detecting Fraud in a Survey Sample Recruited Online","Bivariate Spatial Analysis of Temperature and Precipitation from General Circulation Models and Observations","Evaluating the Practice of Assuming Parallelism in Relative Potency Determination with Four-Parameter Logistic Regression","Graphical Ruggedness Testing Using an Unreplicated 3-Cubed Factorial Experiment","An Alternative Modified Hypergeometric Distribution Probability Model Useful in Industrial Quality Control ","Using IRT Models to Estimate and Visualize Spatial Clusters","Current Methods of Weight Trimming in Sample Surveys","Evaluation of Model Fit Indices and Structural Coefficient Bias with Bifactor Model Misspecification","Latent Variable Models for Nonresponse Adjustment and Calibration","Results of Calibration Research for the 2015 American Housing Survey","Improving Precision by Calculating Estimates During the Calibration Process","Calibration for the Census of Agriculture","Combining Nonresponse and Calibration Adjustments in Weighting","Estimation of Finite Population Mean and Total Using Conditional Inclusion Probabilities Given the Population Ranks","Approaches for Missing Data in Ordinal Multinomial Models","A Different Approach to the Problem of Missing Data","The Midpoint Mixed Model with a Missingness Mechanism: A Likelihood-Based Framework for Relative Quantification Mass Spectrometry Experiments","Estimation in Closed Capture-Recapture Models with Missing Covariate Data","Using the Whole Cohort in the Analysis of Countermatched Samples","Recovering Marginal Treatment Effects from a Transition Model for Longitudinal Data with Drop Out Using Path Analysis","Effect of Compliance on Analysis of Longitudinal Randomized Clinical Trials","Multiple Imputation Using the Weighted Finite Population Bayesian Bootstrap","Two-phase sampling approach to fractional hot deck imputation","Calibration Weighting for Nonresponse that is Not Missing at Random: Allowing More Calibration than Response-Model Variables","Calibration in Missing Data Analysis","Calibrated Propensity Score Method for Survey Nonresponse in Cluster Sampling","Novel Application of Statistical Tools for Big Data Analyses of Solar Physics","Net Lift Modeling vs. Propensity Modeling for Skewed Data","WITHDRAWN: A Genetic Association Study of Osteopontin and Metabolic Syndrome Using Structural Equations Modeling","Big Data for the Social Sciences","Massive and Missing Sampling Frames","Sketches for Stratified Sampling","A Critical Threshold for Design Effects in Respondent-Driven Sampling","Branching Process Tools for Exploring Respondent-Driven Sampling","Generalizing the Network Scale-Up Method: A New Estimator for the Size of Hidden Populations","Nonresponse Analysis for School Surveys","Assessing Nonresponse Bias and the Efficacy of Weighting Class Adjustments in a Survey of Adult Educational Attainment with a Nonresponse Follow-Up Sample","Nonresponse Bias Study Approaches for Equal Opportunity Survey","Longitudinal Patterns of Nonresponse Bias in the Current Population Survey ","Reducing Unit Nonresponse in Controlled Access Situations: An Experimental Study in South Korea","A Method to Assess Nonresponse Bias During Fieldwork","Response Rates Using Mass Mailing Tools in the National Children's Study","Improving Inferences from RDS Data by Incorporating Geo-Reference Information","Spatial-Temporal Multivariate Sampling Design for the June Area Survey","Geo-Sampling: Refining the Grid-Based Sampling Method Using Geographic Information Systems Layers and Spatial Data","Using Census Public Use Microdata Areas (PUMAs) as Primary Sampling Units in Area Probability Household Surveys","Enhancing the June Agricultural Survey Pre-Screening Through the Use of County Assessor's Information","Review of the 2010 Sample Redesign of the Consumer Expenditure Survey","Calibration and Model-Robustness: How Close Can We Get to Full Efficiency?","Range-Restricted Calibration Weights and Related Inference Problems","Weighted Estimating Equations Based on Response Propensities in Terms of Covariates That Are Observed Only for Responders","Improving Efficiency Under Two-Phase Sampling","Longitudinal Functional Additive Model with Continuous Proportional Outcomes","Regression Tree Analysis of Survey Data","Using Classification and Regression Trees to Model Survey Nonresponse","A Linear Representation of Regression Trees with Applications to Survey Data","Monitoring Response Data and Respondent Representativeness to Develop Adaptive Survey Design Interventions","Implementing Static Adaptive Design in the National Survey of College Graduates Using the Results of an Incentive Timing Experiment","Correcting for Preferential Recruitment in Respondent-Driven Sampling","Evaluating Variance Estimators for Respondent-Driven Sampling","Monitoring Field Procedures to Develop Adaptive Survey Design Interventions","Examining the Predictive Power of Response Propensity Models in Varied Survey Designs","Robust Bayesian Dose-Finding Design for Phase I/II Clinical Trials","Association of Dosimetric Parameters with Toxicities in Breast Brachytherapy Treatment with Multi-Lumen Balloon","Using Simulation to Compare Performance of Various Prognostic Propensity Scores, Propensity Scores, and Inverse Probability Treatment Weighting (IPTW) Using Propensity Scores","Challenges and Considerations on Sample Size Estimation in Preclinical Discovery Research: Replace, Reduce, Refine","A Dynamic Alpha Spending (DAS) Function with Informative B-Value for a Stratified Study Design","Evaluation of Multiplicity Control Strategies for a New Study with Multiple Endpoints and Two Doses","A Novel Tipping Point Approach ","Exploratory Data Analysis of Economic Census Products: Methods and Results","Implementation of Ratio Imputation and Sequential Regression Multivariate Imputation on Economic Census Products","Implementation of Hot Deck Imputation on Economic Census Products","Evaluation of Alternative Imputation Methods for Economic Census Products: The Cook-Off","Decomposing the Interviewer Variance Introduced by Standardized and Conversational Interviewing ","Assessing Measurement Error in an Energy Use Survey of Manufacturing Businesses","Quality and Measurement Error Assessment of Juvenile Interviews in the NCVS","Repeating After You: Dependent Interviewing in Establishment Surveys","Improving Editing Efficiency: How a Comprehensive Program Interface Reduces the Time Cost of the Comment Review Process","Using Mixture Models for Heaped Data with Rounded Responses and True Spikes","Characterizing Discrepancies in Reported Acreage Between the Census of Agriculture and June Agricultural Survey","Examples of Singletons for Which Variance Software Fix-Ups Are Not Adequate ","Use of Smartphones as a New Survey Mode: A Feasibility Study","Better Mean Estimation After Post-Stratification","Targeted Sampling, Mixed Mode, Incentives, and Paying for Completion: What Works for Reaching Hard-to-Survey Low-Income Households with Civil Legal Needs?","Adjusting for Effects of Survey Mode Difference Across a Longitudinal Mixed-Mode Study","State Sampling Allocation Strategies for the 2016 Redesigned National Health Interview Survey (NHIS)","Semiparametric Estimation for Generalized Linear Models with Missing Covariates","A Dynamic Systems Approach to Patterns of Affect and Cognitive Difficulty in Interviewer-Respondent Interactions","Health Care Access for Adults with Disabilities","Using Calibration Training to Assess the Quality of Interviewer Performance","Box-Cox Transformed Linear Mixed Models for Small-Area Estimation","The Accuracy of a National Generalized Variance Function for Subnational Estimation","Developing Generalized Variance Functions for Estimates of Recidivism Rates","Income Interpolation from Categories Using a Percentile-Constrained Inverse-CDF Approach","Factors Associated with Change in Retrospective Reports of Life Events ","Weighting Approach for a Statewide Dual-Frame RDD Survey","A Data Management Model for Multinational Surveys Toward Meta-Analysis","A Practical Balancing for a Random Sample from a Finite Population by Systematic Selection","On the Use of Recursive Residuals for Testing the Goodness of Fit of Small Area Estimation Models","Record Linkage: Introductory Overview","Bayesian Estimation Under Informative Sampling","Bayesian Modeling and Imputation for Missing Mixed Ordinal-Categorical Data in Large-Scale Surveys","A Bayesian Semiparametric Area-Level Model for Small-Area Estimation","Bipartite Matching Estimation for Record Linkage","Bayesian Simultaneous Edit and Imputation for Categorical Microdata","A Composite Likelihood Approach in Testing for Hardy Weinberg Equilibrium Using Family-Based Genetic Survey Data","Bayesian Hierarchical Models for Smoothing in Two-Phase Studies, with Application to Small-Area Estimation","Analysis of Biased Sampling in Longitudinal Data","Using the Additive Hazards Model with Two-Phase Sampling in Atherosclerosis Risk in Community Study","Who Are the Non-Voters?","Census Tract-Level Disparities: Examining Food Swamps and Food Deserts","Exploring the Modifiable Areal Unit Problem","Determinants of Poverty in U.S.","An Assessment of Developmental Trajectory of Baby Boomers in the United States: A Latent Growth Curve Modeling Application","Optimal and Coherent Data Visualization in R for the Empirical Study of CPI-U Standard Errors","Results from a CATI Follow-Up of Respondents from a Face-to-Face 2013 National Survey of Egypt","Sensitivity Analysis of Bias of Estimates from Web Surveys with Nonrandomized Panel Selection","Travel Price Indexes: Joys and Headaches of Online Collection","The Effect of CE Sample Sizes on CPI Standard Errors","Challenges and Rewards of Editing Complex Survey Data from the National Ambulatory Medical Care Survey","Gravimetric Anomaly Detection Using Compressed Sensing","Application of Industry-Specific Sample Strata in PPI Variance Estimation","On the Range of Self-Normalized Cramer-Type Moderate Deviations","Field-Testing the Collection of New Data Elements in the Occupational Employment Statistics Survey","Decomposing Wage Inequality Using OES Data","Trend Estimation of Multivariate Time Series with Controlled Smoothness","Insurgency Prediction Using Multiple High-Volume Social Media Data Sources","Accessing and Exploring NCES Data Through Online Training Modules and Data Tools ","Interactive X-13ARIMA-SEATS Seasonal Adjustment Using R","2014 Census Test Results on Alternative Methods to Optimize Self-Response for the 2020 Census","Asking About Prescription Drugs: Order and Encouragement Experiments","The Income Gap in Survey Research: Nonresponse to Income Questions in Online Panel Research","Purposefully Mobile: Experimentally Assessing Device Effects in an Online Survey","The Matrix Lives On: Improving Grids for Online Surveys ","Understanding School-Level Nonresponse and Developing Strategies to Maximize Participation in School-Based Substance Use Surveys ","A Compositional Approach to Survey Inference","A Model-Based Approach for Achieving a Representative Sample","Variance Estimation for Surveys from Internet Panels","Matching Nonprobability Internet Panel Samples with Probability Samples","Occupational Requirements Survey Sample Design","Occupational Requirements Survey (ORS) Data Review Process","Striking the Balance Between Respondent Protection and Ease of Use: Findings from Testing of the Consumer Expenditure Survey's Web Diary Design","Using Alternative Mailing Strategies to Boost Internet Response in an Establishment Survey","Impact of Internet-only Reporting on Response in the Business R&amp;D and Innovation Survey","Sampling Design for the Primary Farm Household Survey of the Taiwanese Agriculture Study","Results and Data Analysis for the 2013 Fishing, Hunting, and Wildlife--Associated Recreation Survey's Pre-Screener Test","Survey Weighting Adjustments and the Design Effect: A Case Study","Assessing the Utility of Interviewer Observations for Nonresponse Adjustments in the National Health Interview Survey","Geo-Sampling Weights and Design Effect","An Alternative Raking Approach to Reduce Design Effects","Design, Sampling Weights, Reweighting for Unit Nonresponse, and Monitoring of the Texas Adolescent Tobacco and Marketing Surveillance (TATAMS) Study","Efficiency of Standard Regression Model-Based Ratio-Synthetic Estimators in Sample Surveys Combining Time Series and Cross-Sectional Data","Fractional Imputation with Missing Data Analysis","SRMI Multiple Imputation in the CPS ASEC","Exact Balanced Random Imputation","Tailoring Outreach to Boost Mail Self-Response in Geographic Areas with Similar Low Response Scores ","Exploring the Census Bureau's 2014 Planning Database Using Topological Data Analysis","Informing Natural Disaster Response with Census Data","Optimizing Survey Cost-Error Tradeoffs: A Multiple Imputation Strategy Using the Census Planning Database","Multiple Imputation for Data That Are Missing Not at Random: Extending the Fully Conditional Specification Procedure","Addressing Item Nonresponse in a Complex Survey Using Full Information Maximum Likelihood Methods","Bayesian Multiple Imputation of Count Data with Zero Inflation","Methods to Impute Household Income in the National Crime Victimization Survey","A Fresh Imputing Survey Methodology Using Sensible Constraints on Study and Auxiliary Variables","Latent Class Analysis with Planned Missingness: Best Approach?","Restricted Latent Class Multiple Imputation Method of Categorical Missing Data","Size-Based Probability Sampling with Constraints on Costs","Optimal Sampling Fractions for Two-Phase Sampling for Nonresponse in the Real World","A Simple and General Algorithm for Exact Optimal Sample Allocation That Is More Efficient Than Neyman Allocation","A Re-Evaluation of the Statistical Learning Approach to Optimal Sample Allocation","Constructing Strata of PSUs for the Residential Energy Consumption Survey","Empirical and Constrained Empirical Bayes Variance Estimation Under a One-Unit-Per-Stratum Sample Design","Survey Data, Big Data, State Space Models, and Official Statistics","Crop Acreage Prediction Combining Several Sources of Information ","Integrating Survey Data with Auxiliary Sources of Information to Estimate Crop Yields","Robust Small-Area Estimation Under Semiparametric Mixed Models","Improving Small-Area Estimates of Disability: A Model-Based Approach to Combining the American Community Survey with the Survey of Income and Program Participation","Prediction Intervals of Small-Area Means Under Semiparametric Measurement Error Models","Robust Bayesian Small-Area Estimation for Area-Level Data","Mixture Model and EM Algorithm in Small-Area Estimation","Synthetic Data Generation for Firm Links","Assessing the Data Quality of Public Use Tabulations Produced from Synthetic Data: Synthetic Business Dynamics Statistics","Editing, Imputation, and Synthesis: A Public Use File for the Census of Manufactures","Differential Privacy and Verification of Results","Forecasting Survey Panel Turnover Using Discrete Time Survival Analysis","The Implications of Questionnaire Redesign on Trend Estimates in the 2011 Police Public Contact Survey","Survival Modeling of Cumulative MLB Season Audience ","Modeling Incomplete Longitudinal Bounded Outcomes: An Application Study","Improvements in the Medicare Current Beneficiary Survey Sample Design","Using Extant Lists to Improve CAPI Instruments and Expedite Record Linkage ","Enhancements to the Collection of Data on Race, Ethnicity, Primary Language, and Other Characteristics to Improve Research on Health Disparities","Development of a MCBS Public Use File: Ensuring Ease of Use and Access for Researchers","Improving Cross-National/Cultural Comparability Using the Total Survey Error Paradigm","New Ideas in Sampling for Surveys in the Developing World","Innovations in Data Collection in Resource-Poor Settings","Case Studies on Monitoring Interviewer Behavior in Cross-National and International Surveys","AL-PUF: A Natural Generalization of K-Anonymization and Micro-Aggregation to Improve Analytic Utility","A Forced Odds Ratio (To Be Equal to One) Leads to a New Estimator for Randomized Response Sampling","On Estimating at Least Seven Measures Using Randomized Response Sampling: Cramer-Rao Lower Bounds of Variances","On Security Properties of Random Matrix Masking","The Christofides' Randomized Response Technique for Multiple Sensitive Attributes","A Two-Stage Sampling Model for the Estimation of Population Proportion and Cheating with Randomized Response and Direct Questioning","Encouraging Early Participation in a Lengthy Survey That Collects Sensitive Personal Data: Do Large Monetary Incentives Make a Difference?","Using Local Knowledge During Data Collection: Does It Make a Difference Who Applies It and When?","The Reliability of American Community Survey Five-Year Estimates of Race Groups and American Indian and Alaska Native Populations","Tailored Assignment of Internet and Mail Self Response Modes Using a Model-Based Stratification","Investigating Methods to Support Subannual Estimates in the American Community Survey","Preliminary Investigation of Variance Issues Related to Generalized Regression Estimation Used for American Community Survey Five-Year Estimates","Multilevel Regression and Post-Stratification for Small-Area Estimation of Population Health Outcomes Using BRFSS: An Evaluation of Cross-Level Inference","NaN","NaN","Dual Frame RDD Survey Costs: Landline vs. Cell Phone Comparisons","Survey Costs: The Missing Half of the 'Cost-Error' Tradeoff","Can Microsimulation Help Us Understand (and Control) Survey Costs?","Results of Recent Improvements in the Sampling Design for the MCBS","Design and Analytic Considerations for the MEPS Longitudinal Insurance Component","A Demonstration of How to Combine Access to Care and Cost and Use Medicare Current Beneficiary Survey Files for Analysis","Utilizing Linked Administrative and Survey Data for Survey Design and Assessment of Measurement Error: The Medicare Current Beneficiary Survey","Changes in Health Care Use Reporting in the 2nd vs. 1st year: MEPS Household Component Overlapping Panel Design","Estimating Design Effects in Small Areas and Domains through Aggregation","Bayesian Rank Estimation Between Radio Stations for Small Demographic Groups","Using a Power Prior to Improve County-Level Diabetes Incidence Estimation","Standard Regression Model-Based Ratio-Synthetic Estimators Assuming Unequal or Equal Unit Error Variances and Their Use in Survey Practice ","Nonparametric Bayes Modeling with Sample Survey Weights","The Use of Sampling Weights in Bayesian Hierarchical Models for Small-Area Estimation","Survey Integration and Estimation of Joint Distributions with Conditionally Representative Data Sources","Scalable Bayes Under Informative Sampling","A Bayesian Approach for Regression Analysis of Complex Survey Data with Missing Values","Removing Survey Questions Through Aggregation: Assessing the Balance Between Specificity, Accuracy, and Burden Through Cognitive Interviewing","Testing New Interview Protocols: Lessons Learned About Interviewers, Respondents, and Survey Content","Demonstrating Feasibility: Results from the Consumer Expenditure Survey Redesign Proof of Concept Test","Survey Redesign Recommendations from a Survey of Data Collection Field Staff","Calibration on Partly Known Counts in Frequency Tables with Application to Real Data","Calibration Estimators Including Fractional Exponents of Auxiliary Variables","Applying Post-Stratification Raking Adjustments to Survey Weights Using the High School Longitudinal Survey of 2009 (HSLS:09)","Higher-Order Calibrated Estimators in Two-Stage Sampling","Asymptotics of MDIA Estimates Under Complex Sampling Designs","The Survey of Juveniles Charged in Adult Criminal Court (SJCACC): A Census, Sample, Model-Based, and Model-Assisted Estimation Hybrid","Inference with Cluster Data Under Informative Sampling ","Bayesian Analysis for Cluster Sampling","H-Likelihood Method for Analyzing Clustered Survey Data","Estimating the Variance Due to Hot Deck Imputation for Product Value Estimates in the 2017 Economic Census","Variance Estimation for Product Value Estimates in the 2017 Economic Census Under the Assumption of Complete Response","Using Auxiliary Marginal Information to Deal with Nonignorable Missing Data","Semiparametric Fractional Imputation Using Empirical Likelihood in Survey Sampling","Using Propensity Scores to Inform Respondent Incentive Escalation","Elusive Respondents: Target Interventions for Challenging Geographic 'Hot-Spots'","Recalcitrant Respondents, Data Quality Measures, and Mitigation Strategies","Geographic Distribution and Characteristics of Locked Buildings and Gated Communities: Gaining Access","Within Household Gatekeepers: Overcoming Obstacles to Survey Participation","Estimated Prevalence and Characteristics of Web Users: National Health Interview Survey, 2014-2015","Estimating Mail or Web Survey Eligibility for Undeliverable Addresses: A Latent Class Analysis Approach","Evaluating the Effects of Adding Cell Phone Samples to the Traditional Landline Phone Samples on Prevalence Estimates from a Telephone Call-Back Survey ","Mail Versus Telephone Respondents in a Survey of Minority Populations","What Paradata Can Tell Us About Online Data Reporting by Juvenile Residential Facilities","What Leads to Errors in Surveys? Evidence from Multiple Government Programs","A Method of Correcting for Misreporting Applied to the Food Stamp Program","Measuring Levels and Trends in Earnings Inequality with Nonresponse, Imputations, and Topcoding","Incomes of the Population 65+: A New Look with Linked Survey-Administrative Data","The Wealth of Wealthholders","Using the 2015 Census Test Evaluation Follow-Up to Compare Nonresponse Follow-Up with Administrative Records","Comparing National Immunization Survey (NIS) and Immunization Information Systems (IIS) Vaccine Coverage Estimates","Bayesian Decision Theory for Further Optimizing the Use of Administrative Records in the Census NRFU","Using 2010 Census Coverage Measurement Results to Better Understand Possible Administrative Records Incorporation in the Decennial Census","An Association Study Including Measurement Errors Using Linked NHIS and EPA Modeled Data ","Correcting Biases in Auxiliary Data to Produce Better Estimates","Convergence and Stability Properties of Variance-Function Estimators Used in the Integration of Surveys and Alternative Data Sources","Bayesian Model-Based Approaches for Raking","Using Paradata to Inform the Collection of SSN from Survey Respondents","Rapid Response Survey of Blood Donors in Support of Public Health","Assessing the Impact of Respondent Fatigue in the National Crime Victimization Survey","Using Statistical Matching as a Supplement to Exact Record Linkage","Misspecified Sampling Weights in Weight-Smoothing Methods","Comparison of MCMC and ADM Methods for Hierarchical Bayesian Estimates in Small-Area Estimation","Seizing Upper Extremity Function: How Does PROMIS Do?","An Integrated Approach to Providing Access to Confidential Social Science Data","The Challenge of Reproducible Science and Privacy Protection for Statistical Agencies","Spatio-Temporal Change of Support with Application to American Community Survey Multi-Year Period Estimates","Benchmarking and Assessment for Multiple Imputation","Interactions and Squares: Don't Transform, Just Impute!","Optimal Split Questionnaire Survey Design in the Longitudinal Setting","Optimal Missing-by-Design Patterns with Genetic Algorithms","Survey Estimators of Ordered Domain Means","Structural Equation Mixed Models with an Application to Small-Area Estimation","Model-Based County-Level Crop Estimates Incorporating Auxiliary Sources of Information","A Design-Based Approach to Small-Area Estimation Using Semiparametric Generalized Linear Mixed Model","Parametric Bootstrap Mean Square Error Estimates for Different Small Areas in the\u00a0Annual Survey of Public Employment and Payroll","Small-Area Estimation Methods for County-Level Vaccination Coverage Rates Using the NIS","Multilevel Small-Area Estimation of Health Behaviors: An Extension of Multilevel Regression and Poststratification (MRP) Approach via Bootstrapping","A Method of Statistical Disclosure Limitation for Design and Strata-Defining Variables in Surveys","Statistical Calibration of Engineering-Based End-Use Electricity Consumption Estimates: A Bayesian Multilevel Model Approach","A Comparative Analysis of Approaches for Quantifying the Weight of Evidence in Forensic Science","The Development of a Variance Estimation Methodology for Large-Scale Dissemination of Quality Indicators for the 2016 Canadian Census Long Form Sample","Estimated Covariance Matrices Associated with Calibration","Defining a Strategy for National Weighting of Data from the Behavioral Risk Factor Surveillance System (BRFSS)","Determining the Appropriate Sample Weights for Diabetes Estimates","Alternative Goals for Adaptive Survey Design","The Operational Design of the 2020 Census: Overview of the Current Status","An Overview of the Master Address File Coverage Study ","Optimizing Self-Response for the 2020 Census","Using Administrative Records to Identify Occupied and Vacant Units","Field Re-Engineering for the 2020 Census","Testing for No Effect in Nonparametric Regression with Survey Data","A Propensity Score Approach in a Study of WIC Families","Approaches for Improved Power with Generalized Estimating Equations in Small-Sample Longitudinal Study Settings","Model-Assisted Estimation Using Time-to-Event Models","Constructing Generalized Variance Functions with Linear Regression Trees","Empirical likelihood inference for regression parameters when modeling hierarchical complex survey data","Confidentiality Approaches for Real-Time Systems Generating Aggregated Results","Using Regression Trees to Model Characteristics of Nonresponse and Measurement Error in a Longitudinal Survey ","Modeling Survey Data with Regression Trees","Using Machine Learning to Correct for Survey Nonresponse Bias","Matchmaker, Data Scientist, or Both? Using Unsupervised Learning Methods for Matching Nonprobability Samples to Probability Sample","A Case Study in Machine Learning Approaches to Survey Nonresponse Adjustments","Inferences from Internet Panel Studies and Comparisons with Probability Samples","An Empirical Method to Establish Usability of Nonprobability Surveys for Inference","The Efficacy of Nonprobability Online Samples","Design of Sample Surveys That Complement Observational Data to Achieve Population Coverage","Ratio of Vector Lengths as an Indicator of Sample Representativeness","Exploration of Methods for Blending Unconventional Samples with Traditional Probability Samples","Machine Learning Applications for Survey Design, Collection, and Adjustment: Going Beyond the Trees to See Clusters, Forests, and Neighbors","Practical Issues Related to Model-Based Small Area Estimation","Recent Developments in Survey Design for Rare and Hard-to-Survey Populations","Recent Developments in Fractional Imputation","Responsive Design: Side Effect Reduction of Prior Information on Survey Design","Adapting Responsive Design Strategies in a Recurring Adult Literacy Assessment","Toward an Adaptive Design for the Survey of Doctorate Recipients","Implementing Adaptive Design on a Longitudinal Survey of At-Risk Youth:Empirical Evidence Based on a Deep-Dive Analysis","Selecting a Sample from a Changing Frame","New Perspectives on Sampling Rare Populations","Sample Design for Longitudinal Multiphase Samples with Misclassification","Imputation Process Used in the Occupational Requirements Survey","An Evaluation of Backwards Imputation for the Annual Survey of Public Employment &amp; Payroll","A Prediction Approach to Missing Data from the Exponential Family ","Multiple Imputation Methods to Enhance the NHANES-CMS Medicaid Linked Data - Demonstrated by Examining Cotinine as a Biomarker for Second-Hand Smoke Among Children Ages 3--17","Effects of number of imputations on fraction of missing information in multiple imputation","Do Imputed Earnings Earn Their Keep? Evaluating SIPP Earnings and Nonresponse with Administrative Records","Introducing a New Calibration Procedure for the Census of Agriculture ","Estimating Interviewer Effects in the Absence of Interpenetration","Causal Inference with Unequal Sampling Weights: Investigating Policy Effect Using Population Health Surveys","Methods for Allocating and Targeting Small Areas in a Large Dual-Frame Telephone Survey","General and Specific Utility Measures for Synthetic Data","Analyzing Heterogeneous Causal Mediation Effects in Multi-Site Trials With Application to the National Job Corps Study","Approximate Median Regression for Complex Survey Data","Response Propensity and Motivated Under-Reporting: Do Persons Likely to Respond Give Better Answers to Filter and Eligibility Questions?","Modeling Preferential Recruitment for Respondent-Driven Sampling","Pilot Surveys of Shore Fishing on Oahu, Hawaii","How Can a Clothing Price Index Be Enhanced? Statistics Canada's Recent Experience","Accuracy in Effect Size Estimation for IID Observations","Sampling with Minimal Strata Sample Size Requirements","Predicting and Preventing Break-Offs in Web Surveys","Assessing the Reliability of Conversational Interviewing","Using official surveys to reduce bias of estimates from nonrandom samples collected by web surveys","Calibration of Design Weights Using a Power Transformation","Calibration Weighting for Nonresponse with Proxy Frame Variables (So that Unit Nonresponse CanBe Missing Not at Random) ","Alternate Methods for Constructing BRR Weights with National Health and Nutrition Examination Survey (NHANES) Single-Year Samples","Generating Correlated Synthetic Binary Indicators of Radio Listening Behavior for Long-Term Projections","Access and Explore NCES Survey and Administrative Data Through Self-Guided Online Training Modules","A Composite Likelihood Approach in Testing for Hardy Weinberg Equilibrium Using Family-Based Genetic Survey Data","Extension of the Peters-Belson Method to Estimate Health Disparities Among Multiple Groups Using Logistic Regression with Survey Data ","Regression Models and Tests for Recruitment Dynamics in Respondent-Driven Sampling","A Comparison Between Standard Regression and Multilevel Modeling Techniques to Analyze Complex Survey Data Based on the Monte Carlo Simulation Study","Ranking Question Design and Data Analysis","Are Utility Subsidies Accurately Predicting Assisted Households' Actual Utility Expenditures? A Methodological Examination","Statistical Inference Based on Judgment Post-Stratifed Samples in Finite Population","A Bayesian Hierarchical Model for Combining Several Crop Yield Indications","Producing Labor Statistics by Detailed Geography and Occupation: Experiences from the New Canadian Job Vacancy and Wage Survey","Inference from Complex Survey-Embedded Field Experiments","Model-Based Evaluation of Local Alcohol Prevention Programs for Under-Age Drinking ","Alternative Variance Component Analyses for a Three-Stage Sample Design","Using Feature Vectors to Cluster Social Networks","Modeling the Effects of Network Attributes on Subgroup Integration","Longitudinal Latent Space Network Model with VAR Evolution ","Causal Mediation Analysis of Social Networks","Spatiotemporal Modeling with Applications to Stroke Mortality and Data Privacy","Predicting Coverage Error on the Master Address File Using Spatial Modeling Methods at the Block Level","Particle Swarm Optimization--Assisted Metropolis Hastings Algorithms","Small-Area Estimation for High-Dimensional Non-Gaussian Dependent Data","A Multivariate Spatio-Temporal Model for U.S. Migration ","Controlling Identification Disclosure Risk in Microdata Release Through Unbiased Post- Randomization","Data Synthesis and Perturbation for the American Community Survey at the U.S. Census Bureau","Estimating Regression Parameters from a Sensitive Variable with Noise Multiplication","Challenges Facing the Disclosure Review Board at Census","A Bayesian Approach to Smoothed Lexis Diagram with Applications to Breast Cancer","The Selection of the Constraint for Smoothing Cohort Model","Parameter Constraints and Impact on Parameter Estimation in ANOVA and Age-Period-Cohort Models","Resolving the Identifiability Problem with the Lasso Regularization Method in Age-Period-Cohort Analysis","Bias Correction in Modeling Confounded Age, Period, and Cohort Effects","Linking Federal Administrative Data to General Population Survey Samples","Quality and Analysis of Sets of Files","Modeling Similar Nonmatches in Record Linkage with Mixture Models ","Linked Designs of the MEPS Medical Provider and Organization Surveys","Multiple Imputation for Survey Integration Under Informative Sampling","Evaluating Record Linkage Software for Agricultural Surveys","How Can a Clothing Price Index Be Enhanced? Statistics Canada's Recent Experience","Subsample Weights Studies - Alternate Methods for Constructing BRR Weights for NHANES Single Year Samples","Field Observation of Survey Data Collection: Experiences and Lessons Learned","Current Population Survey Sample Size Study","An Iterative Composite Estimator in the Current Population Survey ","An Overview of Current Population Survey Variance Methodology ","Calculating Generalized Variance Functions with a Single-Series Model in the Current Population Survey","Update on Current Population Survey Research, Discussion","Measurement of Sexual Orientation and Gender Identity in the Federal Statistical System","Improving Measurement of Same-Sex Couple Households for Census 2020","Results from Cognitive Interviews to Assess Items to Measure Gender and Sexual Identity Among English- and Spanish-Speaking Older Adults in Surveys","Testing of Sexual Orientation and Gender Identity Questions for the National Crime Victimization Survey","Identifying the Transgender Population in the Medicare Program","Urban Analytics: A Case Study in Philadelphia","Racial and Ethnic Disparities in Health Care: An Examination of State Inpatient Databases in the Utilization of and Outcomes Following Total Knee Arthroplasty ","Applications of Statistical Techniques to Smoking Cessation Studies Using National Survey Data","A Randomized Response Model for Continuous Data","A Revisit to Two-Deck Randomized Response Model","Simultaneous Estimation of Means of Two Sensitive Quantitative Variables","Effects of Nonresponse Bias Adjustments on Survey Estimates of Health Conditions and Behaviors in a Community Sample in Chicago","Collecting Proxy Measures of Key Survey Variables to Estimate, Reduce, and Adjust for Nonresponse Bias ","Do Secondary Data and Multiple Attempts of Survey Data Collection Reduce Nonresponse Bias?","Are Initial Respondents Different from the Nonresponse Follow-Up Cases? A Study of Probability-Based Web Panelists","Randomly Split Zones for Samples of Size One as Reserve Replicates and Random Replacements for Nonrespondents","The Relationship Between Nonresponse and Revisions in the CES: Anticipating the Size and Direction of Revisions Using Firm Characteristics and Their Employment Reporting History","A Calibrated Bayesian Method for Propensity Score Estimation","Weighting and Variance Estimation Under Adaptive and Responsive Survey Designs","Cost-Benefit Analysis of a Responsive Sampling Strategy in MEPS","Experimenting with Contact Strategies to Aid Adaptive Design in Business Surveys","Survey of Income and Program Participation Case Prioritization Experiment Results","Assigning Cases Effectively for the Current Population Survey","Identifying Risk Factors for Interstate Crashes Using Spatial Statistics","Modeling Transportation Characteristics: Small-Area Estimation Using the National Household Travel Survey","Geographical Predictors of GPS-Based Transportation Survey Response Rates","Direct and Indirect Effects of Alcohol Impairment and Safety Belt Use on Head Injuries and Their Associated Charges in Illinois","Interviewer Effects and the Measurement of Financial Literacy","Occupation Coding During the Interview","Medical Event Reporting in the 2005 to 2013 MEPS in Terms of Self and Proxy Reporting According to the Person's Relationship to the Respondent","Motivated Misreporting in Web Panels","Evaluating the Quality of Survey and Administrative Data with Generalized Multitrait-Multimethod Models","Stratified Sampling from Electronic Health Records: Approaches to Addressing Errors in Stratum Definitions","Total Survey Error in a National Survey of Influenza Vaccination with Recall Error","Simultaneous Model Selection and Inference in Small-Area and Other Complex Models","A Unified Monte-Carlo Jackknife for Small-Area Estimation After Model Selection","New Bootstrap Bias Corrections with Application to Estimation of Prediction Mean Square Error in Small-Area Estimation","A Weighted, Squared-Distance Function for Two Decks of Cards in Randomized Response Technique","A Comparison of Three New Randomized Response Models for Simultaneous Estimation of Three Sensitive Dependent Characteristics and Their Overlaps","Unrelated Question Model with Two Decks of Cards","Estimation of Odds Ratio and Attributable Risk Using Randomized Response Techniques","Comparison of Multiple Imputation Methods for Categorical Survey Items with High Missing Rates: Application to the Family Life, Activity, Sun, Health, and Eating Study","Nonparametric Imputation for Nonignorable Missing Data","Fixed Choice Design and Augmented Fixed Choice Design for Missing Data in Social Networks","A More Unified Statistical Approach to Nonresponse and Weighting Adjustments","An Investigation of Weighting Procedures for Unit-Nonresponse","Weighting for Nonresponse in the Overseas Citizen Population Survey: The Effects of Sample Size and Adjustment Method on Accuracy of Estimates","A New Approach for Collecting Data from Long-Term Nonrespondents in the Multiple Worksite Report","Data Fusion for Accurate State-Level Diabetes and Prediabetes Prevalence Estimation","Response Rates for the Pre-Production Test of the Occupational Requirements Survey","Nonresponse Bias Analysis for the U.S. Census Bureau's Quarterly Financial Report","A Case Study on the Use of Propensity Score Adjustments with Web Survey Data","Sending Pre-Notice Postcards to Increase Reponse and Decrease Cost for the Current Population Survey","Nonresponse Bias Analysis and Sample Selection Evaluation in the Survey of Business Owners","NaN","NaN","NaN","Methodological Challenges of Environmental Surveys Compared with Human or Establishment Surveys","Spatio-Temporal Balanced Sampling Design for Longitudinal Natural Resources Survey","Estimation of Marine Recreational Fishery Characteristics Using Survey and Logbook Data","Non-Probability Sampling: The Good, the Bad, and the Oh So Interesting","Using Online Panel Surveys to Estimate Population-Level Health Statistics","Redirected Inbound Call Sampling - an Example of Fit for Purpose Non-Probability Sample Design","Improving the External Validity of Nonprobability Samples","Data Editing: How Much Is Too Much?","Lifestyle Segments, Social Marketing, and Hard-To-Survey Populations: Understanding Participation in the 2015 Census Test","Supplementing Record Linkage with Statistical Matching for Non-Consent Bias Reduction","Statistical Analysis with Linked Data","Calculating Boundary Limits for the Hidiroglou-Bertholet Edit Used to Flag Adminstrative Receipts","Estimating Net Coverage of ABS Frames","Using Commercial Data to Enhance Survey Eligibility: The Amerispeak Experience","More Information Is Better! Where Do We Get it and How Do We Use It?","Modeling Nonresponse Bias Likelihood and Response Propensity: The Design and Implementation of Statistical Models to Identify Cases for Interventions During Data Collection","An assessment of the utility of a Bayesian framework to improve response propensity modelling","Optimization of Adaptive Survey Design from a Bayesian Perspective. Two Case Studies","Using Bayesian Methods to Rank Cases Based on Response Propensity During Data Collection","Test of Adaptive Survey Design from a Bayesian Perspective in a Longitudinal Survey","Model-Assisted Estimation with Regression Trees","Estimating the Number of Tax Returns Required to Be Filed: Nonparametric Methods to Address Respondent Rounding","Recursive Partition Split Selection Using a Permutation Test for Complex Sample Data","B-Spline Imputation Procedures for the Treatment of Item Nonresponse in Surveys","Bayesian Regression Using an Approximated Solution to the Penalized Least Squares Minimization Problem","Accuracy in Contact Information for Website Registrations","Identifying Out of Business Records on the NASS List Frame Using Boosted Regression Trees","Analyzing the Need for an Oversample of Low Income Beneficiaries in the Medicare Current Beneficiary Survey","Software Solutions to Sample Design Problems That Are Equivalent to Solving a Transshipment Problem","Analysis and Results of a Bonded Labor Prevalence Survey in Tamil Nadu, a State of India","Design and Implementation of a Rapid, Population-Based Survey for the Zika Emergency Response Effort in Puerto Rico","Exploring Sampling Techniques to Reduce Respondent Burden","Overview of SAMSI Program on Statistical, Mathematical and Computational Methods for Astronomy (ASTRO)","A Multi-Resolution 3D Map of the Intergalactic Medium via the Lyman-Alpha Forest","Testing Bayesian Galactic Mass Estimates Using Outputs from Hydrodynamical Simulations","Quantifying Discovery in Astro/Particle Physics: Frequentist and Bayesian Perspectives","Computer Model Calibration to Enable Disaggregation of Large Parameter Spaces, with Application to Mars Rover Data","The Association Between Copy Number Aberration, DNA Methylation, and Gene Expression","Rerandomization: a Flexible Framework for Experimental Design","IMs for IVs: An Inferential Model Approach to Instrumental Variable Regression","Detecting Differential Gene Expression by Single-Cell RNA Sequencing","Statistical Science and Policy at the EPA","Approximate Message Passing Algorithms for High-Dimensional Regression","Generalized Fiducial Inference for High-Dimensional Data","The Combination of Confirmatory and Contradictory Statistical Evidence at Low Resolution","Approximate Confidence Distribution Computing: An Effective Likelihood-Free Method with Statistical Guarantees","R Package TDA for Statistical Inference on Topological Data Analysis","Teaching a Large, Project-Based Statistical Consulting Class","Transforming Undergraduate Statistics Education Through Experiential Learning: It's Essential!","The Geometry of Synchronization Problems and Learning Group Actions","Sufficient Markov Decision Processes with Alternating Deep Neural Networks","Optimal Dynamic Treatment Regimes Using Decision Lists","Predicting Phenotypes from Microarrays Using Amplified, Initially Marginal, Eigenvector Regression","Computer Vision Meets Television","Generalized Fiducial Inference for Nonparametric Function Estimation","A Phylogenetic Transform Enhances Analysis of Compositional Microbiota Data","Bayesian Multispecies Ecological Models for Paleoclimate Reconstruction Using Inverse Prediction ","Fast Maximum Likelihood Inference for Spatial Generalized Linear Mixed Models","Fair Prediction with Disparate Impact: a Study of Bias in Recidivism Prediction Instruments","I Ran a Nonresponse Follow-Up Survey; Now What Do I Do? ","Recent Advancements and Remaining Challenges of Adaptive Design in Sample Surveys and Censuses","Multiply Robust Nonparametric Multiple Imputation for the Treatment of Missing Data","Hot Deck Imputation of Multinomial Distributions When There Are Fewer Donors Than Recipients","Semiparametric Estimation of Longitudinal Data with Nonignorable Attrition Using Refreshment Samples","CLASSIFICATION and REGRESSION TREES and FORESTS for INCOMPLETE DATA from SAMPLE SURVEYS","Sales and Income Tax Data Collected Comparability: Quarterly vs. Annual Surveys","Implementation of a Youth Health Survey via the Internet","Moving Establishment Surveys from Mail to Web","Assessing the Impact of Adopting a Mixed-Mode Design: Examining Respondent Skip Logic Errors in the 2015 Residential Energy Consumption Survey","Weighting Mixed Mode Data for the 2015 Residential Energy Consumption Survey (RECS)","Adjusting for Mode Effects in Longitudinal Studies Utilizing a Bayesian Prior on Within Subject Correlation","Comparing Data Collection Methods for the June Area Survey","Model-Based Standardization to Adjust for Unmeasured Cluster-Level Confounders with Complex Survey Data","Modeling Count Data from a Complex Survey using Pseudo Maximum Likelihood ","Analysis of Multilevel Complex Survey Data: Comparison of Procedures from Several Software","Poisson Cokriging as a Generalized Linear Mixed Model, Applications in Public Health ","Empirical Estimation of Survey Attrition Phases","Predicting Sample Attrition in a National Study of Medicare Beneficiaries","Refining an External-Factor Model of Government Survey Refusal Rates","Adaptive Sampling Using Neyman Allocation","Developing a Web-Based Tool to Evaluate Survey Performance Using R Shiny","An Embedded Experiment for Targeted Nonresponse Follow-Up in Establishment Surveys","Adaptive Design by Benchmarked Sequential Sampling and Benchmarked Multiple Imputation","Operational Aspects of Sampling Foreign-Born Ethnic Minorities Using Respondent Driven Sampling","Benchmark Assessment of Respondent Driven Sampling Data for Foreign Born Korean Americans","The Use of Imputation and Commercial Data to Improve the Efficiency of Income Stratified Sampling of Households with Young Children","Sampling Late Baby Boomers: Increasing Cluster- and Household-Level Eligibility Rates with External Data","Why Independent Surveys with the Same Objective Yield Different Estimates","Integrating Early Stage Scoping Techniques into Traditional Pretesting Methods: Inside the Development of a Survey on Small Business Lending","Exploring the Effectiveness of Indirect Questioning in Reducing Social Desirability Bias ","Challenges in Collecting Military Family Data on a Nationally Representative Survey","Resolving Balance Complex Discrepancies in the Presence of Negative Data","Small Area Model Diagnostics and Validation with Applications to the Voting Rights Act Section 203","Small Area Models for Over-Dispersed Poisson Counts","Hybrid BRR and Parametric-Bootstrap Variance Estimates for Small Domains in Large Surveys","Multilevel Regression and Poststratification (MRP) for Small Area Estimation: An Application to Estimate Health Insurance Coverage Using Geocoded American Community Survey","A Joint Spatial Factor Analysis Model to Accommodate Data from Misaligned Nested Areal Units with Application to Louisiana Social Vulnerability","A Robust Interrupted Time Series Model for Analyzing Complex Healthcare Intervention Data","Stable Balancing Weights for Marginal Structural Models","Checking Validity of Constrained Survey Estimators","Comparative Study of Differentially Private Data Synthesis Methods","Changes to the Sample Design and Frame of a National Survey of People Living with HIV","Evaluating the Effect of Weighting Adjustments on HIV Prevalence Estimates in Three Population-Based HIV Impact Assessments Surveys","Evaluation of Nonresponse Weighting Adjustment Methods for a National Survey of People Living with HIV","Bayesian sparse propensity score estimation for unit nonresponse","Deriving a Person-Level Weight for Analyzing MEPS Supplemental Data from a Linked Medical Organization Survey","Assuring Quality in Dual Frame RDD National or Sub-National Surveys Using Cell Phone Numbers Without Area Codes in South Korea","TRUMP: Tuned Ratio Unbiased Mean Predictor","Measuring Cross-Country Differences in Misallocation","Analyzing the Multiply Imputed Accelerometer Data in the 2003-2004 National Health and Nutrition Examination Survey","Recommended Methods for Handling Missing Item Values in Regression Analyzes of the National Survey on Drug Use and Health","Regression Power Analysis for a Linear Regression Model When Regressors Are Matrix-Sampled","Comparison of Model-Based to Design-Based Ratio Estimators","Three Methods for Occupation Coding Based on Statistical Learning","The Interaction Between Question Order and Delivery Mode (All Mail vs. Mixed Mode, Web+Mail)","Sample Design and Weighting for Estimating a Dose-Response Curve","Challenges in Linking Demographic Data at Different Geographic Levels","Effects of Link Misspecification in Propensity Score Weighting Adjustment","A Simulation Study to Evaluate How Sample Weight Adjustment for the National Health and Nutrition Examination Survey (NHANES) Affects Nonresponse Bias","Psychometric Development and Validation of a Tool for Pediatric Patient Caregivers to Provide Feedback About Emergency Physician Interpersonal and Communication Skills ","Combining Identifying Assumptions to Handle Nonignorable Missing Data","Parametric, Semiparametric and Non-Parametric Methods for Sensitivity Analysis with Incomplete Data","Sensitivity Analysis for Longitudinal Clinical Trials with Nonmonotone Missingness","Inverse Probability Weighting Estimators for Functions of Data Missing Non-Monotonically and Not at Random","Maximum Likelihood Multiple Imputation: a More Efficient Approach to Repairing and Analyzing Incomplete Data","Fractional Imputation for Dummies","Hot Deck Imputation: Proper or Improper?","A Cross-Disciplinary Review of Record Linkage Methodologies","The Estimation of Match Validity Under the Fellegi-Sunter Paradigm Without Assuming Identifier-Agreement Independence","File linking with faulty matching information","Enhancement of Health Surveys with Data Linkage","Spatial Small Area Smoothing Models for Handling Survey Data with Nonresponse","Small Area/Domain Estimation Assuming One Auxiliary Variable, Equal Unit Error Variances and Post-Stratification ","Developments in Model-Based County-Level Estimation of Agricultural Cash Rental Rates","Unit-Level Logistic Mixed Effects Models for Small Area Estimation of Poverty Estimates","Asymptotic Efficiency of REML Error Components as a Function of Unbalancedness","A Unit Level Model for Prediction of Categorical Data with Auxiliary Information","Small Area Estimation in Government Surveys (U.S. Census Bureau)","The Role of the Sampling Program for Survey Statisticians in Training Statisticians Around the Globe","A Substitution Procedure for Missing Not at Random Mechanism","Small Area Estimation by Combining Two Surveys with Special Consideration of Cell Only Households","Interviewer Effects on Racially-Sensitive Questions and Biomeasures: Results from a Semi-Interpenetrated Design","An Overview of Trend Testing Methods and Applications in NSDUH and Other Studies","Comparison of Trend Testing Methods: a Simulation Study with NSDUH Data","Trend Analysis Using the National Crime Victimization Survey","Development of Data Products and Adjustment Methods to Allow for the Evaluation of Historical Trends in the National Crime Victimization Survey","Time Trend Analysis with Complex Survey Data","Sampling Methodology and Coverage Issue for a National School-Based Survey","Comparing Alternative Methods for the Random Selection of a Respondent Within a Household for Online Surveys","Randomized Multi-Stage Stratified Cluster Sampling Methodology to Estimate Public Transit Fare Compliance","The Link Between Nonresponse Bias and the Length of the Field Period in a Mixed-Mode General Population Panel","Highly Robust Multiple Imputation Models Using BART","Multiple Imputation in Longitudinal Studies with Circularity","Multiple Imputation of Non-Ignorable Binary Missing Data Using a Censored Bivariate Probit Model","Multiple Imputation to Evaluate the Impact of an Assay Change in National Surveys","Donor Selection for MCBS Asset Imputation ","Imputation of Missing Data in Surveys and Studies in Dental Research","Evaluating Imputation Techniques for Longitudinal Study of Effectiveness of an Anti-Smoking Campaign","A Novel Approach to Developing 5-Star Ratings for Home Health Care Systems","Imputation Classes as a Framework for Inferences  From Non-random Samples","Investigating the Performance of Inverse Sampling for Model Estimation","The Heckman Selection Model with Complex Survey Data","Analysis of Familial Aggregation with Complex Survey Data","Effect Modifications of Blood Lead-Cardiovascular Disease Mortality Association by Time-Related Factors Studied Using NHANES Linked Mortality Files","MINIMIZING ERROR in MEGA-POLLS: LESSONS from the 2016 ELECTION","Assessing the Impact of the Final Housing Unit Followup on the 2010 Census Coverage Measurement Housing Unit Estimates","Missing Data Imputation Using Regression and Classification Tree Software GUIDE","Differentiated Effects of Data Analyses on Between- and Within-Imputation Variances in Multiple Imputation","Balancing Bias, Precision, and Sample Size Recovered in Determining a Practical Missing Data Imputation Approach ","The MEPS Medical Organizations Survey: Lessons Learned on Non-Response and Data Quality from a Linked Survey Data Collection Strategy","Correlates of Nonresponse in the 2010 and 2012 Medical Expenditure Panel Survey","Attrition and Its Implications in the National Longitudinal Survey of Youth 1979","Empirical Likelihood Inference for Complex Surveys and the Design-Based Oracle Variable Selection Theory","The Interplay Between the Practice and Theory of Survey Sampling: Past, Present, and Future","Global-Local Shrinkage Priors for Small Area Estimation","Bootstrap Approach to the Application of First-Digits Analysis ","Statistical Agency Use of Macro Editing in Industry-Area Employment Estimation ","Construction and Assessment of Generalized Variance Functions for an Establishment Survey","Testing Models for Weight Smoothing in the Current Employment Statistics Survey","Evaluation of a New Approach for Estimating the Number of U.S. Farms","On the Development of a Parametric Approach for the Estimation of Totals and Means for Complex Survey Sample Data in the Presence of Full Response","Analyzing Residuals in a Survey Logistic Model with SAS","Confidence Intervals for Population Attributable Fractions Using Complex Survey Data","Bootstrap Confidence Interval Bands for Estimates in Measurement Error Model with Linked NHIS and EPA Data ","Comparing Alternative Methods for the Random Selection of a Respondent Within a Household for Online Surveys","Randomized Multi-Stage Stratified Cluster Sampling Methodology to Estimate Public Transit Fare Compliance","Imputation for Longitudinal Study of Effectiveness of an Anti-Smoking Campaign","Multiple Imputation for Handling Measurement Errors","Data Fusion for Correcting Measurement Errors","Multiple Imputation in Data Fusion: Making Better Assumptions Than Conditional Independence","Addressing Differential Measurement Error in Self-Reported Dietary Data Using an External Validation Study: Application to a Longitudinal Lifestyle Intervention Trial","How Suited Is Propensity Score Matching for Combining Data from Different Sources?","A Review of Weighting Assumptions for Geosampling-Based Survey Designs: a Case Study in Nigeria ","One-Versus Two-Step Approaches to Survey Nonresponse Adjustments","Collapsing Weighting Adjustments in the National Immunization Survey (NIS) - Is There an Advantage?","Variance When Using Weights for Full Probability Sample to Analyze Arbitrarily Combined Sub-Samples: NHANES 2011 - 2014","A Comparison of Variance Estimates Using Random Group and Taylor Series Methods for a Large National Survey of Businesses","Survey Designs with Small Stratum Sample Sizes","Variance Estimation for Trend of Nurse Diversity Using ACS PUMS Data 2000-2015","Enhancing the NHANES CMS Medicaid Linked Data with Multiple Imputation","Calibrated Multiple Imputation","Gaussian-based routines for imputing categorical variables in complex data","Small Area Estimates for End-Of-Season Agricultural Quantities","Measurement Error in Small Area Estimation: Functional vs. Structural vs. Naive Models","Hierarchical Bayesian Models for Noisy Size Responses from Small Areas","Bayesian analysis of sparse counts under the unrelated question design","A Multinomial and a Uniform Unit-Level Model for Small Area Estimation","The Use of Propensity Weight Adjustments for a Mobile Panel Sample","Estimation from Purposive Samples with the Aid of Probability Supplements but Without Data on the Study Variable","Combining Probability and Non-Probability Samples Using Small Area Estimation","Blending of Probability and Convenience Samples as Applied to a Survey of Military Caregivers","Correcting for the Multiplicity Issue in a Probability Sample of Homeless Youth","Assessment of Bias in Estimates using Data from a Sample of Self-Reported Web Users","Using Passive Data Collection, System-to-System Data Collection, and Machine Learning to Improve Economic Surveys","Using Big Data to Enhance US Census Bureau Economic Data Products"," Improving Regional Personal Consumption Expenditure Estimates Using Credit Card Transaction Data","Bayesian Model Averaging in Multiple Imputation Under Informative Sampling","Bayesian Finite Population Inference for Skewed Survey Data Using Skew-Normal Penalized-Spline Regression","Bayesian Modeling for Weighting Adjustment and Inference in Sample Surveys","Elusive Respondents: Characteristics and Interventions","Propensity Modeling for Early Respondent Incentive Escalation","Hesitant Respondents and Data Quality on a Financial Survey","A Practical Exploration of Relationships Between Physical Barriers to Entry and Successful Contact Attempts ","Within Household Gatekeepers and Obstacles to Survey Participation","Evaluation of Medical Care Event Reporting in a National Household Survey","Seasonal Adjustment of the QTAX","Outlier Robust Estimation of the Total Private Cost of Payment Methods for Large Businesses","Model-Calibration Estimator Using Time-To-Event Models","Extending Hansen and Hurwitz's Approach for Nonresponse in Sample Survey","Missing Values in Linear Dynamic Panel Models - a Bayesian Approach Towards Model Comparison","Semiparametric Adaptive Estimation with Nonignorable Nonresponse Data"],"topic_key":[0,7,19,9,0,10,18,19,1,19,10,11,12,19,19,4,0,12,9,19,1,10,19,2,2,2,16,19,18,10,19,12,12,10,0,10,8,18,19,7,18,10,19,0,8,4,0,1,6,4,12,4,10,12,10,10,19,19,18,18,10,6,10,16,19,19,19,18,7,19,12,19,18,16,18,9,18,8,1,1,1,1,10,10,10,10,10,4,19,4,11,4,11,11,19,10,19,2,10,3,13,3,5,5,5,3,10,1,18,10,2,2,19,6,9,10,10,10,1,0,3,0,15,4,1,0,13,4,6,0,18,0,14,10,10,18,19,18,2,18,18,0,19,19,10,10,8,19,10,10,9,18,14,0,1,0,14,14,18,10,18,7,0,0,19,18,8,3,19,3,8,3,10,10,15,10,17,13,15,1,1,1,1,8,14,19,14,11,0,9,1,10,10,1,10,10,10,10,5,18,9,2,6,19,19,19,2,18,0,6,6,6,11,6,4,4,2,2,2,11,18,4,1,10,16,1,0,1,1,10,7,17,17,12,5,5,10,10,9,18,8,12,9,0,10,10,10,10,10,3,11,18,10,10,11,19,10,0,6,8,12,18,8,7,1,7,11,7,7,7,12,0,17,2,2,2,7,10,14,14,10,10,5,10,8,10,18,15,19,18,19,19,19,7,0,11,8,11,15,15,11,15,10,17,17,17,17,17,18,19,18,9,8,0,1,8,0,5,0,11,7,4,11,2,10,5,0,14,5,10,12,12,10,18,18,12,10,13,19,10,18,9,12,12,2,2,0,1,10,10,0,10,10,10,12,4,4,4,18,18,18,0,11,1,15,8,5,18,1,13,10,10,12,10,8,19,9,1,13,9,8,12,10,19,19,18,13,13,15,8,10,4,13,9,9,9,9,9,8,9,5,10,10,10,15,17,17,13,10,10,2,15,1,10,11,1,8,1,10,10,7,19,7,10,0,19,3,19,1,1,11,8,1,10,7,9,10,14,18,8,5,5,5,5,5,16,0,10,10,10,3,10,10,9,9,0,9,18,10,10,10,18,9,7,8,8,8,8,8,4,7,17,7,7,14,7,13,13,13,13,13,13,9,6,6,16,0,1,16,1,16,10,1,8,8,5,2,10,18,2,2,1,2,5,4,4,10,10,10,10,19,19,0,10,10,10,11,10,4,10,1,17,4,1,10,10,1,3,9,19,3,1,18,19,1,9,3,18,3,10,15,8,0,1,5,4,15,0,18,19,9,18,15,6,14,7,7,9,4,3,19,14,18,14,10,19,4,2,19,7,1,8,7,18,4,11,4,4,6,18,11,9,14,3,3,9,7,14,14,14,14,10,12,7,4,6,15,2,12,9,9,9,7,19,10,9,11,19,10,16,10,10,16,3,10,1,5,5,5,5,19,17,17,18,19,8,4,14,14,14,14,9,14,19,18,11,19,0,14,2,0,0,10,10,9,7,10,8,18,0,1,6,18,10,15,5,13,2,2,6,14,14,14,14,17,1,1,18,10,3,19,3,19,19,18,19,18,18,5,2,10,10,19,11,4,4,13,14,5,10,4,5,1,0,0,3,3,5,8,10,13,13,13,14,10,10,6,0,7,10,18,3,7,10,10,10,10,10,1,1,1,1,2,2,2,2,10,10,10,8,0,8,8,8,0,18,19,9,3,3,13,3,8,3,0,18,13,10,10,0,5,16,16,19,13,8,13,6,10,10,2,10,4,2,2,13,9,9,18,18,0,10,12,15,8,4,17,17,17,0,10,4,4,8,14,8,5,2,16,8,6,1,19,19,18,1,9,10,10,0,0,17,18,18,12,18,18,18,19,10,19,18,10,6,6,6,3,10,6,10,5,15,10,15,15,19,19,18,14,10,1,10,19,1,7,13,7,19,1,0,11,10,9,0,10,12,18,12,19,4,2,19,2,2,18,10,1,6,6,6,4,11,11,0,4,4,4,0,0,16,0,16,15,14,7,17,19,10,1,1,8,8,1,7,18,2,18,10,0,10,10,18,1,0,4,15,11,11,14,14,14,14,14,14,14,0,13,7,6,7,1,10,10,0,19,10,1,10,19,8,0,14,0,10,13,1,3,10,12,11,12,15,18,19,18,18,2,2,2,2,2,9,0,0,0,4,0,8,14,8,5,5,5,18,11,3,1,1,9,9,0,9,9,2,11,11,11,2,5,19,16,19,14,6,6,14,4,17,4,15,9,4,1,12,5,19,0,18,0,5,10,8,16,4,4,14,15,7,10,10,8,7,9,15,12,10,2,12,10,18,8,0,13,5,12,10,12,4,10,0,18,6,9,9,19,0,19,0,0,3,5,18,13,13,12,13,19,7,11,4,11,0,14,14,14,6,0,8,10,1,14,14,14,14,10,13,0,0,5,19,18,10,1,4,1,15,15,15,10,10,18,9,0,9,0,16,16,10,1,17,17,6,6,0,2,2,18,2,19,2,5,15,12,10,18,2,2,16,2,1,1,10,0,3,8,19,1,8,8,5,3,3,3,8,3,8,19,10,19,19,10,10,1,13,0,10,0,0,1,17,17,17,17,17,18,19,9,18,10,10,18,18,12,18,1,9,9,8,0,10,10,14,14,10,0,17,1,6,17,2,18,0,0,0,0,0,13,7,16,9,9,18,6,11,11,11,19,18,9,10,19,18,12,2,12,2,15,0,9,3,19,6,17,10,10,10,13,4,10,16,8,19,10,5,4,16,16,16,8,17,8,8,5,8,13,16,13,10,10,15,15,10,4,15,12,2,18,19,0,7,18,9,9,18,4,1,2,6,19,0,0,1,1,4,4,0,0,0,6,14,0,0,5,0,14,8,2,2,18,1,2,2,8,19,19,18,19,19,18,19,10,19,10,14,10,10,16,5,14,0,5,13,6,6,6,19,19,3,11,10,15,12,15,4,18,15,7,1,7,7,3,0,14,6,6,14,10,10,10,5,10,5,5,5,11,5,5,10,13,18,13,9,9,5,5,5,5,9,10,10,11,5,5,10,12,12,10,10,13,13,13,13,13,4,7,8,0,6,3,10,7,18,19,18,9,15,0,5,5,12,1,10,10,10,10,10,9,0,19,3,2,10,10,19,19,14,0,14,14,1,6,12,5,5,5,12,8,7,3,3,8,10,8,1,8,0,8,6,1,10,10,10,6,4,13,6,6,12,12,10,0,3,0,13,13,13,10,9,1,0,2,13,10,4,2,11,14,14,4,14,6,6,6,2,9,18,11,18,11,7,17,1,1,1,1,10,10,10,10,1,10,8,13,5,3,3,9,3,3,3,18,19,10,2,12,4,4,4,4,4,4,9,3,19,3,19,10,1,0,3,15,10,10,18,10,12,0,9,19,19,19,5,5,5,5,3,3,3,3,3,12,9,10,10,16,16,0,9,0,9,9,8,10,4,10,16,8,3,2,5,10,11,10,16,13,10,9,19,16,0,1,0,12,1,14,18,10,12,5,5,5,5,11,1,1,16,7,16,10,10,10,19,19,0,19,3,18,3,19,19,18,10,1,11,0,0,0,0,3,8,15,13,13,13,13,13,19,8,10,19,11,11,13,3,3,19,13,9,1,9,12,14,4,1,3,9,0,9,11,4,14,6,18,1,8,13,3,10,10,13,10,10,2,2,10,2,16,12,19,0,15,11,15,17,7,9,7,7,7,14,6,6,6,6,7,2,7,9,10,10,10,10,5,1,0,10,10,10,18,0,9,7,0,16,10,8,7,19,10,19,10,10,1,18,10,7,16,10,10,13,18,19,9,9,10,19,18,19,10,19,19,0,18,5,0,9,3,0,2,2,19,2,17,17,2,1,8,10,10,10,12,10,1,4,9,0,7,14,14,4,14,0,7,11,13,8,19,18,9,14,19,9,0,2,2,2,2,18,18,10,19,10,19,8,3,8,5,10,2,9,18,9,11,7,7,7,1,11,18,9,9,9,0,7,18,18,6,6,4,14,14,0,4,11,16,16,16,5,16,3,3,3,8,19,8,9,8,11,13,8,0,1,7,18,17,19,19,19,10,10,5,10,16,16,16,19,9,9,10,10,13,10,10,17,18,10,7,13,3,13,9,0,9,7,0,18,0,3,10,14,15,15,10,6,6,6,6,6,6,6,7,13,10,0,14,0,0,14,10,10,19,19,15,15,17,15,15,10,10,10,10,18,0,0,14,8,3,18,4,0,14,14,0,7,1,18,13,10,10,19,15,12,15,15,1,19,19,19,10,5,5,19,19,5,13,2,9,2,5,10,9,18,18,19,4,7,19,6,15,19,10,19,8,8,8,19,14,8,7,7,0,9,2,2,2,17,2,10,2,18,10,11,9,10,18,5,18,19,13,4,4,11,7,1,10,4,10,2,2,7,2,19,0,7,9,8,19,15,4,17,10,10,19,16,16,5,12,12,15,13,12,10,6,6,10,6,10,19,18,2,0,17,0,5,14,14,16,8,16,8,4,13,4,15,16,16,16,16,18,19,18,18,16,8,5,10,5,10,15,17,0,10,10,14,17,17,17,17,13,3,9,12,3,2,2,11,9,7,2,1,10,10,10,0,18,10,10,10,18,11,17,17,15,11,17,17,12,11,4,11,5,4,11,18,9,18,7,0,16,7,18,10,10,0,0,1,11,7,1,18,1,4,1,1,10,0,8,10,10,1,10,10,19,8,8,14,17,15,16,8,10,14,4,0,9,6,15,10,11,10,10,7,1,15,5,2,6,16,5,8,5,12,8,1,5,11,13,10,13,2,10,0,2,14,12,11,2,3,2,12,10,0,1,10,16,4,1,1,18,1,15,2,10,2,14,12,4,5,8,10,10,14,16,1,10,4,7,4,17,8,2,10,2,16,5,10,10,9,10,9,0,9,10,9,7,18,4,3,3,19,9,18,10,9,3,10,3,10,10,0,10,0,1,1,0,10,10,15,18,18,6,8,17,2,4,7,12,2,11,10,18,10,14,16,2,18,14,4,13,16,11,4,15,3,2,13,12,10,2,18,7,0,17,1,16,0,0,11,11,14,0,0,15,14,14,0,10,5,15,3,7,10,19,2,10,10,8,0,8,0,14,9,18,19,3,3,10,6,5,19,6,7,0,10,18,0,2,14,0,5,17,15,14,11,5,7,4,17,3,5,14,0,14,14,4,7,7,8,0,13,10,17,6,17,4,17,17,4,6,6,14,6,1,6,1,15,12,17,12,15,2,0,17,13,5,1,19,16,10,16,14,8,3,5,3,3,13,9,4,19,7,5,10,10,10,18,18,19,18,10,19,3,3,19,19,7,19,10,19,19,9,18,19,10,19,6,19,19,4,19,19,7,19,7,7,7,5,6,0,2,2,19,18,7,18,10,0,2,7,9,11,10,19,10,10,0,10,10,0,9,4,11,4,11,4,4,4,0,0,10,3,3,5,9,7,7,18,19,18,11,13,0,10,0,9,15,4,18,10,18,0,18,18,2,2,2,5,2,15,12,12,13,15,10,10,9,6,9,17,17,1,19,15,1,13,19,8,8,10,1,6,10,0,3,16,0,2,19,16,12,0,3,10,10,5,5,5,10,10,10,17,11,10,7,10,18,13,18,13,13,10,19,10,10,4,15,4,4,4,10,0,0,8,1,0,13,13,4,4,0,4,7,11,0,7,10,9,2,2,2,4,10,10,11,2,11,2,11,2,2,2,0,4,0,0,18,9,3,3,19,3,8,12,19,3,10,13,2,10,10,12,7,18,0,1,1,1,12,0,10,15,10,9,9,10,19,15,10,15,8,4,8,9,8,0,14,6,12,10,10,1,1,16,1,3,3,3,9,7,7,18,13,19,15,15,4,13,8,9,7,7,0,19,18,0,18,2,2,10,18,4,4,15,15,15,1,14,6,4,5,12,10,11,10,10,5,8,16,5,1,14,9,0,16,4,11,16,7,18,10,10,10,8,2,2,2,2,18,3,8,3,3,19,3,18,19,10,9,7,1,7,12,5,14,4,16,5,18,10,9,19,8,0,10,13,13,11,0,11,0,18,0,0,0,18,14,3,18,2,10,4,4,10,0,0,0,2,2,4,16,2,16,5,15,1,3,10,18,18,12,19,3,10,0,0,13,15,17,9,14,0,10,10,0,18,10,1,10,18,9,19,13,10,10,0,18,19,19,19,10,5,18,19,19,10,5,18,10,4,9,19,9,9,16,16,16,1,18,16,10,0,10,0,9,9,8,13,10,17,10,10,10,10,1,18,19,19,9,11,11,8,4,0,13,11,9,4,5,15,13,10,5,4,10,15,15,1,15,12,0,12,19,3,9,7,9,19,9,2,19,10,11,11,11,13,8,13,11,17,15,11,10,7,10,12,0,4,0,1,8,0,10,5,16,16,0,14,14,4,19,4,10,11,11,9,18,19,2,18,6,14,10,0,13,4,0,10,10,19,19,19,18,18,18,6,10,10,18,10,18,18,10,10,18,10,18,12,18,10,10,19,19,10,11,17,2,2,18,11,10,10,17,17,17,17,14,19,18,7,19,4,15,10,0,10,4,11,0,18,10,14,8,10,10,1,10,3,3,3,3,10,19,19,18,10,7,11,7,18,1,6,9,5,2,1,2,19,18,17,14,7,11,11,10,11,18,18,9,2,2,2,16,16,16,1,3,0,8,8,12,19,5,0,11,3,15,18,10,10,10,13,0,4,8,17,19,2,2,1,4,2,2,1,19,0,18,9,10,8,5,2,2,2,1,1,15,18,0,3,15,8,13,7,14,19,7,8,12,4,8,11,2,12,12,12,18,0,11,7,7,18,9,8,16,7,2,8,12,19,3,19,7,0,0,0,14,1,5,5,13,2,18,7,4,4,15,15,15,1,10,9,19,4,11,9],"x":{"__ndarray__":"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","dtype":"float32","shape":[3045]},"y":{"__ndarray__":"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","dtype":"float32","shape":[3045]}},"selected":null,"selection_policy":null},"id":"eeda0dd3-17e8-4e45-b11e-477771b769a7","type":"ColumnDataSource"},{"attributes":{"items":[{"id":"4b9e4122-8f24-4e05-ae7b-5dc8a77a90e5","type":"LegendItem"}],"location":"top_left","plot":{"id":"f0a0f016-e948-44d8-ad1f-069fca06ef32","subtype":"Figure","type":"Plot"}},"id":"bb7121ef-0e18-4429-b342-e6ac7344dc69","type":"Legend"},{"attributes":{"data_source":{"id":"eeda0dd3-17e8-4e45-b11e-477771b769a7","type":"ColumnDataSource"},"glyph":{"id":"0e75ba78-04d6-4ed9-9826-dd36619e19c9","type":"Circle"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"7d8308e1-5940-4a45-94ad-90b552cce448","type":"Circle"},"selection_glyph":null,"view":{"id":"6eac7fa1-393d-4733-85a9-a6d49b9a5bcc","type":"CDSView"}},"id":"98906f39-e2c6-4e50-91a4-b56deae18c0c","type":"GlyphRenderer"},{"attributes":{},"id":"b15b62b7-45fa-4b47-80ce-a202bf89ac54","type":"SaveTool"},{"attributes":{"overlay":{"id":"72ce5960-8657-4c5b-952c-a6c60b7e0579","type":"BoxAnnotation"}},"id":"f9e5dfc1-e8c0-465d-85a0-d88a7648dffd","type":"BoxZoomTool"},{"attributes":{},"id":"50c41a6e-5481-4aca-935f-c15320a42923","type":"PanTool"},{"attributes":{"fill_alpha":{"value":0.8},"fill_color":{"field":"color"},"line_alpha":{"value":0.8},"line_color":{"field":"color"},"size":{"units":"screen","value":10},"x":{"field":"x"},"y":{"field":"y"}},"id":"0e75ba78-04d6-4ed9-9826-dd36619e19c9","type":"Circle"},{"attributes":{"min_border":1,"plot_height":900,"plot_width":1600,"renderers":[{"id":"72ce5960-8657-4c5b-952c-a6c60b7e0579","type":"BoxAnnotation"},{"id":"bb7121ef-0e18-4429-b342-e6ac7344dc69","type":"Legend"},{"id":"98906f39-e2c6-4e50-91a4-b56deae18c0c","type":"GlyphRenderer"},{"id":"bb7121ef-0e18-4429-b342-e6ac7344dc69","type":"Legend"}],"right":[{"id":"bb7121ef-0e18-4429-b342-e6ac7344dc69","type":"Legend"}],"title":{"id":"9d460809-f665-4d9f-816e-6d07ef818ca3","type":"Title"},"toolbar":{"id":"190c7ac2-e506-4866-b6df-76575f5368c3","type":"Toolbar"},"x_range":{"id":"fa74f7fe-0ca8-49bf-9223-a61d5c92d757","type":"DataRange1d"},"x_scale":{"id":"7bb1fedf-707d-477d-9261-d0ab6efa75f5","type":"LinearScale"},"y_range":{"id":"8742ef31-c651-47bd-8fa3-798247ec103b","type":"DataRange1d"},"y_scale":{"id":"db0d06ab-05d0-491c-ac6d-8b08adaff7d1","type":"LinearScale"}},"id":"f0a0f016-e948-44d8-ad1f-069fca06ef32","subtype":"Figure","type":"Plot"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"72ce5960-8657-4c5b-952c-a6c60b7e0579","type":"BoxAnnotation"},{"attributes":{},"id":"db0d06ab-05d0-491c-ac6d-8b08adaff7d1","type":"LinearScale"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"50c41a6e-5481-4aca-935f-c15320a42923","type":"PanTool"},{"id":"e89b6f68-109e-4651-bb4c-a29f9ecdadba","type":"WheelZoomTool"},{"id":"f9e5dfc1-e8c0-465d-85a0-d88a7648dffd","type":"BoxZoomTool"},{"id":"8e029c36-0813-425a-b51b-29f7df4a2e6e","type":"ResetTool"},{"id":"3c216f1e-4a84-4384-84fa-32f1fbd49e0c","type":"HoverTool"},{"id":"b15b62b7-45fa-4b47-80ce-a202bf89ac54","type":"SaveTool"}]},"id":"190c7ac2-e506-4866-b6df-76575f5368c3","type":"Toolbar"},{"attributes":{"label":{"field":"topic_key"},"renderers":[{"id":"98906f39-e2c6-4e50-91a4-b56deae18c0c","type":"GlyphRenderer"}]},"id":"4b9e4122-8f24-4e05-ae7b-5dc8a77a90e5","type":"LegendItem"}],"root_ids":["f0a0f016-e948-44d8-ad1f-069fca06ef32"]},"title":"Bokeh Application","version":"0.12.15"}}
        </script>
        <script type="text/javascript">
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('a24b363a-261a-4175-bdfd-570566801880').textContent;
                  var render_items = [{"docid":"1c061917-626f-4654-b535-72395343af79","elementid":"db30cde3-916c-4ee7-83bc-f2b9211d8ad3","modelid":"f0a0f016-e948-44d8-ad1f-069fca06ef32"}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        embed_document(root);
                        clearInterval(timer);
                      }
                      attempts++;
                      if (attempts > 100) {
                        console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing")
                        clearInterval(timer);
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != "loading") fn();
            else document.addEventListener("DOMContentLoaded", fn);
          })();
        </script>
    </body>
</html>